{
  "chunks": [
    "A Guide to Event-Driven Design for Agents and Multi-Agent Systems By Sean Falconer, AI Entrepreneur in Residence, Confluent © 2025 Confluent, Inc. Contents 3 Introduction 4 Part I: Understanding the Evolution of AI The First Wave: Predictive AI The Second Wave: Generative AI Compound AI Bridges the Gap The Third Wave: Agentic AI The Power of Singular Agent Systems 9 Part II: The Case for Event-Driven Agentic Systems The Anatomy of an Agent Persona (Job Function) Perception (Sensing) Reasoning and Decision-Making Memory Planning Action Learning Coordination and Collaboration Tool Interface Why Event-Driven Matters for Agents From Singular to Multi-Agent Systems 13 Part III: Design Patterns for Multi-Agent Systems Multi-Agent Design Patterns Orchestrator-Worker Pattern Traditional Approach Event-Driven Approach Hierarchical Agent Pattern Traditional Approach Event-Driven Approach Blackboard Pattern Traditional Approach Event-Driven Approach Market-Based Pattern Traditional Approach Event-Driven Approach The Role of the Data Streaming Platform in Multi-Agent Systems 23 Part IV: Building Event-Driven Systems for Agents with a Data Streaming Platform Architecting Singular and Multi-Agent Systems Challenges and Solutions in Event-Driven Agent Design [Stream] Next-Level Data Streaming with a Fully Managed, Cloud-Native Service [Connect] Seamless Integration: Connecting Agents with Diverse Systems [Process] Ensuring Data Freshness: Handling Dynamic Data Streams [Govern] Data Quality, Security, and Compliance Real-World Applications of Event-Driven Agentic Systems 1. Automating Web Scraping with AI Agents 2. Intelligent Business Copilots 3. Workflow Automation with a Drag-and-Drop Agent Builder Broader Industry Impact Why a Data Streaming Platform Is Essential for AI Agents 28 The Future of AI is Event-Driven 29 Take the Next Step © 2025 Confluent, Inc. 2 Introduction AI has come a long way. We’ve moved from By processing data close to the source, it predictive models that analyze patterns to reduces latency and enhances decision- generative AI that creates new content. Now, making, allowing AI agents to operate with we’re entering the next phase: agentic AI— the freshest, most relevant data, while systems that don’t just generate, but have the maintaining compliance and control across agency to act, adapt, and collaborate, in real the organization. time. EDA transforms AI from isolated models into For AI agents to be useful, they need a dynamic system of agents that react to more than just intelligence. They need events in real time. By using a data streaming infrastructure. They need security. They need platform, agents can consume, process, governance. and emit events asynchronously, ensuring scalable, flexible, and resilient interactions. An agent must consume data, use tools, Whether it’s a single agent handling customer make decisions, and share outputs across queries, or a network of agents optimizing a an organization. But rigid, request-driven supply chain, event-driven design ensures architectures can’t keep up. APIs and they stay in sync, without breaking down synchronous workflows create bottlenecks, under complexity. limit scalability, and make coordination between multiple agents a nightmare. The This ebook is a guide to building event- future of AI agents isn’t about making models driven AI agents and multi-agent systems. better, it’s about making AI systems work We’ll cover the evolution of AI, the anatomy together seamlessly. of an agent, design patterns for scalable",
    "agents a nightmare. The This ebook is a guide to building event- future of AI agents isn’t about making models driven AI agents and multi-agent systems. better, it’s about making AI systems work We’ll cover the evolution of AI, the anatomy together seamlessly. of an agent, design patterns for scalable architectures, and real-world applications. At This demands an event-driven architecture the core of this approach is a data streaming (EDA) powered by a data streaming platform, platform, which enables agents to consume, which enables agents to react dynamically process, and act on real-time data, while to changes, share state efficiently, and maintaining governance and scalability. The scale without the constraints of traditional goal is simple: to demonstrate why event- request-response patterns. A data streaming driven design—powered by a data streaming platform not only facilitates real-time event platform—is the backbone of AI’s next processing, but also ensures governance, evolution, and how to architect systems that enforces data quality, and connects are not only effective today, but are built for disparate data sources into a unified stream. long-term scalability and adaptability. © 2025 Confluent, Inc. 3 Part I: Understanding the Evolution of AI AI has evolved through three distinct phases, each unlocking new capabilities, while introducing its own limitations. The First Wave: Predictive AI The first wave of AI revolved around traditional machine learning, focusing on predictive capabilities for narrowly defined tasks. The Traditional Machine Learning Workflow Building these models required significant expertise, as they were crafted specifically for individual use cases. They were domain-specific, with their domain specificity embedded in the training data, making them rigid and tough to repurpose. Adapting a model to a new domain often meant starting from scratch—an approach that lacked scalability and slowed adoption. The Second Wave: Generative AI Generative AI, driven by deep learning, marked a turning point. Instead of being confined to single domains, these generative models were trained on vast, diverse datasets, giving them the ability to generalize across a variety of contexts. They could generate text, images, and even videos, opening up exciting new applications. Whereas predictive AI relies on traditional batch-based statistical models to solve specific problems, generative AI uses foundation models like LLMs that are broadly capable and reusable: © 2025 Confluent, Inc. 4 The Generative AI Workflow However, this wave came with its own challenges. Generative models are fixed in time—unable to incorporate new or dynamic information— and are difficult to adapt. Fine-tuning can address domain-specific needs, but it’s expensive and error-prone. Fine-tuning requires vast data, significant computational resources, and ML expertise, making it impractical for many situations. Additionally, since LLMs are trained on publicly available data, they don’t have access to domain-specific information, limiting their ability to accurately respond to questions that require context. For example, suppose you ask a generative model to recommend an insurance policy tailored to a user’s personal health history, location, and financial goals. Simple Prompt and Response with an LLM In this scenario, you prompt the LLM and it generates a response. Clearly the model can’t deliver accurate recommendations, because it lacks access to the",
    "generative model to recommend an insurance policy tailored to a user’s personal health history, location, and financial goals. Simple Prompt and Response with an LLM In this scenario, you prompt the LLM and it generates a response. Clearly the model can’t deliver accurate recommendations, because it lacks access to the relevant user data. Without it, the response will either be generic or flat-out wrong. © 2025 Confluent, Inc. 5 Compound AI Bridges the Gap To overcome these limitations, Compound AI systems integrate generative models with other components like programmatic logic, data retrieval mechanisms, and validation layers. This modular design allows AI to combine tools, fetch relevant data, and tailor outputs in a way that static models cannot. For instance, in the insurance recommendation example: • A retrieval mechanism pulls the user’s health and financial data from a secure database. • This data is added to the context provided to the LLM during prompt assembly. • The LLM uses the assembled prompt to generate an accurate response. Simple RAG Architecture This process, known as Retrieval-Augmented Generation (RAG), bridges the gap between static AI and real-world needs by dynamically incorporating relevant data into the model’s workflow. While RAG effectively handles tasks like this, it relies on fixed workflows, meaning every interaction and execution path must be predefined. This rigidity makes it impractical to handle more complex or dynamic tasks, where workflows cannot be exhaustively encoded. Encoding all possible execution paths manually is labor-intensive and ultimately limiting. The limitations of fixed-flow architectures have led to the rise of the third wave of AI: agentic systems. The Third Wave: Agentic AI As HubSpot CTO Dharmesh Shah put it, “Agents are the new apps.” Salesforce CEO Marc Benioff echoed this sentiment on The Wall Street Journal’s “Future of Everything” podcast, emphasizing that the future of AI lies with autonomous agents—systems that can think, adapt, and act independently. Agents bring something fundamentally new: dynamic, context-driven workflows. Unlike traditional AI models that follow predefined paths, agentic systems determine the best course of action on the fly, adapting in real time to the challenges they face. This makes them particularly well-suited for solving complex, interconnected problems, in enterprise environments. © 2025 Confluent, Inc. 6 Control Logic: Programmatic vs. Agentic Agents flip traditional control logic on its head. Instead of rigid programs dictating every move, agents use LLMs to drive decisions. They can reason, use tools, and access memory—all dynamically. This flexibility allows for workflows that evolve in real time, making agents far more powerful than anything built on fixed logic. Agent Architecture (inspired by https://arxiv.org/pdf/2304.03442) © 2025 Confluent, Inc. 7 The Power of Singular Agent Systems A single agent can be powerful when designed correctly. Effective AI agents share key characteristics: • Autonomy: They operate without constant human intervention. • Adaptability: They adjust to new data and changing conditions. • Decision-Making: They evaluate multiple options and select the best course of action. However, traditional architectures make deploying such agents challenging. AI systems struggle with data freshness, integration complexity, security, governance, and real- time responsiveness. Many still rely on batch-based processing,",
    "Adaptability: They adjust to new data and changing conditions. • Decision-Making: They evaluate multiple options and select the best course of action. However, traditional architectures make deploying such agents challenging. AI systems struggle with data freshness, integration complexity, security, governance, and real- time responsiveness. Many still rely on batch-based processing, leading to decisions made based on stale data, while fragmented data landscapes make it difficult to establish contextualized and trustworthy data. These challenges create a data mess, where agents lack the reliable, real- time inputs needed to make effective decisions. This highlights the need for an event-driven foundation powered by a data streaming platform, where agents can securely react to live events, enforce governance, and integrate data from disparate sources in real time—eliminating bottlenecks, and enabling AI systems to operate with speed, accuracy, and compliance. Before we explore why event-driven design is critical, we need to break down the fundamental components of an agent. Understanding the anatomy of an agent provides the foundation for architecting scalable, interoperable AI systems. © 2025 Confluent, Inc. 8 Part II: The Case for Event-Driven Agentic Systems In artificial intelligence, agents have a long history, from early theoretical considerations by Alan Turing and John McCarthy, to rule-based reasoning agents in the 1960s. These agents were designed to act autonomously within a defined context, but their capabilities were limited by narrow applications and rigid logic. Today, the emergence of foundation models has transformed what’s possible. The Anatomy of an Agent Just like humans, agents solve problems by combining their senses, memory, reasoning, and ability to act. But before diving into these mechanics, there’s one foundational element that underpins everything: their persona. The Anatomy of an Agent © 2025 Confluent, Inc. 9 Persona (Job Function) The persona of an agent defines its job function and expertise. It’s like a detailed job description embedded into the system prompt, shaping the agent’s behavior and responses. The system prompt sets expectations, and influences the model’s probability distribution over tokens, to align outputs with the defined role. Perception (Sensing) With a clear role, the first step to solving any problem is understanding the environment. For agents, perception is their sensory input—how they gather data from the world around them. Humans use eyes, ears, and touch; agents use APIs, sensors, and user inputs. Reasoning and Decision-Making Once information is gathered, it needs to be processed and understood. Reasoning is the agent’s ability to analyze data, derive insights, and decide what to do next. For humans, this happens in the brain. For agents, it’s powered by models like LLMs, which dynamically adapt to inputs and contexts. Memory Memory allows agents to retain domain-specific information across interactions. It’s not about learning, which is a separate part of the anatomy. Humans rely on both short-term memory (like recalling the start of a conversation) and long-term memory (like remembering a skill learned years ago). Agents work the same way. Short-term memory allows the agent to keep track of the immediate context within a conversation, which might be stored temporarily in memory buffers during the session. Meanwhile, long-term memory",
    "recalling the start of a conversation) and long-term memory (like remembering a skill learned years ago). Agents work the same way. Short-term memory allows the agent to keep track of the immediate context within a conversation, which might be stored temporarily in memory buffers during the session. Meanwhile, long-term memory involves storing historical data, such as user preferences or past interactions. This could be a vector database like MongoDB, Elasticsearch, Pinecone, or another permanent storage. A vector database enables semantic search, where embeddings allow the agent to retrieve relevant information efficiently. Planning Once the agent knows what needs to be done, it devises a plan to achieve its goal. This step mirrors how humans strategize: breaking a problem into smaller steps and prioritizing actions. Action Planning is worthless without execution. Action is where agents interact with the world, whether by sending a message, controlling a device, or updating a database. The agent’s execution handlers are responsible for ensuring these actions are performed accurately, and for validating the outcomes. Learning Humans improve by learning from mistakes and adapting to new information. Agents do the same, using machine learning to refine their reasoning, improve predictions, and optimize actions. This process may involve adjusting the agent’s context dynamically during prompt assembly, allowing it to refine its responses based on situational feedback, without making permanent changes to the model’s weights. Alternatively, learning can also occur through reinforcement © 2025 Confluent, Inc. 10 learning, where decision-making is optimized using rewards or penalties tied to specific actions. In many cases, adapting context provides a flexible and efficient way for agents to improve, without the overhead of fine-tuning. Coordination and Collaboration Humans rarely work alone—we collaborate, share knowledge, and divide tasks. In multi-agent systems, coordination enables agents to do the same, working together to achieve shared goals. Tool Interface Humans use tools to amplify their capabilities—doctors use stethoscopes, and programmers use integrated development environments (IDEs). Agents are no different. The tool interface is their bridge to specialized capabilities, allowing them to extend their reach, and operate effectively in the real world. These interfaces often rely on modular API handlers or plugin architectures, allowing the agent to extend its functionality dynamically and efficiently. Agents sense their environment, process inputs, recall relevant information, make decisions, and take action. These steps mirror human cognition and problem-solving, but operate within digital ecosystems. Why Event-Driven Matters for Agents Agents, at their core, function much like microservices—as modular, independent units that execute specific tasks. However, unlike traditional microservices, agents don’t just process requests; they reason, plan, and take actions based on stateful information. Without proper coordination, this complexity can quickly spiral out of control. Imagine deploying hundreds of microservices without guardrails—without standardized communication, state synchronization, or failure recovery mechanisms. The result would be chaos. The same applies to multi-agent systems: without a structured framework, agents become fragmented, inefficient, and unreliable. Microservices architecture evolved to solve similar challenges by shifting from tightly coupled, request/response-based communication to event-driven design. Early monolithic applications struggled to scale, because every component had direct dependencies on others. Microservices addressed this by",
    "same applies to multi-agent systems: without a structured framework, agents become fragmented, inefficient, and unreliable. Microservices architecture evolved to solve similar challenges by shifting from tightly coupled, request/response-based communication to event-driven design. Early monolithic applications struggled to scale, because every component had direct dependencies on others. Microservices addressed this by decoupling services, allowing them to operate independently. But managing interservice communication through APIs still introduced bottlenecks. The breakthrough came with event-driven architectures (EDA), where services could react to changes asynchronously, enabling real-time responsiveness and scalability. Tightly Coupled Microservices to Event-Driven Microservices © 2025 Confluent, Inc. 11 Agents need the same shift. Instead of rigid, API-driven interactions, they should operate in an event-driven ecosystem, consuming and emitting events dynamically. EDA provides the necessary foundation for scalable, adaptive agents by ensuring: • Asynchronous Processing: Agents can process tasks as events arrive, avoiding bottlenecks caused by synchronous API calls. • Scalability: New agents can join the system without disrupting existing workflows, much like adding new microservices to an event-driven infrastructure. • Loose Coupling: Agents interact via event streams rather than direct dependencies, reducing fragility and enabling modular development. • Real-Time Responsiveness: Agents react instantly to events, ensuring that decisions are made based on the latest available data. By borrowing from microservices architecture, event-driven agentic systems enable flexibility, resilience, and efficiency at scale. Just as microservices rely on message brokers like Apache Kafka® for asynchronous communication, agents leverage event streaming to collaborate without rigid dependencies. A data streaming platform takes this further by not just streaming data, but also connecting disparate data sources, processing events in motion, and enforcing governance. With a data streaming platform, agents operate on real-time, contextualized data, avoiding stale insights from batch processing. It enables dynamic filtering, transformation, and secure data sharing, ensuring decisions are made with the freshest, most relevant information. This keeps agent ecosystems adaptive, scalable, and ready for real-world challenges—moving beyond static, request-driven workflows, to truly autonomous AI systems. From Singular to Multi-Agent Systems As powerful as a single agent can be, its capabilities are inherently limited by its scope. No single agent can handle every possible task with full expertise, just as no individual worker in a company can effectively perform every job. The real promise of agentic AI lies in multi-agent systems (MAS), where multiple specialized agents work together, coordinating actions, exchanging information, and dynamically adapting to changes. A MAS is a network of agents that collaborate (or sometimes compete) to solve complex problems more effectively than any single agent could on its own. These systems are designed to distribute workloads, balance specialization, and enable decentralized decision-making. Multi-agent systems allow: • Task Delegation: Agents specialize in specific domains and distribute workloads efficiently. • Parallel Processing: Agents execute tasks simultaneously without bottlenecks. • Dynamic Role Allocation: Responsibilities shift based on changing requirements. To function effectively, multi-agent systems require seamless coordination and communication. Traditional API-based integrations create tight coupling, where agents must know exactly which other agents to interact with. This approach does not scale. As the number of agents grows, the complexity of interactions increases exponentially. Event-driven architecture",
    "based on changing requirements. To function effectively, multi-agent systems require seamless coordination and communication. Traditional API-based integrations create tight coupling, where agents must know exactly which other agents to interact with. This approach does not scale. As the number of agents grows, the complexity of interactions increases exponentially. Event-driven architecture ensures seamless coordination, enabling agents to operate as part of an adaptive, resilient ecosystem. In the next section, we’ll explore the design patterns that make multi- agent interactions scalable and efficient. © 2025 Confluent, Inc. 12 Part III: Design Patterns for Multi-Agent Systems Enterprises require networks of agents that collaborate, share context, and execute workflows together. However, scaling from a single agent to a multi-agent system introduces significant challenges: • Context and Data Sharing: Agents must exchange information efficiently without duplication, loss, or inconsistency. • Scalability and Fault Tolerance: Systems should handle a growing number of agents, while ensuring recovery from failures. • Integration Complexity: Agents interact with diverse tools and data sources, requiring seamless interoperability. • Timely and Accurate Decisions: Agents need access to fresh, real-time data to make informed decisions without delay. • Safety and Validation: Guardrails are necessary to prevent unintended behaviors and ensure reliable, high-quality outputs. • Security and Governance: Organizations must enforce data security, compliance, and lineage to maintain trust and control. • Operational Overhead: Managing infrastructure for data pipelines, integration, and compute at scale adds significant burden. • Lack of Global Availability: AI systems require real-time data access and processing across regions to support global operations. Confluent Data Streaming Platform is the future-proof foundation for addressing these challenges. It acts as the communication layer and data enabler in the agentic AI stack: © 2025 Confluent, Inc. 13 By combining four key pillars—Stream, Connect, Process, and Govern—Confluent enables agents to seamlessly consume, process, and act on data in motion. • Streaming ensures real-time event flow, allowing agents to react instantly to changes. • Connectors integrate disparate data sources, removing bottlenecks in interoperability. • Stream Processing transforms and enriches data in motion, enabling contextualized decision- making. • Stream Governance enforces security, compliance, and data quality, ensuring trust and reliability. This section explores key design patterns that enable multi-agent systems to function as scalable, resilient, and adaptable networks, leveraging a data streaming platform to unlock real-time AI capabilities without infrastructure headaches. Multi-Agent Design Patterns Multi-agent design patterns define how autonomous agents communicate, collaborate, or compete to solve problems. These patterns structure interactions between agents, ensuring efficient decision-making and workload distribution. Below, we examine four essential patterns: Orchestrator-Worker, Hierarchical Agent, Blackboard, and Market-Based, along with how event-driven architectures transform them into scalable, loosely coupled systems. A data streaming platform not only enables any multi-agent system design pattern, but goes far beyond just Apache Kafka®, providing all the tools to connect and unlock data from any system, enforce governance for quality and compliance, and process data into high-value streams that fuel real-time, intelligent agent decision-making. Orchestrator-Worker Pattern In the Orchestrator-Worker pattern, a central agent assigns tasks to worker agents and manages execution. This is akin to the Master-Worker pattern in distributed",
    "and unlock data from any system, enforce governance for quality and compliance, and process data into high-value streams that fuel real-time, intelligent agent decision-making. Orchestrator-Worker Pattern In the Orchestrator-Worker pattern, a central agent assigns tasks to worker agents and manages execution. This is akin to the Master-Worker pattern in distributed computing, where an orchestrator coordinates multiple independent workers that execute specific jobs. Orchestrator-Worker Pattern © 2025 Confluent, Inc. 14 Traditional Approach: • The orchestrator assigns tasks to worker agents. • Workers execute tasks and return results to the orchestrator. • If a worker fails, the orchestrator must reassign tasks manually. Event-Driven Approach: Using data streaming, we can adapt this pattern to make the agents event-driven. Confluent Data Streaming Platform offers key-based Kafka partitioning strategies, so the orchestrator can use keys to distribute command messages across partitions in a single topic. Worker agents can then act as a consumer group, pulling events from one or more assigned partitions to complete the work. Each worker agent then sends output messages into a second topic, where it can be consumed by downstream systems. The pattern now looks like this: Event-Driven Orchestrator-Worker Pattern © 2025 Confluent, Inc. 15 While this diagram looks more complex, it dramatically simplifies the operations of the system. The orchestrator no longer has to manage its connections to worker agents, including managing what happens if one dies, or handling more or fewer worker agents. Instead, it uses a keying strategy that distributes work across partitions. For events that should be processed by the stateful worker agent as some previous message, the same key can be used for each event in a sequence. The worker agents gain the benefits of any consumer group. The worker agents pull from one or more partitions, and the Kafka Consumer Rebalance Protocol assures that each worker has similar workloads, even as worker agents are added or removed. In the event of a worker failure, the log can be replayed from a given partition for a saved offset. The orchestrator no longer needs bespoke logic for managing workers; instead, it simply specifies work and distributes it with a sensible keying strategy. Similarly, the worker agents inherit the functionality of a Kafka consumer group, so they can use common machinery for coordination, scaling, and fault recovery. This pattern allows for dynamic scaling, automatic fault recovery, and efficient workload distribution without the need for complex management logic. Hierarchical Agent Pattern The Hierarchical Agent pattern organizes agents into layers, where higher-level agents oversee or delegate tasks to lower-level agents. This is ideal for breaking down complex problems into smaller, manageable parts. Hierarchical Multi-Agent Pattern Traditional Approach: • A central decision-making agent controls multiple subordinate agents. • Subordinate agents handle specialized tasks but require direct coordination. © 2025 Confluent, Inc. 16 Event-Driven Approach: To make the hierarchical pattern event-driven, we apply the same techniques for decomposing work in the orchestrator-worker pattern recursively in the agent hierarchy, such that each non- leaf node is the orchestrator for its respective subtree. Event-Driven Hierarchical Multi-Agent Pattern By making hierarchical coordination event-driven, agents publish and subscribe",
    "Approach: To make the hierarchical pattern event-driven, we apply the same techniques for decomposing work in the orchestrator-worker pattern recursively in the agent hierarchy, such that each non- leaf node is the orchestrator for its respective subtree. Event-Driven Hierarchical Multi-Agent Pattern By making hierarchical coordination event-driven, agents publish and subscribe to event streams, rather than rely on direct supervision: • Higher-level agents publish objectives as events. • Mid-tier agents consume events, break down tasks, and issue new events to lower-tier agents. © 2025 Confluent, Inc. 17 • Execution agents consume low-level tasks, process them, and publish results. • Sibling agents form consumer groups to process shared workloads dynamically. With this approach, hierarchy is no longer rigid, agents can be added or removed dynamically without modifying the system’s core logic. Asynchronous event processing ensures scalability while maintaining structured delegation. Blackboard Pattern The Blackboard Pattern introduces a shared knowledge base—a “blackboard”—where agents asynchronously post and retrieve information. This pattern is widely used in complex problem- solving, such as collaborative AI systems and robotics. Blackboard Pattern Traditional Approach: • Agents must explicitly query a database or communicate directly with other agents. • Coordination becomes a bottleneck, leading to synchronization challenges. Event-Driven Approach: • The blackboard is implemented as a streaming topic in Kafka. • Agents publish knowledge updates as events instead of direct database writes. • Other agents subscribe to these updates dynamically, consuming only relevant information. © 2025 Confluent, Inc. 18 The event-driven version looks like this: Event-Driven Blackboard Pattern This approach allows real-time collaboration without agents needing to track each other’s state explicitly. The blackboard acts as a memory layer, ensuring that shared context is always available without excessive network calls. Market-Based Pattern The Market-Based Pattern models a decentralized system where agents negotiate or compete for tasks and resources. This is commonly used in autonomous trading, logistics, and distributed optimization problems. Market-Based Pattern © 2025 Confluent, Inc. 19 Traditional Approach: • Agents communicate directly with each other to place bids or negotiate terms. • A central system is often required to coordinate interactions. Event-Driven Approach: • Bidding agents publish offers and requests as events. • A market-making service matches events, executing transactions asynchronously. • Agents listen for matched events and adjust their strategies dynamically. The pattern now looks like this: Event-Driven Market-Based Pattern This removes the quadratic complexity of direct peer-to-peer communication, as agents interact through a central event log instead of maintaining individual connections. For example, in financial markets, a data streaming platform is used as a real-time event broker, allowing thousands of trading agents to execute bids, match orders, and react to price fluctuations in milliseconds. © 2025 Confluent, Inc. 20 EXAMPLE Multi-Agent AI Sales Development Representative (SDR) Here’s an event-driven, multi-agent system that automates the SDR workflow. Apache Flink® with AI Model Inference is used to orchestrate communication with a series of AI agents. The system consists of the following agents: • Lead Ingestion Agent: Captures incoming leads from web forms, enriches them with external data (e.g., company website, Salesforce), and generates a report that can be used for scoring.",
    "AI Model Inference is used to orchestrate communication with a series of AI agents. The system consists of the following agents: • Lead Ingestion Agent: Captures incoming leads from web forms, enriches them with external data (e.g., company website, Salesforce), and generates a report that can be used for scoring. • Lead Scoring Agent: Uses enriched lead information to score leads and generate a short summary for how to best engage. Determines the appropriate next step and triggers downstream agents. • Active Outreach Agent: Creates personalized outreach emails, incorporating insights from the lead’s online presence, in order to book a meeting. • Nurture Campaign Agent: Dynamically creates a sequence of emails based on where the lead originated, and what their interest was. • Send Email Agent: Currently sends to a terminal, but in a real application would send via email relay or email service. → Visit this GitHub repo to learn more. © 2025 Confluent, Inc. 21 The Role of the Data Streaming Platform in Multi-Agent Systems For multi-agent systems to function efficiently, they must operate under a shared event-driven model that standardizes communication and decision-making. This model consists of three primary components: 1. Input: Agents consume structured events or commands. 2. Processing: Agents apply reasoning, use tools, or retrieve additional context. 3. Output: Agents produce new events or take actions in external systems. By following this event-driven framework, multi-agent systems gain modularity, resilience, and the ability to scale dynamically. Maintaining state consistency across multiple agents requires event persistence and replayability. This is where immutable logs and event sourcing come into play. • Every event is recorded as an immutable entry, ensuring no data loss. • If an agent fails, it can replay events from a saved offset, restoring its state seamlessly. • Multiple agents can consume the same event stream, allowing parallel processing without interference. This model dramatically improves reliability, making it easier to debug failures, scale workloads, and maintain real-time synchronization. As multi-agent architectures become more prevalent, event-driven patterns are essential to ensuring they remain scalable, adaptable, and efficient. By applying the lessons from microservices architecture, AI agents can be designed to operate in a loosely coupled, decentralized manner, reacting to events rather than relying on rigid dependencies. This enables the creation of AI-driven ecosystems where agents collaborate seamlessly across complex workflows, unlocking new possibilities for enterprise automation and large-scale AI systems. In the next section, we’ll explore how to build and deploy these event-driven agents in real-world applications, leveraging a data streaming platform with fully managed Kafka and Flink for scalable execution. © 2025 Confluent, Inc. 22 Part IV: Building Event-Driven Systems for Agents with a Data Streaming Platform AI agents are only as effective as the infrastructure that supports them. No matter how sophisticated an agent’s reasoning and decision-making capabilities are, they depend on access to the right data, tools, and communication channels. Traditional architectures—built around request/response patterns, rigid APIs, and batch data processing—create bottlenecks that limit an agent’s ability to act in real time. EDAs solve this by treating data as a continuously moving asset, rather than",
    "decision-making capabilities are, they depend on access to the right data, tools, and communication channels. Traditional architectures—built around request/response patterns, rigid APIs, and batch data processing—create bottlenecks that limit an agent’s ability to act in real time. EDAs solve this by treating data as a continuously moving asset, rather than a static snapshot. By using Confluent Data Streaming Platform as the backbone, agents can consume, process, and emit events asynchronously, making them adaptable, scalable, and resilient for enterprise use cases. In this section, we’ll explore how to architect enterprise-ready event-driven AI systems, tackle the challenges of real-time agent interaction, and apply these concepts to real-world applications. Architecting Singular and Multi-Agent Systems The shift to event-driven architectures marks a pivotal moment in building scalable agent systems. Instead of waiting for direct instructions, agents are designed to emit and listen for events autonomously. Events act as signals that something has happened—a change in data, a triggered action, or an important update—allowing agents to respond dynamically and independently. At their core, AI agents function much like microservices, operating autonomously but needing structured communication to collaborate effectively. Confluent Data Streaming Platform acts as the “central nervous system” for agents, enabling them to function in a loosely coupled, but highly coordinated manner. © 2025 Confluent, Inc. 23 Stream Connect Process Govern Continuously capture Integrate disparate Use Flink stream Use data lineage, and share real-time data from any processing (e.g., join, quality controls, and events with AI systems environment, with filter) to enrich data traceability to ensure and agents anywhere, 120+ pre-built and with real-time context data for agents is built on Kora, the cloud- custom connectors, at query execution, secure and verifiable. native Apache Kafka® bringing real-time enabling agentic RAG. engine. data to agents. The Central Nervous System for Agentic Systems By integrating Apache Kafka®, Apache Flink®, and an existing agent framework like LangGraph, we can build architectures that scale horizontally, process high-throughput data streams, and ensure real-time responsiveness. Here’s what Confluent Data Streaming Platform provides: • Horizontal Scalability: Agents operate independently, allowing for seamless scaling. Adding more agents doesn’t require rewriting the entire system. • Low Latency and High Throughput: Streaming platforms ensure that agents receive and act on fresh data without waiting for batch updates. • Fault Recovery and Isolation: Failures in one part of the system don’t bring down the entire workflow. Agents replay events from logs, ensuring resilience. • Shift Left: Data is processed closer to the source, reducing latency and improving AI performance. A tightly integrated, event-driven pipeline ensures that agents not only receive fresh data, but also act on it dynamically, making real-time AI applications possible. © 2025 Confluent, Inc. 24 Challenges and Solutions in Event-Driven Agent Design While the benefits of an event-driven approach are clear, building AI-powered agentic systems comes with its own set of challenges. Below, we address key roadblocks and how Confluent Data Streaming Platform provides solutions. Stream Next-Level Data Streaming with a Fully Managed, Cloud-Native Service In an agent-driven system, failures, whether from network issues, hardware faults, or software crashes, are inevitable. The challenge is ensuring",
    "comes with its own set of challenges. Below, we address key roadblocks and how Confluent Data Streaming Platform provides solutions. Stream Next-Level Data Streaming with a Fully Managed, Cloud-Native Service In an agent-driven system, failures, whether from network issues, hardware faults, or software crashes, are inevitable. The challenge is ensuring agents can recover seamlessly without disrupting workflows. Solution: • Kafka’s log-based architecture (on Kora) enables agents to replay events and recover from failures. • Idempotent processing ensures that retried operations don’t result in duplicate actions. • Dead-letter queues handle failure scenarios gracefully, allowing human oversight when needed. Connect Seamless Integration: Connecting Agents with Diverse Systems AI agents often interact with multiple external systems, such as databases, APIs, vector stores, and enterprise applications. These interactions require seamless, low-latency connectivity without introducing unnecessary dependencies. Solution: • Use connectors to integrate disparate data sources and event streams. This allows agents to consume and produce events without hardcoded dependencies. • Leverage LangGraph, Microsoft AutoGen, CrewAI and similar frameworks for tool integration, enabling agents to call APIs, databases, and models dynamically. • Use data streaming through Kafka topics and stream processing with Flink, ensuring structured collaboration between different AI agents. Process Ensuring Data Freshness: Handling Dynamic Data Streams Agents require up-to-date, real-time data for optimal decision-making. Traditional architectures that rely on static data retrieval lead to outdated responses and inefficient workflows. Solution: • Flink SQL and Table API to process incoming data streams, ensuring AI models and agents work with the latest context. • Flink AI Model Inference ensures model predictions update dynamically as new data flows in. • Embedding pipelines transform unstructured text into vector representations in real time, stored in vector databases such as MongoDB, for rapid retrieval in RAG. Govern Data Quality, Security, and Compliance AI agents often interact with sensitive data, requiring strong governance, auditing, and compliance measures. Solution: • Stream Governance ensures clean, structured data flows through the system, enforcing policies and data lineage tracking. • Encryption and access control at the field level prevent unauthorized data exposure. • Fine-grained retention policies ensure data is handled in compliance with regulatory requirements like GDPR. By addressing these challenges with an event-driven approach, we ensure that agents remain scalable, reliable, and capable of handling complex, multi-step workflows. © 2025 Confluent, Inc. 25 EXAMPLE Agentic RAG Here’s an event-driven research agent that mines source materials and leverages RAG to create a podcast interview brief. The workflow: 1. Stream unstructured data (e.g., website URLs, blogs, podcasts) into Confluent, retrieve the text, chunk it, generate embeddings using Flink, and store them in a vector database like MongoDB Atlas using a sink connector. 2. For all text extracted, pull out the most interesting questions, and store those. 3. Call the LLM to generate a research brief combining the most relevant context based on the embeddings. → Visit this GitHub repo and blog to learn more. © 2025 Confluent, Inc. 26 Real-World Applications of Event-Driven Agentic Systems Event-driven architectures unlock powerful AI applications across industries by ensuring agents operate with fresh, real-time data. Below are a few examples of",
    "most relevant context based on the embeddings. → Visit this GitHub repo and blog to learn more. © 2025 Confluent, Inc. 26 Real-World Applications of Event-Driven Agentic Systems Event-driven architectures unlock powerful AI applications across industries by ensuring agents operate with fresh, real-time data. Below are a few examples of how companies are using a data streaming platform to build event-driven multi-agent systems. 1. Automating Web Scraping with AI Agents Traditional web scraping is brittle, requiring manual efforts to handle dynamic pages, extract relevant data, and adapt to site changes. This complexity increases when supporting GenAI models, which need structured and unstructured data in real time. Static scraping workflows fail to keep up, leading to outdated or incomplete information. Reworkd tackled this problem by building an agentic system for web scraping. AI agents write code to extract relevant data, while test and validation agents verify that the generated code is correct. These agents operate asynchronously, consuming and producing events in a continuous feedback loop. The agents seamlessly process and stream real-time data into downstream applications. The result: a scalable, fault-tolerant system that dynamically adjusts to website changes, and ensures high-quality data feeds for AI models. 2. Intelligent Business Copilots Teams need self-serve, real-time access to data for faster, smarter decision-making. Yet, business stakeholders often rely on engineering and data science teams to integrate and query data, introducing bottlenecks, batch-based delays, and stale insights. Airy transforms this process by enabling AI-powered copilots that provide a natural language interface for exploring and working with real-time data. Agents convert plain language into Flink jobs, continuously monitoring and processing data streams. By leveraging real-time context from a data streaming platform, these agents use LLMs to generate precise Flink SQL queries, empowering teams to extract and analyze live data instantly. This shifts data interaction from manual queries to conversational, real-time insights, making knowledge more accessible than ever. 3. Workflow Automation with a Drag-and-Drop Agent Builder When starting to build multi-agent systems, teams face the complexity of integrating and orchestrating AI agents and tools, as well as managing intricate tasks at scale. Agent Taskflow provides a no-code platform that helps users get up and running with all the features that go into an agent–memory, knowledge bases, and tools–in just a few clicks. The drag-and-drop UI allows for effortless creation of workflows. Built on a data streaming platform, the solution enables agents to make context-informed decisions and adapt to real-time events, for more efficient and intelligent automation. From automating customer support to marketing campaigns, this frees teams to focus on high-value work while democratizing AI agents, making it accessible to users without programming expertise. © 2025 Confluent, Inc. 27 Broader Industry Impact Event-driven AI agents enable businesses to automate complex, real-time tasks across industries: • E-commerce: Agents continuously track price changes, product availability, and competitor trends, ensuring businesses make informed pricing decisions. • Market Research: Streaming data feeds allow companies to monitor customer sentiment, competitive shifts, and industry trends in real time. • Finance: Agents aggregate financial data, news sentiment, and stock movements, helping analysts make more accurate, timely",
    "price changes, product availability, and competitor trends, ensuring businesses make informed pricing decisions. • Market Research: Streaming data feeds allow companies to monitor customer sentiment, competitive shifts, and industry trends in real time. • Finance: Agents aggregate financial data, news sentiment, and stock movements, helping analysts make more accurate, timely decisions. By adopting event-driven multi-agent systems, businesses gain resilient, adaptive AI workflows that evolve dynamically. As this paradigm becomes standard, companies that integrate streaming-first architectures will gain a competitive edge in automating complex, data-intensive processes. Why a Data Streaming Platform is Essential for AI Agents At the heart of every scalable agentic system is a real-time data streaming platform. Unlike traditional request/response architectures that introduce bottlenecks and stale data, streaming enables continuous, low-latency access to the information agents need. Why streaming matters: • Real-Time Access: Eliminates batch delays and ensures agents make decisions based on the latest available data. • Decoupled Architecture: Agents interact through event streams rather than direct calls, reducing complexity and interdependencies. • AI-Ready Data: Streaming platforms transform unstructured data into embeddings stored in vector stores like MongoDB, making it AI-friendly. • Scalable AI Interactions: AI workloads are distributed across streaming pipelines, preventing bottlenecks and improving efficiency. • Modular and Future-Proof: Use any model, vector database, or AI framework of choice, with the flexibility to swap in new technologies as they evolve, ensuring long-term adaptability. By adopting a shift-left approach—moving computation closer to the data source—organizations can reduce latency, improve AI performance, and create a more adaptive architecture. The Future of AI is Event-Driven AI-powered agents will define the next era of automation, but only if they can think, act, and collaborate in real time. Event-driven architectures ensure that agents are no longer limited by outdated batch processes, rigid APIs, or stale data. Instead, they operate dynamically— processing, analyzing, and acting on real-time events as they happen. As AI adoption accelerates, companies that embrace streaming-first architectures will have a massive advantage. They’ll build AI systems that are smarter, more adaptable, and infinitely scalable, unlocking true agentic intelligence across industries. © 2025 Confluent, Inc. 28 Take the Sign up for Confluent Cloud and 1 receive $400 in free credits. Next Step 2 Visit the GenAI hub for more resources. To get started with building event-driven 3 Apply for Confluent’s AI Accelerator Program, multi-agents: which provides technical and business mentorship to help AI startups fast-track innovation and growth. About Confluent Confluent is pioneering a fundamentally new category of data infrastructure focused on data in motion. Confluent’s cloud-native offering is the foundational platform for data in motion–designed to be the intelligent connective tissue enabling real-time data from multiple sources to constantly stream across the organization. With Confluent, organizations can meet the new business imperative of delivering rich digital front-end customer experiences and transitioning to sophisticated, real-time, software-driven back-end operations. To learn more, please visit www.confluent.io. © Confluent Inc. 2025 29 © Confluent Inc. 2025 29",
    "transitioning to sophisticated, real-time, software-driven back-end operations. To learn more, please visit www.confluent.io. © Confluent Inc. 2025 29 © Confluent Inc. 2025 29",
    "RAGAnything là một framework RAG tất cả trong một. Nó hỗ trợ xử lý đa phương thức (hình ảnh, bảng biểu, công thức). Việc sử dụng Local LLM giúp bảo mật dữ liệu và tiết kiệm chi phí API. Llama 3.1 8B là một mô hình ngôn ngữ mạnh mẽ của Meta.",
    "Tong Xiao Jingbo Zhu Natural Language Processing Neural Networks and Large Language Models NATURAL LANGUAGE PROCESSING LAB NORTHEASTERN UNIVERSITY & NIUTRANS RESEARCH https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Copyright©2021-2025TongXiaoandJingboZhu NATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY & NIUTRANS RESEARCH https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook LicensedundertheCreativeCommonsAttribution-NonCommercial4.0UnportedLicense(the “License”). YoumaynotusethisfileexceptincompliancewiththeLicense. Youmayobtain acopyoftheLicenseathttp://creativecommons.org/licenses/by-nc/4.0. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissionsandlimitationsundertheLicense. December1,2025 3 Preface Naturallanguageprocessing(NLP)isoneofthecoresubfieldsofartificialintelligence(AI). Foralongtime,researchinNLPprimarilyfocusedonsolvingspecificproblemsinlanguage understanding and generation, such as parsing and machine translation. This task-driven research approach dominated the development of NLP for several decades. However, with theriseofdeeplearning,theresearchparadigmofNLPhasundergoneafundamentaltrans- formation. The application of deep neural networks has enabled us to tackle increasingly complextasks. Moreimportantly,researchershavediscoveredthatbyconductinglarge-scale pretrainingonabasemodelwithmassivedatasets,andthenfine-tuningitwithasmallamount oftask-specificdataandknowledge,itispossibletoconstructgeneral-purposemodelscapable ofhandlingmultipletaskssimultaneously. Thisnewparadigmisgreatlychangingtheresearch landscapeofNLP,andeventhebroaderfieldofAI. ThisbookfocusesonmodernNLPmethodscenteredaroundneuralnetworksandfounda- tionmodels. Itaimstoprovideapracticalguidetounderstanding,building,andapplyingthese powerfulmodels. UnliketraditionalNLPtextbooksthatorganizechaptersbasedonspecific tasks,thisbookisstructuredfromtheperspectiveofconstructingneuralNLPmodelsandis dividedintothreemainparts: • FoundationsofMachineLearningandNeuralNetworks(Chapters1-2): Thispart introduces the core concepts and methods of machine learning and neural networks, layingafoundationforthesubsequentchapters. Itisrelativelyself-containedandcan bestudiedindependentlyorusedasbackgroundmaterialwhenneeded. • Basic Neural Models for Natural Language Processing (Chapters 3-6): This part explainstheneuralnetworksusedinNLPtasks,includingwordrepresentationmodels, sequence models, and sequence-to-sequence models. In addition, Transformers are introducedinadedicatedchapter. ThesemodelsarenotlimitedtoindividualNLPtasks; rather,theyserveasgeneral-purposetoolsacrossmanyapplications. • LargeLanguageModels(Chapters7-11): Thispartfocusesonlargelanguagemodels (LLMs), coveringtopicssuchaspretraining, generativemodels, promptengineering, alignment,andinference. Thisbookisintendedforseniorundergraduates,graduatestudents,researchersinrelated fields, and anyone interested in NLP. We strive for clear and accessible writing, aiming to introducecoreconceptsandfundamentalmethodsratherthanprovidinganin-depthexploration ofallcutting-edgetechniques. Therefore,thisbookcanservebothasanintroductorytextfor newcomersandasareferencemanualforkeyconceptsandmethodsinNLP. The content of this book has gradually taken shape through our years of teaching and researchexperience. Initially,weonlyplannedtowritethefirsttwoparts. However,therapid 4 riseandgrowingimportanceofLLMsledustoincludethistopicasakeypartofthebook. At thesametime,wearedelightedtowitnesstherapiddevelopmentofNLP,andthewritingof thisbookisalsoourresponsetothisexcitingtrend. Somechaptersofthisbookhavebeenpreviouslypublishedonline,suchasIntroductionto Transformers: AnNLPPerspective(Chapter6)andFoundationsofLargeLanguageModels (Chapters7-11),andwearegratefulforthevaluablefeedbackfrommanyreaders,whichhas greatly contributed to the refinement of the book. Furthermore, during the writing process, wedrewsignificantinspirationfromclassicworks,includingMachineLearningbyMitchell [1997], Foundations of Statistical Natural Language Processing by Manning and Schütze [1999],PatternRecognitionandMachineLearningbyBishop[2006],andSpeechandLan- guageProcessingbyJurafskyandMartin[2008]. Manyinsightsfromtheseworksprofoundly influencedthewritingapproachofthisbook. Lastly,wewouldliketoexpressourheartfeltthankstoallthosewhoprovidedsuggestions andrevisionstothecontentofthisbook. Theyare: WeiqiaoShan,YongyuMu,Chenglong Wang,KaiyanChang,YuchunFan,HangZhou,ChuanhaoLv,XinyuLiu,TaoZhou,Huiwen Bao,TongZheng,JunhaoRuan,YingfengLuo,YuzhangWu,andYifuHuo. TongXiaoandJingboZhu June,2025 NaturalLanguageProcessingLab,NortheasternUniversity Thisbookisdedicatedtoourfamilies. Contents I Preliminaries 1 Foundations of Machine Learning ........................... 17 1.1 Math Basics ................................................ 18 1.1.1 LinearAlgebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.1.2 ProbabilityandStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.2 Designing a Text Classifier.................................... 26 1.2.1 ProblemStatement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 1.2.2 DocumentsasFeatureVectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.2.3 LinearClassifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . 28 1.2.3 LinearClassifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 1.2.4 GenerativevsDiscriminative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 1.2.5 OOVWordsandSmoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 1.3 General Problems ........................................... 37 1.3.1 SupervisedandUnsupervisedModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.3.2 InductiveBias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.3.3 Non-linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 1.3.4 TrainingandLossFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 1.3.5 OverfittingandUnderfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 1.3.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 1.4 Model Selection and Evaluation................................ 50 1.4.1 StrategiesforModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 1.4.2 Training,ValidationandTestData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 1.4.3 PerformanceMeasure . . . .",
    ". . . . . . 51 1.4.2 Training,ValidationandTestData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 1.4.3 PerformanceMeasure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 1.4.4 SignificanceTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 1.5 NLP Tasks as ML Tasks....................................... 59 1.5.1 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 1.5.2 SequenceLabeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 1.5.3 LanguageModeling/WordPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 1.5.4 SequenceGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 1.5.5 TreeGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 1.5.6 RelevanceModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 1.5.7 LinguisticAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 1.5.8 Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . 65 1.5.8 Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 1.5.9 Others . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 1.6 Summary................................................... 68 2 Foundations of Neural Networks ............................ 71 2.1 Multi-layer Neural Networks ................................... 71 2.1.1 Single-layerPerceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 2.1.2 StackingMultipleLayers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 2.1.3 ComputationGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 2.2 Example: Neural Language Modeling........................... 78 2.3 Basic Model Architectures .................................... 83 2.3.1 RecurrentUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 2.3.2 ConvolutionalUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 2.3.3 GateUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 2.3.4 Normalization(Standardization)Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 2.3.5 ResidualUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "88 2.3.5 ResidualUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 2.4 Training Neural Networks ..................................... 90 2.4.1 GradientDescent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 2.4.2 Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 2.4.3 ParameterInitialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96 2.4.4 LearningRateScheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97 2.5 Regularization Methods ...................................... 99 2.5.1 Norm-basedPenalties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 2.5.2 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 2.5.3 EarlyStopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 2.5.4 SmoothingOutputProbabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 2.5.5 TrainingwithNoise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 2.6 Unsupervised Methods and Auto-encoders..................... 108 2.6.1 Auto-encoderswithExplicitRegularizers . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . . . . . . . . 105 2.6 Unsupervised Methods and Auto-encoders..................... 108 2.6.1 Auto-encoderswithExplicitRegularizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111 2.6.2 DenoisingAuto-encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 2.6.3 VariationalAuto-encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 2.7 Summary.................................................. 119 II Basic Models 3 Words and Word Vectors .................................. 123 3.1 Tokenization ............................................... 124 3.1.1 TokenizationviaRulesandHeuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125 3.1.2 TokenizationasLanguageModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 3.1.3 TokenizationasSequenceLabeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 3.1.4 LearningSubwords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 3.2 Vector Representation for Words.............................. 137 3.2.1 One-hotRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 3.2.2 DistributedRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138 3.2.3 CompositionalityandContextuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 3.3 Count-based Models ........................................ 142 3.3.1 Co-occurrenceMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 3.3.2 TF-IDF . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . . . . 142 3.3.2 TF-IDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 3.3.3 Low-DimensionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 3.4 Inducing Word Embeddings from NLMs........................ 153 3.5 Word Embedding Models .................................... 154 3.5.1 Word2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 3.5.2 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157 3.5.3 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161 3.6 Evaluating Word Embeddings ................................ 163 3.6.1 ExtrinsicEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 3.6.2 IntrinsicEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164 3.6.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167 3.7 Summary.................................................. 168 4 Recurrent and Convolutional Sequence Models ............. 171 4.1 Problem Statement ......................................... 172 4.2 Recurrent Models........................................... 173 4.2.1 AnRNN-basedLanguageModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 4.2.2 Training . . . . . . . .",
    "Recurrent Models........................................... 173 4.2.1 AnRNN-basedLanguageModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173 4.2.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175 4.2.3 LayerStacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178 4.2.4 Bi-directionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180 4.3 Memory ................................................... 181 4.3.1 MemoryasASystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 4.3.2 LongShort-TermMemory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183 4.3.3 GatedRecurrentUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 4.4 Convolutional Models ....................................... 187 4.4.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 4.4.2 CNNsforSequenceModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190 4.4.3 HandlingPositionalInformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 4.5 Examples.................................................. 198 4.5.1 TextClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.5.2 End-to-EndSpeechRecognition .",
    "4.5.1 TextClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 4.5.2 End-to-EndSpeechRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200 4.5.3 SequenceLabelingwithLSTMandGraphicalModels . . . . . . . . . . . . . . . . . . 203 4.5.4 HybridModelsforLanguageModeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207 4.6 Summary.................................................. 207 5 Sequence-to-Sequence Models ............................ 211 5.1 Sequence-to-Sequence Problems ............................. 212 5.2 The Encoder-Decoder Architecture............................ 213 5.2.1 EncodingandDecoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 5.2.2 Example: NeuralMachineTranslation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215 5.3 The Attention Mechanism.................................... 218 5.3.1 ABasicModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219 5.3.2 TheQKVAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 5.3.3 Multi-headAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226 5.3.4 HierarchicalAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229 5.3.5 Multi-layerAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 5.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . 232 5.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 5.4 Search .................................................... 238 5.4.1 TheLengthProblem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 5.4.2 PruningandBeamSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242 5.4.3 OnlineSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250 5.4.4 ExactSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 5.4.5 DifferentiableSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256 5.4.6 HypothesisDiversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258 5.4.7 CombiningMultipleModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260 5.4.8 MoreSearchObjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262 5.5 Summary.................................................. 265 6 Transformers ............................................. 269 6.1 The Basic Model............................................ 269 6.1.1 TheTransformerArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269 6.1.2 PositionalEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . 269 6.1.2 PositionalEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273 6.1.3 Multi-headSelf-attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 6.1.4 LayerNormalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276 6.1.5 Feed-forwardNeuralNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277 6.1.6 AttentionModelsontheDecoderSide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278 6.1.7 TrainingandInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281 6.2 Syntax-aware Models........................................ 283 6.2.1 Syntax-awareInputandOutput . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284 6.2.2 Syntax-awareAttentionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 6.2.3 Multi-branchModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 6.2.4 Multi-scaleModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290 6.2.5 TransformersasSyntaxLearners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 6.3 Improved Architectures...................................... 295 6.3.1 LocallyAttentiveModels . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . 291 6.3 Improved Architectures...................................... 295 6.3.1 LocallyAttentiveModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295 6.3.2 DeepModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299 6.3.3 NumericalMethod-InspiredModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305 6.3.4 WideModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 6.4 Efficient Models ............................................ 312 6.4.1 SparseAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 6.4.2 RecurrentandMemoryModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 6.4.3 Low-dimensionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322 6.4.4 ParameterandActivationSharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327 6.4.5 AlternativestoSelf-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 6.4.6 ConditionalComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336 6.4.7 ModelTransferandPruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 6.4.8 SequenceCompression . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 341 6.4.8 SequenceCompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343 6.4.9 HighPerformanceComputingMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344 6.5 Applications ............................................... 347 6.5.1 LanguageModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348 6.5.2 TextEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349 6.5.3 SpeechTranslation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350 6.5.4 VisionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 6.5.5 MultimodalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355 6.6 Summary.................................................. 357 III Large Language Models 7 Pre-training .............................................. 365 7.1 Pre-training NLP Models..................................... 366 7.1.1 Unsupervised,SupervisedandSelf-supervisedPre-training . . . . . . . . . . . . . . 366 7.1.2 AdaptingPre-trainedModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 7.2 Self-supervised Pre-training Tasks ............................ 372 7.2.1 Decoder-onlyPre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372 7.2.2 Encoder-onlyPre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 7.2.3 Encoder-DecoderPre-training.",
    ". . . . . 372 7.2.2 Encoder-onlyPre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373 7.2.3 Encoder-DecoderPre-training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380 7.2.4 ComparisonofPre-trainingTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386 7.3 Example: BERT ............................................ 388 7.3.1 TheStandardModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388 7.3.2 MoreTrainingandLargerModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 7.3.3 MoreEfficientModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393 7.3.4 Multi-lingualModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394 7.4 Applying BERT Models ...................................... 396 7.5 Summary.................................................. 401 8 Generative Models ........................................ 403 8.1 A Brief Introduction to LLMs ................................. 404 8.1.1 Decoder-onlyTransformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405 8.1.2 TrainingLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 8.1.3 Fine-tuningLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409 8.1.4 AligningLLMswiththeWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 8.1.5 PromptingLLMs. . . . . . . . . . .",
    "AligningLLMswiththeWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 8.1.5 PromptingLLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420 8.2 Training at Scale............................................ 425 8.2.1 DataPreparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425 8.2.2 ModelModifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 8.2.3 DistributedTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430 8.2.4 ScalingLaws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 8.3 Long Sequence Modeling.................................... 436 8.3.1 OptimizationfromHPCPerspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 8.3.2 EfficientArchitectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438 8.3.3 CacheandMemory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441 8.3.4 SharingacrossHeadsandLayers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450 8.3.5 PositionExtrapolationandInterpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 8.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . 452 8.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 8.4 Summary.................................................. 466 9 Prompting ................................................ 467 9.1 General Prompt Design...................................... 468 9.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468 9.1.2 In-contextLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471 9.1.3 PromptEngineeringStrategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473 9.1.4 MoreExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478 9.2 Advanced Prompting Methods................................ 489 9.2.1 ChainofThought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489 9.2.2 ProblemDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492 9.2.3 Self-refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499 9.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505 9.2.5 RAGandToolUse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . 505 9.2.5 RAGandToolUse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509 9.3 Learning to Prompt ......................................... 515 9.3.1 PromptOptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515 9.3.2 SoftPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519 9.3.3 PromptLengthReduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528 9.4 Summary.................................................. 530 10 Alignment ................................................ 533 10.1 An Overview of LLM Alignment ............................... 534 10.2 Instruction Alignment ....................................... 535 10.2.1 SupervisedFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536 10.2.2 Fine-tuningDataAcquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541 10.2.3 Fine-tuningwithLessData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546 10.2.4 InstructionGeneralization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547 10.2.5 UsingWeakModelstoImproveStrongModels . . . . . . . . . . . . . . . . . . . . . . . 549 10.3 Human Preference Alignment: RLHF .......................... 553 10.3.1 BasicsofReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553 10.3.2 TrainingRewardModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 10.3.3 TrainingLLMs . . . . . . . . . . . . .",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 10.3.3 TrainingLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563 10.4 Improved Human Preference Alignment........................ 568 10.4.1 BetterRewardModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568 10.4.2 DirectPreferenceOptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575 10.4.3 AutomaticPreferenceDataGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578 10.4.4 Step-by-stepAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580 10.4.5 Inference-timeAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583 10.5 Summary.................................................. 584 11 Inference ................................................. 587 11.1 Prefilling and Decoding...................................... 588 11.1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 588 11.1.2 ATwo-phaseFramework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593 11.1.3 DecodingAlgorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 11.1.4 EvaluationMetricsforLLMInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607 11.2 Efficient Inference Techniques................................ 608 11.2.1 MoreCaching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ". . . . . . . . . . 607 11.2 Efficient Inference Techniques................................ 608 11.2.1 MoreCaching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608 11.2.2 Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 11.2.3 Parallelization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619 11.2.4 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619 11.3 Inference-time Scaling....................................... 621 11.3.1 ContextScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622 11.3.2 SearchScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623 11.3.3 OutputEnsembling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623 11.3.4 GeneratingandVerifyingThinkingPaths . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624 11.4 Summary.................................................. 632 I Preliminaries 1 Foundations of Machine Learning .......... 17 1.1 MathBasics 1.2 DesigningaTextClassifier 1.3 GeneralProblems 1.4 ModelSelectionandEvaluation 1.5 NLPTasksasMLTasks 1.6 Summary 2 Foundations of Neural Networks ........... 71 2.1 Multi-layerNeuralNetworks 2.2 Example: NeuralLanguageModeling 2.3 BasicModelArchitectures 2.4 TrainingNeuralNetworks 2.5 RegularizationMethods 2.6 UnsupervisedMethodsandAuto-encoders 2.7 Summary https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 1 Foundations of Machine Learning Thegoalofmachinelearningistodevelopmethodsthatcanautomatically detectpatternsindata,andthentousetheuncoveredpatternstopredict futuredataorotheroutcomesofinterest. –Murphy[2012] Machinelearningcanbebroadlydefinedascomputationalmethodsusing experiencetoimproveperformanceortomakeaccuratepredictions. –Mohrietal.[2018] Data-drivenNLPfitstheabovedefinitions1. Itteachescomputerstolearnlanguageexperience fromcorpora,andtounderstandandutilizelanguagebasedonthatexperience. Connecting machine learning (ML) with natural language processing is much more than a means that makescomputersmimichumanlanguageintelligencefromdata. Itisleadingarevolutionin bothareas: naturallanguageprocessingevolvesbyusingapowerfultoolofderivingmeaning fromcorpora,andmachinelearningevolvesbyaddressingtheNLPchallengesandtestingon real-worlddata. Inthischapter,wepresentseveralbasicconceptsandmodelsinmachinelearning. There arenotoughbitsbutsomepreliminariesforthesubsequentchapters. Herewefocusonhow to apply machine learning to NLP problems, in particular how to define an NLP problem as a statistical learning problem.",
    "experiencetoimproveperformanceortomakeaccuratepredictions. –Mohrietal.[2018] Data-drivenNLPfitstheabovedefinitions1. Itteachescomputerstolearnlanguageexperience fromcorpora,andtounderstandandutilizelanguagebasedonthatexperience. Connecting machine learning (ML) with natural language processing is much more than a means that makescomputersmimichumanlanguageintelligencefromdata. Itisleadingarevolutionin bothareas: naturallanguageprocessingevolvesbyusingapowerfultoolofderivingmeaning fromcorpora,andmachinelearningevolvesbyaddressingtheNLPchallengesandtestingon real-worlddata. Inthischapter,wepresentseveralbasicconceptsandmodelsinmachinelearning. There arenotoughbitsbutsomepreliminariesforthesubsequentchapters. Herewefocusonhow to apply machine learning to NLP problems, in particular how to define an NLP problem as a statistical learning problem. To do this, we start with classification — one of the most widely-usedexamplesinmostintroductorybooks. Wethenpresentseveralfundamentalissues ofmachinelearning. TheyarefollowedbyadiscussiononNLPproblemsfromthemachine learningperspective. 1Wedropthetermdata-drivenfromnowonandassumethatallNLPmodelsaredata-drivenintheremainder ofthisdocument. 18 Chapter 1. Foundations of Machine Learning 1.1 Math Basics Intheremainderofthischapterandthefollowingchapters,wewilltalkaboutmachinelearning problems using the tool of applied mathematics. Here are the math basics. If you find the detailstrivial,youcanskiptoSection1.2directly. 1.1.1 Linear Algebra 1. Vectors and Matrices Scalarmayormaynotbethesimplestconceptinlinearalgebra,butissurelythemostcommon conceptthatonelearnsinhighschoolorinuniversity. Ascalarisanumber. Itisaquantity thathasamagnitudebuthasnodirection. Forexample,height,weight,distance,temperature areallexamplesofscalars. Hereweuseanitalicnumbertodenoteascalar,forexample,a,b, x,A,andsoon. Vectorandmatrixaredefinedontopofscalar. Avectorisanarrayofscalars,orsimplya numberlist. Amatrixisarectangulararrayofscalars. Inthisbook,wefollowtheconvention ofusingboldletterstodenotevectorsandmatrices. Forexample,ann-dimensionalvectorcan bewrittenas (cid:104) (cid:105) a = a a ... a (1.1) 1 2 n where{a ,a ,...,a }aretheelements(orentries)ofthevector. Eachindicatesadimension. 1 2 n Forconvenienceofnotation,wewritea asa(i)sometimes. Avectorisareal-valuedvector i onlyifalltheelementsarerealnumbers(i.e.,a ∈Rforeachi),denotedasa∈Rn. Likewise, i wecanwriteanm×nmatrixas   A A ... A 11 12 1n A A ... A  A =  21 22 2n (1.2)  ... ... ... ...    A A ... A m1 m2 mn where m is the number of rows and n is the number of columns. A is the entry (i,j) of ij the matrix. A real-valued matrix is denoted as A∈Rm×n. Occasionally, we use A to m×n emphasizethattheshapeofthematrixism×n. Thereareafewspecialmatrices. Forexample,amatrixwhoseelementsareallzerosisa zeromatrix,denotedas0. Anotherexampleisidentitymatrix,denotedasI. Itisasquare matrixwhosediagonalelementsareall1,andotherelementsare0. Vectorscanbetreatedasa specialsortofmatrices,too. Forexample,thevectorinEq. (1.1)isamatrixwithonlyonerow. 1.1 Math Basics 19 2. Matrix Transpose ThetransposeofamatrixA isamatrixB subjecttoA =B foreachpairofiand m×n n×m ij ji j. Often,A’stransposeisdenotedasAT. Forexample,foramatrix (cid:34) (cid:35) 8 0 0 A = (1.3) 2 9 7 thetransposeis   8 2 AT = 0 9 (1.4)   0 7 Onecantransposeavectoraswell. Foravector (cid:104) (cid:105) a = 1 9 7 3 (1.5) thetransposeis   1 9 aT =   (1.6) 7   3 Ingeneral,avectorwithonlyonerowiscalledarowvector(asinEq. (1.5)),andavector withonlyonecolumniscalledacolumnvector(asinEq. (1.6)). Inthisbook,allvectorsare rowvectorsbydefault. 3. Element-wise Operations on Matrices SupposeAandBaretwomatricesofthesameshape,sayA,B∈Rm×n. Thematrixaddition of A and B is written as A+B. A+B is a matrix in Rm×n such that each element is the sumofthecorrespondingelementsofAandB. Hereisanexample. (cid:34) (cid:35) (cid:34) (cid:35) 8 0 0 1 1 1 A+B = + 2 9 7 1 0 4 (cid:34) (cid:35) 9 1 1 = (1.7) 3 9 11 Inasimilarway,wecandefineelement-wiseminus(A−B),product(A⊙B),division (A⊘B)andotheroperations. Aspecialcaseofelement-wiseproductisthatwemultiplya matrixAwithanothermatrixwhoseelementsareallthesame(sayk). ItisequaltoscalingA withascalark,denotedask×AorkA. Thisisalsocalledscalarproduct. Seebelowforan 20 Chapter 1. Foundations of Machine Learning (cid:34) (cid:35) 8 0 0 examplefork=2andA= . 2 9 7 (cid:34) (cid:35) 8 0 0 kA = 2 2 9 7 (cid:34) (cid:35) 16 0 0 = (1.8) 4 18 14 LetA,BandCbematricesinRm×n,andk andlbescalarsinR. Somepropertiesofthe matrixoperationsare: • Propertyofthezeromatrix: A = A+0 = 0+A (1.9) • Commutativity: A+B = B+A (1.10) klA = lkA (1.11) • Associativity: (A+B)+C = A+(B+C) (1.12) (kl)A = l(kA) (1.13) • Distributivity: k(A+B) = kA+kB (1.14) (k+l)A = kA+lA (1.15) 4. Dot Product (cid:104) (cid:105) (cid:104) (cid:105) Thedotproductoftwosame-sizedvectorsa= a a ... a andb= b b ... b 1 2 n 1 2 n isdefinedtobe: n (cid:88) a·b = a b i i i=1 = a b +a b +...+a b (1.16) 1 1 2 2 n n Ingeometry,areal-valuedvectoracanbeseenasageometricobjecthavingbothmagnitude (denotedas|a|)anddirection. Thedotproductofaandbcanalsobedefinedas a·b = |a|×|b|×cos(θ) (1.17) 1.1 Math Basics 21",
    "a a ... a andb= b b ... b 1 2 n 1 2 n isdefinedtobe: n (cid:88) a·b = a b i i i=1 = a b +a b +...+a b (1.16) 1 1 2 2 n n Ingeometry,areal-valuedvectoracanbeseenasageometricobjecthavingbothmagnitude (denotedas|a|)anddirection. Thedotproductofaandbcanalsobedefinedas a·b = |a|×|b|×cos(θ) (1.17) 1.1 Math Basics 21 whereθ istheanglebetweenaandb. 5. Matrix Product Matrix product (or matrix-matrix product) operates on two matrices. Given a matrix A ∈ Rm×p and a matrix B ∈ Rp×n, the matrix product of A and B produces a matrix C∈Rm×n whoseelementsaredefinedas: p (cid:88) C = A ×B ij ik kj k=1 = A 1×B +A ×B +...+A ×B (1.18) i 1j i2 2j ip pj MatrixproductrequiresthatthenumberofcolumnsinAisexactlythesameasthenumber ofrowsinB. InthisbookweuseABtodenotethematrixproductofAandB. Herearea fewpropertiesofmatrixproduct. • Distributivity. ForA∈Rm×p andB,C∈Rp×n,theleftdistributivityisdefinedas A(B+C) = AB+AC (1.19) ForA,B∈Rm×p andC∈Rp×n,therightdistributivityisdefinedas (A+B)C = AC+BC (1.20) • Associativity. ForA∈Rm×p,B∈Rp×q andC∈Rq×n,theassociativitydefines (AB)C = A(BC) (1.21) • Transpose. ForA∈Rm×p andB∈Rp×n,wehave (AB)T = BTAT (1.22) Matrixproductisnotcommutative,i.e.,wedonothaveAB=BAforallAandBeven ifAandBaresquarematriceswiththesameshape. Basedonmatrix-matrixproduct,wecan definevector-matrixproductandmatrix-vectorproductaccordingly. Thisistrivialbecauseall weneedistoseeavectorasamatrixinmultiplication. 6. Norm (cid:104) (cid:105) Inavectorspace,normisameasureofvector“length”. Givenavectora= a a ... a ∈ 1 2 n Rn,thenormonaisafunctionthatmapsfromRn toR. Itiswrittenas∥·∥ ,orl forshort. p p ∥a∥ isdefinedas p (cid:16) (cid:17)1/p ∥a∥ = (cid:80)n |a |p (1.23) p i=1 i Itiscalledp-normorl norm. Thepopularversionsofp-normarethosewhenp=1,2and p 22 Chapter 1. Foundations of Machine Learning ∞: n (cid:88) ∥a∥ = |a | (1.24) 1 i i=1 (cid:118) (cid:117) n (cid:117)(cid:88) ∥a∥ 2 = (cid:116) |a i |2 (1.25) i=1 ∥a∥ = max{|a |,|a |,...,|a |} (1.26) ∞ 1 2 n 2-normand∞-normarealsocalledEuclideannormandmaximumnorm. p-normcanalso beusedinmeasuringthedistanceoftwopointsinann-dimensionalspace. Letbbeanother vectorinRn. Thep-normdistancebetweenaandbisgivenbytheequation: (cid:16) (cid:17)1/p ∥a−b∥ = (cid:80)n |a −b |p (1.27) p i=1 i i 1.1.2 Probability and Statistics 1. What is Probability Probabilityisamatterofuncertainty. Itisaquantitythatdescribeshowlikelyaneventisto happen. Forexample,iftheeventiscertaintohappen,wewillsaythattheprobabilityis1;if theeventwillneverhappen,wewillsaythattheprobabilityis0. Then,whatisanevent? Inshort,aneventisanexperimentaloutcome. Itcouldbesimply the result of everything. For example, an event could be the action that you raised your arms, the scene that you were seeing the sunset, the result that you figured out for a math quiz, andsoon. Asetofrelatedeventsisdescribedbyarandomvariableorvariablefor short. For example, we can define the outcome of tossing a coin as a variable x. As there are two outcomes (heads or tails), we have two choices for the value of x. We can define thatx=1whenthecoinlandsheads,andx=0otherwise. Hence,xisabinaryvariableor morepreciselya0-1variable. Avariablechoosingavaluemeansthataneventhappens. For example,x=1meanstheeventofthecoinlandingheads. Inmathematics,probabilityisameasureontheprobabilityspacecomprisingevents(call itaprobabilitymeasure). Asameasure,probabilityshouldsatisfycertainproperties,such as countable additivity [Ash and Doléans-Dade, 1999]. This means that not all functions definedontheinterval[0,1]couldbeaprobabilitymeasure. Herewedonotdiscusstheprecise definition of probability measure. We just simply treat it as a function that outputs a real numberin[0,1]. Usually,aprobabilitymeasureisdenotedasafunctionPr(·),calledaprobabilityfunction. When the input of Pr(·) is defined on a discrete set of events, the output of the function is the probability that an event happens. For example, Pr(x=1) means the probability of x equalling1. NotethatPr(x)isafunctionthatvariesitsoutputbychoosingdifferentvalues of x. Suppose x is a value that x can take. When we write Pr(x ), it means Pr(x=x ). 1 1 1 Becausetheprobabilityoveralleventsshouldbe1,anyprobabilityfunctionshouldbesubject 1.1 Math Basics 23 to: (cid:88) Pr(x ) = 1 (1.28) i xi∈X where X is the set of all events. A probability function can be defined on two variables or more. Hereareafewwidely-usedcases. • JointProbability. Itistheprobabilitythattwoeventsx andy happenatthesame 1 1 time,denotedasPr(x ,y ). Asaspecialcase,thejointprobabilitywillbedecomposed 1 1 intotheproductoftheprobabilitiesofx andy ,ifx andy areindependentofeach 1 1 1 1 other. Pr(x ,y )=Pr(x )Pr(y ) (1.29) 1 1 1 1 • Conditional Probability.",
    "all events. A probability function can be defined on two variables or more. Hereareafewwidely-usedcases. • JointProbability. Itistheprobabilitythattwoeventsx andy happenatthesame 1 1 time,denotedasPr(x ,y ). Asaspecialcase,thejointprobabilitywillbedecomposed 1 1 intotheproductoftheprobabilitiesofx andy ,ifx andy areindependentofeach 1 1 1 1 other. Pr(x ,y )=Pr(x )Pr(y ) (1.29) 1 1 1 1 • Conditional Probability. It is the probability that x happens in the presence of y 1 1 happening,denotedasPr(x |y ). Pr(x |y )canbedefinedas: 1 1 1 1 Pr(x ,y ) 1 1 Pr(x |y ) = (1.30) 1 1 Pr(y ) 1 • MarginalProbability. Itisanotherwaytodefinetheprobabilityofasinglevariable. Giventhejointprobabilityontwovariables,themarginalprobabilitydefinesthat (cid:88) Pr(x ) = Pr(x ,y ) (1.31) 1 1 j yj∈Y whereY istheeventspaceofy . Eq(1.31)saysthatPr(x )isunconditionedonY. j 1 Another note on joint probability. In some cases, one would like to use conditional probabilitiestorepresentajointprobability. Tothisend,onecanrewritethejointprobability bytheproductruleorthechainrule,likethis Pr(x ,y )=Pr(x |y )Pr(y ) (1.32) 1 1 1 1 1 Sofar,wehavedefinedseveralkindsofprobabilityondiscretevariables. Forcontinuous variables,wedonothavea“probability”atacertainpoint. Instead,wehaveadensityforthat point. More formally, given a continuous variable x, a probability density of x is written as Pr(x). Suppose x∈R. The probability of x lying in the interval [a,b] is defined via an integral: (cid:90) b Pr(x∈[a,b]) = Pr(x)dx (1.33) a 24 Chapter 1. Foundations of Machine Learning Obviously,wehave (cid:90) +∞ Pr(x)dx = 1 (1.34) −∞ Forotherproperties,suchasjointprobabilityandconditionalprobability,theformsfor continuousvariablesarealmostthesameasthosefordiscretevariables. Wejustneedtoreplace thesumsintheformulaswiththeintegrals. 2. Distribution and Expectation Aprobabilitydistribution(ordistributionforshort)istheprobabilitiesofdifferentvalues for a variable. It is defined by probability functions (for discrete variables) or probability densityfunctions(forcontinuousvariables). Forexample,auniformdistributiononadiscrete variable x that chooses values from {x ,x } can be described as Pr(x ) = 1/2 because 1 2 i Pr(x )=Pr(x )andPr(x )+Pr(x )=1;Auniformdistributiononacontinuousvariable 1 2 1 2 in[−2,2]canbedescribedasPr(x)=1/4becausePr(x)isaconstantforanyx∈[−2,2]and (cid:82) 2 Pr(x)dx=1. Statisticians have developed many distributions for describing the world −2 we are living in, such as binomial distribution, Bernoulli distribution and Gaussian/normal distribution. Onecanfinddetailsofthesedistributionsinmosttextbooksonstatistics[McClave andSincich,2006;Freedmanetal.,2007]. Fordescribingpropertiesofavariable,apopularmeansistocomputetheexpectedvalue orexpectationofthevariable. Letxbeadiscretevariablethattakesvaluesfrom{x ,...,x }, 1 n andPr(x)beadistributiononx. Theexpectedvalueofxisdefinedtobe n (cid:88) E (x) = x ·Pr(x ) (1.35) x∼Pr(x) i i i=1 wherethesubscriptx∼Pr(x)indicatesthatxfollowsthedistributionPr(x). Inmanycases, we can drop the subscript and rewrite it as E(x). E(x) is essentially the weighted average valueofxunderthedistributionPr(x). Itisameasureofcentraltendency,andissometimes calledthemeanofavariable(denotedasµ). Then, we can define the variance of a variable as the squared variation of the variable fromthemeanvalue,likethis Var(x) = E[(x−E(x))2] (1.36) Informally,itdescribeshowfarthevaluesarefromthemean. Var(x)isusuallywrittenasσ2, whereσ iscalledstandarddeviation. Foracontinuousvariablex∈R,theexpectedvalueisdefinedas: (cid:90) +∞ E(x) = x·Pr(x)dx (1.37) −∞ wherePr(x)isaprobabilitydensityfunction. Forcomputingthevarianceofx,wejustreuse 1.1 Math Basics 25 Eq. (1.36). 3. Entropy Entropy is one of the most important tools of describing random variables and processes [Shannon, 1948b]. It is a measure of expected surprise. The more deterministic the events occur, the less surprise and less information there will be. For simplicity, we restrict the discussionondiscretevariableshere2. LetxbeavariableandPr(x)beadistributiononx. Theentropyiswrittenas: n (cid:88) H(x) = − Pr(x )·log Pr(x ) (1.38) i b i i=1 wherebisthebaseofthelogarithmfunction. Thevalueofbistypicallysetto2,10ande. Inadditiontoobtainingtheentropyofasingledistribution,wecandeterminethesimilarity oftwodistributionsfromtheentropypointofview. Supposep(x)andq(x)aredistributions onx. Therelativeentropyofpwithrespecttoq isdefinedtobe: n (cid:88) p(x i ) D (p||q) = p(x )·log (1.39) b i b q(x ) i i=1 We can treat p(x ) as a weight to the log likelihood ratio log p(xi) . Hence, D (p||q) is a i b q(xi) b weighted sum of the likelihood ratios over all possible values. A smaller value D (p||q) b indicates that distributions p and q are closer. For example, p and q will be",
    "a weight to the log likelihood ratio log p(xi) . Hence, D (p||q) is a i b q(xi) b weighted sum of the likelihood ratios over all possible values. A smaller value D (p||q) b indicates that distributions p and q are closer. For example, p and q will be identical if D (p||q)=0. TherelativeentropyisalsocalledtheKullback-Leibler(KL)divergence. Note b thattherelativeentropyisasymmetric,i.e.,wecannotguaranteeD (p||q)=D (q||p). b b Anotherconceptthatispopularinmachinelearningiscross-entropy. Itisameasureof theinformation(intermsofthetotalnumberofbits)thatweneedtotransittheeventsfroma sourceinonedistributionwithanotherdistribution. Moreformally,wewritethecross-entropy ofthedistributionpwiththedistributionq asH (p,q). Itisdefinedtobe: cross n (cid:88) H (p,q) = p(x )·log q(x ) (1.40) cross i 2 i i=1 Likerelativeentropy,cross-entropyisasymmetrictoo. Bothrelativeentropyandcross-entropy arewidelyusedindesigningtheobjectiveoflearningNLPsystemsalthoughtheyaredifferent quantities. Thedifferenceliesinthatrelativeentropycalculatestheaveragenumberofbits whenreplacingpwithq,whilecross-entropycalculatesthetotalnumberofbitsinthesame process. 2Forcontinuousvariables,wehavesimilarcalculations. 26 Chapter 1. Foundations of Machine Learning 1.2 Designing a Text Classifier Classificationisoneofthemostcommonproblemsinmachinelearning. Itaimsatautomat- ically categorizing something into a set of classes. These classes are called labels, or tags, or categories. In general, a program of classification is called a classifier or classification system. Thereareavastnumberofpracticalapplicationsofclassifiers. Asimpleexampleis spamfilteringinthatoneneedstolabelanemailas“spam”or“not-spam”. Morechalleng- ingexamplesincludeclassifyingcomputedtomographyimagesoforgansinto“normal”or “not-normal”,determiningwhetherapieceofChinesetextiswrittenbynativespeakersornot, labelingapatentapplicationwithasetofIPCcodesitbelongsto,andsoon. Manymachinelearningtheoriesandalgorithmsaremodeledandtestedonclassification tasks. Followingthisconvention,weconsidertextclassificationasanexampletogetstarted. Assumethatwehaveacorpuslikethis. Text Label Thegamewaswonderful. Not-food I’vetriedmybesttorecreateitinmykitchen. Ittastesheavenly. Food Forcenturiesseaweedwasconsideredafoodfornormalpeople. Food Haveyoufinishedyourcodingworktoday? Not-Food Iwaswonderinghowyoucouldmissthebus. Not-Food Ilikefruitbecauseitisgoodforhealth. Food Naturallanguageprocessingresearchisamazing. Not-Food ... ... Eachlineofthecorpusisatupleofapieceoftext(wesimplycallitadocument)anda labelthatindicateswhetherthetextisaboutfoodornot. Wecallsuchtuplessamples,ormore preciselylabeledsamples. Labeledsamplesareessentiallyquestion-answerpairsalthough theydonotstrictlyfollowthegeneralformsofquestionsandanswers. Forexample,inthe samples presented above, one can take a document as a question and take its label as the answer. Inthenextfewchapters,wewillshowthatsuchaformofdescribingmachinelearning problemsisgeneralandfitsmostproblemsinNLP. Next, let us assume that we have a classifier that learns from those samples the way of labeling documents. The classifier is then used to label every new document as “Food” or “Not-Food”. Forexample,forthetext FruitisnotmyfavoritebutIcanenjoyit. theclassifierwouldcategorizeitas“Food”. However, text classification, though seems simple on the surface, is much more than classifying or sorting unlabeled samples into classes. It presents a wide variety of issues, especiallywhenconsideringtheambiguitiesandrichnessoflanguage. Modernclassifiersare not a system comprising a set of hand-crafted rules. They instead model the classification probleminaprobabilisticmanner,makingitpossibletolearntheabilityofclassificationfrom large-scalelabeleddata. 1.2 Designing a Text Classifier 27 1.2.1 Problem Statement Letxbeadocumentandcbealabel. Hereweassumeaprobabilisticclassifierwhichwould estimatehowlikelywechoosecasthelabelofx,denotedasPr(c|x). Pr(c|x)isingenerala classificationmodel. Itdescribesadistributionoverthesetofallpossiblelabels,satisfying (cid:88) Pr(c|x) = 1 (1.41) c∈C whereC isthelabelset. Foranydocument,wechoosethemostprobablelabelasoutputvia theclassificationmodel,likethis cˆ = argmaxPr(c|x) (1.42) c∈C wherecˆisthe“best”labelpredictedbythemodel. argmaxistheabbreviationofthearguments ofthemaxima. Itreturnsthevalueoftheargumentthatmaximizessomefunction. Eq. (1.42)isthefundamentalequationofclassification. Itimpliesthreeproblems • Themodelingproblem. Pr(c|x)isacomputationalchallengebecauseitisnotobvious how to obtain the value of Pr(c|x) for each pair of x and c. To make an adequate model,onemayneedtorepresentxandcinsomewaythatiseasytouse,andmayneed to develop some mathematical form connecting x and c together with the algorithms necessarytocomputetheform. • Thelearningproblem. Fromastatisticallearningpointofview,thegeneralformof Pr(c|x)representsarangeofmodelsconfiguredwithdifferentvariablesorparameters. Thesemodelsareessentiallyofthesameformbutwouldbehavedifferentlyifwechoose differentvaluesofthoseparameters. Thus,weneedtochoosea“good”modelamong them. Thisistypicallyaddressedbyoptimizingtheparametersonlabeleddatabysome criterion. • The prediction problem. We are addressing a binary classification problem here. Predictingdocumentclassisthustrivialaswejustneedtodeterminewhichclassismore probablethantheother. However,onecanhardlyimaginehowdifficulttheprediction problemisintherealworld,especiallywhenpredictingtreeorgraph-likestructuresand othernon-linearstructures3. FormanyNLPproblems,predictionneedseffectiveand efficientsearchalgorithms. These problems are general and cover many machine learning and natural language processingtasks. Binaryclassification,thoughisoneofthesimplestcases,canfullycomplete thegoalofgettingfamiliarwithmachinelearning. Ontheotherhand,classificationhasseveral variants. Herearetwoexamples. • Multi-classclassification. Itisanupdatedversionofbinaryclassification. Inmulti-class 3Predictingtreesorotherstructuresisnotrecognizedasastandardsub-problemofclassification.Itistypically referred to as structure prediction. We will show in the later sections that both classification and structure predictioncanshareasimilarmachinelearningparadigm. 28 Chapter 1. Foundations of Machine Learning classification,oneneedstoclassifysamplesintooneofthreeormoreclasses. • Multi-labelclassification. Thismightbeconfusingbecausemulti-classclassification and multi-label classification seem to be the same thing. By conventional use of the terms,multi-labelclassificationisreferredtoasassigningmultiplelabelstoasample. Bycontrast,theproblempresentedinEq. (1.42)isasingle-labelclassificationproblem. Classification would be more interesting and challenging if we extend it to the case of dealingwithhierarchicaldata. Forexample,forbiologicalandpatentdata,someclassescan begroupedintoasuper-class. Thismakesahierarchyoftheclassesandrequiresahierarchical classificationschema. 1.2.2 Documents as Feature Vectors The first problem we confront in designing text classification models is how to represent a document. Treatingxasastringissimplynotagoodsolution. Onemaywantarepresentation bywhichahumanbeingcanunderstandthetext. Forexample,wecanparseeachsentencein adocumentintoasyntaxtreeandusetreesasatextrepresentation. This,however,requires effortsfordevelopingadditionalNLPtools(suchassyntacticparsers). Representation, of course, is a fundamental issue in NLP. We skip here those diverse, state-of-the-artmodels,butpresentasimpleandeffectivemodel—thebag-of-words(BOW) model.",
    "of dealingwithhierarchicaldata. Forexample,forbiologicalandpatentdata,someclassescan begroupedintoasuper-class. Thismakesahierarchyoftheclassesandrequiresahierarchical classificationschema. 1.2.2 Documents as Feature Vectors The first problem we confront in designing text classification models is how to represent a document. Treatingxasastringissimplynotagoodsolution. Onemaywantarepresentation bywhichahumanbeingcanunderstandthetext. Forexample,wecanparseeachsentencein adocumentintoasyntaxtreeandusetreesasatextrepresentation. This,however,requires effortsfordevelopingadditionalNLPtools(suchassyntacticparsers). Representation, of course, is a fundamental issue in NLP. We skip here those diverse, state-of-the-artmodels,butpresentasimpleandeffectivemodel—thebag-of-words(BOW) model. The bag-of-words model is a feature-based model of representing documents. In machinelearning,afeatureisapropertyofasample. Onecandefineafeaturenotonlyas someconcreteattribute,suchasanameandagender,butalsoasaquantitythatiscountable formachinelearningsystems,suchasarealnumber. Inthebag-of-wordsmodel,afeaturecorrespondstotheoccurrencetimesofaword. LetV beavocabulary. Adocumentcanberepresentedasa|V|-dimensionalfeaturevector. Each dimensiondescribesawordcountfeature. Itcountstheoccurrenceofthei-thwordofV inthe document. Moreformally,letxbeafeaturevector. Thei-thentryofxisdefinedas: x = count(V ) (1.43) i i wherecount(·)isacountingfunction. Consider,forexample,thefollowinglinesoftext4. AsIwenttoBonner Imetapig Withoutawig, Uponmywordandhonor. 4ThetextisfromMotherGooserhymes. 1.2 Designing a Text Classifier 29 Wethenhaveavocabularyextractedfromthecorpus5,likethis V = {“a”, “and”, “as”, “Bonner”, “honor”, “I”, “met”, “word”, “my”, “pig”, “to”, “upon”, “went”, “wig”, “without”, “,”, “.”} Eachlineofthetextcanbeseenasadocumentandrepresentedasafeaturevector. See belowforthefeaturevectorsgeneratedbyusingthebag-of-wordsmodel. a dna sa rennoB ronoh I tem drow ym gip ot nopu tnew giw tuohtiw , . AsIwenttoBonner [ 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 ] Imetapig [ 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 ] Withoutawig, [ 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 ] Uponmywordandhonor. [ 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 ] Thebag-of-wordsmodeldefinesavectorspace6. Inthisspace,thesimilarityoftwovectors ismeasuredinsomewaylikedot-product. Ithelpswhenonewantstoestablishtherelationship betweendocuments—twodocumentswithmoreoverlappingwordsaremoresimilar. This intuitivepicturehasguidedmanypeoplewhenbuildingclassificationsystems. The beauty of the bag-of-words model comes from its simple form in that any word is independent of other words. The independent assumption makes it possible to encode a document with almost infinite word relations as a countable, feasibly sized vector. On the otherhand,representingadocumentasafeaturevectorofwordcountsisnottheonlyoption. Asanimprovement,onemighttakemorecontextintoaccount,and/ordesignmorepowerful features. Thisleadstoanactivelineofresearchontextrepresentationmethods,rangingfrom heuristics-basedmethodstorepresentationlearningmethods. Wewillseeafewoftheminthe subsequentchapters. 1.2.3 Linear Classifiers Linearclassifierisoneofthesimplestclassificationmodels. Supposewetakeafeaturevector (cid:104) (cid:105) (cid:104) (cid:105) x= x ... x as input, and take a weight vector w= w ... w and a scalar b as 1 n 1 n parameters. Alinearfunctionhasaformlikethis s(x,w,b) = w·x+b = w x +w x +...+w x +b (1.44) 1 1 2 2 n n (cid:104) (cid:105) wherebisabiasterm. Thedotproductx·wisalinearcombinationof x ... x where 1 n eachx isweightedbyw . Foramorecondensedformulation,wecandefineanewinputvector i i 5Weremovedthecaseofthewordatthebeginningofeachline. 6Avectorspaceshouldbeclosedundervectoradditionandscalarmultiplication. 30 Chapter 1. Foundations of Machine Learning (cid:104) (cid:105) (cid:104) (cid:105) x′= x ... x 1 andanewweightvectorw′= w ... w b . WethenrewriteEq. 1 n 1 n (1.44)as: s(x′,w′) = s(x,w,b) = w′·x′ (1.45) Inthefollowing,wedropthebiastermbforsimplicityanduses(x,w)todenotealinear function. Forclassification,alinearfunctionisusedtodescribeclassmembership. Eachclass isassignedascorebythefunctionequippedwithauniqueweightvector. Consideragainthe binaryclassificationasanexample. Letc andc betwoclasses. Wecandefinetwoweight a b vectorsw andw sothatthefunctioncandiscriminatebetweenc andc . a b a b Forprediction,wecaninferaclassbasedons(x,w). Toachievethis,activationfunctions ψ(·)areingeneralusedformappingthevalueofs(x,w)toaclass. Forexample,forbinary classification,wecandefineanactivationfunctionlikethis, (cid:40) c x>0 a ψ(x) = (1.46) c otherwise b Then,wemakeapredictionby (cid:40) c s(x,w )−s(x,w )>0 a a b ψ(s(x,w )−s(x,w )) = (1.47) a b c otherwise b As s(x,w )−s(x,w )=s(x,w −w ), the final prediction function is ψ(s(x,w −w )). a b a b a b We call it a discriminant function. Note that s(x,w −w ) is linear. So this is a linear a b discriminantfunction. Adiscriminantfunctionassignsaninputvectorxdirectlytoaclassbutitdoesnotdescribe howlikelyaclasswouldappeargivenx. Thereareotheractivationfunctionsforgeneratinga desirableoutput. Forexample,wemaywantaprobability-likeoutput(seeEq. (1.42)),and thusdefineψ(·)asanormalizedfunction7. Then,theclassificationprobabilitiesaregivenby theequation (cid:104) (cid:105) (cid:16)(cid:104) (cid:105)(cid:17) Pr(c |x) Pr(c |x) = ψ s(x,w ) s(x,w ) a b a b (cid:104) (cid:105) = s(x,wa) s(x,w b ) (1.48) s(x,wa)+s(x,w b",
    "s(x,w −w ) is linear. So this is a linear a b discriminantfunction. Adiscriminantfunctionassignsaninputvectorxdirectlytoaclassbutitdoesnotdescribe howlikelyaclasswouldappeargivenx. Thereareotheractivationfunctionsforgeneratinga desirableoutput. Forexample,wemaywantaprobability-likeoutput(seeEq. (1.42)),and thusdefineψ(·)asanormalizedfunction7. Then,theclassificationprobabilitiesaregivenby theequation (cid:104) (cid:105) (cid:16)(cid:104) (cid:105)(cid:17) Pr(c |x) Pr(c |x) = ψ s(x,w ) s(x,w ) a b a b (cid:104) (cid:105) = s(x,wa) s(x,w b ) (1.48) s(x,wa)+s(x,w b ) s(x,wa)+s(x,w b ) where ψ(·) is a vector function8. It normalizes the entries of the input vector by the sum of these entries. The decision rule is simple: we predict c if Pr(c |x) > Pr(c |x), and a a b c otherwise. Since s(x,wa) and s(x,w b ) share the same denominator, the b s(x,wa)+s(x,w b ) s(x,wa)+s(x,w b ) predictioncanalsobemadebycomparingthenumerators,i.e.,wearedoingthesamething as that in Eq. (1.47). In subsequent chapters, we will show that the trick of transforming 7Anormalizedfunctionisafunctionwhoseintegraloveritsdomainisequalto1. 8Avectorfunctionreadsavectorandreturnsanewvector. 1.2 Designing a Text Classifier 31 8 7 6 5 4 3 2 1 0 0 1 2 3 4 5 6 7 8 x 1 x 2 Food Not-food 2 Hyperplane3 Hyperplane 1 e n a pl er p y H Figure1.1: Datapointsandseparatinghyperplanesintwodimensions. Therearetwoclasses of data points (food and non-food). Both hyperplanes 1 and 2 separate the space into two sub-spaceswherethetwoclassesofdatapointsareisolated. Inthissense,theproblemhereis linearlyseparable. Ontheotherhand,hyperplane3failstoseparatethetwoclasses,thatis,the datapointsinthesameclassareclassifiedintotwodifferentclasses. comparingprobabilitiestocomparingreal-valuedscoresisfrequentlyusedforaddressingNLP problems. Foreaseofunderstanding,onecanseealinearclassificationmodelasahyperplane(or decision surface, or decision boundary) that separates data points into different groups. Figure1.1showsexamplehyperplanesina2Dspacewhereeachxisadatapoint. Hyperplanes 1and2successfullyseparatethedatapointsintothecorrectclasses,whilehyperplane3fails todoso. Inthissense,thetaskofclassificationistofindhyperplanesthatmakethecorrect separationofdatapoints. Itisworthyofnotethatlinearityisthebasisofmanyclassifiersalthoughmostofthem do not exist in the same form as Eqs. (1.44 - 1.45). A linear model can be nearly a perfect solutioniftheproblemislinearlyseparable9. Evenfornon-linearlyseparableproblems,linear modelscanbemarriedtoothermodelswithanon-linearseparationability. Agoodexample isthatonecanachievenon-linearclassificationbymarryingalinearmodelwithanon-linear activationfunction. 9Linearseparabilitychecksifthereisawaythatweputahyperplanetoseparateagroupofdatapointsfromthe remainingdatapoints.Forexample,theproblemshowninFigure1.1islinearlyseparable. 32 Chapter 1. Foundations of Machine Learning 1.2.4 Generative vs Discriminative There are two ways, though not restricted to linear models, to make use of linearity in classification — generative models and discriminative models. While most statistical classifiersareofthemodelingvariety,theychoosethebackbonedesignfromeitherorbothof thesetwotypesofmodels. 1. Generative Models A goal of classification is to learn Pr(c|x). Generative models do not explicitly model this conditionalprobability. Instead,theymodelthejointprobabilityPr(x,c),andusetheBayes’ ruletocomputePr(c|x). Thisisgivenbythefollowingequation. Pr(x,c) Pr(c|x) = Pr(x) Pr(c)Pr(x|c) = (1.49) Pr(x) wherePr(x,c)isrewrittenasPr(x|c)Pr(c). Foranoptimalclasscˆ,wechooseaclasscby maximizingPr(c|x)(seeEq.(1.42)). Sincethedenominatorxisaconstantforanyc,wejust needtomaximizethenumerator. Then,werewriteEq.(1.42)intheform cˆ = argmaxPr(c|x) c∈C Pr(c)Pr(x|c) = argmax Pr(x) c∈C = argmaxPr(c)Pr(x|c) (1.50) c∈C wherePr(c)isthepriorofc,andPr(x|c)istheconditionalprobabilityoftheinputdocument vectorxgivenc. ComputingPr(c)iseasy. Forexample,themaximumlikelihoodestimation (MLE)definesPr(c)asarelativefrequency: count(c) Pr(c) = (1.51) (cid:80) count(c′) c′∈C wherecount(c)countstheoccurrencesofcinacorpus. But computing Pr(x|c) is non-trivial as data sparseness prevents us from accurately estimatingtheprobabilityofahigh-dimensionaldocumentvector. Recallthatthebag-of-words modeldefinesx asthewordfrequencyofV inthedocument. Weassumeherethatthefeature i i (cid:104) (cid:105) vectorx= x ... x isgeneratedbyamultinomial(p (c),p (c),...,p (c)),wherep (c) 1 n 1 2 n i istheprobabilityofV occurringgivenc. BasedonMLE,wecanestimatep (c)bytherelative i i frequencyestimation: count(V ,c) i p (c) = (1.52) i (cid:80) count(V ,c) 1≤i′≤n i′ 1.2 Designing a Text Classifier 33 where count(V ,c) is the number of occurrences of V in all documents labeled as c. Then, i i Pr(x|c)isgivenby Pr(x|c) = (cid:0)(cid:80)n i=1 x i (cid:1) ! · (cid:89) n p (c)xi (1.53) i x !·x !·...·x ! 1 2 n i=1 SubstitutingEq. (1.53)intoEq. (1.50),wehave cˆ = argmax Pr(c)· (cid:0)(cid:80)n i=1 x i (cid:1) ! · (cid:89) n p (c)xi (1.54) i x !·x !·...·x ! c∈C 1 2 n i=1 (cid:0) (cid:1) Note that (cid:80)n i=1 xi ! is independent of any c. We drop it in argmax, and rewrite the x1!·x2!·...·xn! right-handsideoftheequationinlogscale: n (cid:88) cˆ = argmax log(Pr(c))+",
    "i=1 x i (cid:1) ! · (cid:89) n p (c)xi (1.54) i x !·x !·...·x ! c∈C 1 2 n i=1 (cid:0) (cid:1) Note that (cid:80)n i=1 xi ! is independent of any c. We drop it in argmax, and rewrite the x1!·x2!·...·xn! right-handsideoftheequationinlogscale: n (cid:88) cˆ = argmax log(Pr(c))+ x ·log(p (c)) (1.55) i i c∈C i=1 Obviously,thisisalinearmodel. Itdefinesthefeaturevectorandweightvectorasbelow (cid:104) (cid:105) x = 1 x ... x (1.56) 1 n (cid:104) (cid:105) w = log(Pr(c)) log(p (c)) ... log(p (c) (1.57) 1 n Suchaformissometimescalledalog-linearmodel,asthelinearitycomesfromtransforming theoriginalproblemviaalogisticfunction. Ingeneral,Eq(1.55)iscalledthemultinomialnaiveBayesapproach. Thereare,ofcourse, the naive Bayes variants for other types of feature vectors. For example, for binary value featurevectors,onecanassumeaBernoullidistributiononeachentryofavectoranddesign a Bernoulli naive Bayes classifier; for vectors with continuous features, one can assume a GaussiandistributionovercontinuousdataanddesignaGaussiannaiveBayesclassifier. 2. Discriminative Models Pr(x,c) Pr(c)Pr(x|c) The model defined by Pr(c|x)= = (see Eq. (1.49)) is called generative Pr(x) Pr(x) becauseitassumessomewayofgeneratingdataxgivenlabelc. TheideaistousePr(x,c)as apivottocomputePr(c|x). AsanalternativepossibilityformodelingPr(c|x),discriminative modelsdonottrytomodelthedistributionofxbutestimatePr(c|x)directly. Anexampleis logisticregression. Forbinarytextclassification,anaiveBayesclassifierpredictslabelc for a agivendocumentxonlyifthefollowingfunctionispositive: Pr(c |x) a f (x) = log (1.58) a Pr(c |x) b 34 Chapter 1. Foundations of Machine Learning Onecanassumethatthisquantityfollowsalinearmodel: Pr(c |x) a log = w·x (1.59) Pr(c |x) b SincePr(c |x)=1−Pr(c |x),wehave b a 1 Pr(c |x) = (1.60) a 1+exp(−w·x) This is a logistic function, or more precisely a Sigmoid function. With such a model, we predictc onlyifw·xispositive. Eq. (1.60)isalsocalledthelogisticregressionclassifier. It a issimplyadiscriminativeanalogofthenaiveBayesclassifier. Anadvantageofdiscriminativemodelsisthattheyofferflexibilityinviewingclassification (orothermachinelearningproblems)fromdifferentangles. Whilegenerativemodelstryto estimatethedatadistributionofx,discriminativemodelstrytofindagoodboundarybetween classes. Discriminative models, therefore, care more about which class is prioritized over another given x, or in possibility language which class is more likely to appear, instead of makingassumptionsonindividualdatapoints. Thismakesitpossibletolearnaclassifierby minimizingthenumberofsomeerrors,notnecessarilyguaranteeingthemaximumlikelihood onthosedatapoints. Thisapproachisgenerallycallederror-drivenlearning. Therearemanywaystodefineerrors. Likegenerativemodels,onecanlearnadiscrim- inative model by fitting parameters w to maximize the likelihood on the training data. Let {(x(1),c(1)),...,(x(K),c(K))} be a set of labeled documents, where x(k) is a document and c(k) isthecorrespondingclass. Thebestparametervectorwˆ isgivenbytheequation K (cid:88) wˆ = argmax logPr(c(k)|x(k)) (1.61) w k=1 TakingEq. (1.60),theprocesscanbeseenasmaximizingthelikelihood K (cid:88) wˆ = argmax logδ(c ,c(k))Pr(c |x)+logδ(c ,c(k))Pr(c |x) a a b b w k=1 K (cid:88) 1 = argmax logδ(c ,c(k)) + a 1+exp(−w·x(k)) w k=1 (cid:16) 1 (cid:17) logδ(c ,c(k)) 1− (1.62) b 1+exp(−w·x(k)) where δ(·,·) is an indicator function that returns 1 if the two arguments are equal, and 0 otherwise. Alternatively,wecantrainthemodelbyminimizing0-1errors,thatis,wecountanerror 1.2 Designing a Text Classifier 35 whentheoutputisnotthecorrectlabel. Thisprocesscanbeformulatedas K (cid:88) wˆ = argmin δ(cˆ(k),c(k)) (1.63) w k=1 wherecˆ(i) isthepredictionmadebytheclassifier. The0-1errorcanbefurtherextendedby takingtheposteriorintoaccount: K (cid:88) wˆ = argmin Pr(cˆ(k)|x(k))·δ(cˆ(k),c(k)) (1.64) w k=1 Thetrainingobjectiveplaysanimportantroleindiscriminativemodels. Thistopic,however, is so broad and beyond the scope of this section. We will present some in Section 1.3.4. As another bonus, discriminative models do not restrict features to forming a probabilistic generative story. In a broader sense, x could be any feature vector that is designed by researchers and engineers. One does not even need to guarantee the probabilistic meaning for these features. For example, let g(x) be the output of another system, say some scores. FollowingEq. (1.60),anewbinaryclassificationmodelcanbedesignedinalogisticregression manner: 1 Pr(c |x) = (1.65) a 1+exp(−w·g(x)) Forlearningg(x),onecaneitherpre-trainitonsomeadditionaldata,ortrainitjointlywithw (i.e.,theparametersoftheupper-levelmodelPr(c |x)). a 1.2.5 OOV Words and Smoothing Theout-of-vocabulary(OOV)problemoccurswhensomeofthewordsofadocumentarenot foundinthevocabulary. OOVwordsarecommoninNLPbecausenewwordsarealwaysthere nomatterhowmuchtextwehaveseen. Figure1.2givestwocurvestoillustratethisproblem. AsshowninFigure1.2(a),newwordscontinuouslyappearwhenmoredataisavailable. When wefixthedatathatisusedfortestingthecoverageofavocabulary,OOVwordsremainevenif wehaveanextremelylargevocabulary(Figure1.2(b)). Forpracticalsystems,OOVwordsarecommonbecausethevocabularyisoftenrestricted toa“small”numberofentries. Astandardmethodistokeepthetop-nmostfrequentwords and discard the rest. In this case, OOV words are treated as unknown words. For example, a new symbol <unk> is introduced",
    "1 Pr(c |x) = (1.65) a 1+exp(−w·g(x)) Forlearningg(x),onecaneitherpre-trainitonsomeadditionaldata,ortrainitjointlywithw (i.e.,theparametersoftheupper-levelmodelPr(c |x)). a 1.2.5 OOV Words and Smoothing Theout-of-vocabulary(OOV)problemoccurswhensomeofthewordsofadocumentarenot foundinthevocabulary. OOVwordsarecommoninNLPbecausenewwordsarealwaysthere nomatterhowmuchtextwehaveseen. Figure1.2givestwocurvestoillustratethisproblem. AsshowninFigure1.2(a),newwordscontinuouslyappearwhenmoredataisavailable. When wefixthedatathatisusedfortestingthecoverageofavocabulary,OOVwordsremainevenif wehaveanextremelylargevocabulary(Figure1.2(b)). Forpracticalsystems,OOVwordsarecommonbecausethevocabularyisoftenrestricted toa“small”numberofentries. Astandardmethodistokeepthetop-nmostfrequentwords and discard the rest. In this case, OOV words are treated as unknown words. For example, a new symbol <unk> is introduced into the vocabulary so that all OOV words are denoted as<unk>. Amoreaggressiveideaistobuildanopen-vocabularysystemthatacceptsevery possibleword,butitwouldrequiremoresophisticatedalgorithmsandprobablyatask-specific designofdatastructures. The<unk>trickisstillthedefactostandardforthedevelopmentof currentNLPsystems. Forwordsthatarealreadyinthevocabulary,therearealsounseenwordsthatareabsent intheparameterestimationphasebutappearinanewdocument. Anaiveimplementationof 36 Chapter 1. Foundations of Machine Learning 20 16 12 8 4 0 200 400 600 800 1,0001,200 Datasize ezisyralubacoV (·104) 100 80 60 40 20 (·105) 0 4 8 12 16 20 Vocabularysize egatnecrepVOO (%) (·104) Figure 1.2: Data size (in number of words), vocabulary size and percentage of OOV. The statisticsarecollectedontheEnglishdataprovidedintheWMT21Zh-Entranslationtask. The moredataweuse,thelargervocabularywehave. Theincreaseinvocabularysizecontinues evenifwebuildthevocabularyondataofmorethan100millionwords. However,theslope ofthecurvetendstobesmallerasmoredataisinvolved. Interestingly,theOOVpercentage convergestoacertainlevelasthevocabularysizeincreases,indicatingthatnewwordsalways occurnomatterhowmanywordswehaveobserved. themodeldescribedintheprevioussectionwouldbetoughwhendealingwithunseenwords. Forexample,Eq. (1.52)willsimplygiveazeroprobabilityifthewordV doesnotoccurin i anytrainingexamplelabeledwithc. Inconsequence, foranewexamplecontainingV , Eq. i (1.53)willassignitazeroprobability. Thisresultisobviouslyunreasonable. However,we shouldnotsimplyattributeittothemodeldesignitself. Instead,aprimaryreasonforthisis theinsufficientdatausedforparameterestimation. ThisisalsoexplainedbyZipf’sLaw: a smallnumberofwordsoccurquiteoften,whilealargenumberofwordsoccurrarely. However,wecannotsupposethatwealwayshaveaccesstosomedatawhereeveryword occurs sufficiently. Alternatively, one couldadopt smoothing techniquesto redistribute the probability over the vocabulary. Consider Eq. (1.52) as an example. We can add a small numbertoeachV ,andrewritetheequationas: i count(V ,c)+α i p (c) = (1.66) i (cid:80) count(V ,c)+n·α 1≤i′≤n i′ whereαisthedefaultcountthatweassigntoeachword. Inthisway,p (c)givesanon-zero i probability even if count(V ,c)=0. Eq. (1.66) is doing something like subtracting word i counts from high-frequency words and reassigning the subtracted counts to low-frequency words. This method is called additive smoothing or add-α smoothing. It is one of the simplestsmoothingmethods. Forothermethods,wereferthereadertolanguagemodeling paperswheresmoothingisinheavyuse[ChenandGoodman,1999]. 1.3 General Problems 37 1.3 General Problems Buildingasimpletextclassifierisagoodstartbutnotenoughforsolvingcomplicated,diverse real-worldproblems. Foramoregeneralpictureofhowmodernmachinelearningsystems work,wenowdiscusssomeproblemsthatareimportantwhendesigningsuchsystems. 1.3.1 Supervised and Unsupervised Models Supervised learning deals with labeled samples, by which we mean that an input x is associatedwithanoutputy. Givenasetofinput-outputpairs{(x(1),y(1)),...,(x(K),y(K))}, thetaskhereistolearnafunctionf(·)thatmapseachx(k) toy(k): y(k) = f(x(k)) (1.67) Thisprocessiscalledsupervisedbecauselearningf(·)isguidedbythemanuallyannotated answery(k) forx(k). Ingeneral,y(k) iscalledtheground-truthorgold-standardlabelofx(k). Afterthat,whenanewinputx comes,weusethelearnedfunctionf(·)topredicttheoutput. new Wewillsaythatthesupervisedlearningsucceedsifthepredictionf(x )isthesameasthe new ground-truthy . new The vast majority of NLP can be framed as supervised learning problems. Assigning a classtoadocumentisnodoubtoneofthesimplestcases. OtherNLPtasksincludebutarenot limitedtoproducingasequenceoflabels,apieceoftext,asyntaxtree,andagraph. Incontrasttosupervisedlearning,unsupervisedlearningdealswithunlabeledsamples,in otherwords,foreachsample,wehaveaninputxintheabsenceofthecorrectoutputy. Inthis case,weneedanalgorithmthatlearnsfromtheunlabeleddata{x(k)}themappingfunction fromx(k) tosomeoutput. Sincethereisnohumaninterventionontheoutputofthefunction, algorithmsofthiskindneedtodiscoverpatternsin{x(k)}andoptimizethewaywerepresent {x(k)}andfunctionoutputsbysomecriteria. Thesecriteriaaretypicallyinspiredbyhuman priorknowledgesothattheresultingfunctioncouldoutputsomethingthatweexpect. Acommonexampleinunsupervisedlearningiswordclustering. Itgroupsasetofwords byassigningsimilarwordsintothesamecluster10. Basedonsuchacriterion,wecanbiasf(·) towardsoutputtingthesameclusterforsimilarwords. Amoredifficultcaseisunsupervised bilingual dictionary induction. It learns a word-level mapping between two languages withouttheneedofparalleldata. Theproblemisusuallyaddressed,inpart,bymakinguseof theisomorphismofwordrepresentationspacesofdifferentlanguages. A halfway between supervised learning and unsupervised learning is semi-supervised learning. It deals with the case in which some of the input data is labeled and the rest is unlabeled. Thus, it can receive benefits from both supervised and unsupervised learning. For example, in machine translation, we may have a certain amount of parallel data (i.e., labeleddata)andordersofmagnitudelargermonolingualdata(i.e.,unlabeleddata). Often, a base system is learned on the parallel data in a supervised learning fashion. On top of it, improvements can be made from training components of the base system on large-scale 10Itmightbedifficultandambiguoustodetermineiftwowordsaresimilarornot.WeleavethisissuetoChapter 3. 38 Chapter 1. Foundations of Machine Learning monolingualdata. Thisisimplementedbyeithercombiningatranslationmodellearnedon the bilingual data and a language model learned on the target-language monolingual data, orpre-trainingpartsofthetranslationmodelonmonolingualdataandfine-tuningtheentire modelonthebilingualdata. Broadlyspeaking,alllearningalgorithmsneedsupervision. Thissoundsweirdbecause unsupervisedlearningseemstonotbesignaledbyanyground-truthdata. However,froma general learning perspective, we should not restrict ourselves to labeled data for receiving supervisionsignals. Evenforanunsupervisedlearningproblem,westillneedtosupervisethe learningprocessbypriorknowledgeandhiddenpatternsintheinputdata{x(k)}. Inthissense, unsupervisedlearningisnot“learningwithoutsupervision”. Taking “supervision” as a concept in a broader sense, more paradigms can be seen as instancesofmachinelearning,thoughnotnecessarilybelongingtoeithersupervisedlearningor unsupervisedlearning. Anexampleisreinforcementlearning. Itmodelshowasystemmakes asequenceofdecisions. Thisisachievedbyoperatinganagentinanenvironment. Theagent receivesafeedback(orareward)fromtheenvironmentwhenmakingadecision(ortakingan action). Thegoalofreinforcementlearningistolearnadecisionmodelthatmaximizesthe rewardalongthestepstheagenttakes. Therealrewardhereisavailableonlywhentheagent reachessomestate,suchastheendofagame. Assuch,reinforcementlearningcandescribe problemswheretherewardisoveralongerperiod(callitdistantreward). Thisdifferentiates reinforcementlearningsharplyfromstandardsupervisedlearning,traditionallyconcernedwith instantsupervisionsignalsthatareencodedinlabeleddatainadvance. Thischaracteristicfits manyNLPproblems.",
    "we should not restrict ourselves to labeled data for receiving supervisionsignals. Evenforanunsupervisedlearningproblem,westillneedtosupervisethe learningprocessbypriorknowledgeandhiddenpatternsintheinputdata{x(k)}. Inthissense, unsupervisedlearningisnot“learningwithoutsupervision”. Taking “supervision” as a concept in a broader sense, more paradigms can be seen as instancesofmachinelearning,thoughnotnecessarilybelongingtoeithersupervisedlearningor unsupervisedlearning. Anexampleisreinforcementlearning. Itmodelshowasystemmakes asequenceofdecisions. Thisisachievedbyoperatinganagentinanenvironment. Theagent receivesafeedback(orareward)fromtheenvironmentwhenmakingadecision(ortakingan action). Thegoalofreinforcementlearningistolearnadecisionmodelthatmaximizesthe rewardalongthestepstheagenttakes. Therealrewardhereisavailableonlywhentheagent reachessomestate,suchastheendofagame. Assuch,reinforcementlearningcandescribe problemswheretherewardisoveralongerperiod(callitdistantreward). Thisdifferentiates reinforcementlearningsharplyfromstandardsupervisedlearning,traditionallyconcernedwith instantsupervisionsignalsthatareencodedinlabeleddatainadvance. Thischaracteristicfits manyNLPproblems. Forexample,atextgenerationsystemgenerallygeneratesasequenceof words from left to right, but it is hard to determine if a word is properly predicted until the wholetexthasbeengenerated. Anotherexampleisself-supervisedlearning. Itaddressesunsupervisedlearningproblems inasupervisedlearningmanner. Ageneralideaistoframetheunsupervisedlearningtaskasa pretexttaskthatcanbeusedinsolvingtheoriginalproblem. Inthepretexttask,groundtruth canbegeneratedfrominputdata. Thelearningthereforereceivessupervisionfromself-made signalsinsteadofmanuallabels. Self-supervisedlearninghasindeedbeenquitesuccessfulinNLP.Perhapspre-training isoneofthosewhichhavemadethemostincredibleprogress. Inpre-training,onecantrain somemodel(suchasalanguagemodel)viaself-supervisedlearning,andthenapplypartsof themodeltosomedownstreamsystem(suchasasentimentanalysissystem). Itofferstwo advantages. First,theself-supervised,pre-trainedmodelscanbescaledtoahugeamountof data because they require no labeled data. Second, pre-training is general itself and can be applied to a wide range of downstream tasks. In Chapter 7, we will see a few examples of applyingself-supervisedlearningtoNLPmodels. 1.3.2 Inductive Bias Weinformallydescribe(supervised)machinelearningproblemsasaninductivereasoning (orinductiveinference)process: weusespecificobservations(suchaslabeleddocuments) tomakeageneralizedmodel(suchasaclassifier). Forexample,wemayobservethatword 1.3 General Problems 39 cookingfrequentlyoccursinsomedocumentstalkingaboutfood. Wewouldsaythat,basedon inductivereasoning,cookingisanimportantindicatorforallfooddocuments. This“specific- to-general”methodisalsocalledinductionsometimes. Oncewehaveaninducedmodel,we canapplyittodescribenewobservations,asadeductionprocess. Induction is the most widely-used principle in designing learners of modern machine learning systems11. Imagine that there is a hypothesis space (or model space, or learnable function space) consisting of all possible models that we could make. Learning a model is thusthesameasselectingamodelinthehypothesisspacebyinducingfromthegiventraining samples. However,wecannotsimplyassumeanoraclemodelthatworkswellonallunseen samples. Areasonforthisisthatsearchingforthe“best”modelinahugehypothesisspaceis computationallyinfeasible. Therewouldbeaninfinitenumberofdimensionsalongwhichwe candesignmodelsifthehypothesisspaceisunconstrained. Thisproblemisessentiallysome sortofthecurseofdimensionality12. Anotherreasonisthatmanymodelsinthehypothesis spacecanfittrainingsampleswell,butonlysomeofthemcanfitunseensamples. Thereisa riskthatweselecta“weak”modelfortestdataalthoughitis“strong”fortrainingdata. Thisis relevanttooverfitting,aconceptthatwewilldiscusslater. Anaturalsolutionistodefinepriorsonthehypothesisspaceinawaythatallowssome modelstobemorepreferablethanothers. Asimpleexampleisthatwerestrictclassifiersto linearmodels(seeSection1.2.3). Itisdoingsomethinglikeweimposeapriorthatexcludes allnon-linearmodelsfromthehypothesisspace. Suchapriorisgenerallycalledaninductivebias. Inanutshell,aninductivebiasisaset ofassumptionsontheproblem13. Forexample,onecandesignmodelsincertainmathematical forms(i.e.,modelbias);onecanchoosespecificalgorithmsforlearningamodel(i.e,algorithm bias);onecanassumethewayofgeneratingsamples(i.e,samplebias),andsoon14. Inductive biases try to tell in what way we should describe a problem. Better results aregenerallyfavorablewheninductivebiasesmeetwhatreallyhappens. Thisexplainswhy solutionstosomeproblemsprefercertainmodelarchitectures(ormodelbiases). Ofcourse, moreandstrongerinductivebiasescouldmakeiteasiertosolveaproblem. However,inductive biasesarenotalwayshelpful,especiallywhentheyarenotclosetothereality. Letusconsideradicerollinggame. Supposeyouhavea6-sideddice. Beforerollingthe dice,youguessaside(sayanumberfrom1to6). Youwillwinifthedicelandsonthesame sideyouguess. Youareagamblerandtrytowinasmanytimesaspossible. Inyourexperience, arandomguessisthebestchoiceinthisgamebecauseallsidesshouldhaveanequalchance ofappearing(i.e. achanceof1/6). Thisistruewhenyouplayfairdice. However,oneday, 11Notallmachinelearningmethodsshouldfollowaninductionprocessforlearningamodel.Thereareother optionsfordifferenttypesofproblems,includingdeductivereasoning,abductivereasoning,analogicalreasoning (ortransduction),andsoon[Hurley,2011]. 12Thecurseofdimensionalityreferstotheproblemsthatgenerallyappearasthedimensionalityofthehypothesis spaceincreases.Forexample,datasparsenessisacommonproblemthatariseswhenprocessinghigh-dimensional data,andisthusakindofthecurseofdimensionality. 13Amoreformaldefinitioncanbefoundinmachinelearningtextbooks[Mitchell,1997] 14Asanasideitisworthnotingthatthetermbiasisusedinmanydifferentways,andthereareothermeanings forbiasincertaincontexts.Wewillmakeitclearwhenadifferentmeaningisused. 40 Chapter 1. Foundations of Machine Learning weplayedweighteddice,anditwasnoteasytowinasbefore. Youfoundthattheappearance of different sides did not follow a uniform distribution. Then, you assumed a multinomial distribution(becauseyouhadtheexperienceofdevelopingnaiveBayesclassifiers). Beforethe gamestarted,yourolledthedice100times. Youchosethemostfrequentsidefornewgames, andyouwonmore. Inthisexample,youmadeaninitialassumptionthatallsixofthesidesare equallylikelytooccur. Thisisaverystronginductivebiasbecauseyourmodelhas0degrees offreedom. Itseemstobeobviousbutdoesnotworkforweighteddice. Thesecondinductive bias,thoughseemsmorecomplicated,isactuallyaweakerassumption,becauseamultinomial distributiondefinesalargerfamilyofmodelsandgivesroomtofindingappropriatemodels. Ingeneral,allmachinelearningmodelsneedsomesortofinductivebias. Manyofthemare implicitassumptions. Sometimes,weareevennotawarethatwearemakingtheseassumptions becausetheyareso“obvious”and“logical”. Ontheotherhand,iftheassumptioniswrongthen itisharmfultoproblem-solving. Sowestillneedsomeexperiencetoavoideasilyneglected mistakes. 1.3.3 Non-linearity Non-linearityisthenatureofmostreal-worldproblems,whereasitisnoteasytousealinear model to solve a non-linear problem. See Figure 1.3 for examples of varying degrees of classificationdifficulty. InFigure1.3(a),thetwoclassescanbeseparatedbyahyperplane. In thiscase,theproblemislinearlyseparablebecausethedecisionboundarycanberepresented asalinearfunction. Incontrast,inFigure1.3(b),wecannotdrawhyperplanestoperfectly separatethetwoclasses. Instead,weneedsomenon-linearityforbetterseparation,suchas hyperspheres. AmoredifficultcaseisshowninFigure1.3(c)wherethedecisionboundaryis highlycomplex. Althoughthetheoryofnon-linearsystemsstillhasnotbeenfullystudied,thereareseveral methodsthathelpusintroducenon-linearityintomachinelearningsystems. • Featuremappingandkernelmethods. Recallthatalinearclassifiercanbeformulated asafunctionf(w·x), wherew istheweightvector, xisthefeaturevector, andf(·) is the function that returns one class (say c ) if its argument >0 and the other class a (say c ) otherwise. The idea of feature mapping is that we map the feature vector x b into a higher-dimensional space so that the problem is linearly separable in the new (cid:104) (cid:105) space. Forexample,letϕ(·):R2→R4 beamappingfunctionandx= x x bea 1 2 2-dimensionalvector. Weassumethat (cid:104) (cid:105) (cid:104) (cid:105) ϕ( x x ) = x2+x2 x x 1 (1.68) 1 2 1 2 1 2 (cid:104) (cid:105) Bychoosingw= 1 −8 −8 28 ,wegetanewclassifier: (cid:104) (cid:105) (cid:104) (cid:105) (cid:0) (cid:1) f(w·ϕ((x))) = f 1 −8 −8 28 · x2+x2 x x 1 1 2 1 2 = f (cid:0) (x −4)2+(x −4)2−4 (cid:1) (1.69) 1 2 1.3 General Problems 41 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Linearity 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Non-linearity 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Non-linearity (moredifficult) Figure",
    "General Problems 41 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Linearity 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Non-linearity 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Non-linearity (moredifficult) Figure 1.3: Linearity and non-linearity in binary classification. The first problem (left) is linearlyseparablebecausethereexists(atleast)ahyperplanethatperfectlyseparatesthedata points in the two classes. The property of linear separability does not hold in the second problem(middle). Rather,weneedacircle-likedecisionboundary. Thedecisionboundary wouldbemorecomplexifthereareareaswherethetwoclassesofthedatapointsaremixed andmoreorlessindistinguishable(right). It defines a decision boundary (i.e., a hypersphere (x −4)2+(x −4)2 = 4) that 1 2 perfectlyclassifiesthesamplesinFigure1.3(b). Inotherwords,weusealinearmodel on the mapped feature space to create a non-linear model. However, computing the mapping function might be inefficient. This is typically addressed by using kernel methods. Inkernelmethods,thecalculationofvectordot-productinthenewspaceis performedefficientlybyusingakernelfunctionintheoldspace. Thismethodiscalled thekerneltrick. Ithasbeensuccessfullyadoptedinclassificationandothermachine learningmodels,suchassupportvectormachines[CortesandVapnik,1995]. • Non-linearactivationfunctions. Anotherwaytoaddnon-linearityistouseactivation functions. A common method is to stack a non-linear activation function on top of a linearmodel. Forexample,thefunctionf(·)usedintheaboveexampleisitselfanon- linearfunction. Therearemanykindsofnon-linearactivationfunctions. Wecanchoose from them, depending on what form of the output we want. For more sophisticated models,moreactivationfunctionscanbeinsertedintotheintermediatecomputingsteps todevelopamorepowerfulandexpressivemodel. Forexample,adeepneuralnetwork isastackofsub-models(callthemlayers)whereeachsub-modelmayinvolveoneor moreactivationfunctions. • Non-parametricmethods. Non-parametricisatermthatisoriginatedfromstatistics. Innon-parametricstatistics,statisticalinferencesaremadewithoutanyassumptionon underlyingdistributionsofdata. Inmachinelearning,non-parametricmethodsfollow the same idea. They do not assume any mapping function from input to output as Eq. (1.69). Thisdifferentiatesthemfromparametricmethodsthatexplicitlylearna mathematicalformofvariables(orparameters)todescribetheproblem. Anexampleof 42 Chapter 1. Foundations of Machine Learning non-parametricmethodsisk-nearestneighbors. Itmakesapredictionforanewsample basedonthek nearestneighboringsamplesintrainingdata. Notethatnon-parametric doesnotmeanparameter-free. Rather,itmeansthatparameterscanchange. Onanother hand,non-parametricmethodsdonotensureafixedmodel. Theygrowinmodelsize asmoretrainingsamplesareavailable. Asareward,theycanhandlehighlynon-linear problemswhentrainingsamplesaresufficient. Still,non-linearmethodsdonotworkalone. Linearityissurelyanimportantcomponent formostpracticalmachinelearningsystems. Thishastwoflavors. First,morenon-linearityis notalwaysbetter. Wedonotneedtocomplicatethemodelingifalinearmodelisenoughfor solvingtheproblem. Anexampleisthatmoststate-of-the-artmachinelearningmodelsarea combinationoflinearandnon-linearsub-models. ThisisalsoaninstanceofOccam’sRazor— thesimplestsolutionisalmostalwaysthebest. Thesecondflavoristhelinearapproximationof non-linearbehaviors. Linearmodelsareagoodalternativeifthenon-linearityoftheproblemis notobvious. Insuchcases,usinglinearfunctionstoapproximateprecisesolutionsisprobably moreefficientforpracticalpurposes. 1.3.4 Training and Loss Functions Almostallmachinelearningalgorithmsinvolveatrainingstep. Typically,itreferstotheprocess ofestimatingthemappingfunctionandtheassociatedparametersfromdata. Herewefollow a conventional definition of the training problem: given a model or mapping function, we improvesomeobjectivebyevaluatingthemodelthroughsometrainingexperience[Mitchell, 1997]. Forexample,traininganaiveBayestextclassifierrequiresmaximizingalikelihood function(i.e.,theobjective)onanumberoflabeleddocuments(i.e.,thetrainingexperience). Often, the training problem can be framed as an optimization problem. As such, we optimizesomeobjectivefunctionviasometrainingalgorithm. Althoughanidealobjective functionisaperformancemeasureontestsamples,wecannottakeitinoptimizationsincethe test samples and corresponding labels are assumed to be inaccessible in the training phase. Practicalobjectivefunctionsareinsteaddefinedasasurrogateforthemeasureontestdata. On theotherhand,theseobjectivefunctionsarenotnecessarilysomesortofperformancemeasure, butsomemetricsthatareassumedtocorrelatewiththeperformanceontestdata. Letusconsiderageneralcase. Supposey =f (x)isamodelthatreadsafeaturevector θ θ x and produces an n-dimensional vector y . For example, in text classification, x is the θ bag-of-wordsrepresentationofadocument, andy isadistributionoverasetofclasses. θ θ is the parameters of the model. The subscript emphasizes that the model is determined by θ. We further suppose that y is the gold-standard vector. Then, we define the objective gold functionasafunctionthatcountserrorsiny withrespecttoy ,denotedasL(y ,y ). It θ gold θ gold measureshowbaditwouldbeifwepredicty insteadofy . Givenamodel,thetraining θ gold problemcanbedescribedasfindingthe“best”parametersθˆ sothatL(y ,y )isminimized: θˆ gold θˆ = argminL(y ,y ) (1.70) θ gold θ 1.3 General Problems 43 ThisformulationcanbeeasilyextendedtothecaseofK trainingsamples: K θˆ = argmin 1 (cid:88) L(y (k) ,y (k) ) (1.71) K θ gold θ k=1 Onceweobtainθˆ ,wecanusef (x)asafixedmodelforprediction. θˆ L(y ,y )and 1 (cid:80)K L(y (k) ,y (k) )areusuallycalledlossfunctions(orcostfunc- θ gold K k=1 θ gold tions). A loss function can be defined in many ways, depending on what type of problem weaddressandwhatpriorwewanttoimposeupontraining. Here,wefirstconsiderthecase in which y is a probability distribution. It is quite common in NLP, e.g., y could be a θ θ distributionoveravocabulary,adistributionoveralistofdocuments,adistributionoveraset ofsyntacticlabels. Forsuchatypeofmodeloutput,themostcommonly-usedlossfunctions aremeasuresofdivergence: • Divergence-basedLoss. Divergence-basedlossfunctionscomputethedegreeofdif- ference between the two distributions y and y . For example, cross-entropy (see θ gold Section1.1.2)isoneofthemostpopularlossfunctionsusedinNLP.Onecan,ofcourse, choose other divergence-based measures, such as the KL divergence and the Jensen- Shannon (JS) divergence, which",
    "quite common in NLP, e.g., y could be a θ θ distributionoveravocabulary,adistributionoveralistofdocuments,adistributionoveraset ofsyntacticlabels. Forsuchatypeofmodeloutput,themostcommonly-usedlossfunctions aremeasuresofdivergence: • Divergence-basedLoss. Divergence-basedlossfunctionscomputethedegreeofdif- ference between the two distributions y and y . For example, cross-entropy (see θ gold Section1.1.2)isoneofthemostpopularlossfunctionsusedinNLP.Onecan,ofcourse, choose other divergence-based measures, such as the KL divergence and the Jensen- Shannon (JS) divergence, which can be found in most statistics textbooks. Note that MLEisalsoaspecialinstanceofthedivergence-basedobjective. Itisthesameasthe cross-entropylossify isaone-hotvectorwheretheentryofthecorrectlabelis1 gold andotherentriesareall0. However,machinelearningsystemsarenotalwaysrestrictedtodistribution-likeoutput. Rather, y could be a vector in Rn. An example is the discriminant functions used in clas- θ sification (see Section 1.2.3). They assign a score to each class, indicating how strong the modelbelievesitistheanswer. Onewaytodefinethelossfunctionsonreal-valuedvectors is to transform them into distribution-like forms15, and resort to the divergence-based loss. However,normalizationisnotalwaysnecessary,especiallywhenweneedascoreoutofthe rangeof[0,1]. Itismorecommontocomputelossesontherawoutputofthesemodels. Here aresomeexamples. • Distance-basedLoss. Itisnaturaltotakelossassomesortofdistanceingeometry. A generalexampleisthep-normdistance(seeSection1.1.1): n (cid:16)(cid:88) (cid:17)1/p L(y ,y ) = |y (i)−y (i)|p (1.72) θ gold θ gold i=1 For example, we would have a Euclidean distance-based loss function if p=2. The distance-based loss intrinsically describes a curve fitting problem: we learn a curve y =f (x) to fit the points {(x(k),y (k) )}. It is also called regression 16. A simple θ θ gold 15Forexample,wecannormalizetheentriesofavectorbythesumoftheseentries. 16Whenthemodeloutputisavectorwithtwoormoredimensions,theproblemiscalledmultivariateregression. 44 Chapter 1. Foundations of Machine Learning exampleisqualityestimationofmachinetranslation17. Itlearnstopredicttranslation quality (i.e., y ) for any pair of source and target sentences (i.e., x). We would say θ that the prediction is accurate if the predicted score is close to that made by humans. By using Eq. (1.72), one can design many loss functions for regression models. For example,meansquareerror(MSE)isapopularregressionlossfunction. Itisthesum ofsquaredEuclideandistancesbetweenthepredictionandthegoldstandard: n (cid:88) L(y ,y ) = |y (i)−y (i)|2 (1.73) θ gold θ gold i=1 Anotherexampleismeanabsoluteerror(MAE).ItispreciselytheformofEq. (1.72) whenp=1. • 0-1 Loss. The 0-1 loss is widely used in classification problems. It chooses a value of either 1 (penalty) or 0 (no penalty), and penalizes the case in which the predicted label and the gold-standard label are not the same. Let c = argmax y (c) be the θ c θ labelthatispredictedbyselectingtheentryiny withthehighestvalue. Likewise,let θ c =argmax y (c)bethegold-standardlabel. The0-1lossisdefinedtobe: gold c gold L(y ,y ) = L (c ,c ) θ gold 0-1 θ gold (cid:40) 1 c ̸=c θ gold = (1.74) 0 c =c θ gold • Margin-basedLoss. Amarginisthedifferencebetweenthepredictedscoresofthe correctlabelc andanincorrectlabelc: gold magin(c,c ) = y (c )−y (c) (1.75) gold θ gold θ It indicates a distinction between c and c. So, a natural idea is to ensure that the gold marginissufficientlylarge,oratleastexceedsaminimum. Thisiscalledlarge-margin training. Let △(c,c ) be a predefined cost of replacing label c with label c, gold gold satisfying△(c,c )≥0,and△(c,c )=0onlyifc=c . Ourgoalistoenlarge gold gold gold margin(c,c )−△(c,c ), in other words, the larger this value is, the smaller the gold gold lossis. Then,themargin-basedlossisgivenby: (cid:16) (cid:17) (cid:0) (cid:1) L(y ,y ) = max 0,max− magin(c,c )−△(c,c ) θ gold gold gold c (cid:16) (cid:17) (cid:0) (cid:1) = max 0,max y (c)−y (c )+△(c,c ) θ θ gold gold c (cid:16) (cid:17) = max 0,y (c)−y (c )+△(c,c ) (1.76) θ θ gold gold c 17Inmachinetranslation,qualityestimationcomprisesseveraldifferenttasks(seehttps://www.statmt. org/wmt21/quality-estimation-task.html).Hereweusethetermtorefertothetaskthatpredicts anevaluationscoredirectly. 1.3 General Problems 45 Designing△(c,c )dependsontheproblem. Asimplechoiceis△(c,c )=1for gold gold c̸=c . ThismakesEq. (1.76)atypeofthehingeloss. AnothervariantofEq. (1.76) gold isusingasuminsteadofamax: (cid:88) (cid:16) (cid:17) L(y",
    "y (c)−y (c )+△(c,c ) θ θ gold gold c (cid:16) (cid:17) = max 0,y (c)−y (c )+△(c,c ) (1.76) θ θ gold gold c 17Inmachinetranslation,qualityestimationcomprisesseveraldifferenttasks(seehttps://www.statmt. org/wmt21/quality-estimation-task.html).Hereweusethetermtorefertothetaskthatpredicts anevaluationscoredirectly. 1.3 General Problems 45 Designing△(c,c )dependsontheproblem. Asimplechoiceis△(c,c )=1for gold gold c̸=c . ThismakesEq. (1.76)atypeofthehingeloss. AnothervariantofEq. (1.76) gold isusingasuminsteadofamax: (cid:88) (cid:16) (cid:17) L(y ,y ) = max 0,y (c)−y (c )+△(c,c ) (1.77) θ gold θ θ gold gold c • Ranking-basedLoss. Ranking-basedloss(orrankingloss)isusedinseveraldifferent areas, such as information retrieval, classification and metric learning. It deals with theproblemwherewewanttoorderasetofscoreditems. Supposethemodeloutput y corresponds to a set of n items {c }, each for an entry of y . We define {ψ (c )} θ i θ θ i as the order of {c } by {y (i)}. For example, given {y (i)}={0.3,−2,1}, we have i θ θ ψ (c )=2, ψ (c )=3 and ψ (c )=1. Likewise, we can define {ψ (c )} as the θ 1 θ 2 θ 3 gold i gold-standardranks. Notethat{ψ (c )}canbeinducedinsomewaywithouttheneed gold i ofy iftheproblemonlyrequiresorders,ratherthanscores. Anideaofranking-based gold lossistomodeltherankingmistakesin{ψ (c )}withrespectto{ψ (c )}. Thereare θ i gold i manywaysto“count”themistakes. Asimplemethodistopenalizethecaseinwhicha pairofitemsareorderedincorrectly. Assuch,theranking-basedlosssomewhatshares thesamespiritofthatusedinbinaryclassification—wecategorizeapairofitemsas correctorincorrect. LetΩbeasetofordereditempairs: Ω = {(i,j)|ϕ (i)<ϕ (j)} (1.78) gold gold Thelossfunctionisgivenbytheequation: (cid:88) L(y ,y ) = L (y (i),y (j)) (1.79) θ gold pair θ θ (i,j)∈Ω whereL (y (i),y (j))isaclassificationloss,suchasthehingelossusedin[Collobert pair θ θ andWeston,2008]: (cid:0) (cid:1) L (y (i),y (j)) = max 0,y (i)−y (j)+1 (1.80) pair θ θ θ θ This method is called the pairwise method. Also, one can define the ranking-based loss in a pointwise or listwise manner. These loss functions are extensively used in developingsystemstorankobjects. • ContrastiveLoss. Contrastivelossistypicallyusedincontrastivelearning. Itassumes that, given a sample, there is a similar sample that is labeled as “positive”, and there are a number of dissimilar samples that are labeled as “negative”. A natural idea is to minimize the distance between similar samples and simultaneously maximize the distancebetweendissimilarsamples. Returntotheformulationhere. Foramodeloutput y ,lety+ bethepositiveoutputandY−={y−}bethesetofnegativeoutputs. Also, θ weusey todenotethetupleofy+ andY− insteadofasinglegold-standardvector. gold 46 Chapter 1. Foundations of Machine Learning Aformofthecontrastivelossfunctionisgivenbytheequation: L(y ,y ) = L(y ,y+,{y−}) θ gold θ (cid:88) = logD(y ,y+)−log D(y ,y−) θ θ y−∈Y− D(y ,y+) θ = log (1.81) (cid:80) D(y ,y−) y−∈Y− θ where D(α,β) is a measure of the distance between α and β. For example, we can define D(α,β) as the Euclidean distance (see Eq. (1.72)). A problem here is how to generatepositiveandnegativemodeloutputs. Inthesupervisedlearningsetup,onecan simplytreatthegold-standardvectorasthepositiveoutput. Fornegativeoutputs,the modelf(x)canoutputanumberofythroughacceptingdifferentx. Intheunsupervised learningsetup,y+ andY− areoftendefinedbasedonsome“natural”annotation. For example,f(·)canbeafunctionthatmapsxtosomethingandbacktox(callitauto- encoding). Then,y+ isxitselforsomeneighborsofx,andY− isasetofrandomly generatedvectors. • Error-basedLoss. Evaluationmetrics,asgenerallyusedincountingerrorsinsystem output,canalsobetakentobepartofalossfunction. Forexample,inmachinetranslation, apopularevaluationmetricisBLEU18.Thus,wecantakeminimizing1−BLEUasthe objective. LetError(y ,y )bethe“number”oferrorsincomparingy withy . θ gold θ gold Theerror-basedlossisjustthesameasthisnumber: L(y ,y ) = Error(y ,y ) (1.82) θ gold θ gold So far wehave presented several lossfunctions fora wide variety of problems, such as classification, regression, andranking. Aswewillseeinthisbook, differentlossfunctions havedifferenteffectsonmodelbehavior. However,testingallpossiblelossfunctionsissimply impracticalbecause there aresomanyofthem. Usersinsteadneed tochooseor designthe mostsuitablelossfunctionsfortheirownproblems. Thismaytaketimebutisnecessary. Onanotherhand,therearegeneralmethodstoimprovethedesignoflossfunctions. For example, wecanassumethatthemodeloutputy isnotasinglevectorbutavariablewith θ someprobability. ThelossL(y ,y )isthustreatedasavariabletoo. Then,weredefinethe θ gold lossfunctionastheexpectationofL(y ,y )underthedistributionofy : θ gold θ L({y },y ) = E (cid:2) L(y ,y )·Pr(y |x) (cid:3) (1.83) θ gold y ∼Pr(y |x) θ gold θ θ θ where the use of {y } means that",
    "Onanotherhand,therearegeneralmethodstoimprovethedesignoflossfunctions. For example, wecanassumethatthemodeloutputy isnotasinglevectorbutavariablewith θ someprobability. ThelossL(y ,y )isthustreatedasavariabletoo. Then,weredefinethe θ gold lossfunctionastheexpectationofL(y ,y )underthedistributionofy : θ gold θ L({y },y ) = E (cid:2) L(y ,y )·Pr(y |x) (cid:3) (1.83) θ gold y ∼Pr(y |x) θ gold θ θ θ where the use of {y } means that y is not fixed. By accessing the space of possible y , it θ θ θ offersabetterestimationoftheloss. ThisisessentiallyaninstanceoftheBayesianapproach. L({y },y )iscalledtheBayesianriskorriskforshort,sometimes. θ gold Anotherwaytoimprovetrainingisintroducingpriorsintotheobjective. Atypicalmethod 18BLEUisaprecision-likescorebetween0and1.Thehigherthebetter. 1.3 General Problems 47 istoaddaregularizationtermRtotheobjective,likethis: θˆ = argminL(y ,y )+α·R (1.84) θ gold θ where R could be another function that describes some aspect of the problem, such as the numberofparameters. αisahyperparametercontrollinghowmuchwerespectRintraining. ThedesignofRisitselfanimportantproblemformanypracticalmachinelearningsystems. Althoughwedonotdiscussthemhere,wewilllookatafewlaterinthisbook. Oncetheobjectiveisdetermined,weneedsometrainingalgorithmtoperformoptimization. Thisisaverybroadtopicinmachinelearning,suchthatwedonoteventrytodescribeanyof themindetailinthischapter. Anyway,oneshouldnotexpectauniversalalgorithmthatcan solvealltrainingproblems,andthereareindeedsomealgorithmsthataresuitableforcertain typesofproblems. Forexample,wecanusegradientdescenttotrainaneurallanguagemodel withthecrossentropy-basedloss[Bengioetal.,2003a],canusequadraticprogrammingto trainanSVMmodelwiththehingeloss[CortesandVapnik,1995],andcanuseminimum error-ratetraining(MERT)totrainastatisticalmachinetranslationmodelwiththe1−BLEU loss[OchandNey,2002]. 1.3.5 Overfitting and Underfitting The standard process of (supervised) machine learning comprises a training step and a test step. Whileonemaytrytominimizethelossontrainingsamples,thelearnedmodelisused to deal with new samples that are never seen before. It is like what we experienced in our lives,forexample,astudentstudieshardandwishestogetgoodgradesinfinalexams. Yes, studying hard = good grades should always be true, but it does not mean that memorizing allthequestionsandanswersintextbooksisagoodwaytoperformwellinexams. Italways happensthatthetestquestionsaresomethingdifferentfromwhatwelearned. Wetherefore needsomeabilityofgeneralization. Inmachinelearning,generalizationisusedtodescribehowwellamodellearnedthrough experiencepredictsonnewdata. Asystemisthoughttobeofexcellentgenerationperformance ifitlearnslittlefromtrainingdatabutformsitspredictionabilitybasedonsome“god”inductive biases on the problem. However, good generalization does not mean less training. Instead, practitionerswouldliketotrainamachinelearningmodelonmoretrainingdatatoprevent it from memorizing all the things. Generalization is a very complex issue determined by severalfactors, includingproblemcomplexity, modelarchitecture, amountoftrainingdata, trainingalgorithmandsoon. Whiletherearenostandardrulestoensuregoodgeneralization, researchersalwaystrytoaddressitsomehow. To describe how well a model generalizes to new data, there are two important terms, underfittingandoverfitting. Underfittingreferstothephenomenonthatamodeldoesnotlearn sufficientlyfromthetrainingdataandthushaspoorperformanceonnewdata. Forexample, weinterrupttrainingaccidentallyanddeploytheimmaturemodelforprediction. Themodel cannotperformwelloneitherthetrainingdataorthetestdata. Ifamodelunderfitsthetraining data,thenonecouldimproveitinsomesimpleways. Forexample,onecouldtrainthemodel 48 Chapter 1. Foundations of Machine Learning 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Underfitting 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Fittingjustright 8 6 4 2 0 0 2 4 6 8 x 1 x 2 Overfitting Figure1.4: Decisionboundariesofabinaryclassificationproblem. left=underfitting,right= overfitting,andmiddle=fittingjustright. Intheunderfittingcase,thereareseveralobvious mistakesthataremadeinseparatingthetwoclassesofdatapoints. Byshiftingthedecision boundaryupabit(middle),weobtainasatisfactoryseparationresult,wheremostofthedata pointsbelongingtothesameclassareplacedonthesamesideofthedecisionboundary. By contrast,aperfectseparationrequiresahighlycomplexdecisionboundaryinstead(right). foralongertime;onecouldremoveunimportantportionsfromthetrainingdata;onecould useamodelwithasimplerarchitectureinstead. In contrast to underfitting, overfitting refers to the phenomenon that a model fits the trainingdatawellbutgeneralizespoorlyonthetestdata(seeFigure1.4). Asimpleexample of overfitting here is the OOV problem (see Section 1.2.5). It would be a disaster if a text classificationmodeljustfitsthosewordsthathavebeenseenbutgetsstuckwhennewwords appear. Thecausesofoverfitting arediverse. Anexampleislearningacomplexmodelonasmall training dataset. The model complexity often matters when we design a machine learning model. If the model is complex and has many parameters, then it would be much easier to overfitasmallnumberofsamples(seeFigure1.4). Theproblemwouldbemoredifficultif thereisnoisydata,becauseoftheerrorsof“garbagein,garbageout“intraining. Inaddition, excessivetrainingisanothercauseofoverfitting. Forexample,wecanheavilytuneasystem toenforceittomodelthedatawithnoerrors. Thesystemwouldbefragilefornewsamples, evenwhentherearesmallfluctuationsininput. Overfittingcanbealleviatedinmanyways. Herearesomecommonly-usedtechniques. • Usingmore(high-quality)trainingdata. Large-scaletraininghelpsthemodelcapture thetruepatternsindata. However,addingnoisydatawoulddothisinanegativeway. • Using validation data. Validation data is some test data but used in training. For example,adatasetcanbedividedintoheld-outdataandtrainingdata. Onecansimply earlystopthetrainingprocesswhentheperformancedropsontheheld-outdata. • Usingsimplermodelarchitectures. Asnotedpreviously,Occam’sRazorisaprinciple 1.3 General Problems 49 we can follow in model design. Models with more complex architectures, though powerful, would be more likely to fit the noisy data points if the problem is not so difficultitself. Usingasimplermodelarchitectureinsteadcouldmakeiteasiertomodel thedominantpatternsinthedata. • Regularization. Regularizationisanotherwaytocontrolthemodelcomplexity. Typi- cally,itregularizesmodelparametersbypriors. Anexampleissmoothing(seeSection 1.2.5). Itre-estimatesthedistributionofwordsaftertraining. Amoregeneralmethod isregularizedtraining(seeEq. (1.84)). Forexample,wecandefinetheregularization factorasthel normoftheparameters,andbiasthemodeltothosewhoseparameters 1 arenotinlargeabsolutevalues. • Combining multiple models. A better prediction can also be made by ensembling multiplemodels. Thesemodels(callthemcomponentmodels)areingeneralofdifferent parametersorarchitectures,and/oraretrainedwithdifferentportionsofthedata. The varianceinmodelscanreducetheriskthatallthesemodelsoverfitthedatainexactly thesamemanner. Thesemodelsare,therefore,lesslikelytomakesimilarmistakesin prediction. 1.3.6 Prediction Although we restricted our discussion to classification in previous sections, (supervised) machinelearningisnotjustataskofpredictingalabelforaninputobject. Therearemany typesofmachinelearningproblems,dependingonwhatformofthepredictionisdefined. • Classification. Classification is perhaps one",
    "isregularizedtraining(seeEq. (1.84)). Forexample,wecandefinetheregularization factorasthel normoftheparameters,andbiasthemodeltothosewhoseparameters 1 arenotinlargeabsolutevalues. • Combining multiple models. A better prediction can also be made by ensembling multiplemodels. Thesemodels(callthemcomponentmodels)areingeneralofdifferent parametersorarchitectures,and/oraretrainedwithdifferentportionsofthedata. The varianceinmodelscanreducetheriskthatallthesemodelsoverfitthedatainexactly thesamemanner. Thesemodelsare,therefore,lesslikelytomakesimilarmistakesin prediction. 1.3.6 Prediction Although we restricted our discussion to classification in previous sections, (supervised) machinelearningisnotjustataskofpredictingalabelforaninputobject. Therearemany typesofmachinelearningproblems,dependingonwhatformofthepredictionisdefined. • Classification. Classification is perhaps one of the most common machine learning problems. Aclassificationsystemisrequiredtoassignoneormoreclassestoaninput object. • Regression. In statistics, regression studies the relationship between a dependent variable (or an outcome) and an independent variable. While regression has many applications,itisoftenframedasscorepredictioninNLP.Forexample,takingamovie reviewasinput(i.e.,anindependentvariable),theregressionmodellearnstopredicta recommendationscore(i.e.,adependentvariable). • Ranking. Arankingmodelistopredicttheorderofasetofinputobjects. Forexample, amodelranksanumberoftranslationsintermsoftranslationquality. • Structureprediction. Manymachinelearningmodelsarerequiredtooutputnotonlya realvalueoraclassbutatreeorasequence. Thetaskofpredictingstructuredoutputs iscalledstructureprediction. Forexample,asyntacticparserisastructureprediction system,asitsoutputisatreestructure. Inadditiontothese,miningisatermthatisfrequentlyusedinthecommunity,although it is somehow not a standard machine learning problem. The problem of mining refers to discoveringunknownpatternsinthedata. Anexamplewewouldliketocategorizeintothis iswordclustering. Givenanumberofwords,theclusteringsystem“predicts”theclusterfor eachword. Theoutputofsuchsystemsisnotpre-defined. Patternsindataarethemselveshard 50 Chapter 1. Foundations of Machine Learning todescribe. Thus,theterm“mining”couldcoverarangeofproblems. Toavoidconfusion,we willusemorespecificterms(suchaswordclustering)torefertomining-relatedproblems. Despiteafundamentalaspectofmachinelearning,predictionisconventionallyassumed to be trivial, given that many models and methods are tested on standard classification and regressiontasks. Ontheotherhand,predictionisnon-trivialinstructureprediction,suchas parsingandmachinetranslation,whichareverycommoninNLP.Essentially,predictingatree orasequenceisasearchproblem. Forexample,thereexistatheoreticallyinfinitenumber oftranslationsgivenasource-languagesentence. Evenifwehaveamodeltoevaluateevery translation,findingtheoptimaltranslationinthesearchspaceisobviouslyacomputational challenge. In such cases, we need some way to make it feasible to perform search. This is implementedbyeitherresortingtothegeneralsearchalgorithmsinartificialintelligenceor developingnewalgorithmsforspecificproblems. Asanaside,thestudyonthesearchproblem offersanewviewonthemistakesmadebyamachinelearningmodel: someoftheerrorsare duetoinaccuratemodeling(callthemmodelerrors),andtherestareduetoinaccuratesearch (callthemsearcherrors). Forprediction,eliminatingsearcherrorsisagoalbutoftenatthe costofaconsiderablylargeamountofsearcheffort. Wesometimesmusttradeoffbetween efficiencyandaccuracyifamachinelearningmodelisdeployedforpracticalpurposes. We willseeafewexamplesinChapter5. 1.4 Model Selection and Evaluation Formostmachinelearningproblems,thegoalistofindamodelthatwouldperformthebest onnewdata. Twoproblemscanbeseparatedoutfromthisgoal[Hastieetal.,2009]: • Modelselection. Selectingthebestmodelontrainingdatabysomecriteria. • Modelevaluation. Estimatingtheperformanceofagivenmodelonnewdata. AsnotedinSection1.3.4,lossfunctions(orerrorfunctions)arecommonwaysofmeasuring errors in a prediction y =f (x) with respect to a gold-standard y . Given K labeled θ θ gold trainingsamples{(x(1),y (1) ),...,(x(K),y (K) )},thetrainingerrorisgivenby gold gold (k) (k) Err = L({y },{y }) (1.85) train θ gold (k) (k) where{y }arethepredictionsoverthetrainingdataset,and{y }arethecorresponding θ gold (k) (k) gold-standards. L({y },{y })isingeneraldefinedastheaveragedlossoveralltraining θ gold samples: K (k) (k) 1 (cid:88) (k) (k) L({y },{y }) = L(y ,y ) (1.86) θ gold K θ gold k=1 ordefinedasasinglemeasureontheentiresetoftrainingsamples. Likewise,wecandefinethe testerroronthetestdataset,denotedasErr . Err isalsocalledgeneralizationerror. test test Itindicateshowwellamodelgeneralizestonewdata. 1.4 Model Selection and Evaluation 51 8 6 4 2 0 10 20 30 40 50 60 TimeElapsed(intrainingepochs) ssoL 75 70 65 ]%[UELB−1 Trainingerror(Err =cross-entropy) train Testerror(Err =1−BLEU) test Figure1.5: Curvesoftrainingerrorandtesterrorforamachinelearningsystem. Thetraining errorismeasuredintermsofthecross-entropyloss,andthetesterrorismeasuredintermsof 1−BLEU. Allstatisticsarecollectedbyrunninganeuralmachinetranslationsystemonthe IWSLTDe-Enbenchmark. Thetrainingerrorcontinuestodropasmoretrainingepochsare involved. Thetesterror,ontheotherhand,followsatrendoffirstgoingdownandthengoing up. Whenthetesterrorstartstoincrease,themodelislikelytooverfitthetrainingdata. IntheprecedingsectionsweassumedthatminimizingErr istheobjectiveoftraining, train i.e., θˆ=argmin Err . However, we cannot assume that f (·) can obtain the minimum θ train θˆ Err inthesameway. SeeFigure1.5forlearningcurvesofamachinetranslationsystem. test Clearly, Err does not correlate with Err well. The training error keeps reducing as test train trainingproceeds. However,thetesterrorgoesupafterfollowingthesametrendasthetraining errorforaperiodoftime,indicatingoverfittingofthemodel. Thismakestheproblemabit morecomplicated,aswecannotalwaystrustErr althoughitisandshouldbethemeasure train ofthegoodnessoftraining. Surely,weneedsomewaytoselectabettermodel,inadditionto lookingatErr only. train 1.4.1 Strategies for Model Selection Choosingtheoptimalmodelonthetrainingdataischallengingbecausethemotivationhereis “greedy”itself—wehopethatamachinelearningmodelcangeneralizefromafinite,evena “small”numberofsamples. Fromthestatisticallearningpointofview,thechallengeisdueto thewaywedefinethelearningproblem. Animplicitassumptioninmachinelearningisthatall dataisgeneratedbysomedistribution. Thus,thelearningproblemisdeterminedbygenerating thetrainingdataviaadata-generationdistributionandthetestdataviaanotherdistribution. Forexample,ifboththetrainingandtestdatasetsaresufficientlylargeandobtainedviathe samedata-generationdistribution,thenthelearnedmodelcanperformonthetestdataaswell asonthetrainingdata. Inthiscase,itiseasytogeneralizethemodelfromthetrainingdatato thetestdata. Bycontrast,ifalltrainingandtestdataisgeneratedinanarbitrarymanner(saya uniformdistributionovertheentirespaceofdatapoints),thenthemodelwillfailtogeneralize, aseverythinglearnedonthetrainingdatadoesnothingwiththetestdata. 52 Chapter 1. Foundations of Machine Learning Itwillbemoreinterestingifweconsiderallpossibleproblems. Thenofreelunchtheorem statesthatalllearningalgorithmswillperformequallywellifweaveragethetesterrorover allproblems19. Inotherwords,alllearningwillmakenosenseifthereisnopreferencefor certain problems. However, developing a universally good machine learning model on all problemsisidealistic. Inreal-worldapplications,thetrainingandtestdataisalwaysassumed toatleastinpartfollowsomedistribution. Therefore,thereareindeedsomewaystocapture thisdistributionandimprovethegeneralizationabilityofamodel. Twoscenariosaregenerally consideredinimprovingmachinelearningsystems: • Giventhemodeldesignandthetrainingalgorithm, howtodeveloporselecttraining datatoreducethetesterror. • Giventhetrainingdata,howtodeveloporselectamodeltoreducethetesterror. Thefirstscenarioiscomplicatedandrelatestomanypracticalissues,e.g.,annotation,data cleaning, data quality estimation and so on. Since these issues are not the focus for model selection,wedonotdiscussthembutleavesometosubsequentsections. Here,wefocuson themodelselectionprobleminthesecondscenario. 1. Model Complexity The simplest method of model selection might be testing the models on validation data. Typically, this data does not overlap with either training or test data, but is assumed to be generatedinthesamewayasthetestdata. However,suchdataisnotalwaysavailable. Insome cases,wedonotevenknowanythingaboutthetestdata. Somanymodelselectionmethods arevalidation-free. Acommonwayistousemodelcomplexity(ormodelcapacity)asanindicatorofthe selection. Inmachinelearning,modelcomplexitycanbeinterpretedinseveraldifferentways. For example, a non-linear model is intuitively more complex than a",
    "simplest method of model selection might be testing the models on validation data. Typically, this data does not overlap with either training or test data, but is assumed to be generatedinthesamewayasthetestdata. However,suchdataisnotalwaysavailable. Insome cases,wedonotevenknowanythingaboutthetestdata. Somanymodelselectionmethods arevalidation-free. Acommonwayistousemodelcomplexity(ormodelcapacity)asanindicatorofthe selection. Inmachinelearning,modelcomplexitycanbeinterpretedinseveraldifferentways. For example, a non-linear model is intuitively more complex than a linear model. Also, a model with more parameters is more complex than a model with fewer parameters under thesamemodelarchitecture. Moreformaldefinitionscouldbefoundinthetheoreticalpart ofmachinelearning, suchastheVapnik-ChervonenkisdimensionortheVCdimension [VapnikandChervonenkis,1971]. Herewesimplytreatmodelcomplexityasameasureofthe expressivepowerofamodel,i.e.,ahighermodelcomplexityindicatesmorehypothesesthat themodelcanexpress. While complex models are usually assumed to be more powerful, higher model com- plexitiesarenotalwayshelpful. Infact,complexmodelsaremorelikelytooverfitthedata, especiallywhenasmalldatasetisusedfortraining. Bycontrast,toosimplemodelsareoften pronetounderfitting. Wethereforeneedtoseekan“optimal”levelofmodelcomplexity. Figure 1.6plotstrainingandtesterrorsagainstmodelcomplexity. An“optimal”complexitycanbe chosen when the training error tends to convergence. While Figure 1.6 shows an intuitive example,itisstillhardtosayatwhatpointwecanchoosethemodel. Thecommonpractice, thoughnotformallydescribedinmostcases,istochooseamongthose“good”modelsbyusing 19Thenofreelunchtheoremwasoriginallypresentedinaclassificationscenario[Wolpert,1996],andwas furtherextendedtosearchandoptimizationproblems[WolpertandMacready,1997]. 1.4 Model Selection and Evaluation 53 ModelComplexity rorrE Trainingerror Testerror Figure1.6: Curvesoftrainingerrorandtesterrorunderdifferentmodelcomplexities. Complex modelshelpinreducingthetrainingerrorastheycancomputecomplexfunctionsinfitting datapoints. However,atoolargemodelcomplexityismorelikelytoleadtooverfittingandis harmfultothegeneralizationabilityofthemodels. Forexample,thetesterrorincreasesas morecomplexityisadded. Occam’sRazor. Supposewehaveasetofmodelsthatperformcomparablywellonthetraining databutareofdifferentcomplexities. AccordingtoOccam’sRazor,thesimplestmodelisthe “best”choice. Manycriteriaareavailabletomeasurethemodelcomplexity. Forexample, • Numberofparameters. Thoughverysimple,countingthenumberofparametersisthe mostintuitiveyeteffectivemethod. Itcanbeextendedtocountingtheeffectivenumber ofparameterswhichisdefinedtobethetraceofthematrixusedtotransformy to gold y . θ • p-normofparameters. Thep-normofaparametermatrixisalsoanindicatorofhow complexamodelis(seeSection1.1.1). Forexample,accordingtothel norm,amodel 1 withlargerabsolutevaluesforparametersismorecomplex. • Descriptionlength. Descriptionlengthisatermusedindatacompression. Forexample, itcouldbethenumberofbitsusedtostoreamodel. Thus,theminimumdescription length(orMDL)indicatesthemostcompressedmodel. • TheVCdimension. Itisoriginallyfromcomputationallearningtheory. Inshort,theVC dimensioncanbedefinedasthemaximumnumberofdatapointsthatcanbeshattered bytheclassifier. Inaddition,thereareotherchoicesfordefiningthecriterion,includingtheAkaikeinfor- mationcriterion(AIC),theBayesianinformationcriterion(BIC),theminimummessage length(MML)andsoon. Theycanbefoundinmosttextbooksonstatisticsand/orstatistical learning[BurnhamandAnderson,2002;KonishiandKitagawa,2007;Hastieetal.,2009]. 54 Chapter 1. Foundations of Machine Learning 2. Bias-Variance Tradeoff Controllingmodelcomplexitytoavoidoverfittingandunderfittingisalsolinkedtothetradeoff betweenbiasandvariance. Bias(orpredictionbias)istheamountthatthemodelprediction differs from the true value. In statistics, bias is a systematic error that cannot cancel out evenifwerunalargenumberofrepeatedexperiments. Ingeneral,biaserrorresultsfromthe wrongassumptionsabouttheproblem,suchasapproximatinganon-linearproblemviaalinear model. Thisisveryinteresting! Wecanestablishtheconnectionofthebiaserrorherewith theinductivebiasusedinmodedesign(seeSection1.3.2). Forexample,giventrainingdata, alargebiasmodelisusuallyduetothefactthattherearemoreassumptionsandthemodel is not complex enough. To make it simple, we would say that more (or stronger) inductive biasescanresultinalowermodelcomplexityandmorebiaserrorinprediction. Occasionally, thetermbiasisusedasashortforbothbiasinprediction(fromastatisticsperspective)and inductivebias(fromamodeldesignperspective),althoughtheyareconsideredtohavedifferent meanings20. Variance,ontheotherhand,describeshowspreadthepredictioniswhentherearevariations in training data. The variance error also correlates with model complexity. For example, a complexmodeltendstoexhibithighervariance. Both bias and variance are sources of errors of a system. A common example is the bias-variancedecompositionofmeansquarederror. Hereweusesomenotationthatdiffers slightlyfromthatusedinprevioussections. LetD beasetofK trainingsamplesandf (·) θˆ(D) beamodelleanedonD. Further,givenanewsamplex,lety =f (x)bethemodel θˆ(D) θˆ(D) predictionandy bethe“true”prediction. Thebiasandvariancearedefinedas: gold bias = E [y ]−y (1.87) D θˆ(D) gold variance = E (cid:2)(cid:0)E [y ]−y (cid:1)2(cid:3) (1.88) D D θˆ(D) θˆ(D) whereE [y ]isthemeanofy overallpossibleK sampletrainingdatasets. Thus,the D θˆ(D) θˆ(D) biasissomesortofdifferencebetweenthemeanandthetruevalue,andthevarianceissome sortofdifferencebetweenthemeanandthepredictedvalue. Takingthemeansquarederroras theerrormeasure,wecanwritetheexpectederroras: error = E (cid:2)(cid:0) y −y (cid:1)2(cid:3) D θˆ(D) gold = bias2+variance (1.89) Forlowermeansquarederror,reducingbothbiasandvariancesimultaneouslyisobviously anidealgoal. However,itisdifficulttomakeamodelthatexhibitsbothlowbiasandvariance. When one of the two decreases, the other increases (see Figure 1.7). Researchers must choosetheoptimallevelofmodelcomplexitywhilepreventingtrainingfromoverfittingand underfitting. This also depends on the problem we intend to solve. For example, a simple 20Biasismoreoftenusedinstatisticstodescribesomeaspectofanestimator. 1.4 Model Selection and Evaluation 55 TestError Bias Variance Complexity Figure1.7: Biasandvarianceagainstmodelcomplexity[Goodfellowetal.,2016]. Thecurves showaconflictinreducingthebiaserrorandthevarianceerrorsimultaneously. Byvarying themodelcomplexity,onecanobtaineitheralow-bias,high-variancemodelorahigh-bias, low-variancemodel. Bothofthetwocasesexhibithightesterror. Forexample,ahigh-variance modelisoftenofalargermodelcomplexity. Whilesuchamodelisabletodealwithcomplex problems, it is more likely to overfit the data. On the other hand, a high-bias model often meansasimplermodelbuttendstounderfitthedata. Toimprovethegeneralizationontest data,onecanseekatradeoffbetweenbiasandvariance. Forexample,thereislowtesterror whena“middlesized”modelischosen. model generally has low variance but high bias. However, if we use the simple model (say alinearmodel)todescribeacomplexproblem(sayanon-linearproblem),thenunderfitting wouldprobablyoccurbecausetheproblemistoo“hard”forthemodel. Returningtothemodelselectionproblem,thebias-variancetradeoffisnotaruleformodel selection,butaprinciplewemustkeepinmind. Often,oneneedstomakecompromisesto createamodelthatmakesreasonablygoodpredictions. Itisalsoworthnotingthat,inmany applications, complex models are usually accompanied with the inefficiency problem. An appropriatemethodmightbetostartwithasimplemodelandonlyaddcomplexitywhenitis needed. 3. Model Combination Selectingfromasetofmodelsisnottheonlywaytoreducegeneralizationerror. Alternatively, onecandothisintheoppositeway,andcombinethesemodelsfora“stronger”model. Sucha methodiscalledensemblelearning[Senietal.,2010;Zhou,2012a]. Akeyideaofensemble learningistocreateasetofcomponentmodels(orensemblemodels),suchthattheycanvote forabetterprediction. Thesimplestoftheseisamixturemodelthataveragesthepredicted scoresofmultiplecomponentmodels(callitmodelaveraging),whereasamoresophisticated methodcancombinethesub-structuresofthesemodels. Component models are in general generated in some way that they can exhibit some diversity. For example, they can be learned on different portions of the training data, or by usingdifferentinitializationsformodelparameters. Interestingly,itisfoundthatsuchmethods 56 Chapter 1. Foundations of Machine Learning canguaranteethereductionofgeneralizationerrorsomehow. Forexample,bagginghelpsto lowervariance[Breiman,1996],andboostinghelpstolowerbias[Schapire,1990]. Theseare linkedbacktowhatwepresentedinSection1.4.1: thegeneralizationerrorcanbereducedby eitherreducingthebiaserrororreducingthevarianceerror. Butdiscussinghowtocombinemodelsisbeyondthescopeofthischapter. Whileitiseven notappropriatetocategorizemodelcombinationasatopicrelatedtomodelselection,itcanbe",
    "methodcancombinethesub-structuresofthesemodels. Component models are in general generated in some way that they can exhibit some diversity. For example, they can be learned on different portions of the training data, or by usingdifferentinitializationsformodelparameters. Interestingly,itisfoundthatsuchmethods 56 Chapter 1. Foundations of Machine Learning canguaranteethereductionofgeneralizationerrorsomehow. Forexample,bagginghelpsto lowervariance[Breiman,1996],andboostinghelpstolowerbias[Schapire,1990]. Theseare linkedbacktowhatwepresentedinSection1.4.1: thegeneralizationerrorcanbereducedby eitherreducingthebiaserrororreducingthevarianceerror. Butdiscussinghowtocombinemodelsisbeyondthescopeofthischapter. Whileitiseven notappropriatetocategorizemodelcombinationasatopicrelatedtomodelselection,itcanbe seenasameansofimprovingthegeneralizationability. Inthissense,bothmodelcombination and model selection address problems on a similar theme. In fact, model combination is remarkablyeffectiveformanyNLPtasks. Forexample,moststate-of-the-artsystemsinNLP arebasedonthecombinationofmultiplemodels. 1.4.2 Training, Validation and Test Data Weturnnowtothedataproblem. Asdiscussedintheprevioussections,inthetrainingstage,a trainingdatasetisusedtofittheparametersofthemodel. Intheteststage,atestdatasetisused toevaluatethelearnedmodel. Closelyrelatedtotestdataisvalidationdata,whichhascome upafewtimesinthischapter. Avalidationdatasetisatestdatasetaswellbutcanbeusedin thetrainingstage. Itiscommonlyusedformodelselectionandtuninghyperparameters. Inmanycases,onemayimaginethatthereissomedatafortrainingandsomeadditional data for validation and test. This assumption, however, is not realistic in many real-world applications. For example, developers cannot always access the data of system use after deploying a system. From a scientific point of view, there is no “real” new data for test — whenyouseenewdata,itisnotnewanymore. Therefore,whatweaddressisessentiallyan analogueoftheproblem. A simple method, as in many research papers, is to verify machine learning models on benchmarktasks. Inthesetasks,alldataispreparedinadvance,andallyouneedistorunyour modelsonthedata. Suchamethodmakesiteasytocomparedifferentsystemsdirectly,asall thesesystemsaretrainedandtestedonthesamedatasets. Occasionally,wearejustgivena numberofsamplesbutnottoldwhicharefortrainingandwhicharefortest. Insuchcases,the datacanbedividedintopartseachofwhichisusedforsomepurposes. Forexample,asplit couldbe60%fortraining,20%forvalidation,and20%fortest. Whiledatasplittingprovidesawaytoassesstheperformanceofamodel,theassessment resultisnotalwaysstableduetosamplingbias. Sometimes,theperformancevariesgreatly acrossdifferentrunsofdatasplitting. Theproblemismoreobviouswhenthedatasetistoo smalltoperformsufficienttrainingortest. Acommonwaytoweakentheeffectofthisbiasiscross-validation. Cross-validationisa resamplingmethod. Eachroundofcross-validationisanewsplitofdataandtheresultisthe combinationoftheassessmentovertherounds. Asimplemethodisrandomsubsamplingthat repeatsrandompartitionofthedataandaveragestheperformanceoverruns. Anothermethod isk-foldcross-validation. Itdividesthedataintok parts. Ineachroundofcross-validation, somepartsareusedastrainingdata,andotherpartsareusedasvalidationandtestdata. For example,in10-foldcross-validation,amodelcanbetrainedandvalidated/testedfor10times, eachchoosingoneofthetenpartsasthetestdataset. 1.4 Model Selection and Evaluation 57 Anothernoteonthescaleofdata. Forpractitioners,oneofthemostfrequentquestions ishowmanysamplesareenoughforlearningagoodmodel. Thismaybethemostdifficult questiononwhichdifferentpeoplecanhaveconsensusanswers. Therearemanytheoretical resultsthatcantelltheboundoferrorsgivenacertainamountofdata,whereasinmostcases we just simply follow the “the more the better” idea. In another line of thought, a system could be sample efficient. In general, a sample efficient system can reach a good level of performancebyusingfewersamplesorseeingthesamesampleforfewertimes. Forexample, tuningapre-trainedmodelissampleefficientbecausethesamplesarenotusedforlearning fromscratchbutamodestupdateofthemodel. Anotherexampleisfew-shotlearning. Itaims togeneralizefromobservingveryfewsamplesforatask. 1.4.3 Performance Measure Asanessentialpartofeverymachinelearningproblem,aperformancemeasuredescribeshow well a system performs given some data. Usually it is used in either designing the training objectivesorevaluatingtheresultofthefinalsystem. Forexample,allthoselossfunctions describedinSection1.3.4aresomekindsofperformancemeasures. Asforevaluatingtheperformanceontestdata,ameasureisoftendesignedinawaythat wecancounttherealerrors. Thus,re-usingthelossfunctionsintrainingmightnotbeagood choiceforreportingthefinalscore. Forexample,thewidely-usedmeasuresforclassification problems are precision, recall and F score. They are proposed to quantify the ability of a 1 classification system in certain aspects: given a class c, precision computes the fraction of correctpredictionsinpredictingc,andrecallcomputesthefractionofcorrectpredictionson allsampleslabeledasc. TheF scoreisameasurethatcombinesprecisionandrecall. 1 Noticethatperformancemeasuresarenotnecessarilydesignedforoptimization. Inthis sense, they may not guarantee some mathematical properties, such as differentiable and continuousfunctions. AnexampleistheBLEUmetricusedinmachinetranslation. BLEUisa functioncombiningprecisionscoresandapenaltyscore[Papinenietal.,2002]. Thisinturn makesthemetricnon-differentiableanddiscontinuous. InNLP,therearemanysuchevaluation measures that are ad-hoc for certain tasks. These raise an interesting problem that the loss functionusedintrainingmaydifferfromwhatweactuallyuseinevaluatingthefinalmodel. Thus, one sometimes needs to take into account the discrepancy between the objectives of trainingandtest. AnotherproblemwithperformancemeasuresinNLPisthattheremightbetwoormore “answers”forthesame“question”. Forexample,therearegenerallymultiplegoodtranslations forasource-languagesentence. Onesolutionistotakemultiplegold-standardsintoaccount whendesigningaperformancemeasure. BLEUissuchacase. Itcountsthemaximumnumber ofthecorrecttranslationsegmentsoverallreferencetranslations. Thesecondsolutioninvolves humanevaluation. Suchawayofevaluationismoreaccuratebutofcourseismoreexpensive. When developing practical systems, practitioners usually train and tune the systems using automaticmeasures,andcallforhumanevaluationsforthefinaltest. 58 Chapter 1. Foundations of Machine Learning 1.4.4 Significance Tests Now, assuming you are improving a system in some way, you might be wondering if the improvement is significant enough or not. All you have is a performance measure. So you cantelltheperformancedifferencebetweenanytwopointsindevelopingthesystem,butyou cannottellifthedifferenceisrealorhappensbychance. Inthisexample,youimplicitlytrytorejectoracceptaclaimthatasystemisbetterthan anothersystem(ornot). Instatistics,significancetestsareamethodtomodelthisproblem. Suppose we have two systems A and B. And there are a number of datasets on each of whichweevaluatethetwosystemsviathesameperformancemeasure. Then,wemaketwo hypotheses H : SystemAperformsworsethanorequallywellassystemB. 0 H : SystemAperformsbetterthansystemB. 1 whereH isthenullhypothesis,andH isthealternativehypothesisthatiscontradictoryto 0 1 thenullhypothesis. Bytestingthesehypotheses,wecanclaimthatsystemAissignificantly better than system B (i.e., reject H and accept H ) or not (i.e., accept H and reject H ). 0 1 0 1 Weprobablymakeerrorsinthetest,forexample,incorrectlyrejectingatruenullhypothesis (typeIerror),orincorrectlyacceptingafalsenullhypothesis(typeIIerror). Thetwotypes oferrorsareatoddswitheachother. Adecreaseofonemayleadtoanincreaseoftheother. Alternatively,wecandecreaseonewhileguaranteeingthattheotherisupperbounded. For example,wecanreducethetypeIIerrorasmuchaspossible,andkeepthetypeIerrorbelow a constant α. α is typically called the significance level of a test. It is standard practice to choose the significance level",
    "(i.e., reject H and accept H ) or not (i.e., accept H and reject H ). 0 1 0 1 Weprobablymakeerrorsinthetest,forexample,incorrectlyrejectingatruenullhypothesis (typeIerror),orincorrectlyacceptingafalsenullhypothesis(typeIIerror). Thetwotypes oferrorsareatoddswitheachother. Adecreaseofonemayleadtoanincreaseoftheother. Alternatively,wecandecreaseonewhileguaranteeingthattheotherisupperbounded. For example,wecanreducethetypeIIerrorasmuchaspossible,andkeepthetypeIerrorbelow a constant α. α is typically called the significance level of a test. It is standard practice to choose the significance level in the interval [1%,5%]. When conducting statistical testing, we can obtain the probability of the type I error (call it a p-value). A p-value that is lower than the significance level can make a rejection of the null hypothesis. For example, in the aboveexample,withasignificancelevelof5%,ap-value=3%meansthattheimprovement isstatisticallysignificant. Formoreinformationaboutthep-value,wereferthereadertoother booksonstatistics[McClaveandSincich,2006;Freedmanetal.,2007;Freedman,2009]. Note that the conclusion of significance tests depends on several factors, such as the number of experiments and the variance in the results of experiments. A problem with applyingsignificanceteststoNLPtasksisthatthereareoftenveryfewdatasetsforrunningthe experiments[Droretal.,2020]. Ideally,weknowthetruedatadistributionandcanconsider it in the test. This method is called the parametric test. If we cannot find the true data distribution,then,asanon-parametrictestmethod,wecangenerateanumberofexperiments bysamplingoveradatasetoraddingrandomnessintothetest. Significancetestsareimportantfordrawingconvincingconclusionsindevelopingmachine learningsystems,althoughtheyareoftenignoredunintentionally. Figure1.8showsevaluation resultsofthreemodels. Eachofthemisrunforseveraltimeswithdifferentinitialparameters. WhilesystemAissuperiortosystemB intermsoftheaveragedperformance,therearelarge variancesintheirresults. Thesignificancetestindicatesthatthedifferenceisnotsignificant. By contrast, the difference between system A and system C is significant because their performancediffersgreatlyenoughinmostcases. Ontheotherhand,researchershavefound 1.5 NLP Tasks as ML Tasks 59 systemC systemB systemA Performance Figure1.8: Performanceofthreemachinelearningsystems. Foreachsystem,therearemany different results because we introduce some randomness into training (e.g., data shuffling, randomstartingpoints,etc). AlthoughitseemsthatSystemAoutperformsSystemB,there is no real distinction between them, because they overlap a lot in the distributions of the performance(seethebottomofthefigure). WhencomparingSystemCwithSystemAorB, thedifferenceinperformanceissignificantbecausewecouldaccepttheH hypothesis(i.e., 1 SystemCoutperformsSystemAorB)givenalargenumberofexperiments. thatthereareindeedsomethresholdsofperformancegaintoindicatesignificanceundercertain circumstances. Forexample,wewouldsaythatthesignificancecanberoughlyindicatedbya certainmetricgainifwecomparesimilarsystems[Berg-Kirkpatricketal.,2012]. 1.5 NLP Tasks as ML Tasks While there are a wide variety of NLP tasks, many of them can be formulated as the same machine learning problem. This enables a universal solution to a group of NLP problems by using a general machine learning approach. Typically, an NLP task can be described as learningtomaplanguageunitstosomeoutput. Followingthenotationusedinthischapter,we usextodenotetheinputfeaturevector(ormatrix)ofanNLPtask,andusef(x)todenotethe functionthatislearnedtoprocessx. HerearesomeofthecommontasksinNLP. 1.5.1 Classification SupposethereareasetofclassesorlabelsC. Eachclassisrepresentedbyadistinctintegerin {1,...,|C|}. Aclassificationmodelisafunctionthatmapstheinputxtoa|C|-dimensional vector y, i.e., y =f(x). Each entry of y is a score corresponding to class i, denoted by y(i). Thetaskhereistoassignxtooneormoreclasseshavingthehighestscores. Consider single-labelclassificationasanexample. Thepredictionisgivenbytheequation cˆ = argmaxy(i) (1.90) 1≤i≤|C| wherecˆisthe“best”classassignedtox. Sometimes,oneneedsaprobability-likeoutput(see 60 Chapter 1. Foundations of Machine Learning Section1.2.1). Letψ(·)beafunctionthatnormalizesavectorintoadistribution21. Wethen obtainaprobabilisticclassifier: y = ψ(f(x)) (1.93) Classification may be the most common problem in NLP. There are many applications inadditiontocategorizingdocumentsintopredefinedclasses. Amongthemarechoosinga senseforaword[Yarowsky,1994],determiningthepolarityofasentence[Pangetal.,2002], checking whether two entities should be linked [Krebs et al., 2018], classifying the way of associatingasemanticargumentwithaverb[GildeaandJurafsky,2002],andsoon. When adaptingaclassificationmodeltothesetasks,allyouneedistodesigntheformofxandthe setofclasses. 1.5.2 Sequence Labeling Anextensiontostandardclassificationistoclassifyasetofsamplessimultaneously. Sequence labelingisanexampleofsuchaproblem. Insequencelabeling,theinputisasequenceofn tokens,suchasasequenceofnwords. Asequencelabelingsystemisrequiredtoassigneach input token x(i) a label l(i). Here the boldface in x(i) is used to emphasize that the token is represented as a feature vector. For convenience, we write x(i) as x and l(i) as l . The i i functionf(·)mapsthesequencex ...x intoanothersequencey ...y ,wherey istheoutput 1 n 1 n i vectorcorrespondingtox . Thiscanbeformulatedas: i (cid:104) (cid:105) (cid:104) (cid:105) y ... y = f( x ... x ) (1.94) 1 n 1 n Forvectory ,eachentryy (c)correspondstothepredictionscoreofaclassc∈C. Note i i thatf(·)allowsfortheuseofalargercontext. Forexample,onecanconditiontheprediction y ontheentireinputsequence[Laffertyetal.,2001]. Thefinaloutputofthesystemcanbe i definedasthe“optimal”labelsequenceinducedfromy ...y . Asimplemethodistochoose 1 n thelabelsequencethatmaximizesthesumofthescoresoverallpositions,likethis n (cid:104) (cid:105) (cid:88) ˆl ... ˆl = argmax y (l ) (1.95) 1 n i i l1,...,ln∈C i=1 AstraightforwardapplicationofsequencelabelingtoNLPistotageachtokenoftheinput sequence,suchaspart-of-speechtagging(orPOStagging). Furthermore,sequencelabeling 21AsimilarideacanbefoundinEq.(1.48).Givenavectora= (cid:2) a(1) ... a(n) (cid:3) ,thenormalizationfunction hastheform: (cid:104) (cid:105) ψ(a) = (cid:80)n a(1 a ) (i) ... (cid:80)n",
    "Thefinaloutputofthesystemcanbe i definedasthe“optimal”labelsequenceinducedfromy ...y . Asimplemethodistochoose 1 n thelabelsequencethatmaximizesthesumofthescoresoverallpositions,likethis n (cid:104) (cid:105) (cid:88) ˆl ... ˆl = argmax y (l ) (1.95) 1 n i i l1,...,ln∈C i=1 AstraightforwardapplicationofsequencelabelingtoNLPistotageachtokenoftheinput sequence,suchaspart-of-speechtagging(orPOStagging). Furthermore,sequencelabeling 21AsimilarideacanbefoundinEq.(1.48).Givenavectora= (cid:2) a(1) ... a(n) (cid:3) ,thenormalizationfunction hastheform: (cid:104) (cid:105) ψ(a) = (cid:80)n a(1 a ) (i) ... (cid:80)n a(n a ) (i) (1.91) i=1 i=1 AnotherwayisusingtheSoftmaxfunction: (cid:104) (cid:105) ψ(a) = (cid:80)n exp e ( x a p (1 (a )) (i)) ... (cid:80)n exp e ( x a p (n (a )) (i)) (1.92) i=1 i=1 1.5 NLP Tasks as ML Tasks 61 Tokens: Most are expected to fall below previous- levels . month POS tags: JJS VBP VBN TO VB IN JJ NNS . Chunk tags: B-NP B-VP I-VP I-VP I-VP B-PP B-NP I-NP O NP VP PP NP Figure1.9: AnexampleofsequencelabelingforPOStaggingandchunking. Theexample isfromthetrainingdataoftheCoNLL2000sharedtask. EachtokenislabeledwithaPOS tagandachunktag. Achunktaghasaninitialcharacterchosenfrom{B,I,O}, whereB= beginningofachunk,I=insideachunk,andO=outsideachunk. So,achunkalwaysstarts witha“B”tag,optionallyfollowedby“I”tags. Forexample,theVP(verbphrase)chunkin theexamplespansoverthechunktagsequence“B-VPI-VPI-VPI-VP”. isabletodealwithmorecomplexproblemsbyusinglabelsinacleverway. Awell-known exampleistheuseofthe“IOB”labelformatinidentifyingchunksspanningmultipletokens (callitchunking). Inthismethod,“I”,“O”and“B”standforatokeninsideachunk,atoken outsideachunk,andtheleftmosttokenofachunk[RamshawandMarcus,1995]. Assuch,a chunkalwaysstartswitha“B“andendsjustbeforethenext“B”oranew“O”.SeeFigure1.9 forPOStaggingandchunkingresultsonanexamplesentence. Assequencelabelingallows thelabelingofbothtokensandspans,ithasbeenappliedwithstrongresultstomanytasks, includingPOStagging[BahlandMercer,1976],chunking[TjongKimSangandBuchholz, 2000],namedentityrecognition(NER)[TjongKimSang,2002],andsoon. 1.5.3 Language Modeling/Word Prediction Statistical language modeling (or language modeling for short) is a task of assigning a probabilityPr(w ,...,w )toasequenceofwordsw ...w . Thisjointprobabilityisgenerally 1 n 1 n decomposedintoaproductofconditionalprobabilities,byusingthechainrule: Pr(w ,...,w ) = Pr(w )·Pr(w |w )···Pr(w |w ,...,w ) 1 n 1 2 1 n 1 n−1 n (cid:89) = Pr(w |w ,...,w ) (1.96) i 1 i−1 i=1 Eq. (1.96)describesaprocedurethatgeneratesawordsequencefromlefttoright(call itauto-regressivegeneration). EstimatingPr(w |w ,...,w )isessentiallyamissingword i 1 i−1 predictionproblem: wemaskoutthelastwordofasequenceandguidethelanguagemodelto predictthecorrectwordatthatposition. Seebelowforawordsequencewherethelastwordis missing. Prideandprejudiceisoneofthebestknown We can reuse the idea in classification to model the probability distribution Pr( | 62 Chapter 1. Foundations of Machine Learning Pride,and,...,known). Letx bethevectorrepresentationofw . Wecandefineafunctionthat i i readsx ...x andproducesavectorh : 1 i−1 i h = f(x ,...,x ) (1.97) i 1 i−1 where h is the intermediate states of the word distribution at position i. For a sounding i distribution,wenormalizeh bysomenormalizationfunctionψ(·). Thus,thedistributionat i positioniwouldbe y = ψ(h ) i i = ψ(f(x ,...,x )) (1.98) 1 i−1 Obviously,y (w )istheprobabilityofw givenpreviouswords,i.e.,y (w )=Pr(w |w ,...,w ). i i i i i i 1 i−1 NotethatEq. (1.96)onlyconsiderstheleftcontextwhenpredictingaword. Anaturalextension tothisistoconditionthepredictiononallavailablecontext. Consider,forexample,asentence withamaskedwordinthemiddle. Prideand isoneofthebest-knownnovels Inthisexample,wecanpredictthemaskedwordbyusingboththeleftcontext(Prideand) andtherightcontext(isoneofthebestknownnovels): y = ψ(f(x ,...,x ,x ,...,x )) (1.99) i 1 i−1 i+1 n Thisisabidirectionalmodel,andiscommonlyusedinauto-encodingmethodsforlearning sequencerepresentationmodels[Devlinetal.,2019]. 1.5.4 Sequence Generation SequencegenerationcoversarangeofNLPproblems, includingmachinetranslation, sum- marization, questionanswering, dialoguesystems, andsoon. Usually, itreferstomapping some data to a sequence. Here we focus on the sequence-to-sequence problem, in that a source-sidesequenceistransformedtoatarget-sidesequence,althoughsequencegenerationis notspecializedtoworkwithchainstructuresonthesource-side. For notation convenience, we use boldface variables to denote sequences from now on. (cid:104) (cid:105) For example, a is a sequence of size n. It can be written as either a ... a or a ...a . 1 n 1 n Lets=s ...s andt=t ...t bethesequencestotransformfromandto. Thesequence-to- 1 m 1 n sequenceproblemcanbedescribedasfindingatarget-sidesequencethatmaximizesPr(t|s): ˆt = argmaxPr(t|s) (1.100) t 1.5 NLP Tasks as ML Tasks 63 Likelanguagemodeling,Pr(t|s)canbeformalizedinanauto-regressivefashion: Pr(t|s) = Pr(t ,...,t |s) 1 n = Pr(t |s)·Pr(t |s,t )···Pr(t |s,t ,...,t ) 1 2 1 n 1 n−1 n (cid:89) = Pr(t |s,t ,...,t ) (1.101) i 1 i−1 i=1 Eq. (1.101) differs from Eq. (1.96) only in the additional condition (i.e., s) introduced totheseprobabilities. Inthissense,wecanuseEqs. (1.97-1.98)tosolvePr(t |s,t ,...,t ). i 1 i−1 On the other hand, involving s makes",
    ",...,t ) 1 2 1 n 1 n−1 n (cid:89) = Pr(t |s,t ,...,t ) (1.101) i 1 i−1 i=1 Eq. (1.101) differs from Eq. (1.96) only in the additional condition (i.e., s) introduced totheseprobabilities. Inthissense,wecanuseEqs. (1.97-1.98)tosolvePr(t |s,t ,...,t ). i 1 i−1 On the other hand, involving s makes the problem more difficult, as we need to model the cross-sequence relationship between s and t . A recent trend in sequence generation is to i formulate Pr(t |s,t ,...,t ) in the encoder-decoder paradigm. There are two steps: an i 1 i−1 encoderisfirstusedtorepresentsassomeintermediateform(e.g.,avector),andadecoderis thenusedtomodelboththetarget-sidewordsandthecorrelationbetweentheencoderoutput andthetarget-sidewords. Puttingthesetogether,theoutputoftheencoder-decodermodelcan bedefinedtobe y = Dec(Enc(s),t ,...,t ) (1.102) i 1 i−1 where Enc(·) is the encoder, and Dec(·) is the decoder. y is a distribution of the target- i side word at position i, i.e., y (t )=Pr(t |s,t ,...,t ). Chapter 5 will provide a detailed i i i 1 i−1 descriptionoftheencoder-decodermodel. 1.5.5 Tree Generation InNLP,treesareusuallyusedtorepresentthestructuresormeaningsofsequentialdata. For example,asyntacticparseranalyzesasentencetoformasyntaxtreeorparsetree. More formally,givenasequenceofwordss=s ...s ,theparsingproblemcanbedefinedas: 1 m dˆ = argmaxPr(d|s) (1.103) d∈D wheredisaparsetree,andDisthesetofallparsetreesyieldings ...s . ComputingPr(d|s)is 1 m challenging,asthemodelingcomplexityincreasesexponentiallywhenmovingfromsequences totrees. Instatisticalparsing,asolutionistomodeldasaderivationofsyntacticrules. In thisway,Pr(d|s)canbeformulatedasaproductofruleprobabilities. Figure1.10presents anexampleofparsingwithcontext-freegrammar(CFG)rules. Alternatively,Pr(d|s)canbe modeledinanend-to-endmanner. Forexample,somerecentapproachesperformparsingby defininganeuralnetworkovertheparsetree. Theprobabilityofasub-treerootingatanodeis computedbyconsideringtheinteractionbetweenthisnodeandchildnodes. Anotherideaistoframeparsingassequencegeneration. Forexample,onecanlinearizea parsetreeandrepresentitasasequenceofwordsandsyntacticlabels,ortransformthetree generation process as a sequence of actions. This allows the use of sequence-to-sequence techniquesinaddressingasequence-to-treeproblem. 64 Chapter 1. Foundations of Machine Learning Parse Tree: CFG Rules: S r : PRP → He 1 r : VBD → declined 2 NP VP . r : TO → to 3 r : VB → elaborate PRP VBD S 4 r : . → . 5 He declined VP . r 6 : NP → PRP r : VP → VB 7 TO VP r : VP → TO VP 8 r : S → VP 9 to VB r : VP → VBD S 10 r : S → NP VP . elaborate 11 P(d|s)=P(PRP→He)·P(VBD→declined)·P(TO→to)·P(VB→elaborate)· P(.→.)·P(NP→PRP)·P(VP→VB)·P(VP→TO VP)·P(S→VP)· P(VP→VBDS)·P(S→NP VP .) = (cid:81)11 P(r ) i=0 i Figure1.10: AnexampleparsetreeandCFGrules. Thesentenceisfromthetrainingdataof theCoNLL2000sharedtask. TheparsetreeisrepresentedasaderivationofCFGrules. The probabilityoftheparsetreeisdefinedastheproductofruleprobabilities. InlinguisticsandNLP,treestructuresareinheavyuseforsyntacticanalysis. Inaddition toparsingsentences,theyarealsoattributedtowords,phrases,anddiscourses. Ontheother hand,treesarenottheonlywayofvisualizingcomplexnon-linearstructures. Amoregeneral conceptisagraph. Whiletreescanbethoughtofasspecialgraphs,therearecasesthattrees cannothandle[Fellbaum,2005;Singhal,2005;Banarescuetal.,2013]. Forexample,inthe semanticrepresentationofasentence,weoftenneedagraphtoconnectverbsandarguments. Whilelearninggeneralgraphsisharderthanparsingasentenceintoatree,wecanreusemany ofthemethodsdevelopedinsequenceandtreegeneration. 1.5.6 Relevance Modeling Generallyspeaking,relevanceisreferredtoashowwellathingrelatestoanother. Theconcept ofrelevanceisusedinmanydifferentsub-fieldsofNLPandinformationscience. Forexample, in information retrieval, relevance is used to describe to what extent a retrieved document meetsthequery. Additionalusesofthisconceptcanbefoundinquestionanswering,dialogue systems,semanticanalysis,andallothertasksthatrequireamatchingorretrievalprocess. Letusconsideramoregeneraldescription. Assumethatwehaveaqueryquery andakey key that representssomething we intendto match withquery. Then, we define thefeature 1.5 NLP Tasks as ML Tasks 65 vectorsofquery andkey as q = Q(query) (1.104) k = K(key) (1.105) Q(·)andK(·)arefeatureextractors. Therelevancebetweenquery andkey isgivenbythe function: r = f(q,k) (1.106) f(·)couldbeononehandsimplyadistancemeasureifqandkareinthesamevectorspace, andontheotherhandamorecomplexmodelthatperformssomenon-lineartransformations. Infact,thewayofdefiningrelevancecanbeadoptedinseveraldifferentscenarios. Sometimes, relevanceisalsotermedassimilarityorcorrelation. Ageneralexampleishowsimilartwo objectsare. Letxandy betwosamples(saytwowords). Thesimilarityofxandy isgivenby r = f(g(x),g(y)) (1.107) where g(·) is a feature extractor, and f(·) is a similarity function. Learning both g(·) and f(·)iscalledsimilaritylearning. Inonesetupofsimilaritylearning,wefixf(·)andlearn g(·)inawaythatsimilarsamplesexhibitsimilaroutputsofg(·). Thelearningofthefeature extractorisnotevenrequiredtoworkwiththesimilarityfunction. Forexample,forobtaining thesimilaritybetweenwords,wecanlearng(·)inalanguagemodelanduseittogetherwith varioussimilarityfunctions. Thisputstheprobleminalargertopicofmachinelearning: the learningofasub-modelisindependentoftheproblemwhereweuseit. Suchanideaiswidely adoptedinpre-training,advancingtherecentstate-of-the-artonmanyNLPtasks. Inanothersetupofsimilaritylearning,wecanlearnf(·)directly. Thiscanbeperformedby eitherjointlylearningf(·)andg(·),orlearningf(·)ontopoffixedg(·). Theproblemisalso relatedtometriclearning. Typically,metriclearningisframedasasupervisedproblem[Kulis, 2013]. A desired similarity function could be learned with the supervision regarding some gold-standardsimilarity. However,inpracticethereisusuallynosuchsupervisedinformation inNLP.Inthiscase,onecouldtakerelativedistanceassomesupervision. Forexample,the similarityfunctioncanbelearnedbyoptimizingacontrastiveloss(seeSection1.3.4). Measuringthesimilaritybetweenobjectsplaysanimportantroleinmanymachinelearning methods, such as clustering and nearest neighbor classification. On the side of NLP, it is usefulforexploringtherelationshipbetweenwords,phrases,sentences,anddocuments,e.g., similarity is a way to examine how word vectors correspond to our understanding of word meanings[Mikolovetal.,2013c;Penningtonetal.,2014]. 1.5.7 Linguistic Alignment Linguisticalignmentisasetofproblemswhereweestablishsomecorrespondencebetween twosetsoflinguisticunits. InNLP,thesequence-to-sequenceandsequence-to-treeproblems aretypicallylinguisticalignmentproblems,astheybothconnecttwolinguisticunits. However, 66 Chapter 1. Foundations of Machine Learning",
    "Forexample,the similarityfunctioncanbelearnedbyoptimizingacontrastiveloss(seeSection1.3.4). Measuringthesimilaritybetweenobjectsplaysanimportantroleinmanymachinelearning methods, such as clustering and nearest neighbor classification. On the side of NLP, it is usefulforexploringtherelationshipbetweenwords,phrases,sentences,anddocuments,e.g., similarity is a way to examine how word vectors correspond to our understanding of word meanings[Mikolovetal.,2013c;Penningtonetal.,2014]. 1.5.7 Linguistic Alignment Linguisticalignmentisasetofproblemswhereweestablishsomecorrespondencebetween twosetsoflinguisticunits. InNLP,thesequence-to-sequenceandsequence-to-treeproblems aretypicallylinguisticalignmentproblems,astheybothconnecttwolinguisticunits. However, 66 Chapter 1. Foundations of Machine Learning byconvention,thetermalignmentisreferredtoasaligningmultipleobjectssimultaneously22. As an example, consider the well-known word alignment task: we align the words of asentencetothewordsofanothersentence. WereusethenotationinSection1.5.4asboth thesequence-to-sequenceandwordalignmenttasksperformonapairofsequences. Givena source-sidewordsequences=s ...s andatarget-sidewordsequencet=t ...t ,theword 1 m 1 n alignmentbetweenthetwosequencesisdenotedasanm×nmatrixA. A(i,j)=1ifthereis analignmentlinkbetweens andt ,andA(i,j)=0otherwise. Theoptimalalignmentcan i j bedefinedas: Aˆ = argmaxPr(A|s,t) (1.108) A wherePr(A|s,t)isthewordalignmentprobability. Likeinothermachinelearningproblems, wecanmodelPr(A|s,t)ineitheragenerativeordiscriminativemanner(seeSection1.2.4). Forexample,inBrownetal.[1993]’swork,thewordalignmentmodelisfactoredintoseveral generativesteps,eachaccountingforsomeassumptionsabouttheproblem23. A0-1alignmentmatrixindicatesahardwayofwordalignment. Aproblemhereisthat thehardmodelmaynotdescribewellthehighlyambiguouswordalignments. Wetherefore can represent A as a real-valued matrix (call it a soft word alignment matrix or a word alignmentweightmatrix). Assumethatthesource-sidewordsarerepresentedasasequence (cid:104) (cid:105) of feature vectors x= x ... x . Likewise, the target-side words are represented as 1 m (cid:104) (cid:105) y= y ... y . Asoftwordalignmentmodelisgivenby: 1 n A = a(s,t) (1.109) wherea(·)isanwordalignmentfunctionthatcomputesthealignmentweightA(i,j)foreach pair of x and y . In fact, all the methods discussed in Section 1.5.6 are applicable to the i j design of a(·). This somehow links the modeling of word alignment with the modeling of similarity,andmakesitpossibletoaddressdifferentNLPproblemsbyusingthesamemachine learningapproach. Eq. (1.109) offers a very general way to discover the underlying connection over pairs of variables. In addition to aligning words in sequences, it is useful for aligning unordered objects. Forexample,inbilingualdictionaryinduction,wecanlearnsuchaweightmatrixto estimatehowstrongawordinonelanguagecorrespondstoawordinanotherlanguage. Here is another note on linguistic alignment models. While linguistic alignment could be thought of as an independent NLP task, it is commonly used in designing sub-models of some downstream systems. Many systems that model word-level relationships involve implicit representation of linguistic alignment. As a consequence, linguistic alignment is treatedassomelatentstates,andisaby-productofthesesystems. Forexample,intheearly 22Theconceptofalignmentiswide-ranging.Weusethetermlinguisticalignmentheretodifferentiateitfrom thealignmentoflargelanguagemodelsdiscussedinsubsequentchapters. 23Moreprecisely,Brownetal.[1993]modelPr(A,s|t)whichisasurrogateofPr(A|s,t),asPr(A|s,t)= P P r( r A (s , | s t | ) t) = (cid:80) A P ′ r P (A r( , A s| ′ t , ) s|t) 1.5 NLP Tasks as ML Tasks 67 ageofstatisticalmachinetranslation,wordalignmentisahiddenvariableusedinmodeling the mapping between sequences. The word alignment result can be easily induced from a machine translation model. More recently, neural sequence-to-sequence models — most notablyattentionalmodels[Bahdanauetal.,2014]—haveattemptedtodosomethingsimilar towordalignmentbycomputingattentionweightsamongwords. 1.5.8 Extraction InNLP,extractionisnotakindoftaskbutakindofbehaviorthatasystemexhibits. Informally, it denotes a process of gathering, distilling structured information from some information sources. So,thetermextractiongenerallyappearstogetherwithothertermstoformaspecific task,suchaskeywordextraction,eventextraction,andrelationextraction. Manyofthese taskscanbecategorizedintoanarea—informationextraction. Informationextractionis perhapsthebroadesttopicinNLP.Thereisevennoexhaustivelistofinformationextraction tasks. AccordingtoJurafskyandMartin[2008]’sbook,itincludesbutisnotlimitedtonamed entityrecognition,referenceresolution,relationextraction,eventextraction,templatefilling, andsoon. However, since information extraction is a “miscellany” of many different problems, it cannot be formulated as a single machine learning problem. Fortunately, most of these problems can be framed as standard machine learning problems, such as classification and sequencelabeling,andcanbesolvedbyusingtheoff-the-shelftools. Insomecases,itmay requireaslightupdateofexistingmodelsforadaptationtoanewtask. Forexample,extracting a specific segment from text may require the system to produce a span that indicates the beginningandendingpositionsoftheextractedsegment(callitspanprediction). Onthepracticalside,machinelearningisnotalwaysnecessaryinextractinginformation fromtext. Manyproblemscanbesolvedbyusinghand-craftedrules. Anexampleisusing regularexpressionstoidentifylocationsanddatesintext. Inpracticereal-worldsystemsare usuallycombinationsofheuristicmethodsandautomaticmachinelearningmethods. 1.5.9 Others Figure 1.11 shows illustrative examples of the above NLP tasks. Note that many of the discussionsherearestillpreliminaryandincomplete. Forexample,weonlytalkedaboutNLP problems in the supervised learning paradigm. Many unsupervised tasks are important for NLPresearchaswell. Forexample,itiscommontoclusterunlabeledwordsordocumentsto easetheprocessingindownstreamsystems. Severalmethodsaredirectlyapplicabletothis task[Murphy,2012]. ArecenttrendinNLPisthatitisnotnecessarytosetastrictboundary betweentheuseofsupervisedlearningandtheuseofunsupervisedlearning. Inmanycases, unsupervised methods help supervised tasks, and vice versa. A notable example is that we learnapre-trainedfeatureextractoronunlabeleddataandbuildasupervisedclassifierontop ofit. Thisleadstoanothertrendrunningtowardsimprovingrepresentationmodels(i.e.,feature extractors)withouttheneedofaccessingdownstreamsupervisedtasks. 68 Chapter 1. Foundations of Machine Learning 1.6 Summary ThischapterhasgiventhebasicideasofmachinelearninganditsapplicationstoNLPproblems. Inparticular,wehavepresentedasimpletextclassificationproblemtogetstartedwithmachine learning. Also,wehavediscussedseveralgeneralproblemsonmachinelearning,e.g.,typesof machinelearningmethods,inductivebiases,lossfunctions,overfittingandsoon. Theyare followedbyadiscussiononmodelselectionandassessment. Inaddition,wehavedescribed howmodelNLPproblemsareframedasmachinelearningproblems. However,machinelearningisahugeresearchfield. Thereareseveralinterestingtopicswe",
    "tasks are important for NLPresearchaswell. Forexample,itiscommontoclusterunlabeledwordsordocumentsto easetheprocessingindownstreamsystems. Severalmethodsaredirectlyapplicabletothis task[Murphy,2012]. ArecenttrendinNLPisthatitisnotnecessarytosetastrictboundary betweentheuseofsupervisedlearningandtheuseofunsupervisedlearning. Inmanycases, unsupervised methods help supervised tasks, and vice versa. A notable example is that we learnapre-trainedfeatureextractoronunlabeleddataandbuildasupervisedclassifierontop ofit. Thisleadstoanothertrendrunningtowardsimprovingrepresentationmodels(i.e.,feature extractors)withouttheneedofaccessingdownstreamsupervisedtasks. 68 Chapter 1. Foundations of Machine Learning 1.6 Summary ThischapterhasgiventhebasicideasofmachinelearninganditsapplicationstoNLPproblems. Inparticular,wehavepresentedasimpletextclassificationproblemtogetstartedwithmachine learning. Also,wehavediscussedseveralgeneralproblemsonmachinelearning,e.g.,typesof machinelearningmethods,inductivebiases,lossfunctions,overfittingandsoon. Theyare followedbyadiscussiononmodelselectionandassessment. Inaddition,wehavedescribed howmodelNLPproblemsareframedasmachinelearningproblems. However,machinelearningisahugeresearchfield. Thereareseveralinterestingtopicswe leftout. Onetopicthatwesaidlittleaboutisreinforcementlearning. Ingeneral,reinforcement learningisverypowerful. Itshouldbeandhasbeenconsideredasanapproachtoaddressing NLPproblems,e.g.,trainingasequence-to-sequencebyusingarisk-basedlossfunction. A reinforcementlearningtextbookwillofferthegeneralideasofreinforcementlearning[Sutton andBarto,2018]. AnothertopicwemissedhereisBayesianlearning[Gelmanetal.,2020; McElreath,2020;Downey,2021]. Itopensupanotablestrandofresearchinstatisticallearning, andhasbeensuccessfullyusedinNLPtasks. Moreover,therearemanyothertopicsthatare specializedincertainaspectsofmachinelearningandareofinteresttoNLPresearchersand engineers. Someofthemareefficientmachinelearning[Tayetal.,2020b],multi-tasklearning [ZhangandYang,2021],few-shot/zero-shotlearning[Wangetal.,2019b;2020c],andsoon. Afinalpointtowrapupthischapter. Weskipthedetaileddiscussiononcertainmachine learning models and algorithms, such as classification and regression models, because the reader interested in them can find several excellent, comprehensive introductions [Bishop, 2006; Hastie et al., 2009; Murphy, 2012; Mohri et al., 2018]. In the next chapter we will discussabitmoreaboutartificialneuralnetworkswhicharethebasisofdeeplearningand recentstate-of-the-artNLPmodels. 1.6 Summary 69 classcˆ ˆl1=B ˆl2=I ˆl3=O ˆl4=B ˆl5=B SequenceLabeler Classifierf(x) (cid:2) (cid:3) f( x ... x ) 1 n Samplex(featurevector) x 1 x 2 x 3 x 4 x 5 (a)Classification (b)SequenceLabeling w 5 t 3 LanguageModel Encoder Decoder Pr(w i |w 1 ,...,w i−1 ) Enc(s) Dec(Enc,t1...ti−1) w 1 w 2 w 3 w 4 s 1 s 2 s 3 t 1 t 2 (c)LanguageModeling (d)SequenceGeneration(seq2seq) 0.82 S SimilarityFunc. f(q,k) Parser NP VP Pr(d|s) NNS VBD NN q=Q(query) k=K(key) s 1 s 2 s 3 s1 s2 s3 query key (e)TreeGeneration(seq2tree) (f)RelevanceModeling t t t span=[2,4] 1 2 3 t1 t2 t3 .8 s1 Aligner Extractor 1 .6 s2 a(s,t) Extracteeverythingneeded,e.g.,asegment .7 s3 s s s s s s s s 1 2 3 1 2 3 4 5 (g)LinguisticAlignment (h)Extraction Figure1.11: Naturallanguageprocessingtasksfromamachinelearningperspective. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 2 Foundations of Neural Networks Artificial neural networks (or neural networks, or neural nets for short) are powerful machinelearningtoolsthathaveadvancedthepreviousstate-of-the-artinNLPinrecentyears. However,althoughthehistoryofneuralnetworkscanbetracedbacktothe1940s[McCulloch and Pitts, 1943], for quite a long time neural networks have not been found to consistently outperformothermachinelearningcounterparts. Thechangebeganaround2006when“new” ideasweredevelopedtolearndeepneuralnetworks[Hintonetal.,2006;Hinton,2007]. Such methodshavesincebeenknownasdeeplearning. Todate,deeplearninghasnodoubtbecome oneofthemostactive,influentialareasinartificialintelligence,whileithasreceivedbenefits fromnotonly“deep”modelarchitecturesbutalsomany,manytechniqueswhichhelptolearn andusesuchmodels. In this chapter, we will present the basic ideas of neural networks and deep learning. The chapter is not cutting-edge but covers several important concepts and techniques that arewidelyusedinimplementingneuralsystems. Thisincludesbasicmodelarchitecturesof neural networks, training and regularization methods, unsupervised learning methods, and auto-encoders. Wewillalsopresentanexampleofusingneuralnetworkstosolvethelanguage modelingproblem. 2.1 Multi-layer Neural Networks Togetstarted,wegiveaquickintroductiontosingle-layerperceptrons,andextendthemtoa moregeneralcasewheremultipleneuralnetworksarestackedtoformamorecomplexone. 2.1.1 Single-layer Perceptrons Single-layerperceptrons(orperceptronsforshort)maybethesimplestneuralnetworksthat havebeendevelopedforpracticaluses[Rosenblatt,1957;MinskyandPapert,1969]. Often,it isthoughtofasabiologically-inspiredprogramthattransformssomeinputtosomeoutput. A perceptron comprises a number of neurons connecting with input and output variables. Figure2.1showsaperceptronwherethereisonlyoneneuron. Inthisexample,therearetwo real-valuedvariablesx andx forinputandabinaryvariabley foroutput. Theneuronreads 1 2 72 Chapter 2. Foundations of Neural Networks y neuron: (cid:40) 1 x ·w +x ·w +b>0 1 1 2 2 y= 0 otherwise w 1 w 2 b x 1 x 2 1 Figure2.1: Aperceptronwithtwoinputvariables{x ,x }andanoutputvariabley. Thereare 1 2 twoweights{w ,w }, eachcorrespondingtoaninputvariable. Theoutputdependsonthe 1 2 sumoftheweightedinputvariablesandthebiastermb,say,y=1ifx ·w +x ·w +b>0, 1 1 2 2 andy=0otherwise. theinputvariablesanddetermineswhichoutputvalueischosen. Thisprocedureislikewhata biologicalneurondoes—itreceiveselectrochemicalinputsfromotherneuronsanddetermines iftheelectrochemicalsignalispassedalong. Inamathematicalsense,aperceptroncanbedescribedasamappingfunction. Letxbea vectorofinputvariables(i.e.,afeaturevector). Anaffinetransformationofxisgivenby1: f(x) = x·w+b (cid:88) = x ·w +b (2.1) i i i wherewisaweightvectorandbisabiasterm. Then,astandardperceptroncanbedefinedto be: y = ψ(f(x)) (cid:40) 1 f(x)>0 = (2.2) 0 otherwise whereψ(·)isabinarystepfunction. Anothernameforψ(·)isactivationfunction. Thislinks theperceptrontotheclassificationmodelsdiscussedinSection1. Inotherwords,Eq. (2.2) isaclassifieritself: ψ(·)isadiscriminatefunctiondefinedoneachinputx,followedbyan activationfunctionψ(·)usedforproducingadesirableoutput2. Incasetherearetwoormoreneurons,wecangrouptheseneuronsintoalayer. Asshown inFigure2.2,alltheneuronsinalayerreceivesignalsfromthesameinputfeaturevectorbut areweightedindifferentways. Theoutputofthelayerisanewfeaturevector,eachentryof 1Inmathematics,alineartransformationmapseachvectorvinaspacetof(v)inanotherspace,satisfying foranyvectorsxandy,andscalarsαandβ,wehavef(αx+βy)=αf(x)+βf(y).Anaffinetransformationis alineartransformationfollowedbyatranslation,oftenwrittenintheformf(x)+b. 2Sincethestepfunctionisalinearcombinationofindicatorfunctions,theperceptronisalinearclassifier. 2.1 Multi-layer Neural Networks 73 y y y y 1 2 3 4 w 1 y x w11 w12 w13 w14 (cid:16) y1 y2 y3 y4 =ψ x1 x2 ×",
    "0 otherwise whereψ(·)isabinarystepfunction. Anothernameforψ(·)isactivationfunction. Thislinks theperceptrontotheclassificationmodelsdiscussedinSection1. Inotherwords,Eq. (2.2) isaclassifieritself: ψ(·)isadiscriminatefunctiondefinedoneachinputx,followedbyan activationfunctionψ(·)usedforproducingadesirableoutput2. Incasetherearetwoormoreneurons,wecangrouptheseneuronsintoalayer. Asshown inFigure2.2,alltheneuronsinalayerreceivesignalsfromthesameinputfeaturevectorbut areweightedindifferentways. Theoutputofthelayerisanewfeaturevector,eachentryof 1Inmathematics,alineartransformationmapseachvectorvinaspacetof(v)inanotherspace,satisfying foranyvectorsxandy,andscalarsαandβ,wehavef(αx+βy)=αf(x)+βf(y).Anaffinetransformationis alineartransformationfollowedbyatranslation,oftenwrittenintheformf(x)+b. 2Sincethestepfunctionisalinearcombinationofindicatorfunctions,theperceptronisalinearclassifier. 2.1 Multi-layer Neural Networks 73 y y y y 1 2 3 4 w 1 y x w11 w12 w13 w14 (cid:16) y1 y2 y3 y4 =ψ x1 x2 × w21 w22 w23 w24 b 1 (cid:17) + × 1 b1 b2 b3 b4 x 1 x 2 1 Figure 2.2: A single-layer perceptron involving four neurons. All these neurons receive informationfromtheinputvariables{x ,x }. Theperceptrondescribesaprocessinthat1) 1 2 wefirsttransformtheinputvectorofvariablesbyanaffinetransformationf(x)=x·w+b; 2)andthencomputetheoutputbyfeedingf(x)intotheactivationfunctionψ(·). whichcorrespondstoaneuron. Moreformally,takingψ(·)andf(·)asvectorfunctions,the mathematicalformofthesingle-layerperceptronisgivenbytheequations: y = ψ(f(x)) (2.3) f(x) = x·w+b (2.4) wherex∈Rm,y∈Rn,w∈Rm×n andb∈Rn. Anothernoteontheactivationfunction. Thestepfunction,thoughextensivelyused,isnot theonlyformoftheactivationfunction. Therearemanydifferentwaystoperformactivation. Forexample,wecanusetheSoftmaxfunctionifwewantaprobabilitydistribution-likeoutput; wecanusetheSigmoidfunctionifwewantamonotonic,continuous,easy-to-optimizeoutput; we can use the ReLU function if we want a ramp-shaped output. Table 2.1 shows several commonly used activation functions. Note that, although a layer of neurons equipped with these activations can be loosely called a single-layer perceptron, it can be categorized as a moregeneralconcept,calledasingle-layerneuralnetwork. Ifnotspecifiedotherwise,we willusethetermsingle-layerneuralnetworkthroughoutthisdocument. 2.1.2 Stacking Multiple Layers Anextobviousstepistocreateaneuralnetworkcomprisingmultiplelayers. Todothis,allwe needistostackmultiplesingle-layerneuralnetworkstoformamulti-layerneuralnetwork. SeeFigure2.3foranexample. Inthismulti-layerneuralnetwork,theoutputofeveryneuron ofalayerisconnectedtoallneuronsofthefollowinglayer. Sothenetworkisfullyconnected. Essentially,amulti-layerneuralnetworkdescribesacompositionoffunctions. Forexample, wecanformulatetheneuralnetworkinFigure2.3asafunctionyieldedbycomposingafew simplefunctions: y = Softmax(Sigmoid(ReLU(x·w )·w )·w +b ) (2.5) 1 2 3 3 74 Chapter 2. Foundations of Neural Networks Name Formula(forentryiofavector) Identity y =s i i (cid:40) 1 s >0 i BinaryStep y = i 0 s ≤0 i exp(s )−exp(−s ) i i HyperbolicTangent y = i exp(s )+exp(−s ) i i  1 s >1  i  HardTangent y = s −1≤s ≤1 i i i  −1 s <−1 i 1 Sigmoid(Logistic) y = i 1+exp(−s ) i (cid:40) s s >0 i i ReLU(RectifiedLinearUnit) y = i 0 s ≤0 i Softplus y =ln(1+exp(s )) i i (cid:16) 1 (s −µ )2(cid:17) i i Gaussian y =exp − · i 2 σ2 i exp(s ) i Softmax y = i (cid:80)n exp(s ) i′=1 i′ Maxout y =max(s ,...,s ) i 1 n Table2.1: Activationfunctions(y=ψ(s),wheres,y∈Rn). Allthesefunctionsarevector functions. We show formulas for entry i of the input and output vectors. µ and σ2 are the i i meanandvariancerespectively. wherew ∈R3×4,w ∈R4×3,w ∈R3×3,andb ∈R3 areparameters. 1 2 3 3 Usually,thedepthofaneuralnetworkismeasuredintermsofthenumberoflayers. Itis calledmodeldepthsometimes. Forexample,takingtheinputvectorasanadditionallayer, thedepthoftheexamplenetworkinFigure2.3is4. Arelatedconceptismodelwidth,which istypicallydefinedonalayer,ratherthanontheentirenetwork. Acommonmeasureforthe widthofalayeristhenumberofneuronsinthelayer. Forexample,thewidthoftheoutput layer in Figure 2.3 is 3. If all layers of a neural network are of the same width n, then we cansimplysaythatthemodelwidthisn. Bothmodeldepthandmodelwidthhaveimportant implicationsforthepropertiesoftheresultingneuralnetwork. Forexample,ithasbeenproven thatevenaneuralnetworkwithtwolayersofneuronsandtheSigmoidactivationfunctioncan computeanyfunction[Cybenko,1989]. Forstrongersystems,promisingimprovementsare generallyfavorablewhendeepeningneuralnetworks. Stackinglayersresultsinaverycommonkindofneuralnetwork—feed-forwardneural networks(FFNNs). Thesenetworksarecalled“feed-forward”becausetherearenocyclesin connectionsbetweenlayersandallthedatamovesinonedirection. Wewillseeinthisbook thatmostoftoday’sneuralnetworksarefeed-forward. Afewexceptionswillbepresentedin Section2.3. 2.1 Multi-layer Neural Networks 75 y y y y 1 2 3 w 3 (cid:16) =Softmax × Layer3(Output) b 3 (cid:17) + × 1 (cid:16) (cid:17) 1 =Sigmoid × Layer2 w 2 (cid:16) (cid:17) =ReLU × Layer1 w 1 x x x x 1 2 3 Layer0(Input) Figure 2.3: A multi-layer neural network. The input layer consists of three variables {x ,x ,x }. These variables are fully connected to all neurons of layer 1. The output 1 2 3 oflayer1isanewvectorh =ReLU(x·w ). Itisthenfullyconnectedtolayer2,performing 1 1 the mapping h =Sigmoid(h ·w ). Its output h is fed into layer 3, which generates the 2 1 2 2 finaloutputy=Softmax(h ·w +b ). Theparametersofthisneuralnetworkarew ,w , 2 3 3 1 2 w andb . 3",
    "layer 1. The output 1 2 3 oflayer1isanewvectorh =ReLU(x·w ). Itisthenfullyconnectedtolayer2,performing 1 1 the mapping h =Sigmoid(h ·w ). Its output h is fed into layer 3, which generates the 2 1 2 2 finaloutputy=Softmax(h ·w +b ). Theparametersofthisneuralnetworkarew ,w , 2 3 3 1 2 w andb . 3 3 2.1.3 Computation Graphs Computation graphs are a common way of representing neural networks. As graphs in mathematics,acomputationgraphismadeupofnodesandedgesbetweennodes. Eachnode representseitheramathematicaloperationoravariable,andeachedgerepresentsthedataflow fromonenodetoanother. Socomputationgraphsaredirected3. Consider,forexample,three 3Whileanumberofmachinelearningmodelscanberepresentedasundirectedcomputationgraphs,theyare notthefocusofthisdocument. 76 Chapter 2. Foundations of Neural Networks functions: y = x+w (2.6) y = Softmax(x·w+b) (2.7) y = Sigmoid(x·w +b )−ReLU(x·w ) (2.8) 1 1 2 Figure2.4showsthecomputationgraphsofthesefunctions. Fromtheparsingpointofview,all neuralnetworkscanbeviewedasmathematicalexpressions. Acomputationgraphistherefore therepresentationoftheresultwhenparsingamathematicalexpression. Inthisway,eachnode ofthegraphyieldsasub-expression,andtherootnodeyieldsthewholeexpression. Inacomputationgraph,anodecanbeconnectedtomultiplenodesbeneathitand/orabove it. Thisenablesthereuseofsub-graphsinrepresentingcomplexfunctions. Forexample,inEq. (2.8),thevariablexisusedtwiceandthecorrespondingnodehastwooutgoingedges. Infact, organizingneuralnetworksintocomputationgraphsresemblesthecompositionalnatureof neuralnetworks—typically,alargenetworkisbuiltbycomposingsmallnetworks. TakeEq. (2.8)asaninstance. Itcanberewrittenasasystemofthreeequations: y = h −h (2.9) 1 2 h = Sigmoid(x·w +b ) (2.10) 1 1 1 h = ReLU(x·w ) (2.11) 2 2 Inthecompositionoperation,thenodesofh andh inEq(2.9)arereplacedbythegraphsof 1 2 Eqs. (2.10-2.11). Themainuseofcomputationgraphsisinexecutingthefunction. Thisisexactlythesame thing as predicting the output of a neural network. The method is quite simple. First, the nodes of the graph are topologically sorted such that they are placed in an order consistent withtheinformationflow. Then,giventhevaluesthatarefedintotheinputnodes,thegraphis traversedinawaythat wecomputetheoutputofeachnode andflushittoitsparentnodes. Thefinalresultisgotoutoftheoutputnode. Thisprocedureistypicallycalledaforwardpass. Aforwardpasscanbeefficient,aseverynodeonlyneedstobevisitedonceanditsoutputcan bereusedbymultiplenodeswithouttheneedofrecomputingtheresult. Moreover,aforward passcanbeoptimizedbyreconstructingthegraph. Thiscandevelopthereuseideaabitmore andavoidunnecessarycomputationandmemoryconsumption. Another use of computation graphs is to compute gradients automatically. In training neural networks, it is in general required the partial derivatives of the loss function L with respecttoeveryweightmatrix(w)andeverybiasterm(b),say ∂L and ∂L. Beforeseeinghow ∂w ∂b thesepartialderivativesareusedinupdatingamodel(seeSection2.4.1),though,wefirstgive anideaofcomputingderivativesinacomputationgraph. Forexample,considerthefunction below: y = ψ(x·w +b )·w (2.12) 1 1 2 2.1 Multi-layer Neural Networks 77 y − y Soft. Sigm. ReLU + + y + · b · b · 1 x w x w x w x w 1 2 (a)y=x+w (b)y=Softmax(x·w+b) (c)y=Sigmoid(x·w +b )− 1 1 ReLU(x·w ) 2 Figure2.4: Computationgraphsofthreeexampleneuralnetworks. Theblackboxesrepresent themathematicaloperations,andthecoloredboxesrepresentthevariables. Amathematical operationnodehasincomingedgesfromothernodes,andeachofthesenodescanbetreatedas anargumentoftheoperation. Forexample,insub-figure(a),theadditionnodehastwochild nodeslabeledwithxandwrespectively. Thisnodereadstheoutputofthenodesxandw,and generatestheoutputy=x+w. Thingsareabitinterestingforlargergraphs. Insub-graph(b), theoutputofthedotnode(i.e.,x·w)ispassedalongtheedgetotheadditionnode. Then,the additionnodecomputesthesumofx·wandbasitsoutput. Wecanrepeatthesameprocess overallthemathematicaloperationnodesinabottom-upmanner,andgetthefinalresultof computingthewholeexpressionoutofthetop-mostnode. To obtain ∂L , ∂L and ∂L , it is natural to use the chain rule of differentiation. For ∂w1 ∂b1 ∂w2 example,foracompositefunctiony=p(q(x)),theformulaofthechainruleisgivenas: ∂y ∂p ∂q = · (2.13) ∂x ∂q ∂x ButtheanalyticformulaofaderivativebasedonEq. (2.13)wouldmakealengthyequation. 78 Chapter 2. Foundations of Neural Networks Instead,wecandecomposeacomplexfunctionintoseveralfunctions,eachstandingforsome operation. Then,Eq. (2.12)canberewrittenas: y = h ·w (2.14) 1 2 h = ψ(h ) (2.15) 1 2 h = h +b (2.16) 2 3 1 h = x·w (2.17) 3 1 All these variables can be understood in a better way from a computation graph: each variableisanodeofthegraph,andnodesareconnectedbyalgebraicoperationsandfunction compositions. Taking Eq. (2.13) and some basic knowledge of calculus, we compute the derivativesofthevariables,likethese: ∂L node1:= = δ (2.18) y ∂y ∂L ∂L node2:= = ·wT (2.19) ∂h ∂y 2 1 ∂L ∂L node3:= = hT· (2.20) ∂w 1 ∂y 2 ∂L ∂L node4:= = ⊙ψ′(h) (2.21) ∂h ∂h 2 1 ∂L ∂L node5:= = (2.22) ∂h ∂h 3 2 ∂L ∂L node6:= = (2.23) ∂b ∂h 1 2 ∂L ∂L node7:= = ·wT (2.24) ∂x ∂h 1 3 ∂L ∂L node8:= = xT· (2.25) ∂w ∂h 1 3 where δ is the derivative of the loss with respect to the model output. δ depends on the y y choiceofthelossfunction,e.g.,ifweusethesquaredlossL= 1(y−y )2,wherey is 2 gold gold thebenchmark,thenδ =y−y .",
    "∂L ∂L node7:= = ·wT (2.24) ∂x ∂h 1 3 ∂L ∂L node8:= = xT· (2.25) ∂w ∂h 1 3 where δ is the derivative of the loss with respect to the model output. δ depends on the y y choiceofthelossfunction,e.g.,ifweusethesquaredlossL= 1(y−y )2,wherey is 2 gold gold thebenchmark,thenδ =y−y . Theaboveprocessisessentiallyabackwardpass,asthe y gold gradientsarepassedinatop-downfashion. Anothernameforthisiserror-propagation. It hasbeenthedefactostandardfortrainingdeepneuralnetworks. Forabetterunderstandingof howforwardandbackwardpasseswork,Figure2.5showstworunningexamples. 2.2 Example: Neural Language Modeling Languagemodelingisawell-knownNLPtaskthatestimatesaprobabilitydistributionover sequencesofwords. Givenasequenceofmwordsw ...w ,theprobabilityPr(w ,...,w )is 1 m 1 m 2.2 Example: Neural Language Modeling 79 1 y(Output) 1 LossL ∂L ∂y 8 8 1 1 y=h ·w · y=h ·w · 1 2 1 2 ∂L = ∂L·wT ∂L =hT·∂L ∂h1 ∂y 2 ∂w2 1 ∂y 2 3 2 3 h 1 =ψ(h 2 ) ψ(·) w 2 h 1 =ψ(h 2 ) ψ(·) w 2 ∂L = ∂L ⊙ψ′(h) ∂h2 ∂h1 4 4 h =h +b + h =h +b + 2 3 1 2 3 1 ∂L = ∂L ∂L = ∂L ∂h3 ∂h2 ∂b1 ∂h2 5 6 5 6 h =x·w · b h =x·w · b 3 1 1 3 1 1 ∂L = ∂L ·wT ∂L =xT· ∂L ∂x ∂h3 1 ∂w1 ∂h3 7 8 7 8 x w x w 1 1 (a)ForwardPass (b)BackwardPass Figure2.5: Theforwardpassandbackwardpassforanexamplecomputationgraph. Inthe forwardpass(left),thenodesarevisitedinanorderfromtheinputtotheoutput,say,fromnode 8to1. Oneachnode,weexecutethecorrespondingfunction,suchasaddition,togeneratethe output,whichisthenconsumedbythesubsequentnodes. Incontrast,inthebackwardpass (right),thenodesarevisitedinthereverseorder,say,fromnode1to8. Duringthisprocess,we passthegradientoftheloss(orerror)fromtheoutputtotheinput,thatis,foreachnode,we computethegradientattheinputpointofthenodebyusingthechainrule,giventhegradient attheoutputpointofthenode. givenbytheequation: m (cid:89) Pr(w ,...,w ) = Pr(w |w ,...,w ) (2.26) 1 m i 1 i−1 i=1 As such, the language modeling problem is framed as predicting the next word given all previouscontextwords. AsimplemethodofmodelingPr(w |w ,...,w )istoconditionthe i 1 i−1 80 Chapter 2. Foundations of Neural Networks predictiononacontextwindowthatcoversatmostacertainnumberofwords,likethis: Pr(w |w ,...,w ) ≈ Pr(w |w ,...,w ) (2.27) i 1 i−1 i i−n+1 i−1 where n is the window size. One way to estimate the probability is the n-gram language modeling approach: we compute the relative frequency for each n-gram w ...w , i.e., i−n+1 i Pr(w |w ,...,w )= count(wi−n+1...wi) . While n-gram languagemodels have domi- i i−n+1 i−1 count(wi−n+1...wi−1) natedtheNLPfieldforalongtime,theyusuallyrequirehugetablesforrecordingallthose n-gramprobabilities. Inconsequence,themodelswillbeverysparseifmoreandmoretexts areusedintrainingsuchmodels. Thisisalsoknownasakindofthecurseofdimensionality. Hereweconsiderneuralnetworksinaddressingthelanguagemodelingproblem[Bengio et al., 2000; 2003b]. Unlike n-gram language models, neural language models do not generalizeinadiscretespacethatrequiresanexponentiallylargenumberofdistinctfeature vectorsasmorewordsandalargecontextareinvolved,butinacontinuousspacethatencodes wordsviadense,low-dimensionalrealvectors. Inparticular,afeed-forwardnetworkisutilized heretopredicthowlikelyw occursgivenw ...w . i i−n+1 i−1 Figure2.6presentsthearchitectureofthefeed-forwardneuralnetworkbasedlanguage model(FFNNLM).Theinputisthecontextwordsw ...w . Eachisadiscretevariable i−n+1 i−1 choosingvaluesfromavocabularyV. Sincetheneuralnetworkoperatesonvectors,allwords arevectorizedasone-hotrepresentations. Inthiscase,thewordw=V isa|V|-dimensional k vectorinwhichentryk is1andotherentriesareall0. Forexample, consideravocabulary V ={“I”,“you”,“he”,“’she”,“they”}. Theone-hotrepresentationof“you”is (cid:104) (cid:105) w(“you”) = 0 1 0 0 0 (2.28) Whiletheone-hotvectorsmakewordrepresentationsdistinguishable,itmaynotappear thatwecangaintoomuchbythisbecausesuchrepresentationscannotdescribethecloseness betweenwords,e.g.,similarwordsshouldtendtobecloseinthevectorspace. Ifwerelaxthe indicator-based representations to real-valued representations, then it turns out that we can obtainsomewordrelationshipbycomputingsimilaritiesbetweenthesevectors. Tothisend, aneffectivetechniqueistotransformone-hotrepresentationstodistributedrepresentations. Moreformally,letxbeaone-hotvectorofawordw. Thedistributedrepresentationofthe wordisareal-valuedvector,givenby: e = x·C (2.29) where the representation e is a vector ∈ Rde, and d e is the number of dimensions of the representation. Each dimension of e can be viewed as some countable aspect of the word, thoughitisnotrequiredtobeinterpretedbylinguistics. Cisa|V|×d matrix,ofwhichthe e k-th row corresponds to the vector for V . Hence, w·C is to “select” a row from C. For k 2.2 Example: Neural Language Modeling 81 Pr(·|w ,w ,w ) 1 2 3 OutputLayer y=Softmax(h ·W +B ) 1 2 2 (Layer3) h 1 0 .9 .6 .6 −1 HiddenLayer h =TanH(h ·W +B ) 1 0",
    "V . Hence, w·C is to “select” a row from C. For k 2.2 Example: Neural Language Modeling 81 Pr(·|w ,w ,w ) 1 2 3 OutputLayer y=Softmax(h ·W +B ) 1 2 2 (Layer3) h 1 0 .9 .6 .6 −1 HiddenLayer h =TanH(h ·W +B ) 1 0 1 1 (Layer2) h 0 −1 .2 .8 0 7 .3 .2 8 1 e 1 −1 .2 .8 e 2 0 7 .3 e 3 .2 8 1 EmbeddingLayer e =x ·C e =x ·C e =x ·C 1 1 2 2 3 3 (Layer1) 0 1 0 0 0 0 0 1 1 0 0 0 x (w ) x (w ) x (w ) 1 1 2 2 3 3 Figure2.6: Aneurallanguagemodel[Bengioetal.,2003b]. Blueboxesrepresentthelayers of the neural network. The input is three context words in their one-hot representations {x ,x ,x },andtheoutputistheprobabilitydistributionofthenextwordPr(w |w ,w ,w ). 1 2 3 4 1 2 3 First,anembeddinglayerisusedtomapeachwordintothedistributedrepresentation(i.e.,the wordembedding). Theembeddingsofthesewordsareconcatenatedtoformabiggervector h suchthattheconcatenatedvectorencodesallinputinformation. Then,h istakenasthe 0 0 inputtoanormallayerthatperformsthemappingh =TanH(h ·W +B ). Thefinallayer 1 0 1 1 readsh andproducesadistributionoverthevocabulary,i.e.,y=Softmax(h ·W +B ) 1 1 2 2 wherey =Pr(V |w ,w ,w ). k k 1 2 3 example,givenC∈R5×3,thedistributedrepresentationof“you”isgivenby: e(“you”) = w(“you”)·C   73 12 0.1 12 0.5 18 (cid:104) (cid:105)   = 0 1 0 0 0 ·37 0.7 28     61 0.4 23   62 11 0.4 (cid:104) (cid:105) = 12 0.5 18 (2.30) 82 Chapter 2. Foundations of Neural Networks Eq. (2.29)impliesanideaoflearningtorepresentwords,leadingtoabigdevelopment of NLP. Typically, the vector e is called the word embedding, and the parameter matrix C is called the embedding matrix. A number of methods may be used for learning word embeddings,thoughwewilltendtonotfocusonsuchmethodsinthischapter. Thereadercan refertoChapter3foramoredetaileddiscussiononthistopic. Toencodethecontextwords{w ,...,w }(or{x ,...,x }),asimplemethodisto i−n+1 i i−n+1 i concatenatethewordembeddings{e ,...,e }asanewvectorh : i−n+1 i−1 0 h = [e ,...,e ] 0 i−n+1 i−1 Thenextpartofthemodelisa2-layerfeed-forwardneuralnetwork. Thefirstlayer,called ahiddenlayer,isastandardlayerofneurons,followedbythehyperbolictangentactivation function. Thelayerproducesad -dimensionalvector: h h = TanH(h ·W +B ) (2.31) 1 0 1 1 Thesecondlayeristheoutputlayer. ItproducesadistributionoverV. Thiscanbeformulated as: Pr(·|w ,...,w ) = Softmax(h ·W +B ) (2.32) i−n+1 i−1 1 2 2 The parameters of the model are C∈R|V|×de, W 1 ∈R(n−1)de×d h, B 1 ∈Rd h, W 2 ∈ Rd h ×|V|,andB 2 ∈R|V|. Apopularwaytooptimizetheseparametersistominimizethecross- entropylossviagradientdescent. Additionally,trainingcanbeimprovedviaregularization. ThesemethodswillbediscussedinSections2.4and2.5. Afewremarksontheneurallanguagemodel. First,byusingdistributedfeaturevectors, “senses”canbesharedinpartbydifferentwords. Thisenableslearnablewordsensesbywhich thesimilaritybetweenwordsisimplicitlyconsidered. Anadvantageofsuchamodelisthat a small change in word vectors would not lead to a big change in the result. For example, supposewehaveseen“grapesarefruits”manytimesbuthaveneverseen“peachesarefruits”. If “grapes” and “peaches” are close in the vector space, then we would say that n-grams “grapesarefruits”and“peachesarefruits”aresomethingsimilar. Thisdifferentiatesneural languagemodelsgreatlyfromn-gramlanguagemodelsinwhichdifferentsurfaceformsmean differentmeanings. Second,thedenserepresentationofwordsmakesasmallermodel. Forexample,acommon setting of d and d is less than 1000, making the number of parameters under control. By e h contrast,thesizeofann-gramlanguagemodelincreasesbyafactorof|V|asnincreases. For example,therewillbeahugetableofprobabilitiesforacommonvocabularyifnislargerthan 3. Third,theneurallanguagemodeliscomputationallyexpensivebecauseoftheheavyuseof vectorandmatrixoperations,suchasmatrixmultiplication. Thisisacommonproblemwith mostofdeepneuralnetwork-basedsystems. Acommonsolutionistobreakthecomputation problemintoindependentsub-problemssothatthesesub-problemscanbehandledinparallel. 2.3 Basic Model Architectures 83 At a lower level, one can use GPUs or other parallel computing devices to speed up linear algebraoperators. Atahigherlevel,onecandistributepartsofthemodelorpartsofthedatato multipledevicesformodel-levelortask-levelspeed-ups. 2.3 Basic Model Architectures Wenowdescribe,inmoredetail,severalbasicbuildingblocksforneuralnetworks. Theyare widelyusedindevelopingstate-of-the-artneuralmodelsinNLP. 2.3.1 Recurrent Units Recurrentneuralnetworks(RNNs)areaclassofneuralnetworksthatreadand/orproduce sequentialdataortimeseriesdata. Aswithafeed-forwardneuralnetwork,anRNNcomprises layersofneuronsandconnectionsbetweenneurons[Hopfield,1984;Rumelhartetal.,1986; WilliamsandZipser,1989;Elman,1990]. Someoftheneuronsareusedasa“memory”that keeps the state of the problem when the processing moves on along a sequence of signals. Asaresult,itisstraightforwardtouseRNNstodealwithvariablelengthproblems,suchas machinetranslationandspeechrecognition. ThemainideabehindRNNsistorepeatedlyutilizearecurrentunit(orrecurrentcell) to compute the output at each position of",
    "to speed up linear algebraoperators. Atahigherlevel,onecandistributepartsofthemodelorpartsofthedatato multipledevicesformodel-levelortask-levelspeed-ups. 2.3 Basic Model Architectures Wenowdescribe,inmoredetail,severalbasicbuildingblocksforneuralnetworks. Theyare widelyusedindevelopingstate-of-the-artneuralmodelsinNLP. 2.3.1 Recurrent Units Recurrentneuralnetworks(RNNs)areaclassofneuralnetworksthatreadand/orproduce sequentialdataortimeseriesdata. Aswithafeed-forwardneuralnetwork,anRNNcomprises layersofneuronsandconnectionsbetweenneurons[Hopfield,1984;Rumelhartetal.,1986; WilliamsandZipser,1989;Elman,1990]. Someoftheneuronsareusedasa“memory”that keeps the state of the problem when the processing moves on along a sequence of signals. Asaresult,itisstraightforwardtouseRNNstodealwithvariablelengthproblems,suchas machinetranslationandspeechrecognition. ThemainideabehindRNNsistorepeatedlyutilizearecurrentunit(orrecurrentcell) to compute the output at each position of an input sequence. To be more precise, given a sequenceofvectorsx ...x ,astandardrecurrentunitcanbedescribedasafunctionRNN(·) 1 m thatconsumesaninputx andastates ateachtimeandgeneratesanewstates ,likethis: i i−1 i s = RNN(s ,x ) (2.33) i i−1 i Thestates canbeviewedasa“memory”thatsummariesthepastdata,andwouldbeupdated i whenthenewdatacomes. SeeFigure2.7(a)forvisualizationofEq. (2.33). Thecirclehere indicatesthereuseoftherecurrentunit. ThiscanbeunderstoodbyrewritingEq. (2.33)ina sequenceofcallsofthefunctionRNN(·): RNN(s ,x ) = RNN(RNN(s ,x ),x ) i−1 i i−2 i−1 i = RNN(RNN(RNN(s ,x ),x ),x ) i−3 i−2 i−1 i = ... = RNN(RNN(...(RNN(s ,x ),x )...x ),x ) (2.34) 0 1 2 i−1 i Figure 2.7 (b) shows the structure of this network. This is sometimes referred to as an unrolled (or unfolded) structure of RNNs. Basically, Figures 2.7 (a) and (b) are the same thing. WhilearolledRNNhasasimpleandwell-explainedform,anunrolledRNNismore suitableforvisualizingthedataflowthroughthenetwork. So,wewillusetheunrolledversion ofRNNsthroughoutthisdocument. Moreover,itisworthnotingthatanunrolledRNNisin factadeepfeed-forwardneuralnetwork. Forexample,eachuseoftherecurrentunitcreatesa “layer”thatreceivesinformationfromaprevious“layer”. Inthissense,anRNNisastackof 84 Chapter 2. Foundations of Neural Networks s s s s i i−1 i i+1 RNN RNN RNN RNN x x x x i i−1 i i+1 (a)AnRNNunit (b)UnrollingtheRNN Figure2.7: ExampleofRNN(rolledvsunrolled). AnRNNunitreadstheinputateachtime stepiandtheoutputatthelasttimestepi−1,andproducesanewoutputs . Assuchwecan i reusethesameRNNunittomakepredictionsoverasequenceofinputs(seesub-figure(a)): for eachi,thecurrentinputx andlastoutputs areconsumedandmappedtotheoutputthatis i i−1 fedintothesameRNNunitfortheprocessingatthenexttimestep. Abetterwaytovisualize theRNNistounrollitintoanetworkwithnocycles(seesub-figure(b)). TheunrolledRNN canberegardedasadeepfeed-forwardneuralnetworkinthatallRNNunitssharethesame setofparameters. layers,say,stackinglayersfromlefttoright. AbenefitoftreatingRNNsasdeepfeed-forward neural networks is that one can use the same methods to train and deploy the two types of neural networks. An example is that both RNNs and feed-forward neural networks can be trainedbytheerror-propagationtoolprovidedwithinacommonoptimizer. ThereareanumberofRNNvariants,differinginwaysofdefiningRNN(·). Thesimplest oftheseistoformulateRNN(·)asasingle-layerneuralnetwork. Assumethats andx are i−1 i inRd h. TheformofRNN(·)isgivenby: RNN(s ,x ) = ψ(s ·U+x ·V) (2.35) i−1 i i−1 i whereU∈Rd h ×d h andV∈Rd h ×d h areparameters. Thecommonchoicesfortheactivation functionψ areTanH(·),Sigmoid(·),ReLU(·),andamongothers. Eq. (2.35)isasingle-layer neuralnetworkbecauseithasthesameformasEqs. (2.3-2.4): ψ(s ·U+x ·V) = ψ([s ,x ]·W) (2.36) i−1 i i−1 i where[s i−1 ,x i ]istheconcatenationofs i−1 andx i ,andW∈R2d h ×d h istheparametermatrix (cid:34) (cid:35) U thatisformedby . V RNNsoftenworkasapartofamodel. Forexample,theinputofarecurrentunitcouldbe eitherarepresentationofrealdataoranoutputofanotherneuralnetwork. Also,wecanstack otherneuralnetworksontopofarecurrentunit. Forexample,inmanyreal-worldsystems,an 2.3 Basic Model Architectures 85 additionallayerisgenerallystackedons forprojectingittoadesirableoutput. i 2.3.2 Convolutional Units Convolutional neural networks (CNN) are another well-known class of neural networks [Waibeletal.,1989;LeCunetal.,1989]. Inabiologicalsense,theyareinspiredbyhuman vision systems: neurons react to the stimulus in a certain vision region or patch (call it the receptivefield)[HubelandWiesel,1959]. InCNNs,thereceptivefielddescribestheregion intheinputspacethatisinvolvedingeneratingtheoutputforaneuron. CNNsaretherefore “partiallyconnected”modelsinwhicheachneurononlyconsidersinputfeaturesinarestricted region. This differentiates CNNs from fully connected feed-forward neural networks. In general,CNNscanresemblethehierarchicalnatureoffeaturesdescribingdataandscalebetter incomplexity. WhileCNNshavemanyapplicationsinprocessing2Ddata,suchasimageclassification, we discuss them here in a sequential data processing scenario for a consistent treatment of theprobleminthischapter. Typically, aCNNconsistsofaconvolutionallayer, apooling layer,andotherlayersoptionally. Itbeginswiththeconvolutionallayerwherethereceptive fieldisdefinedbyasetofconvolutionkernelsorfilters. Afilterisalinearmappingfunction thatconvolvestheinputfeaturesinthereceptivefieldtoformanoutputfeature. Forexample, considerasequenceofnumbersx ...x . Thefilterrangingfrompositionitopositioni+r−1 1 m isdefinedtobe: υ = Conv(x ,W) [i,i+r−1] = x ·W [i,i+r−1] r−1 (cid:88) = x ·W (2.37) i+k k k=0 where r is the size of the receptive field, x is the sub-sequence x ...x , and [i,i+r−1] i i+r−1 W∈Rr istheparametersofthefilter. Then,asequenceofoutputfeaturescanbegenerated bymovingthefilteralongtheinputsequence. Letstridebethedistancebetweenconsecutive moves. Theoutputformoveiisthendefinedtobe: υ = Conv(x ,W) (2.38) i [stride×i,stride×i+r−1] In this way, the convolutional layer transforms the input sequence x ...x to the output 1 n sequenceυ 1 ...υ ⌊ m ⌋ 4. AremarkhereisthattheparametersWaresharedacrosspositionsof stride theinputsequence. Thismethodisknownasparametersharingorweightsharing. Parameter sharingmakesaCNNefficientbecauseitrequiresfewerparametersthanafeed-forwardneural networkgiventhesamenumberofneurons. Aproblemwiththeaboveformulationisthattheuseofthefiltermaynotbetiledtofitthe inputsequence. Forexample,whenstride×i+r−1>m,theinputofthefilterisincomplete. Acommonlyusedsolutionispadding. Itsimplysetsthefeaturesoutsidetheinputsequence 4⌊·⌋standsforthefloorfunction. 86",
    "Then,asequenceofoutputfeaturescanbegenerated bymovingthefilteralongtheinputsequence. Letstridebethedistancebetweenconsecutive moves. Theoutputformoveiisthendefinedtobe: υ = Conv(x ,W) (2.38) i [stride×i,stride×i+r−1] In this way, the convolutional layer transforms the input sequence x ...x to the output 1 n sequenceυ 1 ...υ ⌊ m ⌋ 4. AremarkhereisthattheparametersWaresharedacrosspositionsof stride theinputsequence. Thismethodisknownasparametersharingorweightsharing. Parameter sharingmakesaCNNefficientbecauseitrequiresfewerparametersthanafeed-forwardneural networkgiventhesamenumberofneurons. Aproblemwiththeaboveformulationisthattheuseofthefiltermaynotbetiledtofitthe inputsequence. Forexample,whenstride×i+r−1>m,theinputofthefilterisincomplete. Acommonlyusedsolutionispadding. Itsimplysetsthefeaturesoutsidetheinputsequence 4⌊·⌋standsforthefloorfunction. 86 Chapter 2. Foundations of Neural Networks v =Conv([x ,x ,x ]·W) 2 1 2 3 =x ·W +x ·W +x ·W 1 1 2 2 3 3 ConvolutionalLayer v 1 v 2 v 3 v 4 Input 0 x 1 x 2 x 3 x 4 0 Padding ReceptiveField Padding Figure2.8: Convolutionoverasequenceofnumbers{x ,x ,x ,x }(r=3andstride=1). 1 2 3 4 Thereceptivefielddefinestheregionintheinputthatistakenincomputingtheoutput. Here thereceptivefieldhasasizeof3,thatis,theconvolutionaloperationcoversthreeconsecutive numbersintheinputsequence. Thefilter(orconvolutionalkernel)outputsaweightedsumof thesenumbers. Eachtimeweslidethereceptivefieldovertheinput,thefiltergeneratesanew output. Assuch,theoutputoftheconvolutionallayerisavectorofnumbers. Also,apadding number(i.e. 0)isaddedtoeachendofthesequencesothattheconvolutionisfeasiblewhen thereceptivefieldisincomplete. to a constant. For example, we can attend dummy feature vectors (say 0) to each end of the sequence so that all convolution operations are feasible. See Figure 2.8 for an example filtercomputedoverasequenceofnumbers. Notethatthereceptivefieldsofdifferentfilter applicationsmayoverlap. Thisisbeneficialsometimesbecauseitreducesinformationlossin featurerepresentationwhenalow-levelfeatureisusedinformingmultiplehigh-levelfeatures. Inaddition,aconvolutionallayercaninvolveanactivationfunctionψ(·)toperformsome non-linearmappingonthefilteroutput. Letm =⌊ m ⌋bethenumberoffilterapplications. k stride Theoutputofaconvolutionallayerisgivenby (cid:104) (cid:105) (cid:104) (cid:105) h ... h = ψ( υ ... υ ) (2.39) 1 m 1 m k k Ingeneral,aconvolutionallayermaynotberestrictedtoascalar-basedinputorasingle filter. Often,wecantakeavectorastherepresentationofatokenintheinputsequence,and takeasetoffiltersasfeatureextractors. Tothisend,wecanadoptthesameformulationasin Eqs. (2.37-2.39),butreplacex ,υ andh bythevectorizedcounterparts. i i i Aconvolutionallayeristypicallyfollowedbyapoolinglayer. Likeconvolution,pooling is a function that sweeps a filter on a sequence. But the pooling operation does not have anyparameters. Itcanbeinsteadthoughtofasanaggregationfunctionthatperformsdown- sampling on the input sequence. There are several ways to design a pooling function. One of the most common methods is max pooling which outputs the maximum value in the 2.3 Basic Model Architectures 87 Pooling Filter2 Filter1 0 padding 0 x6 x5 Features 0 x1 x2 x In 3 x pu 4 t Sequence 0 Figure2.9: ExampleofCNN.Therearetwofiltersfortheconvolutionallayer. Theinputis asequenceof6tokensrepresentedintheirfeaturevectors{x ,...,x }. Totilethefiltersto 1 6 fittheinputsequence,twopaddingvectorsareattachedtoeachendofthesequence. When applyingafilter,wemapthefeaturevectorsinthereceptivefieldtoanewfeaturevector. For example, for filter 1, the receptive field is a 6×3 rectangle in the input, and the output is a 2-dimensionalfeaturevector. Bysweepingthefilteronthesequence,weobtainasequenceof featurevectors,say,asequenceof8featurevectors,eachhaving2features. Thepoolinglayer fuzesfeaturesalongthesequence. Forexample,performingthepoolingontheoutputoffilter 1resultsin2fuzedfeatures. ThefinaloutputoftheCNNistwo2-dimensionalfeaturevectors. receptivefield. Anothermethodisaveragingpoolingwhichoutputstheaveragedvalueover thereceptivefield. ForacompletepictureofhowaCNNworks,Figure2.9depictsarunning examplewhereconvolutionalandpoolingoperationsareperformedonasequenceoffeature vectorsvia2filters. 2.3.3 Gate Units Inneuralnetworks,agateisusedtodecidehowmuchinformationispassedalong[Hochreiter and Schmidhuber, 1997]. Consider a standard RNN as an example. At each time step i, insteadofdirectlypassingthepreviousstates i−1 ∈Rd h totherecurrentunit,itmightbemore 88 Chapter 2. Foundations of Neural Networks interesting to see how much information in s is useful for a next-step decision. In this i−1 case,wewants tobemorelikearealmemory: asthetimegoesby,somethingshouldbe i−1 memorized,andsomethingshouldbeforgotten. A way to achieve this goal is to introduce a coefficient for controlling the scale of data flow. HerewereusethenotationinRNNs(seeSection2.3.1),butourdescriptionisgeneral andcouldbeappliedtoallthecasesthatneedsuchamethod. Letz∈[0,1]d h beacoefficient vector, where z =0 means that nothing is memorized for dimension i, and z =1 means i i that everything is memorized for dimension i. We can set z as a gate on s . This can be i−1 formulatedas: Gate(z,s ) = z⊙s (2.40) i−1 i−1 where⊙istheelement-wiseproductoftwovectorsormatrices. Gate(z,s )isanupdateof i−1 s . Thus,zcanbecalledanupdategate,oraforgetgate,orsomethingsimilar. Alternatively, i−1 wecandefinethegatingfunctioninanotherway: Gate(z,s ) = (1−z)⊙s (2.41) i−1 i−1 Eqs. (2.40)and(2.41)basicallytellthesamestorybuthavedifferentinterpretationsforzin practice. Thekeyproblemhereishowtoobtainz. Ageneralmethodistodefinezastheoutputof anothernetwork. Forexample,forarecurrentunit,zcanbedefinedtobe: z = Sigmoid(s ·W +x ·W +B) (2.42) i−1 1 i 2 TheuseoftheSigmoidactivationfunctionguaranteesthattheoutputfallsintotherangeof [0,1]. NotethatEq. (2.42)describesalearnablegate. Thisinturnmakesthegateapartofthe modelandcanbetrainedtofitthedata. Thereareanumberofmethodstodesignagate,and wewillseeafewinChapter4. 2.3.4 Normalization (Standardization) Units Aneuralnetworkworksbytransformingfeaturevectorslayerbylayer. Whilethemulti-layer, multi-dimensionalnatureofneuralnetworksenablesthemodelstocomputecomplexfunctions, itmightleadtoverydifferentdistributionsofoutputactivationsacrosslayersorfeatures. This isaproblemwithdeepneuralnetworksbecauseamodelofthiskindhastoadapttodifferent distributionsoverdifferentlayersordifferentfeatures[IoffeandSzegedy,2015]. Sometimes, asmodelparametersareinitializedrandomlyinalllayersandinallfeaturedimensions,itis likelyforsomefeaturestobelargevalues. Inthiscase,themodelwouldbebiasedtothose largevaluefeatures. Awaytomitigatethisissueisnormalization,whichstandardizesann-dimensionalfeature vectorsas s−µ Normalize(s)",
    "(2.41) i−1 i−1 Eqs. (2.40)and(2.41)basicallytellthesamestorybuthavedifferentinterpretationsforzin practice. Thekeyproblemhereishowtoobtainz. Ageneralmethodistodefinezastheoutputof anothernetwork. Forexample,forarecurrentunit,zcanbedefinedtobe: z = Sigmoid(s ·W +x ·W +B) (2.42) i−1 1 i 2 TheuseoftheSigmoidactivationfunctionguaranteesthattheoutputfallsintotherangeof [0,1]. NotethatEq. (2.42)describesalearnablegate. Thisinturnmakesthegateapartofthe modelandcanbetrainedtofitthedata. Thereareanumberofmethodstodesignagate,and wewillseeafewinChapter4. 2.3.4 Normalization (Standardization) Units Aneuralnetworkworksbytransformingfeaturevectorslayerbylayer. Whilethemulti-layer, multi-dimensionalnatureofneuralnetworksenablesthemodelstocomputecomplexfunctions, itmightleadtoverydifferentdistributionsofoutputactivationsacrosslayersorfeatures. This isaproblemwithdeepneuralnetworksbecauseamodelofthiskindhastoadapttodifferent distributionsoverdifferentlayersordifferentfeatures[IoffeandSzegedy,2015]. Sometimes, asmodelparametersareinitializedrandomlyinalllayersandinallfeaturedimensions,itis likelyforsomefeaturestobelargevalues. Inthiscase,themodelwouldbebiasedtothose largevaluefeatures. Awaytomitigatethisissueisnormalization,whichstandardizesann-dimensionalfeature vectorsas s−µ Normalize(s) = α⊙ +β (2.43) σ+ϵ 2.3 Basic Model Architectures 89 whereµ∈Rn andσ∈Rn arethemeanandstandarddeviationofs,respectively. ϵisasmall numberusedfornumericalstability[ChiangandCholak,2022]. α∈Rn andβ ∈Rn arethe parametersofthenormalizationunit. Asimplechoiceisα=1andβ =0, whereasamore sophisticatedmethodistolearnαandβ togetherwithotherparameters. WemayimplementEq. (2.43)inseveralwaysthatdifferinhowtodefineµandσ. Letus considerthisforonedimensionins,says,inageneralsetting. Supposethatsisdrawnfroma setoffeaturevaluesΩ . ThemeanandthestandarddeviationonΩ arethendefinedtobe: k k 1 (cid:88) µ = · s (2.44) k |Ω | k s∈Ω k (cid:115) 1 (cid:88) σ = · (s−µ )2 (2.45) k k |Ω | k s∈Ω k SeveralmethodsareavailabletodefineΩ . Forexample,onecandefineΩ asfeaturesin k k thesamelayer[Baetal.,2016],orfeaturesalongthesamedimensionoverdifferentsamples orinputpositions[IoffeandSzegedy,2015;Ulyanovetal.,2016],orsomethinginbetween them[WuandHe,2018]. Anadvantageofnormalizationistoputfeaturesonthesamescaleandmakethemcompa- rable. Thishasbeenfoundtobeveryhelpfulforstabilizingthetrainingprocessandmaking neuralnetworksbetterbehaved. Aswewillseeinsubsequentchapters,normalizationplaysan importantroleinmanysuccessfulsystems. Asanaside,whilethetermnormalizationindeeplearningisusuallyreferredtoasaprocess ofsubtractingthemeananddividingbythestandarddeviation,itisinfactastandardization process. Inotherareas,bycontrast,normalizationismoreoftenreferredtoasatechniquethat scales all entries of a vector to the interval [0,1]. Standardization has no such requirement. Itinsteadtendstohavetheinputcenteredaround0. Inthissense,normalizationmightbea misnomerindeeplearningsomehow. Nevertheless,normalizationandstandardizationareused interchangeablyinthisbookwhenreferredtoprocesseslikeEq. (2.43). 2.3.5 Residual Units Thesuccessofdeepneuralnetworkshasbeenmostlyaccreditedtothemoreandmorelayers usedinformingmorecomplexfunctions. Althoughstackingalargenumberoflayersisthe simplestwaytoobtainadeepmodel,ithasbeenpointedoutthatsuchamodelisdifficulttotrain. Thereareseveralreasonsforthis,e.g.,optimizationalgorithms,gradientvanishing/exploding inpassingthroughstackedlayers,parameterinitialization,andsoon. Even,afurthernotable disadvantage comes with regard to feeding a single representation to upper-level feature extractors,asonemightwantdirectaccesstotheintermediaterepresentationsseverallayers ahead. Residualneuralnetworksareoneofthemosteffectiveapproachestoaddressingthese issues [He et al., 2016a]. They are a special type of neural networks that add residual connections(orskipconnections,orshortcutconnections)overlayersinalayerstack. Let F(x) be a neural network that maps x to some output. A residual neural network build on 90 Chapter 2. Foundations of Neural Networks ResidualConnection ResidualConnection F(x(l)) + F(x(l+1)) + x l xl+1 xl+2 Layerl Layerl+1 xl+1=F(xl)+xl Figure 2.10: A 2-layer residual neural network. For each layer, there is a skip or shortcut connection(inredcolor)thatdirectlyaddstheinputtotheoutput. F(x),givenbysummingthemappingF(x)andtheidentitymappingx: y = F(x)+x (2.46) Amorecommonuseofresidualconnectionsisinaneuralnetworkconsistingofanumber ofidenticallayers. Letxl andyl betheinputandoutputoflayerl inaresidualmulti-layer neuralnetwork. Theoutputoflayerlcanbedefinedas: xl+1 = F(xl)+xl (2.47) or yl = F(yl−1)+yl−1 (2.48) Figure 2.10 shows the architecture of a 2-layer residual neural network. Clearly, the residualconnectionsaddtheoutputsofcurrentlayersdirectlytotheoutputsofthenextlayers. The added identity mapping is generally thought of as one of the most effective ways to simplifythenetworkandeasetheinformationflowinadeepmodel. 2.4 Training Neural Networks In this section, we turn to the training problem, which is fundamental in developing neural network-basedsystems. Mostofthediscussionhereisfocusedonmethodsinasupervised learningsetting. WewilldiscussunsupervisedmethodsinSection2.6. 2.4.1 Gradient Descent Thegradientmethodhasbeenproventobeoneofthemostsuccessfulmethodsfortraining neuralnetworks. Thebasicideaistoiterativelyupdateparameterssothatwecanminimizea differentiablelossfunction. Inanupdate,thevaluesoftheparametersareadjustedinaway thatthelossdegradesthefastest. Inamathematicalsense,itrequirestheupdatetobeinthe 2.4 Training Neural Networks 91 minimum Figure2.11: Gradientdescentina2Dspace(bluelinesstandforlevelsets). Thegoalistofind theparameters(i.e.,valuesalongthetwodimensions)thatminimizethevalueofagivenloss function. Gradientdescentdoesthisbystartingatarandompointandsteppingtotheminimum inanumberofupdatesoftheparameters. Ineachupdate,itadjuststheparametersθ inthe t directionthatmakesthelosslower. Theideahereisthattheupdatechoosesthedirectionof thesteepestascent,thatis,themodelmovesastepinthedirectionofthenegativegradientof theloss(i.e.,− ∂L(θt) ). Thesizeofthemoveiscontrolledbyahyper-parameterlr,calledthe ∂θt learningrate. Thus,theamountofthechangetotheparametersis−lr· ∂L(θt) . Byaddingthis ∂θt toθ ,weobtainthenewparametersθ . Thisprocessrepeatsforanumberofupdatesuntil t t+1 thevalueofthelossfunctionisclosetotheminimum. oppositedirectionofthegradientoftheloss. Thisisknownasgradientdescentorsteepest descent. Let θ be the parameters at step t (call it an update step or a training step), and t L(θ )bethelosscomputedbythemodelparameterizedbyθ . Theupdaterule(ordeltarule) t t ofgradientdescentisgivenbytheequation: ∂L(θ ) t θ = θ −lr· (2.49) t+1 t ∂θ t where ∂L(θt) isthegradientofthelosswithrespecttotheparametersatstept. Itcanobtained ∂θt byrunningtheerror-propagationalgorithmpresentedinSection2.1.3. Sinceθ isusuallyof t multipledimensions, ∂L(θt) couldbeavectorormatrixthathasthesameshapeasθ . lr isthe ∂θt t learningratethatcontrolshowbigastepwetakeinthedirectionoftheminimum. Whilelr canbesimplysettoaconstantvalueduringtraining,itismorecommontoadjustitsvalueas thetrainingproceeds(seeSection2.4.4foradiscussion). SeeFigure2.11foranillustrationof gradientdescent. Eq. (2.49) gives a very basic definition of gradient descent. There are a number of improvementstotheformofEq. (2.49). Someofthemare: 92 Chapter 2. Foundations of Neural Networks • GradientDescentwithMomentum. Inphysics,momentumisavectorquantitythat describesthemassofmotion. Ifwethinkofupdatingparametersasmovinganobjectin aspace,thenweneedtoconsiderthemomentumoftheobjectatapositiontodetermine thedirectionofthenextmove. Thisideacanbeimplementedbyre-definingtheupdate ruleas: θ = θ +v (2.50) t+1 t t where v is the velocity vector of the momentum. In the classic momentum method t [Polyak,1964],v isdefinedtobe: t ∂L(θ ) t v = λ·v −lr· (2.51) t t−1 ∂θ t v",
    "Neural Networks • GradientDescentwithMomentum. Inphysics,momentumisavectorquantitythat describesthemassofmotion. Ifwethinkofupdatingparametersasmovinganobjectin aspace,thenweneedtoconsiderthemomentumoftheobjectatapositiontodetermine thedirectionofthenextmove. Thisideacanbeimplementedbyre-definingtheupdate ruleas: θ = θ +v (2.50) t+1 t t where v is the velocity vector of the momentum. In the classic momentum method t [Polyak,1964],v isdefinedtobe: t ∂L(θ ) t v = λ·v −lr· (2.51) t t−1 ∂θ t v retainssomeofthepreviousmomentum(i.e.,v ),followedbyacorrectionbased t t−1 onthegradient(i.e., ∂L(θt) ). λisascalarforweightingv inanupdate. Awell-known ∂θt t improvementtoEq. (2.51)istotakeintoaccountthemomentuminthegradient,avoiding atoolargevelocitywhenapproachingtheminimum[Nesterov,1983],likethis (cid:2) (cid:3) ∂ L(θ )+λ·v t t−1 v = λ·v −lr· (2.52) t t−1 ∂θ t AmoredetaileddiscussiononthedifferencebetweenEq. (2.51)andEq. (2.52)canbe foundinSutskeveretal.[2013]’spaper. • AdaptiveGradientDescent. Inadaptivemethodsforgradientdescent,theupdaterule isadaptedtoeveryparameter,ratherthanthewholemodel. AdaGradisamethodof thiskind[Duchietal.,2011]. Itscalesupthelearningrateforparametersthathavenot beenupdatedtoomuch,andscalesdownthelearningrateforparametersthathavebeen muchupdated. Assumethatθ and ∂L(θt) arebothd-dimensionalvectors. Wecandefine t ∂θt anewvariableG∈Rd×d asthesumoftheouterproductofthegradientoverthepastt steps5: t (cid:88)(cid:2)∂L(θ i )(cid:3)T ∂L(θ i ) G = · (2.54) t ∂θ ∂θ i i i=1 5Giventwovectorsa= (cid:2) a 1 ··· a d (cid:3) andb= (cid:2) b 1 ··· b d (cid:3) ,theouterproductofaandbis: a⊗b = aT·b a  1 =   . . .   · (cid:2) b 1 ··· b d (cid:3) a d a b ··· a b  1 1 1 d =   . . . ... . . .   (2.53) a b ··· a b d 1 d d 2.4 Training Neural Networks 93 In general, (G t ) 1 2 ∈Rd can be viewed as an indicator that describes to what extent a 1 parameterhasbeenupdatedsofar. However,computing(G t )2 isextremelyexpensive. So it is more common to use the diagonal of G instead. Then, the update rule of t AdaGradisgivenby: lr ∂L(θ ) t θ = θ − ⊙ (2.55) t+1 t (cid:112) diag(G t )+ϵ ∂θ t where diag(G ) is the diagonal of G , i.e., diag(G )(k)=G (k,k). ϵ is a smoothing t t t t factorfornumericalstability. Insteadofsummingoverthosesquaredgradientsinan unweightedmanner,anotherwayistoreducetheimpactof“old”gradientsandmake “recent”gradientsmoreimportant. AdaDeltaconsidersthisbyaccumulatingsquared gradientswithadecayfactor[Zeiler,2012]: (cid:18) (cid:19) ∂L(θ ) ∂L(θ ) g2 = σ·g2 +(1−σ)· t ⊙ t (2.56) t t−1 ∂θ ∂θ t t whereσ isthedecayfactorofavalue<1. LikeEq. (2.55),theupdateruleforAdaDelta canbegivenbyreplacingdiag(G )withg2: t t lr ∂L(θ ) t θ = θ − ⊙ (2.57) t+1 t (cid:112) g2+ϵ ∂θ t t (cid:112) Since g2+ϵcanbeseenastherootmeansquare(RMS)ofthegradient,Eqs. (2.56- t 2.57)arealsoknownastheRMSPropmethod[Hinton,2018]. • Adam(AdaptiveMomentEstimation). TheAdamoptimizercombinesthemeritsof boththeadaptivegradientdescentandmomentummethods[KingmaandBa,2014]. It definesan estimate ofthemean ofthegradient(the firstmoment)andan estimateof the variance of the gradient (the second moment). Let m and v be the two moment t t estimates. Theyaregivenbytheequations: ∂L(θ ) t m = β ·m +(1−β )· (2.58) t 1 t−1 1 ∂θ t (cid:18) (cid:19) ∂L(θ ) ∂L(θ ) t t v = β ·v +(1−β )· ⊙ (2.59) t 2 t−1 2 ∂θ ∂θ t t whereβ ,β ∈[0,1]arehyper-parametersforatrade-offbetweenthepreviousestimate 1 2 andthegradient(orsquaredgradient)atthecurrentstep. β andβ arealsotreatedas 1 2 thedecayfactorsoftheseaverages. Forexample,commonchoicesforβ andβ are0.9 1 2 and0.999. Astheinitialmomentsaresetto0,theseestimatesarebiasedto0vectorsat theverybeginningofthetrainingprocess. Toaddressthisissue, biascorrectionsare 94 Chapter 2. Foundations of Neural Networks usedinAdam,leadingtobias-correctedestimates: m t mˆ = (2.60) t 1−βt 1 v t vˆ = (2.61) t 1−βt 2 Sinceβ ,β <1,thecorrectionswouldbesufficientlysmallifalargernumberofupdates 1 2 areperformed. Theupdateruleisfinallydefinedtobe: mˆ t θ = θ −lr·√ (2.62) t+1 t vˆ +ϵ t Eq. (2.62)resemblesthegeneralformofgradientdescent,butmakesuseofboththe momentum method (i.e., the moving average of the past gradients) and the adaptive method(i.e.,themovingaverageofthepastsquaredgradients). Inpractice,Adamhas becomeapopularoptimizerfortrainingneuralnetworks. Improvinggradientdescentisanactivesub-fieldofdeeplearning, butafulldiscussion of all those techniques is beyond the scope of this document. A few related issues will",
    "areperformed. Theupdateruleisfinallydefinedtobe: mˆ t θ = θ −lr·√ (2.62) t+1 t vˆ +ϵ t Eq. (2.62)resemblesthegeneralformofgradientdescent,butmakesuseofboththe momentum method (i.e., the moving average of the past gradients) and the adaptive method(i.e.,themovingaverageofthepastsquaredgradients). Inpractice,Adamhas becomeapopularoptimizerfortrainingneuralnetworks. Improvinggradientdescentisanactivesub-fieldofdeeplearning, butafulldiscussion of all those techniques is beyond the scope of this document. A few related issues will be discussedintheremainderofthissection. On a last note of this subsection, a practical issue that one should consider in utilizing iterativetrainingmethodsiswhentostoptraining. Stoppingcriterionisageneraltopicin optimization. Forgradientdescentanditsvariants,itiscommonpracticetosetamaximum number of training steps or training epochs6, say 20,000 steps, or 100 epochs. As an alternative, we can perform training until convergence. For example, we can say that the trainingcoveragesifthelosstendstobestableforanumberoftrainingsteps. Whenthereis somedataforvalidatingthemodel,abettermethodmaybetocheckthestatesofthemodel onvalidationdata. Forexample,wecanstopthetrainingwhenthepredictionerrorincreases on the validation data. This method, known as early stopping, is often used as a means of regularization. InSection2.5.3,wewillseemoredetailsabouthowtoearlystopthetraining byusingavalidationdataset. Onthealgorithmicside,therehasbeenmuchinterestandwork in studying the convergence and error bounds for machine learning methods. We refer the interestedreadertoafewtextbooksforfurtherdiscussions[Mohrietal.,2018;Kochenderfer andWheeler,2019]. 2.4.2 Batching Thelossfunctionisanessentialaspectofthetrainingofneuralnetworks. Whileanumber ofmathematicalformsareavailabletodefinethelossfunction(seeSection1),westillneed todecideinwhatscaleofsamplesweusethatlossfunction. Perhapsthesimplestmethodis stochasticgradientdescent(SGD).Ineachupdateofparameters,SGDcomputestheloss functiononasinglesamplethatisrandomlyselectedfromthetrainingdataset. LetD beaset oftrainingsamples,and(x(i),y (i) )bearandomlyselectedsamplefromD. Givenaneural gold 6Atrainingepochmeansthatthetrainergoesoverthewholetrainingdatasetforonetime. 2.4 Training Neural Networks 95 network (cid:16) (cid:17) y (i) =F x(i) (2.63) θ θ thelossofSGDisdefinedtobe: (cid:16) (cid:17) (i) (i) L(θ) = L y ,y (2.64) θ gold (i) (i) (i) whereL(y ,y )isasample-levellossfunctionthatcountserrorsinthemodeloutputy θ gold θ (i) withrespectthebenchmarky . gold SGDhasbeenoneofthemostimportantoptimizationmethodsinmachinelearningdue to its simplicity. However, SGD converges slowly because it is just an analog of the actual gradientontheentiretrainingset. Toestimatethegradientinamorepreciseway,wecantake intoaccountasetofsamples(callitabatch)incomputingtheloss. Thismethodisknownas batching. LetS beasetofsamplesfromD. ThelossfunctionisthendefinedonS,asfollows 1 (cid:88) L(θ) = · L(y ,y ) (2.65) θ gold |S| (y ,y )∈S θ gold IfS =D,thenwehavethebatchgradientdescent(BGD)method,i.e.,thegradientis estimated on the entire set of training samples. In general, batch gradient descent is what wewouldordinarilycallgradientdescent. However, calculatingthelossonallthetraining samples simultaneously is time consuming. In practice, it is more common to use a batch muchsmallerthanD. Thisisknownasmini-batchgradientdescent. Itisadoptedinlearning real-worldsystemsforitsgoodefficiencyandstrongperformance. Asanother“bonus”,batchingisgenerallyusedasawaytomakedensecomputationonma- tricesforsystemspeedup. AssumethatSconsistsofmsamples{(x(i1),y (i1) ),...,(x(im),y (im) )}. gold gold Wecanbatchallinputvectorsandbenchmarkvectorsasmatrices:   x(i1) X =   . . .   (2.66)   x (im) gold   y (i1) gold  .  Y gold =  . .  (2.67)   y (im) gold Then,wecanruntheneuralnetworkonthebatchedinputandoutput,likethis: Y =F (X) (2.68) θ θ 96 Chapter 2. Foundations of Neural Networks Likewise,wecancomputethebatchedloss 1 L(θ)= ·L(Y ,Y ) (2.69) θ gold m (cid:16) (cid:17) where L(Y ,Y ) vectorizes the computing of (cid:80)m L y (i k ) ,y (i k ) . Eqs. (2.68-2.69) θ gold k=1 θ gold preventtherepetitivecallsoftheforwardandbackwardpassesonindividualsamples. They instead pack everything in a single pass through the network. This makes better use of maximum available compute on modern GPUs which are the majority of the devices for runningdeeplearningsystems. 2.4.3 Parameter Initialization Gradientdescentrequiresthatthetrainingprocessstartsfromsomeinitialparameters. Since thetrainingobjectiveinapracticalsystemisoftenanon-convexfunctionwithmanylocalmini- mums,theperformanceoftheresultingmodelishighlysensitivetotheparameterinitialization step. Herewedescribesomeofthemostcommonmethodsofparameterinitialization. • ConstantInitialization. Thefirstmethodcouldassignthesamevaluetoallparameters (orallparametersofaparametermatrix). Thismethod,thoughquitesimple,resultsin thatalloutputentriesofamodelmakenodifference,renderingthemodelmeaningless. Itperformspoorlyinmostcasesifnorandomnessisintroducedintotraining. • InitializationwithPredefinedDistributions. Ausefulwayistorandomlyinitialize parametersbysomedistributions. Thesimplestofthiskindistoassignaparametera valuedrawnfromauniformorGaussiandistribution,e.g.,arandomvalueintheinterval [−0.1,0.1]. Interestingly,thismethodissatisfactoryinmostcasesinpractice. • Layer-sensitiveInitialization. Anextensiontorandominitializationistousetailored distributions for different layers of a neural network. Xavier initialization is a well- knownmethodofthiskind[GlorotandBengio,2010]. Givenalayery=ψ(x·W+B), let d and d be the numbers of the input and output dimensions (i.e., the row and in out columnnumbersofW). ThestandardXavierinitializationmethod,alsoknownasthe LeCun initialization method [LeCun et al., 2012], gives a random number to every parameterofW: (cid:18) (cid:19) 1 1 W∈Rdin×dout ∼ U −√ , √ (2.70) d d in in whereU(−a,a)meansauniformdistributionovertheinterval[−a,a]. Likewise,we caninitializethebiasterminasimilarway. Asanimprovement,thenormalizedXavier initializationmethodconsidersbothd",
    "the numbers of the input and output dimensions (i.e., the row and in out columnnumbersofW). ThestandardXavierinitializationmethod,alsoknownasthe LeCun initialization method [LeCun et al., 2012], gives a random number to every parameterofW: (cid:18) (cid:19) 1 1 W∈Rdin×dout ∼ U −√ , √ (2.70) d d in in whereU(−a,a)meansauniformdistributionovertheinterval[−a,a]. Likewise,we caninitializethebiasterminasimilarway. Asanimprovement,thenormalizedXavier initializationmethodconsidersbothd andd indefiningthedistribution,likethis: in out (cid:18) (cid:114) (cid:114) (cid:19) 6 6 W∈Rdin×dout ∼ U − , (2.71) d +d d +d in out in out Moredetailscanbefoundintheoriginalpaper. Notethattheuniformdistributionscan 2.4 Training Neural Networks 97 bereplacedbythenormaldistributionswithmean=0andvariance= 1 or 6 . din din+dout Manyparameterinitializationmethodsaredesignedforcertaintypesofneuralnetworks. Forexample,XavierinitializationisassumedtoworkwiththeSigmoidandhyperbolictangent activationfunctions. ForReLU,onecanrefertoHeetal.[2015]’swork. Anotherexample isinitializationfordeepneuralnetworks. Ithasbeenfoundthatappropriateinitializationis criticaltothesuccessofextremelydeepmodelsinNLP.Consideringthemodeldepthasan additionalfactorininitialization,wecanmodifyEq. (2.71)tobe: (cid:18) (cid:114) (cid:114) (cid:19) α 6 α 6 W∈Rdin×dout ∼ U − s · , s · (2.72) l d +d l d +d in out in out wherelisthedepthforalayer,andα isahyper-parameter. Apartfromthis,severalmethods s are proposed to address the initialization of deep neural networks, including the Lipschitz initialization [Xu et al., 2020], the T-Fixup initialization [Huang et al., 2020a], the Admin initialization[Liuetal.,2020c],andsoon. Note that in practice we do not have to restrict training to a single starting point. It is commontotryafewstartingpointsbyusingdifferentinitializationmethodsorrandomseeds, andtochoosethebestperformingonefromthesetries. Itgenerallyhelpswhenlocalminimums abound. 2.4.4 Learning Rate Scheduling Toachievedesirableresults,itisessentialtocarefullyconfigurethelearningratethroughout thelearningprocess. Whilesomeoftheupdaterules,asnotedabove,haveconsideredscaling thegradientfordifferentparameters,learningrateschedulingisconventionallyfocusedmore ondesigningheuristicstoadjustlrovertrainingsteps. Inapracticalsense,atoolargelearning rateusuallyleadstoovershootingaroundtheminimum,whileatoosmalllearningrateusually leadstoslowconvergence(seeFigure2.12). Acommonideaistolearnfastatthebeginning (i.e.,alargelearningrate)andlearnslowlywhenthelossisclosetotheminimum(i.e.,asmall learningrate). Herewepresentsomeofthepopularmethodsforlearningratescheduling. • FixedLearningRates. Fixingthelearningrateisgenerallyabadstrategy,butcouldbe usedinprototypingsystems,e.g.,aquicktestofanewmethodbytrainingitforonlya fewepochs. • LearningRateDecay. Decayisacommonly-usedtechniqueforlearningrateschedul- ing. Therearemanyapproachestothisidea. Forexample,onecanhalvethelearning rateaftereachtrainingepoch. Hereweusen todenotethenumberoftrainingsteps, t andτ behowoftenwechangethelearningrate(e.g.,100steps). Table2.2shows decay severaldecayfunctionsforlearningratescheduling. • Warmup and Decay. As noted in Section 2.4.3, it is common to initially set model parameterstorandomvalueswhenaneuralnetworkisbeingtrained. However,learning fromscratchwithalargelearningrateisusuallynotagoodchoicebecausethegradient attheearlystageofthetrainingisnotmuchpreciseandthestateofthemodelisunstable. 98 Chapter 2. Foundations of Neural Networks (a)Asmalllearningrate (b)Alargelearningrate (c)Adesirablelearningrate Figure 2.12: Learning with different learning rates. Small learning rates (left) help us step totheminimuminapreciseway, butrequiremuchadditionaltimeforconvergence. Large learningrates(middle),ontheotherhand,leadtofastlearning,whichisverybeneficialwhen we are far away from the minimum. However, as we get closer to the minimum too large learningratescauseovershooting. Amoredesirablestrategy(right)maybetolearnthemodel inareasonablyfastwaywhenthereisalongwaytogo,andtolearnthemodelslowerwhen weareclosetotheminimum. Thus,itismorereasonabletostartwithasmalllearningrateandgraduallyincreaseit. Then,whenthemodelistrainedforsometime,thelearningratebeginstodecayasusual. Suchathoughtmotivatesthewarmupanddecaymethodforlearningratescheduling. ApopularformofthismethodinrecentstudiesisproposedinVaswanietal.[2017]’s work,asfollows: (cid:18) (cid:19) lr = lr ·min (cid:0) n t (cid:1)−0.5 , n t ·(n )−1.5 (2.73) nt 0 n n warmup decay decay where lr is the initial learning rate, and n is a hyper-parameter that specifies 0 warmup forhowmanystepsweexecutethewarmupprocess. Figure2.13plotsthecurveofEq. (2.73)wheren ,n ,andlr aresetto4,000,1and1. Weseethatthelearning warmup decay 0 rate increases linearly in the first n steps and then decays as an inverse square warmup rootfunction. Choosinganappropriatelearningrateschedulingstrategyisahighlyempiricalproblem, and there are no universally good choices. The problem is even harder if we consider the correlationbetweenthelearningrateandotheraspectsofthetraining,thoughlearningrate schedulingistypicallytakentobeanindividualtask. Forexample,whenalargerbatchisused intraining, alargerlearningrateisdesiredforagoodresult[Ottetal.,2018b;Smithetal., 2018]. So,makinggoodlearningratechoicesisstilldifficultandtime-consuminginneural networkapplications. Occasionallyoneneedsalargenumberoftrial-and-testrunstofinda desirablelearningratesetupfortheparticularproblemathand. 2.5 Regularization Methods 99 Entry Formula Hyper-parameters PiecewiseConstantDecay lr =β values{β ,...,β } nt i 1 m if γ ≤ nt <γ thresholds{γ ,...,γ } i n i+1 1 m decay nt ExponentialDecay lr nt =lr 0 ·λndecay decayrateλ,init. lr. lr 0 ⌊ nt ⌋ (Drop)ExponentialDecay lr nt =lr 0 ·λ ndecay decayrateλ,init. lr. lr 0 NaturalExponentialDecay lr =lr ·exp(−λ· nt ) decayrateλ,init. lr. lr nt 0 n decay 0 InverseTimeDecay lr =lr · 1 decayrateλ,init. lr. lr nt 0 1+λ· nt 0 ndecay (Drop)InverseTimeDecay lr =lr · 1 decayrateλ nt 0 1+λ·⌊ nt ⌋ ndecay (cid:0) (cid:1) CosineDecay lr =lr · (1−α)·c +α coefficientα nt 0 decay c = 1· (cid:0) 1+cos(π· nt ) (cid:1) init. lr. lr decay 2 n 0 decay Table 2.2: Decay functions. λ=decayrate,",
    "0 1+λ· nt 0 ndecay (Drop)InverseTimeDecay lr =lr · 1 decayrateλ nt 0 1+λ·⌊ nt ⌋ ndecay (cid:0) (cid:1) CosineDecay lr =lr · (1−α)·c +α coefficientα nt 0 decay c = 1· (cid:0) 1+cos(π· nt ) (cid:1) init. lr. lr decay 2 n 0 decay Table 2.2: Decay functions. λ=decayrate, lr =initiallearningrate, and {β }, {γ } and 0 i i α=otherhyper-parameters. 0.015 0.010 0.005 0.000 0 4,000 10,000 20,000 30,000 numberofupdatesteps(n ) t ) rl(etargninrael tn Figure2.13: Learningratescheduling: warmupandthendecay(n =4,000,n =1, warmup decay andlr =1). Thelearningrateincreaseslinearlywithn forthefirst4,000steps. Then,the 0 t learning rate follows an inverse square root function and decays as the learning continues. The change of the rate learning will be small if n is sufficiently large, indicating the fine t adjustmentoftheparameterswhenweareapproachingtheminimumoftheloss. 2.5 Regularization Methods Wenowdiscusstheregularizationmethodsforpreventingoverfitting. Whileregularizationisa wide-rangingtopicinmachinelearning,wepresentsomeofthosethatarecommonlyadopted intrainingneuralnetworks. 100 Chapter 2. Foundations of Neural Networks 2.5.1 Norm-based Penalties One of the most popular methods involves a regularization term based on the l norm. A p generalformoftheregularizedobjectivecanbedefinedas: θˆ = argminL(θ)+α·R(θ) (2.74) θ whereR(θ)istheregularizationtermweightedbyacoefficientα. Ingeneral,R(θ)servesas anadditionallossthatpenalizescomplexmodels. Thisismotivatedbythefactthatcomplex modelsaremorelikelytooverfitthedata(seeSection1). Toimposeapenaltyonthemodel complexity,asimplewayistodefineR(θ)asthel normontheparametersθ. Letustreatθ 1 asavectorofparameters. Thel norm-basedregularizationtermisgivenby 1 (cid:88) R(θ) = |θ | (2.75) i i Eq. (2.75)penalizesmodelshavinglargevalueparameters. Thiscanbeunderstoodinaway fromapolynomialfunction: largecoefficientsofvariablesinapolynomialfunctionleadtoa complexcurve. Typically,regularizationwiththel normisreferredtoasthel regularization 1 1 ortheLassoregularization. Suchamethoddoesnotrequireupdatesofthetrainer,andcanbe implementedbystandardgradientdescent. Moreinterestingly,thel regularizationtypically 1 providessparsesolutionstotheoriginaltrainingobjective. Itbiasesthemodeltothosehaving smallvalues(orevenzerovalues)formostoftheparametersandlargevaluesforonlyvery fewparameters. Thisalsoimpliesaninherentabilityoffeatureselectionbecauseparameters areforcedtobeclosetozerofornot-so-importantfeatures. Analternativetothel regularizationisthel regularizationortheRidgeregularization. 1 2 Inthel regularization,theregularizationtermisgivenby 2 (cid:115) (cid:88) R(θ) = |θ |2 (2.76) i i Likethel norm,thel normpenalizesthecasesthatdeviatethemodelparametersfaraway 1 2 from the origin. However, it slightly differs from the l norm in that the l norm enforces 1 2 all parameters to have small values (but not necessarily to be zeros) and there are no large value parameters. In this sense, the use of the l norm does not introduce sparsity into the 2 solutionbutperforms“smoothing”ontheunderlyingdistributionsoffeatures. Notethatthe l regularization has a relatively bigger effect of regularization. So, it is sometimes called 2 weightdecaytoemphasizeitsabilitytopreventthemodelfromlearningparametersoftoo largevalues. Inabroadersenseofmachinelearning,Eq. (2.75)offersageneralmethodtointroduce priorknowledgeintothetrainingofaneuralnetwork. Thereareanumberofwaystodesign theregularizationterm,andaddressingoverfittingisjustonepurposeofthesedesigns. Wecan seemanyapplicationsofthisapproachinNLP,andwillseeafewexamplesintheremaining chaptersofthisdocument. 2.5 Regularization Methods 101 2.5.2 Dropout Inareal-worldneuralnetwork,alayertypicallyinvolveshundredsorthousandsofneuronsand producesafeaturevectoraccordingly. Whileeachofthesefeaturesiscomputedbyasingle neuron,theyworktogethertoformtheinputtoeachneuronofthefollowinglayer. Asaresult, afeatureisforcedtocooperatewithotherfeatures. Itislikeagroupofpeoplesittingtogether andmakingacollectivedecision. Althoughamembercouldhaveopinionsindependently,he orsheoccasionallytriestocorrecttheerrorwhenallothermembershavehadtheirdecisions. Inthiscase, everygroup memberisco-adaptedtoothersin thegroup[Hintonetal.,2012]. Fromafeatureengineeringstandpoint,theco-adaptationofneuronshelpswhenmodeling complex problems, as it implicitly makes some sort of higher order features. Beyond this, thestrongsupervisioninformation(e.g.,propagatingerrorsthroughlayers)couldstrengthen theco-adaptationintraining. Thisexplainsmoreorlesswhyaneuralnetworkwithalarge numberofneuronscanfitcomplexcurves. Attesttime,however,theco-adaptationprevents generalization. Since all neurons of a layer are learned to collaborate well on the training data,asmallchangeintheinputcouldaffectalltheseneuronsandleadtoabigchangeinthe behavioroftheneuralnetwork. Awaytomitigateoreliminatecomplexco-adaptationsistolearnforeachneurontopredict in the absence of other neurons. To this end, one can simply drop some of the neurons in training. Thismethodisknownasdropout[Srivastavaetal.,2014]. Letnbethenumberof neuronsofalayer. Givenaprobabilityρ(call1−ρthedropoutrate), wecangeneratean n-dimensionalmaskvectorM whereeveryentryissetto1withapossibilityofρ,andset drop to0withapossibilityof1−ρ. Then,adropoutlayercanbedefinedas y = M ⊙ψ(x·W+B) (2.77) drop where ψ(x·W+B) is a usual single-layer neural network. Eq. (2.77) only activates the neurons whose masks are 1. For dropped neurons, all connections from/to these neurons are blocked (see Figure 2.14 (a)). During training, M is randomly generated in a call drop of the forward and backward passes. A neuron therefore can learn to work with different neuronseachtimeandwouldnotadapttothesamegroupof“co-workers”. Anotherwayto understand dropout is to view it as learning sub-models of a “big” model. The use of Eq. (2.77)isessentiallyasamplingprocessthatextractsasub-networkfromtheoriginalnetwork. So,trainingwithdropoutisdoingsomethingliketraininganexponentiallylargenumberof sub-networks7. Ontheotherhand,thetrainingisefficientbecausethesesub-networkssharethe sameparametersforthesameneuronandtheupdateofaparametercanbenefitexponentially manysub-networks. At test time, all these sub-networks are combined for prediction. In this case, we do not need to drop any neuron but use the original",
    "neuronseachtimeandwouldnotadapttothesamegroupof“co-workers”. Anotherwayto understand dropout is to view it as learning sub-models of a “big” model. The use of Eq. (2.77)isessentiallyasamplingprocessthatextractsasub-networkfromtheoriginalnetwork. So,trainingwithdropoutisdoingsomethingliketraininganexponentiallylargenumberof sub-networks7. Ontheotherhand,thetrainingisefficientbecausethesesub-networkssharethe sameparametersforthesameneuronandtheupdateofaparametercanbenefitexponentially manysub-networks. At test time, all these sub-networks are combined for prediction. In this case, we do not need to drop any neuron but use the original network as usual. This makes it simple to implement dropout: a neuron is present with some probability on the training data, and all neuronsarepresentandworktogetheronthetestdata. Sincetheconnectionsbetweenneurons areinvolvedwithaprobabilityofρintraining,thelearnedweightsarescaleddownwithρin 7Forasingle-layernetworkhavingnneurons,thereare2npossiblesub-networks. 102 Chapter 2. Foundations of Neural Networks activeneuron allneurons (presentwithρ) areactive droppedneuron weightW weightρW 32 32 (a)Trainingthedropoutnetwork (b)Testingwiththedropoutnetwork Figure2.14: Dropoutforamulti-layerneuralnetwork(trainingvstest). Attrainingtime,every neuronisrandomlydroppedwithaprobabilityof1−ρ,resultinginaslimmednetwork. Inthis sense,dropouttrainingisessentiallyaprocessoflearninganexponentiallylargenumberof sub-networks. Attesttime,thefullnetworkisusedasusual,whichistheresultofcombining allthosesub-networksforprediction. Sinceallconnectionsbetweenneuronsareactivatedwith theprobabilityρduringtraining,theweightsofthepredictingnetworkarescaleddownwithρ. thepredictingnetwork,i.e.,alayerhasaform: y=ψ(x·ρW+ρB) (2.78) SeeFigure2.14foracomparisonoftrainingandapplyingadropoutnetwork. Eq. (2.78) requiresanupdateofthepredictingsystem. Analternativeistotakeintoaccountthescaling issueonlyinthetrainingprocessandleavethepredictingsystemasitis. Forexample,wecan scaleupalltheparameterswith 1 indropouttraining,likethis ρ 1 1 y = M ⊙ψ(x· W+ B) (2.79) drop ρ ρ Sincemultiplying 1WwithρyieldsW(thisalsoholdsforthebiastermB),wecanuse ρ W(andB)astheparametersofthepredictingsystem. 2.5.3 Early Stopping InChapter1wehavediscussedabitofhowtostopthetrainingbymonitoringtheperformance onthevalidationdata. Itcanbetreatedasawayofmodelselectionthatseeksanappropriate state between underfitting and overfitting. Note that early stopping is not just an empirical method. Itisalsowellexplainedfromtheperspectiveofstatisticallearningtheory. Forexample, researchershavefoundthat,undersomeconditions,earlystoppinghasasimilareffectasthe l regularization and restricts the learning to the region of small value parameters [Bishop, 2 1995a;Goodfellowetal.,2016]. Also,otherresearchshowsthatsomeearlystoppingrules 2.5 Regularization Methods 103 haveatightrelationshipwiththebias-variancetrade-offandcouldguaranteenicepropertiesof convergence[Yaoetal.,2007]. Ontheotherhand,earlystoppingrequiresseveralheuristicstomakeitpracticalanduseful. The first problem is the condition of stopping. Ideally, one might imagine that there is a perfectU-shapederrorcurveonthevalidationdata,andthetrainingcanbehaltedimmediately whentheerrorstartstoincrease. Thetruth,however,isthattheerrorcurvecannotbesimply describedasastrictlyconvexfunctionofthetrainingtime. Afterdropsintheerrorinacertain number of training steps, the performance of the model tends to fluctuate, leading to many localminimums. Theproblemwouldbemoreinterestingifonewantstosavetimeandstopthe trainingasearlyaspossible. However,weneverknowwhetherthecurrentchoiceordecision isthebestonebecausewehavenoideaofwhathappensnext. Acommonly-usedmethodis todecidewhetherthetrainingshouldstopbycheckingthemodelstatesforanumberofpast updatesteps(orepochs)[Prechelt,1998]. Someearlystoppingconditionsare: • The change in the performance is below a threshold for a given number of steps (or epochs). • Thechangeinthemodelparametersisbelowathresholdforagivennumberofsteps(or epochs). • Theaverageperformanceoveragivennumberofsteps(orepochs)startstodecrease. • Themaximumperformanceoveragivennumberofsteps(orepochs)startstodecrease. However,usingthemodelatthepointthatwestopthetrainingisnotalwaysagoodchoice. Inpractice,amodeloftenhasalargevarianceingenerationerroraroundthatpoint,making modelselectionmoredifficult. Insteadof“selecting”amodel,analternativewayistocombine multiple models. For example, we can save the model for every run of a given number of training steps (call each copy of the model a checkpoint). The final model is induced by averaging the parameters of the last few checkpoints. For better results, one may use more sophisticatedensemblemethods(seeSection1). 2.5.4 Smoothing Output Probabilities Instatistics,smoothingreferstotheprocessofreducingthevalueofnoisydatapoints(probably of high values) and increasing the value of normal data points. It is typically used when a distributionisestimatedonsmalldataandtheprobabilitiesofrareeventsarenotwellestimated. Forexample,considerthelanguagemodelingproblemdescribedinSection2.2. Alanguage model is trained in a way that enforces the model to output a one-hot distribution, that is, the total probability of 1 is occupied by only one word, leaving other words assigned zero probabilities. Itmaybemoredesirabletodistributetheprobabilitytoallwords,eventhough manyofthemarenotobservedtobetheanswergiventhepreviouswords. Inthisway, the modellearnstomakeasoftpredictionofwordprobabilitiessothatitcangeneralizebetteron unseendata. (cid:104) (cid:105) Givenadistributionp= p ... p ,itisthepurposeofsmoothingthatweobtainthe 1 n newestimatebetweenpandauniformdistribution 1. Acommonapproachtothisideaisto n 104 Chapter 2. Foundations of Neural Networks useashrinkageestimatortoimprovepbymakingitcloserto 1. Forexample,theaddictive n smoothingmentionedinSection1isasimpletypeofshrinkageestimator. Hereweconsider, forexample,smoothingamultinomialdistribution. Letp denotetheprobabilityofeventk k ands denoteaquantitythatdescribessomeobserved“count”oftheevent. Theprobability k p isgivenby k s k p = (2.80) k (cid:80)n s k=1 k Then,thesmoothedversionofp isdefinedas k s +α k pˆ = (2.81) k (cid:80)n (s +α) k=1 k Itsimplyaddsaquantityαtoeachs . Thevalueofαcontrolsthesmoothnessoftheresulting k estimate. Forexample,pˆ =p ifα=0,andpˆ ≈ 1 ifαchoosesanextremelylargevalue. k k k n Apartfromaddictivesmoothing,wecansmoothadistributioninaSoftmaxmanner,as follows exp(s /β) k pˆ = (2.82) k (cid:80)n exp(s /β) k=1 k ThisformisknownasaninstanceoftheBoltzmanndistribution[Uffink,2017],wheres k is viewed as the negative energy of a state, and β is viewed as the temperature indicating the degree of smoothing. Note that s can be interpreted in many ways. For example, in a k neuralnetwork,s istypicallydefinedasthestateofaneuron. Sometimes,s canevenbea k k probability. ThismeansthatwecandirectlyapplyEqs. (2.81-2.82)toanypevenifthereisno priorknowledgeabouthowpisestimated. Then,wecanrewriteEqs. (2.81-2.82)byreplacing",
    "k ThisformisknownasaninstanceoftheBoltzmanndistribution[Uffink,2017],wheres k is viewed as the negative energy of a state, and β is viewed as the temperature indicating the degree of smoothing. Note that s can be interpreted in many ways. For example, in a k neuralnetwork,s istypicallydefinedasthestateofaneuron. Sometimes,s canevenbea k k probability. ThismeansthatwecandirectlyapplyEqs. (2.81-2.82)toanypevenifthereisno priorknowledgeabouthowpisestimated. Then,wecanrewriteEqs. (2.81-2.82)byreplacing s withp : k k p +α k pˆ = (2.83) k (cid:80)n (p +α) k=1 k exp(p /β) k pˆ = (2.84) k (cid:80)n exp(p /β) k=1 k Anothermethodofsmoothingistointerpolatepwiththeuniformdistribution. Aformof theinterpolationisgivenby 1 pˆ = (1−ϵ)·p +ϵ· (2.85) k k n whereϵisahyper-parameterindicatingtowhatextentwerelyontheuniformdistributionin computingpˆ . ToillustratehowEq. (2.85)works,letussupposethatpisaone-hotvector, k say, p =1 if k =z and p =0 otherwise. By using Eq. (2.85), we subtract an amount of k k probability (i.e. ϵ) from p . The subtracted amount of probability is then redistributed to z alldimensionsevenly,makingtheresultingdistributionmoreflat-toppedandsmoother. See Figure2.15foranillustration. 2.5 Regularization Methods 105 1. 0. 0 0. 0. 0. 0. 0. 0. 0. p k ϵ ×n n .82 ϵ→ ... .02 .02 .02 .02 .02 .02 .02 .02 .02 pˆ (ϵ=0.2) k Figure2.15: Smoothingadistributionbyinterpolatingitwiththeuniformdistribution: pˆ = k (1−ϵ)·p +ϵ· 1. Foreachdimensionk,itsubtractsanamountofϵfromtheprobabilityp k n k andredistributesthisamountofprobabilityevenlytoallthevariables,thatis,everyvariable getsaprobabilityofϵ·p k. n InNLP,sincemanysystemsmakeprobability-likepredictions,acommonapplicationof smoothing is to smooth a system’s output. There are two ways. First, we can smooth the benchmarkprobabilitysuchthatthemodelisguidedbythegeneralizederrorratherthanthe errormadebyharddecisions. Forexample,thelabelsmoothingtechniqueadoptsthesame formasEq. (2.85)andimprovesthebenchmarkrepresentationoncategoricaldata[Szegedy etal.,2016]. Second,wecanreducethesteepnessandincreasethetailednessofapredicted distribution8. This method is often used when the posterior probability of the prediction is required, such as minimum Bayesian risk decoding/training [Bickel and Doksum, 2015; Goodman,1996a;KumarandByrne,2004a]. 2.5.5 Training with Noise Above,wehaveshownthataddingsomeamounttoeachobservedcountofeventsinpredicting aprobabilitycanimprovegeneralization. Fromarobuststatisticspointofview[Olive,2022], thisisequivalenttoimprovingtherobustnessofanestimatorwhereaskeweddistributionoften leadstoabiasedmodel. Theadditionofasmallperturbationtotheestimatecanpreventlarge biasescausedbyoutliersandunexpectedobservationsofrareevents. Inthissense,smoothing can be regarded as a way of introducing noise into training, that is, we impose a prior of uniformdistributionontheestimatethoughthecorrectestimatemaynotbeuniform. Noisytrainingworkswithanideathatamodelislearnedtoworkinnon-idealconditions andavoidoverfittingdatapointsofextremevalues. Herethetermnoisehasawidemeaning, and there are a few different ways to regularize training with noise. One of the simplest methodsistousenoise-sensitivetrainingobjectives. Forexample,smoothingthebenchmark 8IngeneralonemaywantadistributiontobeaMesokurticcurve. 106 Chapter 2. Foundations of Neural Networks distribution(e.g.,theone-hotrepresentationofthecorrectprediction)canbeseenasawayof makingnoisyannotations. Alternatively,wecanaddrandomnoisetotheinput,output,and intermediatestateofaneuralnetwork. AcommonchoiceistheGaussiannoise. Supposewe haveavectorx∈Rn. TheadditionoftheGaussiannoisedefinesanewvector,asfollows x = x+g (2.86) noise whereg∈Rn isavectorofnoise. ItfollowsaGaussiandistribution: g ∼ Gaussian(µ,σ2) (2.87) Forentryk ofg,itdefinestheprobabilityPr(g )tobe: k 1 (cid:18) (g −µ )2(cid:19) Pr(g ) = √ ·exp − k k (2.88) k σ 2π 2σ2 k k whereµ isthemeanofthedistribution,andσ isitsstandarddeviation. Often,µ issetto k k k 0. σ is a hyper-parameter that is used to control the amount of noise we want to add. For k example,alargeσ meansthattherandomnoisespreadsoutinalargeregioncenteredaround k µ ,anditismorelikelytogeneratelargenoise. k Eq. (2.86)isgenericandcanbeappliedtoalmosteverywhereinaneuralnetwork. Givena layery=ψ(x·W+B),thenoise(sayg )canbeaddedtotheinput,likethis input y=ψ((x+g )·W+B) (2.89) input Likewise,thenoise(sayg )canbeaddedtotheactivation(oroutput): output y=ψ(x·W+B)+g (2.90) output Forexample,onecansimplymakenoisyinputs(oroutputs)foramodelandrunallhidden layersasusual,orcanaddrandomnoisetoallactivationsthroughouttheneuralnetwork. While itiscommontoaddrandomnoisetothelayerinputsand/oractivationsinaneuralnetwork [Plautetal.,1986;HolmströmandKoistinen,1992;Bishop,1995b],anotherapproachtonoisy trainingistoaddrandomnoisedirectlytomodelparametersorgradients[Gravesetal.,2013b; Neelakantanetal.,2015]. Forexample,theadditionofnoisetothetransformationmatrixhas thefollowingform: y = ψ(x·(W+g )+B) (2.91) w whereg isthematrixofnoiseandhasthesameshapeasW. Also,wecanaddnoise(say w g )tothegradientoflossforW. Letsdenotex·W+B. Thenoisygradientcanbe gradient 2.5 Regularization Methods 107 writtenas: ∂L ∂L = xT· +g gradient ∂W ∂s (cid:18) (cid:19) ∂L ∂y = xT· ⊙ +g gradient ∂y ∂s (cid:18) (cid:19) ∂L = xT· ⊙ψ′(s) +g (2.92) gradient ∂y Theuseofnoisygradientshasbeenfoundtonotonlybehelpfulforrobusttrainingbutalsoto easethegradientflowinthenetwork[Gulcehreetal.,2016]. Itshouldbenotedthatnoiseisonlypresentduringtrainingandthemodelworkswithout the addition of noise when making predictions on new data. In this sense, many of the regularizationmethodscouldfallunderthenoisytrainingframeworkthatisusedtoprevent fittingthetrainingdatapreciselyandenablethepredictingsystemtogeneralizewellonthe testdata. Forexample,dropoutrandomlyinactivatessomeoftheactivationsofalayersothat everyneuronislearnedtoworkinanoisyenvironment. Whenrunningonthetestdata,allthe neuronsworktogetherasinausualneuralnetwork. Thereisanadditionaladvantagewithnoisytraininginthattheuseofrandomnoisemakes “new” training samples. Even for the same sample, different noise could lead to different trainingresults. Inotherwords,weessentiallytrainthemodelonaninfinitenumberofsamples. This idea is also linked to another line of research on training with synthetic data, called dataaugmentation. Insimpleterms,dataaugmentationisasetofmethodstogeneratenew samples from",
    "In this sense, many of the regularizationmethodscouldfallunderthenoisytrainingframeworkthatisusedtoprevent fittingthetrainingdatapreciselyandenablethepredictingsystemtogeneralizewellonthe testdata. Forexample,dropoutrandomlyinactivatessomeoftheactivationsofalayersothat everyneuronislearnedtoworkinanoisyenvironment. Whenrunningonthetestdata,allthe neuronsworktogetherasinausualneuralnetwork. Thereisanadditionaladvantagewithnoisytraininginthattheuseofrandomnoisemakes “new” training samples. Even for the same sample, different noise could lead to different trainingresults. Inotherwords,weessentiallytrainthemodelonaninfinitenumberofsamples. This idea is also linked to another line of research on training with synthetic data, called dataaugmentation. Insimpleterms,dataaugmentationisasetofmethodstogeneratenew samples from existing samples. An example is back-translation [Sennrich et al., 2016a]. WhendevelopingamachinetranslationsystemfromlanguageAtolanguageB,wecanfirst trainareversetranslationsystem(saytheB→Asystem)onthebilingualdata. Then,weusethe B→Asystemtotranslatesomeadditionaltarget-languagedatatosource-languagedata. This resultsinnewbilingualdatawherethetarget-languagedataisrealandthesource-language data is synthetic. This new data can be used together with other bilingual data to train the A→Bsystem. Inadditiontoback-translation,therearemanydataaugmentationmethodsin NLP,includingreplacingwordswithsynonyms,swappingtwowords,deleting/insertingwords, andsoon. Moreover,wecandosimilarthingsonfeaturevectors,suchasreplacingaword embeddingwithasimilarembedding. Sincedataaugmentationcoversawidevarietyoftopics, wereferthereadertoafewsurveypapersformoreinformation[Fengetal.,2021;Shortenand Khoshgoftaar,2019]. One last note on data augmentation. Synthetic data can be made for some purpose. A popularideaisadversarialmachinelearning. Itgeneratesadversarialsamplesonthata modelwouldmakemistakes(callsuchprocessesattacks)[Szegedyetal.,2014a;Goodfellow etal.,2015]. Themodelislearnedtomakecorrectpredictionsonthesesamples,i.e.,itdefends the attacks. For example, in some cases, the output of a machine translation system would be completely wrong if we change the gender of the subject of the input sentence. For a morerobustsystem,onemaytrainthetranslationmodelbyusingmoregender-balanceddata, gathered either manually or automatically. But it is not easy to craft samples that look like 108 Chapter 2. Foundations of Neural Networks normalsentencesbutcanfoolthemodel[Zhangetal.,2020b]. Thisinturnmakesitinteresting yet challenging to generate adversarial samples in NLP, since a small change in a sentence (such as word replacement) could lead to something with a very different meaning9. The challengealsomotivatesathreadofresearchoninvestigatingadversarialsamplesinNLP[Jia andLiang,2017;BelinkovandBisk,2018;Ebrahimietal.,2018;Alzantotetal.,2018]. 2.6 Unsupervised Methods and Auto-encoders Unsupervised learning is concerned with discovering the underlying patterns in a set of unlabeleddatapoints. Anumberofproblemscanbeviewedasclassicalunsupervisedlearning problems,thoughwewillnotdiscussthemindetailthroughoutthischapter. Forexample,data clusteringistofindgroupingsinacollectionofdataobjects,givennosupervisedsignalson whatthecorrectgroupingis. Anotherwell-knownexampleisassociationrulemining. Itis oftenframedasaprocessofestablishingtherelationshipamongsetsofdataobjects. While theseproblemsareindeedcoveredbyunsupervisedlearning,wewillfocusonproblemsof unsupervisedrepresentationlearningorfeaturelearning,thatis,amodelislearnedtomapan objectfromaninputspacetoalow-dimensionalfeaturevectorspace10. Learning low-dimensional representations has been extensively studied in the context of finding a linear transformation from the original space to the new space. For example, principalcomponentsanalysis(PCA)anditsvariantstrytofindalinearmappingfunctionso thata(high-dimensional)dataobjectcanberepresentedasitscoordinatesalongthedirections of the greatest variance [Pearson, 1901; Wold et al., 1987]. Here we extend the mapping function to its natural non-linear generation and use neural networks as a solution to the mappingproblem. Aswithothermachinelearningmodels,aneuralnetworkistypicallylearnedbyoptimizing modelparameterswithrespecttosomelossfunction. Aconsiderablechallengewithunsuper- visedlearningisthatthereisnobenchmarktosignalthelearning. Asolutiontothisissueisto resorttonon-parametricmethodsorheuristics(seeChapter1). However,suchmethodsthem- selvesarenotdesignedtoaddressthelearningissueoflarge-scaleneuralnetworks,particularly whenaneuralnetworkisbuiltupofahugenumberofparameters. Inunsupervisedlearning ofaneuralnetwork,therefore,itismorecommontousethe“supervision”informationfrom the input data itself. While there are several ways to do this [Hopfield, 1982; Ackley et al., 1985;Dayanetal.,1995;HintonandSalakhutdinov,2006],wefocusonauto-encodersinthis section. Wechooseauto-encodersfordiscussionbecausetheyresemblethegeneralformof supervisedmodelsandcanbetrainedviaback-propagation. Anauto-encoderisatypeofneuralnetworksthattriestoreconstructtheinputdatafrom itsrepresentation. Itisinspiredbytheideaofdimensionalityreduction: 9Bycontrast,incomputervision,itismucheasiertocreateadversarialsamplesbymakingasmallchangein theinput(e.g.,pixels),sincetheinputspaceiscontinuousandasmallinputperturbationhasverylittleeffecton thewholeimage. 10Inadditiontolearningtorepresentdataobjects,thissectionalsocoverssometopicsonthegenerationofdata objects.WewillseetheminSection2.6.3. 2.6 Unsupervised Methods and Auto-encoders 109 High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstructhigh-dimensionalinputvectors. –HintonandSalakhutdinov[2006] Thisalsodevelopstheideaofrepresentationlearninginthattheinformationofanobject canbesufficientlyrepresentedbyalow-dimensionalreal-valuedvector. Typically,anauto- encoderinvolvesa(probablynon-linear)dimensionalityreductionfunction(callitanencoder) tomaptheinputobjecttoitslow-dimensionalfeaturevectorrepresentation(callitacode). Also,itinvolvesareversefunction(callitadecoder)thatmapsthecodebacktotheobject. So,althoughanauto-encoderiscalledan“encoder”,itisnotjustanencoderbutacombination ofanencoderandadecoder. Moreformally,letxbetheinputvectorofthemodel,suchasa high-dimensionalrepresentationofaword. Theencoderspitsoutavectordescribingthecode orlow-dimensionalrepresentationofx,asfollows h = Enc(x) (2.93) where Enc(·) is the encoding network. Enc(·) is typically a multi-layer neural network and works as a plugged-in for other systems. Thus, Enc(·) is a general-purpose model. In subsequentchapters,wewillseemanyexampleswhereencodersaretrainedandappliedas partsof“bigger”systems. Onceweobtainthecode,weusethedecodertomapitbacktotheinput: x = Dec(h) (2.94) (cid:101) where x is the reconstruction of the input, and Dec(·) is the decoding network. Given the (cid:101) originalinputxandthereconstructedinputx,theobjectiveoftheauto-encoderistominimize (cid:101) thediscrepancybetweenxandx. Supposethattheencoderandthedecoderareparameterized (cid:101) by θ and ω, denoted as Enc (·) and Dec (·). The training objective over a set of samples θ ω {x(1),...,x(m)}isdefinedas m (cid:88) (cid:16) (cid:17) (θˆ,ωˆ) = argmin L x(i),x(i) (cid:101) (θ,ω) i=1 m (cid:88) (cid:16) (cid:17) = argmin L x(i),Dec (h(i)) ω (θ,ω) i=1 m (cid:88) (cid:16) (cid:17) = argmin L x(i),Dec (Enc (x(i))) (2.95) ω θ (θ,ω) i=1 whereL(·)isthelossfunctionthatcomputesthediscrepancybetweenxandx. Itissometimes (cid:101) calledthereconstructionloss. Popularlossfunctionsforreconstructionincludemeansquared 110 Chapter 2. Foundations of Neural Networks TrainingObjective: m (θˆ,ωˆ)=argmin (cid:80) L( x(i) ,",
    "x(i),x(i) (cid:101) (θ,ω) i=1 m (cid:88) (cid:16) (cid:17) = argmin L x(i),Dec (h(i)) ω (θ,ω) i=1 m (cid:88) (cid:16) (cid:17) = argmin L x(i),Dec (Enc (x(i))) (2.95) ω θ (θ,ω) i=1 whereL(·)isthelossfunctionthatcomputesthediscrepancybetweenxandx. Itissometimes (cid:101) calledthereconstructionloss. Popularlossfunctionsforreconstructionincludemeansquared 110 Chapter 2. Foundations of Neural Networks TrainingObjective: m (θˆ,ωˆ)=argmin (cid:80) L( x(i) , x(i) ) (cid:101) (θ,ω) i=1 Inputx Reconstructionx Codeh (cid:101) (Bottleneck) 7 7 3 0 3 .1 Encoder 5 Decoder .1 2 h=Enc (x) -1 x=Dec (h) 0 θ (cid:101) ω 0 .8 1 .1 .5 Figure 2.16: An undercomplete auto-encoder with an encoder, a decoder and a bottleneck layersandwichedbetweenthem. Aninputx(left)istransformedintoacodeh(middle)and thenareconstructionx(right). Theparametersofboththeencoderanddecoderareoptimized (cid:101) byminimizingthediscrepancybetweentheinputxandthereconstructionxonanumberof (cid:101) unlabeled samples {x ,...,x }. On new samples, we throw away the decoder, and use the 1 m encodertogeneratenewcodesorrepresentations. errorloss,crossentropyloss,etc. Puttingtogethertheencoderandthedecoder,itistemptingtothinkofanetworkinwhich we feed something into the input layer and get back the same thing out of the output layer. Thechallengehereisthatthelow-dimensionalvectorhservesasabottleneckininformation flow. Thereisariskofinformationlossintransformationeitherfromxtohorfromhtox, (cid:101) makingitdifficultto“copy”theinputtotheoutput. Rather,weneedto“squeeze”anobject fromahigh-dimensionalspacetoadense,low-dimensionalspace,andthen“unsqueeze”it fromthenewspacetotheoriginalhigh-dimensionalspace. Aconsequenceofthissqueeze-and- unsqueezeprocessisthattheencoderisforcedtocompressthedatabutretaintheinformation asmuchaspossible. So,theauto-encoderdiscussedhereisalsocalledtheundercomplete auto-encoder,becausehhasasmallersizethanxandx. Figure2.16showsanillustrationof (cid:101) theundercompleteauto-encoderstructure. GiventhelossfunctionL(·),theencoderEnc (·)andthedecoderDec (·),theparameters θ ω θˆ andωˆ canbeoptimizedbyusingthegradientdescentmethodasinsupervisedlearning(see Section2.4.1). Whenapplyingtheauto-encoder,onecansimplydropthedecoderandusethe encoderasafeatureextractor,thatis,givenanewinputx ,wegenerateanewrepresentation new hˆ = Enc (x ) (2.96) new θˆ new Notethattheencoderisnotastandalonesystembuttypicallyworkswithothermodelsfor 2.6 Unsupervised Methods and Auto-encoders 111 acompleteworkingsystem. Forexample,wecantrainanauto-encoderonsomesentences and place a Softmax layer on the output of the encoder to build a sentence classifier. The classifier can be further trained on some task-specific data to solve a new problem, such as taggingasentencewithitssentimentpolarity. Thisalsomakestheapplicationofauto-encoder fallunderthegeneralparadigmofpre-training: asub-model(i.e.,anencoder)isfirsttrained on large-scale, task-irrelevant data, and then used as a component of a bigger model on a downstreamtask. 2.6.1 Auto-encoders with Explicit Regularizers As more complex neural networks are involved, an auto-encoder tends to learn an identity transformationalthoughthebottleneckmakesitabithardertopassthroughwithoutinformation loss. Thisiswhatwewouldordinarilyexpect: wecouldmakehasurrogateofxanddecode htosomethingverysimilartox. Ontheotherhand,learninganexactidentitytransformation requires a highly complicated model and is prone to overfitting. Fortunately, as with other machinelearningmodels,wecanregularizethetrainingbyusingthemethodspresentedin Section2.5. Oneofthemostpopularmethodsisaddinganexplicitregularizationtermtothe lossfunction. TakingtogetherEq. (2.74)andEq. (2.95),wecandefinethetrainingobjective tobe m (cid:88) (cid:16) (cid:17) (θˆ,ωˆ) = argmin L x(i),Dec (Enc (x(i))) +α·R (2.97) ω θ (θ,ω) i=1 whereRistheregularizationtermaccountingforsomepriorknowledgewewanttoimpose ontraining,andαisitscoefficient. AcommonchoiceforRinauto-encodersisthesparsity penalty(alsoknownassparseauto-encoders). Thesimplestwaytoimplementsuchapenalty istoapplythel orl normonthecode,asfollows 1 2 m R = (cid:88)(cid:88)(cid:12) (cid:12)h (i) (cid:12) (cid:12) (2.98) l1 (cid:12) k (cid:12) i=1 k m (cid:115) (cid:88) (cid:88)(cid:0) (i)(cid:1)2 R = h (2.99) l2 k i=1 k It is worth noting that, unlike those penalties on model parameters (see Section 2.5.1), the sparsity penalty regularizes the code h (or the output of the encoder) to be sparse. The ideaofencouragingsparsenessinrepresentationsstemsfromsparsecoding[Olshausenand Field,1997]. Itstatesthattheinformationofanobjectisembeddedincomplexdependencies amongtheoriginalattributes(orfeatures)oftheobject. Adesirablerepresentationlearning systemshouldextractsuchdependenciesandreformthemtobeasetofindependentfeatures. Andthereshouldbeasmallnumberoftheseindependentfeaturesthatareactive,whilethe activefeaturesvarywhenweswitchtoanewobject. Notealsothat,fromaBayesianlearning pointofview,otherpenaltiesinregularizedtrainingcouldbeinterpretedaspriorsovermodels. Thesparsitypenalty,however,isnotapriorbecauseitdoesnotdependonmodels(ormodel 112 Chapter 2. Foundations of Neural Networks parameters) but on the training data [Goodfellow et al., 2016]. In this view, the sparsity penaltyshouldnotbetreatedasa“regularization”term,butsimplysomedistributionoverthe model’sintermediatestates. Ontheotherhand,thesparsenessofthecode,thoughnotwell explainedbyconventionaluseofregularizationterms,isindeedhelpfulinmanyapplications ofauto-encoders,becauseitdirectlymodelsthewayofrepresentingtheinputandimposing “priors” on outcomes of encoders. When considered from an empirical point of view, the sparsitypenaltyisstillthoughtofasaregularizerthatbiasesthetrainingtocertainmodels. ThereareotherchoicesfordefiningtheregularizationtermRinadditiontoEqs. (2.98- 2.99). Forexample,awayofforcingsparsityistopenalizethecaseswheretheaveragevalue ofeachentryh isfarawayfromapredefinedvalue[NairandHinton,2009]. Incasethath k k chooses values from [0,1], the regularization can be implemented by defining R as the KL divergencebetweentheaveragecodeoveranumberofsamplesandtheexpectedcode11. Let h¯ denote the average code over {x ,...,x }, where the value of h¯ is the mean of the k-th 1 m k variableofthecode: m 1 (cid:88) h¯ = · Enc(x(i))(k) (2.100) k m i=1 Also,letqbetheexpectedcode,whereq =τ foranyk. Ifeachentryoftheaveragecode k isviewedasaBernoullirandomvariablewithmeanh¯ ,andeachentryoftheexpectedcodeis k viewedasanotherBernoullirandomvariablewithmeanτ,thentheregularizationtermcanbe definedasthesumoftheKLdivergencebetweenh¯ andq overallentries: k k (cid:88) R = KL(h¯ ,q )",
    "over {x ,...,x }, where the value of h¯ is the mean of the k-th 1 m k variableofthecode: m 1 (cid:88) h¯ = · Enc(x(i))(k) (2.100) k m i=1 Also,letqbetheexpectedcode,whereq =τ foranyk. Ifeachentryoftheaveragecode k isviewedasaBernoullirandomvariablewithmeanh¯ ,andeachentryoftheexpectedcodeis k viewedasanotherBernoullirandomvariablewithmeanτ,thentheregularizationtermcanbe definedasthesumoftheKLdivergencebetweenh¯ andq overallentries: k k (cid:88) R = KL(h¯ ,q ) k k k (cid:88) τ 1−τ = τ·log +(1−τ)·log (2.101) h¯ 1−h¯ k k k Inthisform,Rpenalizesthemodelwhenh¯ deviatesfromq. Asanotherauto-encodervariant,thecontractiveauto-encoder(CAE)triestoimprove therobustnessofrepresentationbyintroducinganewregularizationtermintotraining[Rifai etal.,2011]: (cid:88) m (cid:13) (cid:13)∂h(i) (cid:13) (cid:13) 2 R = (cid:13) (cid:13) (cid:13)∂x(i)(cid:13) (cid:13) (cid:13) i=1 F (cid:88) m (cid:13) (cid:13)∂Enc(x(i)) (cid:13) (cid:13) 2 = (cid:13) (cid:13) (2.102) (cid:13) ∂x(i) (cid:13) (cid:13) (cid:13) i=1 F where ∂Enc(x(i)) (or ∂h(i) ) is the Jacobian matrix of the representation 12, and ∥·∥ is ∂x(i) ∂x(i) F 11Ingeneral,wecansetallentriesoftheexpectedcodetoτ ∈[0,1].Sparsecodeswillbepreferredifτ isclose to0,asfeaturesare“inactive”inmostcases.Bycontrast,densecodeswillbepreferredifτ iscloseto1. 12SupposethattheencoderisafunctionEnc(·):x∈Rdx →h∈Rdh.TheJacobianmatrixofh=Enc(x)is 2.6 Unsupervised Methods and Auto-encoders 113 the Frobenius norm of a matrix 13. The contractive penalty helps resist the influence of smallperturbationstotheinput. Inthegeometricsense,itencouragesthattheneighborhood relationshipholdsforoutputdatapointsiftheinputdatapointsareneighborhoods,inother words, it forces Enc(·) to behave more like a contraction mapping14, hence the name of contractiveauto-encoder. 2.6.2 Denoising Auto-encoders Anothersourceofinspirationforimprovingtherobustnessofamodelarisesfromthedenoising idea: weaddnoisetotheinputandthenremoveittorecovertheoriginalinput. Denoisingauto- encoders(DAEs)aresuchatypeofneuralnetworksthatmarriestheideaofauto-encoding withtheideaofdenoising. First,noiseisaddedtotheinputvectorinastochasticmanner. This canbedescribedasaprocessofgeneratinganoisyinputx giventheoriginalinputx: noise x ∼ Pr (x |x) (2.106) noise noise noise wherePr (·)isadistributionforsamplingx . Forexample,wecanfollowthemethod noise noise presentedinSection2.5.5andtakethenoisyinputasamultivariateGaussianvariable: x ∼ Gaussian(x,σ2) (2.107) noise whereGaussian(µ,σ2)generatesx viaaGaussiandistributionwiththemeanµandthe noise variance σ2. Eq. (2.109) introduces an additive noise to the input. Subtracting x from the mean,wehave x = x+g (2.108) noise g ∼ Gaussian(0,σ2) (2.109) ad h ×dxmatrix: ∂h Jacobian = ∂x (cid:104) (cid:105) = ∂h ... ∂h ∂x1 ∂xdx  ∂h1 ··· ∂h1  ∂x1 ∂xdx =   . . . ... . . .   (2.103)   ∂hdh ··· ∂hdh ∂x1 ∂xdx 13ForamatrixA∈Rdh×dx,theFrobeniusnormisgivenbytheequation: (cid:115) (cid:88) ∥A∥ = A2 (2.104) i,j i,j 14LetXbeametricspacewithametricd.Givenafunctionf(·)fromXtoX,f(·)isacontractionmappingif andonlyifthereisanumberϵsuchthatforanyx 1 ,x 2 ∈X: d(f(x 1 ),f(x 2 ))≤ϵ·d(x 1 ,x 2 ) (2.105) 114 Chapter 2. Foundations of Neural Networks TrainingObjective: m (θˆ,ωˆ)=argmin (cid:80) L( x(i) , x(i) ) (cid:101) (θ,ω) i=1 CorruptedInput Inputx Noiseg x noise =x+g Codeh Reconstructionx (cid:101) (Bottleneck) 7 0 7 7 3 0 3 0 3 .1 + .2 = .3 Encoder 5 Decoder .1 2 0 2 h= -1 x=Dec (h) 0 (cid:101) ω 0 .5 .5 Enc (x ) .8 1 θ noise .1 .1 .2 .5 Figure2.17: Thestructureofadenoisingauto-encoder. Aninputxisfirstcorruptedintoa noisyorcorruptedinputx . Then,itispassedthroughanencodertoformacodeh. Then, noise thecodeispassedthroughadecodertoformareconstructedinputx. Thetrainingisperformed (cid:101) byminimizingthelossbetweenxandx. Thisprocessistermed“denoising”becauseittriesto (cid:101) removethenoisefromx andrecovertheoriginalinputx. noise Sometimes, this process is called the corruption of the input, and x is called the noise corruptedinput. AsidefromadditiveGaussiannoise,thereareafewdifferentwaystocorrupt the input [Vincent et al., 2010]. One of the popular methods is to zero some of the entries of x. For example, we can set each entry to 0 with a pre-defined probability. This is also calledmaskingnoise. Anothermethodistousesalt-and-peppernoiseorimpulsenoisefor corruption. Itrandomlychoosessomeoftheentries,andsetseachofthemtoaminimumor maximumvaluewithapre-definedprobability. Differenttypesofnoiseareappliedtodifferent applicationsofauto-encoders. Forexample,themaskingnoiseispopularintraininglanguage models,andthesalt-and-peppernoiseismorecommonlyusedinimageprocessing. Then,thecorruptedinputx isfedintoanencoder-decodernetwork,andthenetwork noise produces a reconstructed input x=Dec(Enc(x )). The training process is regular. We (cid:101) noise reuseEq. (2.95)tominimizethelossofreplacingxwithx. Thus,wecanrewriteEq. (2.95)to (cid:101) adapttheobjectivetothedenoisingcase: m (θˆ,ωˆ) = argmin (cid:88) L (cid:16) x(i),Dec (Enc (x (i) )) (cid:17) (2.110) ω θ noise (θ,ω) i=1 Eq. (2.110)differsfromEq. (2.95)onlyinthattheinputoftheauto-encoderisx instead noise ofx. Inotherwords,wedenoisethecorruptedinputtorecovertheoriginalinput. SeeFigure 2.17forthestructureofdenoisingauto-encoders. Notethatbothcontractive/sparseauto-encodersanddenoisingauto-encoderscanbethought 2.6 Unsupervised Methods and Auto-encoders 115 of as ways",
    "process is regular. We (cid:101) noise reuseEq. (2.95)tominimizethelossofreplacingxwithx. Thus,wecanrewriteEq. (2.95)to (cid:101) adapttheobjectivetothedenoisingcase: m (θˆ,ωˆ) = argmin (cid:88) L (cid:16) x(i),Dec (Enc (x (i) )) (cid:17) (2.110) ω θ noise (θ,ω) i=1 Eq. (2.110)differsfromEq. (2.95)onlyinthattheinputoftheauto-encoderisx instead noise ofx. Inotherwords,wedenoisethecorruptedinputtorecovertheoriginalinput. SeeFigure 2.17forthestructureofdenoisingauto-encoders. Notethatbothcontractive/sparseauto-encodersanddenoisingauto-encoderscanbethought 2.6 Unsupervised Methods and Auto-encoders 115 of as ways to improve the robustness of auto-encoders. Their difference lies in that they regularize the training at different points of the model. Contractive auto-encoders aim at improving the robustness of encoding, that is, the representation is learned to be not so sensitive to small perturbations to the input. Denoising auto-encoders, on the other hand, aim at improving the robustness of reconstruction. It affects both encoders and decoders simultaneously. In some sense, denoising auto-encoders are direct applications of noisy training to auto-encoders (see Section 2.5.5). It is of course difficult to say which models arebetter. Forexample,contractiveauto-encodershavemoredirectguidanceonlearningthe representation, which is what we are concerned the most about. The training of denoising auto-encoders,thoughhasanindirecteffectonencoding,receivesadditionaldenoisingsignals fromthedecoder. Thisoffersanewviewofrobusttraining: arobustrepresentationcanbe learned in both where it is generated (the denoising encoder) and where it is applied (the denoisingdecoder). 2.6.3 Variational Auto-encoders variationalauto-encoders(VAEs)werenotinitiallyproposedtomodeltheencodingproblem, althoughitistermedan“auto-encoder”. Theyaretypicallyusedtogeneratenewdatasimilar toobserveddata,hencehavingverydifferentformulationsfromtheclassicalauto-encoderswe mentionedabove. Instatisticsandmachinelearning,variationalauto-encodersaremoreoften viewedasinstancesofvariationalBayesianmethodsandusedtoperformefficientstatistical inferenceoverlatentvariableswhentheposteriorprobabilitiesofthesevariablesareintractable [KingmaandWelling,2014;2019]. Ontheotherside,variationalauto-encoders,implicitly orexplicitly,dealwithwhatwedoininducingtheunderlyingrepresentationofanobserved object. Wethereforeinvolveitinthissectionforarelativelycompletediscussion. Webeginwithagenerativestorydescribinghoweachdatapointisgenerated. Suppose that, for an observed sample x in our dataset, there is an unobserved latent variable h that describesx. Nowweintendtodevelopaprobabilisticmodeltomodelthegenerationprocess ofx,say,estimatingtheprobabilityPr(x). Thiscanbeobtainedbycomputingthemarginal distribution: (cid:90) Pr(x) = Pr(x,h)dh (2.111) whereweexplicitlyintroducethelatentvariablehintotheinferenceofx. TosolveEq. (2.111), weuseamodelp (x,h)toapproximatePr(x,h)(i.e.,p (x,h)≈Pr(x,h)),andwehave ω ω (cid:90) p (x) = p (x,h)dh (2.112) ω ω wherep (x,h)isaprobabilitydensityfunctionparameterizedbyω. Wereplacetheleft-hand ω sideofEq. (2.113)withp (x)toemphasizethattheprobabilityisdeterminedbythemodel ω p (·). Therearegenerallymanywaystodefinep (x,h). Herewecansimplythinkofitasa ω ω neuralnetwork. 116 Chapter 2. Foundations of Neural Networks Then,wecanrewriteEq. (2.113)byusingthechainrule: (cid:90) p (x) = p (h)·p (x|h)dh (2.113) ω ω ω wherep (h)istheprioroverh,e.g.,aGaussianprior. Theconditionalprobabilityp (x|h) ω ω describes how likely x is observed given the latent variable h. To model this generation process,p (x|h)isoftenassumedtobeaGaussiandistributionthatisparameterizedwithits ω meanµ andvarianceσ : p p p (x|h) = Gaussian(µ ,σ ) (2.114) ω p p whereµ andσ aredeterminedbyadecodingnetworkDec (·)(wewillexplainlateronwhy p p ω itiscalled“decoding”): (µ ,σ ) = Dec (h) (2.115) p p ω However,Eq. (2.113)isstillintractableeventhoughp (h)andp (x|h)arebothtractable, ω ω because it is impossible to summing over all possible h’s. This also leads to an intractable posterior: p (h)·p (x|h) ω ω p (h|x) = (2.116) ω p (x) ω Itlookslikewearestuckwithp (x)andp (h|x)! Variationalauto-encodersaddressthis ω ω issuebyapproximatingp (h|x)withatractableposteriorq (h|x): ω θ q (h|x) ≈ p (h|x) (2.117) θ ω where θ is the parameter of the new model. Like Eqs. (2.114-2.115), q (h|x) is defined as θ anotherGaussiandistribution: q (h|x) = Gaussian(µ ,σ ) (2.118) θ q q (µ ,σ ) = Enc (x) (2.119) q q θ whereEnc (·)istheencodingnetworkthatreadsxandgeneratesthemeanandvarianceof θ thedistributionq (h|x). Thisisinteresting! WenowhaveafeasiblepathtocomputePr(x): θ we first sample a latent variable h via q (h|x), and then compute p (x) via the product of θ ω 2.6 Unsupervised Methods and Auto-encoders 117 p (h)andp (x|h). Inthiscase,thelog-scaleprobabilityoftheobservationisdefinedtobe ω ω logPr(x) ≡ E logp (x) h∼q (h|x) ω θ p (h)·p (x|h) = E log ω ω h∼q θ (h|x) p (h|x) ω p (h)·p (x|h) q (h|x) = E log ω ω · θ h∼q θ (h|x) p (h|x) q (h|x) ω",
    "117 p (h)andp (x|h). Inthiscase,thelog-scaleprobabilityoftheobservationisdefinedtobe ω ω logPr(x) ≡ E logp (x) h∼q (h|x) ω θ p (h)·p (x|h) = E log ω ω h∼q θ (h|x) p (h|x) ω p (h)·p (x|h) q (h|x) = E log ω ω · θ h∼q θ (h|x) p (h|x) q (h|x) ω θ q (h|x) p (h)·p (x|h) = E log θ +E log ω ω (2.120) h∼q θ (h|x) p (h|x) h∼q θ (h|x) q (h|x) ω θ Thefirsttermoftheright-handsideofEq. (2.120)istheKLdivergence(relativeentropy) betweenq (h|x)andp (h|x),i.e., θ ω q (h|x) D(q (h|x)||p (h|x)) = E log θ (2.121) θ ω h∼q θ (h|x) p (h|x) ω Thus,givenD(q (h|x)||p (h|x))≥0,wehave15 θ ω p (h)·p (x|h) logPr(x) ≥ E log ω ω h∼q θ (h|x) q (h|x) θ (cid:20) (cid:21) p (h) = E logp (x|h)+log ω h∼q θ (h|x) ω q (h|x) θ = E logp (x|h)+D(p (h)||q (h|x)) (2.122) h∼q (h|x) ω ω θ θ The right-hand side of Eq. (2.122) is a lower bound of the likelihood logPr(x). It is also known as the evidence lower bound (ELBO). The first term of the ELBO can be approximatelycomputedbysamplingdifferenth’s. Also,computingthesecondtermisnot difficultbecausethereisananalyticalformforD(p (h)||q (h|x))iftheformsofp (h)and ω θ ω q (h|x))aregiven. LetL(x,θ,ω)denotethenegativeELBO.Then,thetrainingprocessofa θ variationalauto-encodercanbeframedasminimizingL(·)overanumberofobservedsamples {x(1),...,x(m)}: m (cid:88) (cid:16) (cid:17) (θˆ,ωˆ) = argmin L x(i),θ,ω (2.123) (θ,ω) i=1 Notethat,becausesamplinghfromq (h|x)isanon-continuousoperation,E logp (x|h) θ h∼q (h|x) ω θ isnotstraightforwardlydifferentiated. Tofitthetrainingofauto-encodersinstandardback- prorogation,acommonwayistousetheso-calledreparameterizationtrick. Hereweskip thedetailsandreferthereadertoafewpapersformoreinformation[KingmaandWelling, 2014;Doersch,2016]. Figures2.18illustrateshowavariationalauto-encoderworks. Itpresentsuswithatwo-step generationprocess: 15TheKLdivergencebetweenpandqiszeroonlyifp=q,andispositiveotherwise. 118 Chapter 2. Foundations of Neural Networks m TrainingObjective: (θˆ,ωˆ)=argmin (cid:80) L(x(i),θ,ω) (θ,ω) i=1 EncodingStep: q (h|x) DecodingStep: p (x|h) θ ω µ p Inputx 7 ρ2 Inputx µ p q 3 Codeh .1 7 .1 ρ2 .1 7 q .2 3 4 0 .2 3 .1 .1 -1 2 .1 Encoder 5 Decoder .1 .2 .1 .9 4 2 Enc θ (x) .1 -1 Dec ω (h) .3 2 0 .3 .8 .1 0 .1 .1 sampling sampling Gaussian( µ q , ρ2 q ) Gaussian( µ p , ρ2 p ) Figure2.18: Thegenerativestoryofavariationalauto-encoder. Foraninputsamplex, we generate a latent variable h by using an encoder q (h|x). In the encoding step, a neural θ networkEnc (·)isfirstusedtoproducethemeanandvarianceofaGaussiandistribution,say, θ µ and σ2. The latent variable h is then drawn according to Gaussian(µ ,σ2). After that, q q q q weregeneratetheoriginalsamplexbyusingadecoderp (x|h). Inthedecodingstep,like ω thegenerationprocessintheencoder,aneuralnetworkDec (·)isusedtogeneratethemean ω µ andvarianceσ2 ofGaussian(µ ,σ2). Thesameinputxisspittedoutbysamplingfrom p p p p Gaussian(µ ,σ2). p p • Encoding. For an input sample x, we sample a latent variable h from q (h|x). This θ involvesanencodingnetworkEnc (x)thatgeneratesthemeanµ andvarianceσ2 of θ q q the Gaussian distribution q (h|x). The latent variable is then generated by sampling θ fromGaussian(µ ,σ2). q q • Decoding. Forthelatentvariableh,wesampletheoriginalinputxfromp (x|h). It ω follows again a Gaussian sampling process: a decoding network Dec (h) is used to ω determinethemeanµ andvarianceσ2 ofthedistribution. xisgeneratedbyfollowing p p Gaussian(µ ,σ2). p p Sometimes,q (h|x)andp (x|h)arecalledan“encoder”anda“decoder”,astheytryto θ ω “map”aninputtoarepresentationandthen“map”itbacktotheinput. However,q (h|x)and θ p (x|h)themselvesimplysomenon-deterministicmodels,thatis,theyoutputtheprobability ω densityfunctionsofthevariablesratherthanpointestimates. Animportantconsequenceof 2.7 Summary 119 this result is that variational auto-encoders do not tend to find the “best” representation for the input. At first glance it sounds weird as every model we have talked about so far can give",
    "(h|x)andp (x|h)arecalledan“encoder”anda“decoder”,astheytryto θ ω “map”aninputtoarepresentationandthen“map”itbacktotheinput. However,q (h|x)and θ p (x|h)themselvesimplysomenon-deterministicmodels,thatis,theyoutputtheprobability ω densityfunctionsofthevariablesratherthanpointestimates. Animportantconsequenceof 2.7 Summary 119 this result is that variational auto-encoders do not tend to find the “best” representation for the input. At first glance it sounds weird as every model we have talked about so far can give a fixed value output. This, however, is the case of the Bayesian inference — we only learn a distribution over possible values of a latent variable. On the empirical side, if you wanttoobtainsomethinglikeagoodrepresentation,itisfinetojustsampleavaluefromthat distributionyoudeveloped. Itwouldbeahighprobabilitythatyougetanot-so-badoutcomeif yourmodelworkswell[Knight,2009]. Inpractice,themainuseofvariationalauto-encodersisingenerationbutnotrepresentation. Attesttime,providedtheoptimizedparametersθˆ andωˆ,theencoder(i.e.,q (h|x))isremoved, θˆ andthedecoder(i.e.,p (x|h))workswithrandomlygeneratedh’s. Moreprecisely,wesample ωˆ alatentvariableh fromaGaussiandistribution,andinferasamplex byp (x|h ) new new ωˆ new asusual. WewillseeinthesubsequentchaptersthatmanyNLPproblemscanbecategorized as generation problems where sequential or hierarchical data objects are generated on the conditionofsomegivendataobjectsorlatentvariables. 2.7 Summary Inthischapterwehavetalkedaboutwhataneuralnetworkis,aswellasafewbasicarchitec- tures, whicharecommonlyusedasbuildingblocksinconstructingpowerfuldeeplearning systems. Also, we have talked about how to train neural networks, how to regularize the training process, and how to apply neural networks to feature learning in an unsupervised manner. Butneuralnetworksanddeeplearningarewide-rangingtopicsandallofourdiscussionsare alittle“peek”intothem. Foramorecomprehensiveintroductiontothesetopics,Goodfellow etal.[2016]’sbookmaybeagoodchoice. Italsocoversseveraladvancedtechniques,such asdeepstructuredmodelsandrandomizedmethods,fordevelopingstate-of-the-artsystems. However,asalways,thereisabigdifferencebetweenknowingwhatatechniqueisandbeing fluentwithusingitinsolvingreal-worldproblems. So,forpractitionerswhowanttoapply neuralnetworksanddeeplearninginevensimplesituations,thereareanumberofbookson implementationdetailsofdeeplearningsystems[Géron,2019;Zhangetal.,2021;Chollet, 2021],andopen-sourceprojectsthatprovidecode-basesforreference16. In the following chapters, we will dig into how to use neural models to address NLP problems. Alongtheway,wewillseehowtolearntherepresentationofwordsandsentences usingthemethodswehavediscussedsofar(Chapters3-4),andhowtomodeldifferentNLP problemsbyusingseveralinterestingneuralnetwork-basedmethods,includingtheattention mechanismandTransformers(Chapters5-6),pre-training(Chapter7),largelanguagemodels (Chapters8-10),andsoon. 16URLstoafewpopularonlinetutorials:https://pytorch.org/tutorials,https://keras.io/ examples/nlp,andhttps://www.tensorflow.org/tutorials II Basic Models 3 Words and Word Vectors ................. 123 3.1 Tokenization 3.2 VectorRepresentationforWords 3.3 Count-basedModels 3.4 InducingWordEmbeddingsfromNLMs 3.5 WordEmbeddingModels 3.6 EvaluatingWordEmbeddings 3.7 Summary 4 Recurrent and Convolutional Sequence Models 171 4.1 ProblemStatement 4.2 RecurrentModels 4.3 Memory 4.4 ConvolutionalModels 4.5 Examples 4.6 Summary 5 Sequence-to-Sequence Models ........... 211 5.1 Sequence-to-SequenceProblems 5.2 TheEncoder-DecoderArchitecture 5.3 TheAttentionMechanism 5.4 Search 5.5 Summary 6 Transformers ............................ 269 6.1 TheBasicModel 6.2 Syntax-awareModels 6.3 ImprovedArchitectures 6.4 EfficientModels 6.5 Applications 6.6 Summary https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 3 Words and Word Vectors Wordsarebasicunitsoflanguage[Jackendoff,1992]. Mostlanguagesystemsthatpeopleuse toexpresstheirfeelingsandcommunicatewithothersinvolvecreating,mixing,andcombining wordsinsomeway. Beforeunderstandinghowawordisusedinforminglargerlanguageunits, itisworthfirstunderstandingwhatawordis. Thisinvolvestwofundamentalquestions: • Whatisthesurfaceformofaword? • Whatisthemeaningofaword? Butthesequestionsaredifficult,ofcourse,becausetherearenosimplerulestodescribe howawordisformedandhowitsmeaningisdefinedorinduced. Whilethereareavarietyof theoriestoanswerthesequestionsinlinguistics,NLPresearchersareconcernedmorewith twopracticalissues: • Tokenization: givenastring,howtosegmentitintoasequenceofwords(alsocalled tokens)suchthatthesewordscanbeusedasbasicunitsindownstreamNLPtasks? • WordRepresentationLearning: givenacorpus,howtolearntorepresenteachword insomecountableform, andhowtoenableNLPmodelsto“compute”ontopofthis representation? One goal of this chapter is to show how a sentence is segmented in either a linguistic or statistical manner. Specifically, we describe several approaches to tokenizing a string of charactersintowordsorsubwordsbyheuristicrulesorstatisticalmodelslearnedfromdata. The other goal here is to show how words can be represented as real-valued vectors. In particular,wepresentmodernapproachestolearningandevaluatingthesewordvectors. The value of this part is not on drilling on those formulas and models but on showing the core ideaofwordvectorrepresentationwhichisthebasisofmanyNLPsystems. Inthenextfew chapters,wewillseeanaturalgenerationofthisideatomodelingmorecomplicatedproblems, suchasrepresentingsequentialandtree-likedata. 124 Chapter 3. Words and Word Vectors Input: 一直以来，完美的机器翻译是人类的梦想之一。 Chinese Output: 一直/以来/，/完美/的/机器翻译/是/人类/的/梦想/之一/。 Input: 西日本や海はく晴れて、汗ばむ暑さとなる。 Japanese Output: 西日本/や/海/は/く/晴れて/、/汗ばむ/暑さ/と/なる/。 Input: Shesaid,“Deeplearningisnotthesolutiontoallworld’sproblems”. English Output: She/said/,/“/Deep/learning/is/not/the/solution/to/all/world/ ’s/problems/”/. Figure3.1: Tokenizationfordifferentlanguages(slash=wordboundary). ForChineseand Japanese where there are no delimiters between words, tokenization is often called word segmentation. 3.1 Tokenization In computer science and related fields, the term token can be used in many different ways. Herewesimplythinkofatokenasawordinlinguistics,althoughitcanbesomethingdifferent (seeSection3.1.4). InNLP,tokenizationorsegmentationisataskrelatedtomorphological analysis[AronoffandFudeman,2011]. Whilemorphologicalanalyzersorparsersaregenerally usedtostudytheinternalstructureofwords,tokenizationisconcernedwithhowsentencesare brokendownintowords. Itappearsthatweneedtoknowhowwordsarecomposedifwewant toknowhowsentencesareformedbywords. Thingsareevenmoreinterestingbecausethe varietyoflanguagesmakesitdifficulttofindageneralsystemtodescribethemorphologyof everylanguage. Forexample,analyticlanguages(suchasChinese)havelittleinflection,and relyonwordordertoconveymeaning. Bycontrast,syntheticlanguages(suchasFrench)may haverichinflectionandthemeaningofawordishighlyinfluencedbymorphology. Ontheotherhand,dividingsentencesintosmallerlinguisticpiecesisimportantinmany NLPtasks,eventhoughmanyoftheworld’slanguageshavelittlemorphology. Forexample, Chineseisamorphologicallysimplelanguagethathasnoexplicitwordboundaries. Whileit alsomakessensetotakecharactersasunitsinunderstandingwhataChinesetextistalking about,itismoredesirableandreasonabletoconsiderlargerunitsinprocessingthetext. Note that,evenforlanguageshavingdelimitersbetweenwords,suchasEnglish,westillhaveto tokenizesentencessuchthattheyarestandardizedwhenservingastheinputand/oroutputof anNLPsystem. Inthissection,weskipthediscussiononwhatexactlyawordisinmorphologyandsyntax, butsimplyviewtokenizationasataskofaddingwordortokenboundariestoagivenstring (seeFigure3.1). Wewillshowthatasentencecanbebrokendownintowordsortokensin eitheraheuristicorstatisticalmanner. Notethatthisprocessisdesignedtoproducesomeunits that can ease the processing of languages in NLP systems, not necessarily to make strictly linguisticsense. 3.1 Tokenization 125 3.1.1 Tokenization via Rules and Heuristics A common and simple approach to tokenization is to identify every word in a sentence by applying a set of pre-defined rules. In general, these rules",
    "can ease the processing of languages in NLP systems, not necessarily to make strictly linguisticsense. 3.1 Tokenization 125 3.1.1 Tokenization via Rules and Heuristics A common and simple approach to tokenization is to identify every word in a sentence by applying a set of pre-defined rules. In general, these rules are linguistically motivated and reflectourpriorknowledgeofwhattheformofawordshouldbe. Forexample,considerthe EnglishexampleinFigure3.1. Wecandefinethefollowingrulesfortokenizingthesentence: • Words do not contain spaces. In this sense, we can split the sentence into “word candidates”withspace. • EverywordcandidatethatismadeupofEnglishlettersonly(i.e.,a-zandA-Z)isaword. • Everypunctuationmark(i.e.,quote,comma,period,etc.) shouldbeisolatedtoforma word. • ’sisaword,indicatingnounpossessive. This might be one of the smallest rule sets we can use in English tokenization. Surely, morerulescanbeaddedtocovermorelinguisticphenomena,e.g.,wordswithdashes,words containing non-English letters, and so on. However, there are no standards to define such a set of rules. In practice, and particularly in NLP applications, we want a minimal set of rulestodealwithmostproblems,andthetokenizationisusuallyimplementedbyanumberof regularexpressions. Herewewillnotdiscusstheserulesandregularexpressionsindetail, butreferthereadertoafewtextbooksformoredetails[Lawson,2003;Friedl,2006;Jurafsky andMartin,2008]1. Also,itiscommontonormalizethetextbeforetokenizationsothattheinputofthetok- enizeriscanonical. Forexample,forEnglishandotheralphabeticlanguages,normalization orcanonicalizationreferstoaprocessoflowercasingwords,normalizingcharacterrepresen- tation(e.g.,Unicodecharacters),andsoon. Inaddition,wecanmapdifferentformsofaword tothesameformforfurthergeneralizationofthetokenization. Asimplewaytodothisisto conflateallinflectedformsofawordintoitsbaseform. Inlinguistics,thebaseformofaword iscalledlemma,andtheprocessofmappingwordstolemmasiscalledlemmatization. Here aresomeexamplesoflemmatization. learn → learn learning → learn learns → learn best → good Therearewordsthatcorrespondtotwoormoredifferentlemmas(oftenwithdifferentpart-of- speeches). Inthiscase,weshouldselectthecorrectlemmaaccordingtothecontext. Inother words,lemmatizationiscontext-dependent. 1Tokenization scripts can be found in many open-source projects, such as Moses [Koehn et al., 2007] (https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/ tokenizer.perl) and the tokenizers in SacreBLEU (https://github.com/mjpost/sacrebleu/ tree/master/sacrebleu/tokenizers). 126 Chapter 3. Words and Word Vectors Original Shesaid,“Deeplearningisnotthesolutiontoallworld’sproblems”. Normalization shesaid,“deeplearningisnotthesolutiontoallworld’sproblems”. Tokenization she/said/,/“/deep/learning/is/not/the/solution/to/all/world/’s/problems/”/. Lemmatization she/say/,/“/deep/learning/be/not/the/solution/to/all/world/’s/problem/”/. Stemming she/said/,/“/deep/learn/is/not/the/solut/to/all/world/’s/problem/”/. Figure3.2: Normalization,lemmatization,andstemmingofanEnglishsentence. Innormal- ization,thewholesentenceislowercased. Inlemmatization,everywordislemmatizedand rewrittenasitslemma. Instemming,thesuffixesofsomewordsareremoved. Closelyrelatedtolemmatizationisstemming,whichrepresentsawordasitsstem. Like lemmas,astemissomebaseformofaword. However,unlikelemmas,astemisnotnecessarily avalidword,althoughtherearemanywordswhoselemmasandstemsareidentical. Another differencefromlemmatizationisthatstemmingisperformedonindividualwords,withoutthe needofcontextfordisambiguation. So,stemmingiscontext-independent. Thereareseveral efficientalgorithmsforstemming. Apopularoneissuffixstripping[Porter,1980]. Itsimply removesthesuffixesing,ed,ion,etc.,likethese remove → remov removing → remov removal → remov best → best Formoreexamples,Figure3.2showsnormalization,lemmatization,andstemmingresultsfor anEnglishsentence. Itisworthnotingthattheabovemethodsaretypicallyimplementedusingregularexpres- sions, dictionary lookups, and additional heuristics. While in our little exploration here it seems that tokenization is not so difficult, much more work is needed to make it practical. Inparticular,ifwedealwithlanguageswithanon-alphabeticwritingsystem,orlanguages withoutexplicitspacingbetweenwords,thentokenizationwouldbeahardproblem,andin thatcase,usingsimpleruleswouldnotbeagoodstrategy. Inthefollowingsubsections,we willreframetokenizationasamachinelearningproblemwherethewaytotokenizeorsegment sentencesislearnedfromdata. Thesemethodsarelanguage-independentandcanbeappliedto awiderangeoftokenizationorsegmentation-likeproblems. 3.1.2 Tokenization as Language Modeling Nowletusmovetostatisticalmodelingofthetokenizationproblem. Foreaseofdiscussion,in thissubsectiononlylanguages(ormorepreciselywritingsystems)withoutwordboundaries areconsidered,butthemethodshouldbeunderstoodtocoverotherproblemswheredelimiters areusedtoindicatetheendorbeginningofaword. Letx=x ...x beastringofcharacters, 1 l 3.1 Tokenization 127 andy=y ...y beasequenceofwordsortokens. Wewouldsaythatyisatokenizationresult 1 m ofxifydefinesasegmentationonx. ConsiderthefollowingChinesesentence: x = 机器翻译是人类的梦想之一。 Wecandefineasegmentationonthesentence,forexample2, y = 机器翻译/是/人类/的/梦想/之一/。 (cid:104) (cid:105) = “机器翻译” “是” “人类” “的” “梦想” “之一” “。” (3.1) Inthisway,tokenizationcanbeframedasaproblemofmappingxtoy. Givenaninputstring, theoutputisthemostlikelysegmentation: yˆ = argmax Pr(y|x) y = argmax logPr(y|x) (3.2) y Eq. (3.2)describesapredictionmodelwehavebeenreferencingseveraltimesinthisbook. However,theproblemwearedealingwithiseasierbecauseycontainstheinformationofx, andwecanremovetheconditionfromPr(y|x)intheargmaxoperation: yˆ = argmax logPr(y) y = argmax logPr(y ,...,y ) (3.3) 1 m y ItiseasytocheckthatEq. (3.3)infactdescribesalanguagemodelingproblem. There areafewdifferentwaystoestimatethejointprobabilityPr(y ,...,y ). Asimplemethodisto 1 m rewritelogPr(y ,...,y )intoasumoflog-scaleconditionalprobabilities: 1 m logPr(y ,...,y ) = logPr(y )+logPr(y |y )+...+logPr(y |y ,...,y ) (3.4) 1 m 1 2 1 m 1 m−1 EachconditionalprobabilityPr(y |y ,...,y )canbeapproximatedby i 1 i−1 Pr(y |y ,...,y ) = Pr(y |y ,...,y ) (3.5) i 1 i−1 i i−n+1 i−1 thatis,thegenerationofy onlydependsonthen−1previouscontextwords. Tocompute i Pr(y |y ,...,y ),wecaneitherusetherelativefrequencymethodsorneuralnetworks i i−n+1 i−1 (seeChapter2). Nowwecanthinkoftokenizationasasupervisedlearningproblem. Theprocessisoutlined here: • Preparesomesentencesthatarecorrectlysegmented. 2Followingthenotationusedpreviously,weusebothy=y 1 ...ymandy= (cid:2) y 1 ... ym (cid:3) todenoteasequence ofvariables. 128 Chapter 3. Words and Word Vectors • LearnalanguagemodelPr(y)ontheselabeledsentences. • Foranewsentence,findthe“best”tokenizationyˆ thatmaximizesPr(yˆ),asinEq. (3.3). Whilethisprocedurefollowsastandardpipelineofsupervisedlearning,thereareseveral practical issues we have to iron out. First, the language model requires a vocabulary from which y can choose a value, but new words are always around. To handle them, one way i is to segment an unknown substring into characters, that is, we treat characters as words if thesubstringyieldingthesecharactersisnotcontainedinthevocabulary. Analternativeisto takeintoaccountallsubstringsthatarenotcoveredbythevocabulary,andreplacethemwith the<unk>tag. The<unk>trickiswidelyadoptedinstate-of-the-artlanguagemodelsandis usuallyhelpful. Second,thelanguagemodeldescribedabovehasabiastowardsshortsequencesbecause Pr(y",
    "First, the language model requires a vocabulary from which y can choose a value, but new words are always around. To handle them, one way i is to segment an unknown substring into characters, that is, we treat characters as words if thesubstringyieldingthesecharactersisnotcontainedinthevocabulary. Analternativeisto takeintoaccountallsubstringsthatarenotcoveredbythevocabulary,andreplacethemwith the<unk>tag. The<unk>trickiswidelyadoptedinstate-of-the-artlanguagemodelsandis usuallyhelpful. Second,thelanguagemodeldescribedabovehasabiastowardsshortsequencesbecause Pr(y ,...,y )wouldbelargeifmisasmallnumber. Ageneralwaytomitigatethisbiasisto 1 m introducealengthreward(orlengthbonus)tothemodel,forexample, yˆ = argmax logPr(y)+λ·m (3.6) y or logPr(y) yˆ = argmax (3.7) mλ y whereλ·mandmλ rewardlongsequencesandλ>0isahyperparametercontrollinghow muchwerelyontherewardinassessingthegoodnessofy. Interestingly,itisfoundthatthe lengthbiasisnotabigproblemwithtokenizationinpracticebecausethevarianceinlengthis smallforthose“good”tokenizationresults. Forexample,usingaunigramlanguagemodel (i.e.,n=1)withoutanylengthrewardworkswellinmanyreal-worldapplications. Wewill seeafewexamplesinSection3.1.4. Third,performingargmaxisdifficultbecausethereareexponentiallymanytokenization candidates. However, the use of language models here enables efficient search algorithms. Consider, for example, applying a unigram language model to tokenization. For the input string x, we keep, at each position j of x , a state that describes the probability of the best tokenization on x ...x (denoted as p(j)) as well as the last word of this tokenization. At 1 j positionj+1,wecreateanewstateandcomputetheprobabilityofthebesttokenizationon x ...x by 1 j+1 p(j+1) = max p(i)·Pr(x ...x ) i+1 j 1≤i≤j = max p(i)·Pr(w ) (3.8) [i+1,j] 1≤i≤j wherePr(w )istheprobabilityofthewordspanningx ...x . Onthealgorithmicside, [i+1,j] i+1 j Eq. (3.8)describesadynamicprogrammingmethodthathasatimecomplexityofO(l2)for aninputoflengthl. Forthefinaloutput,wecantracebackfromthefinalstateanddumpthe wordsequencealongthepathoftheoptimaltokenization. 3.1 Tokenization 129 Note that the methods here are generic and can be applied to tokenization for other languages. For example, when applying it to English, we only need a slight update on the formatoftheinput: theinputisnotacharactersequencebutasequenceofthesmallestpossible piecesseparatedoutbypunctuationandspaces. Forexample,forthesentenceIsthisTom’s laptop?,wehave (cid:104) (cid:105) x = Is this Tom ’ s laptop ? (3.9) Then,thetokenizationprocesscanproceedasinEqs. (3.2-3.7). 3.1.3 Tokenization as Sequence Labeling OneofthemajorwaysbywhichNLPresearchersgrouptogetherconsecutivelinguisticpieces isthroughtaggingthesequencewithagrouping-inspiredlabelset,oftenknownassequence labeling. Althoughwelimitourselvesheretotheproblemofgroupingcharacterstowords,as wewillseeinthefollowingchapters,suchamethodisagoodsolutiontomanyNLPproblems, suchaspart-of-speechtagging,namedentityrecognition,andsoon. Sincetheideaofsequence labelinghasbeendiscussedinChapter1,wepresentherehowitisadaptedtothetokenization task. Thelabelsetsusedintokenizationareregular. Thesimplestoftheseisthe“IB”set. The “I”labelindicatesalinguisticpieceinsideaword,andthe“B”labelindicatesthebeginning ofaword. Thelabelsetcanbeenrichedbyaddingthe“E”label(i.e.,theendingofaword) and/orsplittingthe“B”labelintosub-labels(e.g.,B andB indicatethefirstandthesecond 1 2 linguisticpiecesofaword)[Zhaoetal.,2006]. Givenaninputsequencexandatokenization resulty, transformingy tothelabelsequenceisfairlysimple. Consideragaintheexample usedintheprevioussubsection. Wecanlabelthesequenceindifferentformats: x: 机 器 翻 译 是 人 类 的 梦 想 之 一 。 y: 机 器 翻 译 是 人 类 的 梦 想 之 一 。 {I,B} B I I I B B I B B I B I B {I,B,E} B I I E B B E B B E B E B {I,B ,B ,E} B B I E B B B B B B B B B 1 2 1 2 1 1 2 1 2 1 2 1 Sincethelabelsequencecanbetreatedasanotherformofthetokenization,wecanrestate theproblemasfindingthebestlabelsequencegivenaninput: cˆ = argmax logPr(c|x) (3.10) c wherec=c ...c isalabelsequence. ManymethodshavebeenproposedtomodelPr(c|x). A 1 l 130 Chapter 3. Words and Word Vectors classicwayisgivenbyrewritingPr(c|x)usingtheBayes’rule: Pr(x|c)Pr(c) cˆ = argmax log Pr(x) c = argmax logPr(x|c)+logPr(c) (3.11) c Pr(x|c)Pr(c) Inthismodel,Pr(x)isaconstantforallc’s,andthuscanberemovedfrom in Pr(x) search. Pr(x|c)istheprobabilityofgeneratingtheinputx(i.e.,observations)giventhelabel sequencec(i.e.,latentvariables),andPr(c)isalanguagemodeldefinedonthelabelsequence. Simplifications are in general required for a tractable model. For example, we can make a Markovassumptionthatthechoiceofc isdependentonlyonthechoiceofc . Thisleads i i−1 tothehiddenMarkovmodel(HMM)whichiswidelyusedingenerativemodelingforNLP problems. An alternative method is discriminative modeling. A common idea is to treat sequence labelingas aseries ofindependentclassification problems. Forexample, wecan develop a local classifier that conditions the prediction of c on a set of features around position i. In i more sophisticated models, such as conditional random fields (CRFs), the context of the entiresequencecanbeusedintheprediction. Whileitmaybeinterestingtogomoredeeply intothedetailsaboutthesesequencelabelingmodels,wesimplyskipthemtomakethetopic inthissectionmoreconcentrated. Instead,thereaderisreferredto[Kupiec,1992;McCallum etal.,2000;Laffertyetal.,2001]forthoroughdiscussionsofhowthesemodelsaredeveloped andapplied. Inaddition,foracomparisonofgenerativemodelinganddiscriminativemodeling, wereferthereadertoChapter1. 3.1.4 Learning Subwords Itisacommonlyheldbeliefthatwordsarethebasicunitsinlanguageuse. Thisdoesnotmean thatwordsarethesmallestlinguisticunits. Rather,wordscanbebrokendownintosmaller pieces that have meanings, such as morphemes. It is this which accounts for the important roleofwordsinthesyntactichierarchyofalanguage,e.g.,wordsaremadeupofmorphemes, andphrasesandsentencesaremadeupofwords. Itisthereforenaturaltothinkofwordsas distinctcomponentsoflanguagesthathavesomefunctioninformingthestructureormeaning ofaphraseorasentence. InNLP,however,viewingsentencesassequencesofwordsisnotso desirablesometimes. Aproblemisthatsomewordsarerare,makingitdifficulttoadequately learnamodelbecauseofdatasparseness. Forexample,uncopyrightableisanEnglishword thatrarelyoccurs. AnNLPsystemmaysimplyrecognizeitasanunknownword(i.e.,anOOV word),althoughwecangetthemeaningofthiswordbydecomposingitintoparts: un,copy,",
    "(CRFs), the context of the entiresequencecanbeusedintheprediction. Whileitmaybeinterestingtogomoredeeply intothedetailsaboutthesesequencelabelingmodels,wesimplyskipthemtomakethetopic inthissectionmoreconcentrated. Instead,thereaderisreferredto[Kupiec,1992;McCallum etal.,2000;Laffertyetal.,2001]forthoroughdiscussionsofhowthesemodelsaredeveloped andapplied. Inaddition,foracomparisonofgenerativemodelinganddiscriminativemodeling, wereferthereadertoChapter1. 3.1.4 Learning Subwords Itisacommonlyheldbeliefthatwordsarethebasicunitsinlanguageuse. Thisdoesnotmean thatwordsarethesmallestlinguisticunits. Rather,wordscanbebrokendownintosmaller pieces that have meanings, such as morphemes. It is this which accounts for the important roleofwordsinthesyntactichierarchyofalanguage,e.g.,wordsaremadeupofmorphemes, andphrasesandsentencesaremadeupofwords. Itisthereforenaturaltothinkofwordsas distinctcomponentsoflanguagesthathavesomefunctioninformingthestructureormeaning ofaphraseorasentence. InNLP,however,viewingsentencesassequencesofwordsisnotso desirablesometimes. Aproblemisthatsomewordsarerare,makingitdifficulttoadequately learnamodelbecauseofdatasparseness. Forexample,uncopyrightableisanEnglishword thatrarelyoccurs. AnNLPsystemmaysimplyrecognizeitasanunknownword(i.e.,anOOV word),althoughwecangetthemeaningofthiswordbydecomposingitintoparts: un,copy, right,andable. Anotherproblemisthatlinguistics-basedtokenizationstandardssomewhat limit the use of computers for automatically learning the way to segment the sentence into unitsinamachinelearningsense. Inthiscase,itishelpfultoconsideridentifying“new”words thatarenotstrictlyconstrainedbylinguisticsbutarebettersuitedtoNLPsystems. 3.1 Tokenization 131 1. Byte Pair Encoding BytePairEncoding(BPE)isoneofthemostsuccessfulmethodstolearnsubwordunitsfrom asetofwordsequences[Sennrichetal.,2016b]. WhiletheBPEapproachstemsfromdata compression[Gage,1994],itismoreoftenusedinNLPasasolutiontotheopenvocabulary problem. ThebasicideaofBPEisthatwerepeatedlyreplacethemostfrequentpairofbytes in the data to form a new byte. As a result, common bytes are often involved in merging substringsofbytes,andrarebytesareoftenisolatedandconsidereduniqueunits. Theoutcome ofBPEisabytevocabularythatcanbeusedtoencodenewdata. InNLP,abytecanroughlycorrespondtoacharacter. Andeachentryofthevocabularyis acharactersequence,calledasymbolorsubword. BPEbeginswithsplittingagiventextintoa sequenceofcharacters,forexample,wecanaddaspaceaftereachoccurrenceofanEnglish letterorapunctuationmark. Thisingeneralresultsinaverylongsequence. WhileBPEitself hasnorestrictionsoninputlength,amorecommonwayistopreventcross-wordsymbolsfor efficiencyconsiderations. Thus,wecanrepresentthetextasalistofspace-separatedwords, eachbeingassociatedwiththefrequencyoftheword. Forexample,considerawordlist: f l o w # : 2 b l o w # : 2 f l a t # : 1 f l a g # : 4 where#isaspecialsymbolindicatingtheendofaword3. Fromthiswordlist,wecancollect aninitialvocabulary: f : 7 b : 2 l : 9 a : 5 o : 4 t : 1 w : 4 g : 4 # : 9 Then,wecounttheoccurrencesofeachsymbolbigram: 3Insteadoftaking#asaseparatesymbol,anotherwayistoconcatenate#withthelastcharacterineachword, likethis f l o w# :2 b l o w# :2 f l a t# :1 f l a g# :4 where“w#”,“t#”,and“g#”representcharactersthatoccurattheendofaword. 132 Chapter 3. Words and Word Vectors f l : 7 a g : 4 l a : 5 g # : 4 l o : 4 b l : 2 o w : 4 a t : 1 w # : 4 t # : 1 Wemergethemostfrequentsymbolbigram“fl”toanewsymbol“fl”andreplaceinthe wordlisteachoccurrenceof“f l”with“fl”: fl o w # : 2 b l o w # : 2 fl a t # : 1 fl a g # : 4 Accordingly,thesymbol“fl”isaddedtothevocabulary: f : 7 b : 2 l : 9 a : 5 o : 4 t : 1 w : 4 g : 4 # : 9 fl : 7 Then,thisprocessisrepeatedagain. Thistime,wemergethesymbolbigram“fl a”and createanewsymbol“fla”. Assuch,wehaveanewwordlist: fl o w # : 2 b l o w # : 2 fla t # : 1 fla g # : 4 andanewvocabulary: f : 7 b : 2 fla : 5 l : 9 a : 5 o : 4 t : 1 w : 4 g : 4 # : 9 fl : 7 Wecanrunthisprocessacertainnumberoftimes. Themoretimesweperformthemerge process, the larger the vocabulary is. The entries of the final vocabulary are reordered by symbolfrequencies. Forexample,ifwesetthenumberofmergeoperationsto6,wewillhave avocabulary,likethis: 3.1 Tokenization 133 l : 9 fla : 5 ow# : 4 # : 9 o : 4 flag : 4 f : 7 w : 4 flag# : 4 fl : 7 g : 4 b : 2 a : 5 ow : 4 t : 1 Itcorrespondstothewordlist: fl ow# : 2 b l ow# : 2 fla t # : 1 flag# : 4 Having obtained a vocabulary like above, we can apply it to tokenize new words. The subwordtokenizationfollowsthesameprocedureofmergingsymbolbigramsasthatusedin buildingthevocabulary. GivenaBPEvocabulary,wefirstsegmenttheinputtextintocharacter symbols. Then,weexamineeachsymbolbigraminthesequence,andmergetheonethathas the highest frequency in the vocabulary. We repeat this operation until there are no further merges. Consider,forexample,thefollowingtext: towaflag Itisfirsttransformedintoacharactersequence: t o w # a # f l a g # ByusingtheBPEvocabularywehaveobtained,wecandoBPEmergingonthissequence, likethis t o w # a # f",
    "it to tokenize new words. The subwordtokenizationfollowsthesameprocedureofmergingsymbolbigramsasthatusedin buildingthevocabulary. GivenaBPEvocabulary,wefirstsegmenttheinputtextintocharacter symbols. Then,weexamineeachsymbolbigraminthesequence,andmergetheonethathas the highest frequency in the vocabulary. We repeat this operation until there are no further merges. Consider,forexample,thefollowingtext: towaflag Itisfirsttransformedintoacharactersequence: t o w # a # f l a g # ByusingtheBPEvocabularywehaveobtained,wecandoBPEmergingonthissequence, likethis t o w # a # f l a g # f l⇒fl −−−−−−−−−→ t o w # a # fl a g # fl a⇒fla −−−−−−−−−→ t o w # a # fla g # o w⇒ow −−−−−−−−−→ t ow # a # fla g # ow #⇒ow# −−−−−−−−−→ t ow# a # fla g # ... ... −−−−−−−−−→ t ow# a # flag# Thissubwordsequencecanbeusedassomeinputand/oroutputofadownstreamNLP task,suchasmachinetranslation. Sometimes,wewanttomapsubwordsbacktowords. Thisis simple: wekeepthespaceaftereachoccurrenceofthe#symbol,andremoveallotherspaces and#. AlsonotethattheBPEmethodwedescribehererequiresword-segmentedinputs,that is,weneedapre-tokenizertoroughlytokenizetheinputsequenceintosomeunits. Thiscanbe donebyusingthemethodspresentedinSections3.1.1-3.1.3. 134 Chapter 3. Words and Word Vectors 2. WordPiece TheWordPiecemethodisverysimilartotheBPEmethodinthatitfirstdividestheinputtext intothesmallestsymbolsandthenprogressivelymergespairsofconsecutivesymbolstoform largersymbols[SchusterandNakajima,2012]. Thedifferencebetweenthemisonlyinthe way of selecting which symbol bigram to merge. In BPE, we merge each time the symbol bigramwiththehighestfrequency. Let(x ,x )beabigraminthesequencex. Themerge i i+1 ruleofBPEcanbedescribedas (x ,x ) = argmax count(x ,x ) (3.12) ˆi ˆi+1 i i+1 i∈[1,|x|−1] where the function count(x ,x ) returns the frequency of (x ,x ) in the corpus, and i i+1 i i+1 (x ,x )isthebigramwiththehighestfrequency. ˆi ˆi+1 TheWordPiecemethod,instead,adoptsamaximumlikelihoodcriterionforbigramselec- tion. Moreprecisely,itmergesthebigramsothatthelikelihoodofthedataismaximized. This canbeformalizedas: Pr(x ,x ) i i+1 (x ,x ) = argmax log ˆi ˆi+1 Pr(x )Pr(x ) i∈[1,|x|−1] i i+1 = argmax [logPr(x ,x )−log(Pr(x )Pr(x ))] (3.13) i i+1 i i+1 i∈[1,|x|−1] logPr(x ,x )−log(Pr(x )Pr(x )) describes the increase in log-likelihood of the i i+1 i i+1 text when we replace consecutive symbols (x ,x ) with a single symbol x x 4. Thus, i i+1 i i+1 applicationsofsuchamergeruleproduceasequenceofcodingsteps,eachofwhichincreases thelikelihoodabitontopofthelaststep. Theoutcomeofthisprocessisacodebook(i.e.,a vocabulary)bywhichwecandefinethemostlikelycodesequenceforthegiventext. 3. SentencePiece Both the BPE and WordPiece methods require that the input text is pre-tokenized in some way. Thismakesitsomewhatcomplicatedtodevelopatokenizationsystem. Asanalternative, SentencePieceisamoregeneralmethodthatdealswithrawtextsandconsidersallcharacters (includingspaces)intokenization[KudoandRichardson,2018]. ThemainideaofSentence- Pieceistoscaledownabigvocabularysothattheunigramprobabilityofthetextisminimized atsomelevelofthevocabularysize5,calledtheunigrammethod[Kudo,2018]. The unigram method frames subword segmentation as a unigram language modeling problem,resemblingthegeneralformofEqs. (3.3-3.4). Letxbeasequenceofcharactersand 4Instatistics, Pr(a,b) iscalledthepointwisemutualinformationofvariablesaandb.Seemoredetails Pr(a)Pr(b) inSection3.3.1.Anothernameforthisisinformationgain.ItcanbeinterpretedbyusingtheKullback-Leibler divergenceorothermeasuresininformationtheory(seeChapter1). 5Thetermvocabularysizemayhavedifferentmeanings.Hereitreferstothenumberofentriesofthevocabulary. Sometimes,ontheotherhand,itisthoughtofasthetotalnumberofbytesusedtostorethevocabulary. 3.1 Tokenization 135 ybeasequenceofsymbolsorsubwordsyieldingx. Theprobabilityofyisgivenby: |y| (cid:89) Pr(y) = Pr(y ) (3.14) i i=1 Then,wecanwritethelikelihoodofxintermsofthejointprobabilityofxandy: (cid:88) Pr(x) = Pr(x,y) (3.15) y∈Y(x) where the sum is over all possible tokenization results Y(x). Since y can be viewed as a segmentation-annotated version of x, the model of Pr(x,y) provides no more information thanthemodelofPr(y)andwehavePr(x,y)=Pr(y). Thus,wecanrewriteEq. (3.15)as: (cid:88) Pr(x) = Pr(y) y∈Y(x) |y| (cid:88) (cid:89) = Pr(y ) (3.16) i y∈Y(x)i=1 Takingthisequation,thelog-likelihoodofasetofstringsX isgivenby |y| (cid:89) (cid:88) (cid:89) Pr(X) = log Pr(y ) i x∈Xy∈Y(x)i=1   |y| (cid:88) (cid:88) (cid:89) = log Pr(y i ) (3.17) x∈X y∈Y(x)i=1 Ifweconsider−Pr(X)asalossfunction,thenthetaskherecanbestatedasfindingthe best estimate for each unigram probability Pr(y) so as to make Pr(X) as large as possible. At first glance this optimization problem looks complicated. Fortunately, there are several powerfultoolstosolveit. ApopularmethodistousetheExpectation-Maximization(EM) algorithm[Dempsteretal.,1977],whichiscommonlyusedwhenonetriestofindastatistical modelthatmaximizesthelikelihoodofthedata. NotethattheEM-basedsolutiontoEq. (3.17) is similar to those for other NLP problems, such as statistical machine translation, and has beenwelldiscussedinthosecontexts. Sowereferthereaderto[Brownetal.,1993]fordetails aboutthesemethods. InthischapterwejusttakeEMasanoff-the-shelftooltoestimatePr(y) givenEq. (3.17). 6 6InEM,wecanviewX asanobservation,andPr(X|θ)asastatisticalmodelthatdescribeshowlikelythe observationoccurs.Hereθisthemodelparametersthatweintendtodetermine.EMisbasedonanobjectiveof maximumlikelihoodestimation,thatis θˆ = argmaxPr(X|θ) (3.18) θ Forthemodelhere,wecanview{Pr(y)}asmodelparameters. WeskipthederivationdetailsabouttheEM 136 Chapter 3. Words and Word Vectors SentencePieceisessentiallya“pruning”methodthatremoveslowprobabilityentriesfrom thevocabulary. ItstartswithabiginitialvocabularyV. Forexample,wecancreatetheinitial vocabularybyenumeratingallstringswithalengthconstraint. Typically,cross-wordstrings areexcludedtoreducethevocabularysize. Then,werunthefollowingsteps: • Estimatetheprobabilityforeachentryy ofV byoptimizingEq. (3.17). • Computethelossforeachentryy ofV viatheremove-onestrategy,thatis,thelossis thereductioninthelikelihood(seeEq. (3.17))wheny isremovedfromthevocabulary. • RemoveacertainpercentageofentriesofV",
    "translation, and has beenwelldiscussedinthosecontexts. Sowereferthereaderto[Brownetal.,1993]fordetails aboutthesemethods. InthischapterwejusttakeEMasanoff-the-shelftooltoestimatePr(y) givenEq. (3.17). 6 6InEM,wecanviewX asanobservation,andPr(X|θ)asastatisticalmodelthatdescribeshowlikelythe observationoccurs.Hereθisthemodelparametersthatweintendtodetermine.EMisbasedonanobjectiveof maximumlikelihoodestimation,thatis θˆ = argmaxPr(X|θ) (3.18) θ Forthemodelhere,wecanview{Pr(y)}asmodelparameters. WeskipthederivationdetailsabouttheEM 136 Chapter 3. Words and Word Vectors SentencePieceisessentiallya“pruning”methodthatremoveslowprobabilityentriesfrom thevocabulary. ItstartswithabiginitialvocabularyV. Forexample,wecancreatetheinitial vocabularybyenumeratingallstringswithalengthconstraint. Typically,cross-wordstrings areexcludedtoreducethevocabularysize. Then,werunthefollowingsteps: • Estimatetheprobabilityforeachentryy ofV byoptimizingEq. (3.17). • Computethelossforeachentryy ofV viatheremove-onestrategy,thatis,thelossis thereductioninthelikelihood(seeEq. (3.17))wheny isremovedfromthevocabulary. • RemoveacertainpercentageofentriesofV withlargelosses. Forexample,wekeep 80%oftheentries,anddiscardtherest. Theoutcomeofthisprocessisanewvocabularyaswellastheprobabilityassignedtoeach subword. Wecanrepeatthisprocessanumberoftimesuntilthevocabularysizeisreducedto adesirablelevel. SentencePiecediffersfromBPEandWordPieceinthatitconsidersallpossiblesubword (cid:80) sequencesforagivenstring(seethesum inEq. (3.15)). Fromthemachinelearning y∈Y(x) pointofview, thiscanbeseenas awayof regularization, thatis, wecanreducethe riskof overestimatingtheparameterscorrespondingtothesingle-bestsubwordsequencethatmay haveerrors. AnalternativewayistoonlyconsidersomeofthesubwordsequencesinY(x)for thesakeofefficiency. Forexample,wecansamplek subwordsequencesaccordingtoPr(y) toformthecandidatesetY(x). NotethattheSentencePiecemethoddoesnotdependonword-separatedinputsequences. While the BPE and WordPiece methods can also deal with raw text if updated, the Senten- cePiecemethodexplicitlytakesthespaceandotherdelimitersaspartsofthesubwords. See Figure3.3forafewtokenizationresultsfortowaflag. Given a learned vocabulary and the corresponding unigram probabilities, we can apply themtodealwithanewtext. Thisisinfactasearchproblem: wefindthemostlikelysubword sequenceintermsoftheunigramprobability. Aslanguagemodelingisawell-studiedtopic in NLP, many search algorithms are directly applicable to the case here. For example, the estimateofPr(y)butdirectlypresenttheresult.TheEMalgorithminvolvestwosteps. • TheExpectationStep(ortheE-step): GiventhecurrentestimateofPr(y)(say,Prt(y)),wecompute theposteriorPrt(y)foreachyaccordingtoEq.(3.14).Then,wecomputethefractionalcountofeach subwordyinthevocabularyV,likethis   |y| (cid:88) (cid:88) (cid:88) fcount(y) = Prt(y) δ(y,y i ) (3.19) x∈Xy∈Y(x) i=1 (cid:80)|y| whereδ(y,y i )returns1ify=y i,and0otherwise. i=1 δ(y,y i )countsthenumberoftimesyoccursin thesubwordsequencey. • TheMaximizationStep(ortheM-step):GiventhefractionalcountsobtainedintheE-step,were-estimate theunigramprobabilitiesbytheequation: fcount(y) Pr t+1 (y) = (cid:80) fcount(y′) (3.20) y′∈V Thetwostepsareiteratedforanumberofroundsuntiltheparametersconvergetosomevalues. 3.2 Vector Representation for Words 137 subwordsequence unigramprobabilities([subword]:probability) t/ow_/a/_flag [t]:0.030 [ow_]:0.002 [a]:0.041 [_flag]:0.001 t/ow/_/a_/f/lag [t]:0.030 [ow]:0.005 [_]:0.113 [a_]:0.093 [f]:0.041 [lag]:0.002 t/ow/_a_/fla/g [t]:0.030 [ow]:0.005 [_a_]:0.084 [fla]:0.003 [g]:0.027 tow/_/a_/f/lag [tow]:0.001 [_]:0.113 [a_]:0.093 [f]:0.041 [lag]:0.002 t/ow_/a_/flag [t]:0.030 [ow_]:0.002 [a_]:0.093 [flag]:0.001 Figure3.3: Differenttokenizationresultsfortowaflag. Everysubwordisassignedaprobability thatisestimatedthroughaunigramlanguagemodel. Everywhitespaceisreplacedwith“_” foraclearpresentation. methodspresentedinSection3.1.2arestraightforwardlyapplicablehere. 3.2 Vector Representation for Words Wordshavemeanings7. Inthebroadestsense,themeaningofawordisthewayinwhichit canbeinterpreted. Thisissomethingbehindthesurfaceformofawordbutcanbeunderstood bylanguagespeakers. Forexample,considerthefollowinglinesoftextfromapoem[Knight, 2018]: Therewasalittlesparrow Whosatonawheelbarrow, Andtweetedtoallherfriendsaround. Acatwithopenjaws Andverypointedclaws, Spiedherasheracedalongtheground. These words are not merely strings of English letters and punctuation marks but have identifiablemeaningsthatareknownbyEnglishspeakers. Forexample,“little”meanssmallin size,“sparrow”meansakindofbird,and“friends”meanspeoplewhoyoulikeandtrust. From anNLPperspective,awordmeaning(orwordsense)isnotjustwhatthewordexpressesin one’sbrainbutsomethingcomputer-readableandcomputable. 7WhilewehavesofardiscussedseverallinguisticelementsusedinNLP,suchassubwords,westillusewords asthebasicunitsinourdiscussionhere.Themethodswewillpresentintheremainingpartofthischaptercouldbe understoodtocoverothertypesoflanguageunitsonemayuseindevelopingNLPsystems,includingcharacters, subwords,andsoon. 138 Chapter 3. Words and Word Vectors 3.2.1 One-hot Representation Anaturalwaytorepresentwordmeaningsistouselanguagetodescribethem. Forexample, wecanfindinadictionarytheabovewordswiththeiridsandmeanings. Someofthemare8: cat 511 A small animal with fur, four legs, a tail, and claws, usuallykeptasapetorforcatchingmice her 5220 Used,usuallyastheobjectofaverborpreposition,to refer to a woman, girl, or female animal that has just beenmentionedorisjustabouttobementioned jaws 6186 Themouth,includingtheteeth ground 6402 Thesurfaceoftheearth sparrow 8331 Acommon,small,gray-brownbird wheelbarrow 9954 Alarge,opencontainerformovingthingsinwithawheel atthefrontandtwohandlesattheback,usedespecially inthegarden To represent a word, the simplest idea may be to replace it with the id number in the dictionary. Inthisway,eachwordrepresentationisauniquenumber. Anequivalentformto thisistheone-hotrepresentation. Itisavectorwhosedimensionalityisequaltothevocabulary size. In this vector, only the entry corresponding to the word has a value of 1 and all other entrieshave0values. Forexample,thewordsparrowcanberepresentedasaone-hotvector basedonitsid(8331),likethis [ 0 0 ... 0 1 0 ... 0 0 ] ↑ id=8331 3.2.2 Distributed Representation However,itappearsthattheone-hotrepresentationonlyprovidesthe“identity”oftheword but not the “description” of what the word is. An obvious problem is that every word is orthogonaltootherwords. Thismakesitdifficultto“compute”therelationshipbetweenwords becausethereisnoconnectionamongtheassociatedwordvectorseventhoughsomeofthe wordsarethoughttobesimilarinouruseoflanguage. Here,ourdesireisamodelinwhich wordsaredescribedascountableattributesandtheclosenessbetweendifferentwordsiswell explained. Awaytodothisistoenrichtherepresentationwiththeworddescription. Consider againthewordsparrowforexample. Inthedictionary,wehaveitsmeaningacommon,small, gray-brownbird. ByusingthetokenizationandnormalizationmethodsmentionedinSection 3.1.1,thistextcanbetransformedintoasequenceofwords (cid:104) (cid:105) a common , small , gray - brown bird 8Allthesewordsandtheirmeaningsarefoundinhttps://dictionary.cambridge.org/. 3.2 Vector Representation for Words 139 Then,wevectorizethissequenceusingthebag-of-wordsmodel(seeChapter1),leadingto anewvectorofnumbers [ 0 0 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 0 0 ] ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ , - a bird brown common gray small wherethevalueofanentryis1ifthecorrespondingwordispresent,and0otherwise. Thisway enablesthesharingofcontentamongwords. Wewouldsaythattwowordsaresimilarifthey haveoverlapsintheirwordvectors. Consideranewwordcuckoo. Wecanfinditsmeaningin adictionary,e.g.,agreybirdwithatwo-notecallthatsoundssimilartoitsname. Itiseasy to know that sparrow and cuckoo are two words that share something similar because they bothmarkthe“bird”dimensionas1andthevectorsimilaritybetweenthetwowordvectorsis greaterthan09. Treatingwordsasvectorsofnumbersoffersageneraltooltorepresentwordsinvarious differentways. Wedonotevenhavetoexplainawordvectorfromtheviewpointofsemantics. Forexample,wecanintroduceanewdimensionintothevectortomarkifthewordbelongsto somesyntacticcategory. Inabroadsense,wecandefineanarbitraryfunctiononeachentry ofthevectorandviewthefunction’soutputasafeaturedescribingtheword. Forexample,a simpleimprovementtotheaboverepresentationistouseafunctioncountingtheoccurrences ofawordinsteadofthebinary-valuedfunctionmarkingthepresenceoftheword. Morefeature functionscanbefoundinSection3.3. Notethatitisnotnecessarytoconstrainthefeaturefunctionstoformsthatmakelinguistic sensealthoughlinguisticallymotivateddesignsofthefeaturefunctionsareusuallyofinterest to NLP researchers.",
    "- a bird brown common gray small wherethevalueofanentryis1ifthecorrespondingwordispresent,and0otherwise. Thisway enablesthesharingofcontentamongwords. Wewouldsaythattwowordsaresimilarifthey haveoverlapsintheirwordvectors. Consideranewwordcuckoo. Wecanfinditsmeaningin adictionary,e.g.,agreybirdwithatwo-notecallthatsoundssimilartoitsname. Itiseasy to know that sparrow and cuckoo are two words that share something similar because they bothmarkthe“bird”dimensionas1andthevectorsimilaritybetweenthetwowordvectorsis greaterthan09. Treatingwordsasvectorsofnumbersoffersageneraltooltorepresentwordsinvarious differentways. Wedonotevenhavetoexplainawordvectorfromtheviewpointofsemantics. Forexample,wecanintroduceanewdimensionintothevectortomarkifthewordbelongsto somesyntacticcategory. Inabroadsense,wecandefineanarbitraryfunctiononeachentry ofthevectorandviewthefunction’soutputasafeaturedescribingtheword. Forexample,a simpleimprovementtotheaboverepresentationistouseafunctioncountingtheoccurrences ofawordinsteadofthebinary-valuedfunctionmarkingthepresenceoftheword. Morefeature functionscanbefoundinSection3.3. Notethatitisnotnecessarytoconstrainthefeaturefunctionstoformsthatmakelinguistic sensealthoughlinguisticallymotivateddesignsofthefeaturefunctionsareusuallyofinterest to NLP researchers. A more general form for word representation is simply a real-valued, multi-dimensionalvector. Itisoftencalledthedistributedrepresentationofaword,orthe wordembedding. Forexample,thewordsparrowcanberepresentedasavectorlikethis (cid:104) (cid:105) 1.9 −7 3 −1.2 ... 2.01 −2.05 Inthemachinelearningpointofview,thisvectorcandescribesomeunderlyingattributesofa word. Theseattributesmaynotbeexplainableinhumanunderstandingbutcanbelearnedfrom data. Oneofthechallengesinlearningsucharepresentationisthatonecanhardlymeasurethe goodnessofavector. Ingeneral,itmakesnosensetoaskwhetherthedistributedrepresentation of a single word is good or not. Rather, we would like to know if the representations of a groupofwordsarewellbehaved. Forexample,itisacommonbeliefthatsimilarwordsshould havesimilarrepresentations. So,therelationshipbetweenwordsisoftenthoughtofassome “distance” between the word representations in a vector space. This leads to a number of methodstovisualizeandevaluatewordrepresentations. InSection3.6,wewillgiveamore detaileddiscussionabouttheseissues. 9Thesimilarityoftwovectorscanbemeasuredbythecosineoftheanglebetweenthem. 140 Chapter 3. Words and Word Vectors On the other hand, word representations typically do not work alone in NLP systems but are used as some intermediate states of a model. A standard approach in NLP, to learn distributedrepresentationsofwords,istotakeitasaby-productoftraininga“big”system. Thatis,therepresentationmodelworksasacomponentofasystem,andisoptimizedtogether with other components when the system is trained in some way. This inspires a promising paradigm of representation learning: the representation model is learned as a sub-model in an easy-to-train system, and can be used as a plug-in for a completely different system. In neural language modeling, for instance, we can force the model to map each input one-hot wordvectorintoareal-valued,low-dimensionaldistributedrepresentation. Thesedistributed representationsarefedintoaneuralnetworkthatpredictsaprobabilitydistributionoverthe vocabulary. Themappingfunctionorembeddingfunctionistrainedsoastominimizetheloss ofthelanguagemodelonsomedata(seeSection3.4). Whenapplyingthelearnedembedding function,wedropallotherpartsofthelanguagemodelandusethefunctiontogeneratethe distributed representation for each word in downstream tasks. An alternative strategy is to specifically tailor the model to the word representation learning problem. Systems of this kindaretypicallynotdesignedtodealwithstandardNLPproblems,butwithanemphasison specificproblemsinwordrepresentationlearning,suchasexplicitlymodelingtherelationship betweenwords(seeSection3.5). 3.2.3 Compositionality and Contextuality WhilewerestrictourdiscussiontowordrepresentationlearninginNLP,studyingthemeanings ofwordsisatraditionalsub-fieldoflinguistics. Inlexicalsemantics,forinstance,researchers areconcernedwithhowwordmeaningsaredefinedandused,andhowthesemeaningsform thesentencemeanings. Infact,thetaskoflearningtorepresentwordsdoesnotconcernitself withtheissueofsemanticsinlinguistics. Instead,itprovidesmachinelearningapproaches totransforminglinguisticunitsintocomputer-friendlyforms. However,thesemanticsissue iscriticalwhenoneunderstandsandusesalanguage. Itisthereforestillworthconsidering semanticsandrelatedproblemsinthedesignofwordrepresentationmodels. Forexample, • Compositionality. Compositionality is a common concept in semantics, logic and relatedfields. Itoftencomesoutwiththeprincipleofcompositionality: The meaning of a complex expression is determined by its structure andthemeaningsofitsconstituents. –Szabó[2020] This offers a useful tool to describe how the meaning of a big thing is built up from themeaningsofitsparts. Theprincipleofcompositionalityisfundamentalandexists everywhereinthelanguageworld. Forexample,whenyouseethephrasewhitecat,itis easytoknowitsmeaningintermsofthemeaningsoftheconstituentwordswhiteandcat. Anotherexampleatahigherleveloflanguageuseiscompoundsentences. Acompound 3.2 Vector Representation for Words 141 sentence forms its meaning by simply connecting multiple independent clauses with conjunctions. Notethattheprincipleofcompositionalityisnotasimplerulebywhichwe usetodescribehowabigitemismadeupofsmallerones,althoughresearchershavetried todefineitformally[Montague,1974]. Thereareevendisagreementsanddebateson howthisprincipleisinterpretedandhowitisadequatelymodeledbysemanticaltheories. Still, if we focus on NLP problems and set aside the theoretical part of linguistics, compositionalityisaveryusefulpropertythatonecanmakeuseofinsystemdesignand evaluation. Sometimes,ifonefindsthataproblemiscompositional,itimpliesthatthere aremanygoodmethodstoaddressitbecauseacomplexthingcanbedividedintosmaller and easier things. For word representation learning, we may wish that the resulting wordrepresentationsexhibitsomecompositionality,inresponsetothecompositional natureoflanguage. InSection3.6,wewillseeafewexamples,e.g.,therepresentations learnedbyneuralnetworksshowmeaningfulresultsunderlinearalgebraicoperations, thoughthemodelsarethemselvesnon-linear. However,ontheotherhand,theprinciple of compositionality is not the principle of everything. There are many situations in whichcompositionalityisnotheld,suchascollocationsandidioms. Inthiscase,natural languagesarenon-compositional. ThisexplainswhytheNLPproblemissochallenging. • Contextuality. Contextualityissomesortofnon-compositionality. Itstatesthataword mayhavemultiplepossiblemeaningsandthe“true”meaningisdeterminedbylooking atthecontextprecedingand/orfollowingthisword. Forexample,considerthefollowing sentences10 Theysatroundthedinnertable,arguingaboutpolitics. Cometothetableeverybody-supper’sready. Hecameinwithfourshoppingbagsanddumpedthemonthetable. ThetablecanhelpyouevaluatethepotentialrisksofinvestingintheFund. Buildingsocietiesdominatethebest-valuetablesformortgages. Thistablerepresentsexportsales. Intheseexamplesentences,tableisapolysemywithtwomeanings: Sense1: aflatsurfaceusedforputtingthingson. Sense2: anarrangementofitemsinrows,orcolumns,orblocks. Inotherwords,tableisanambiguousword. Thisambiguitywouldbeeliminatedifwe considerthesurroundingwords. Forexample,whentablefollowsdinner,itiseasyto figureoutthatitreferstosense1. Theambiguityalsoexistswhenawordstemsfroma fewdifferentformsorlexemes(callitahomonymy). Forexample,bearcanbeeithera 10All these sentences are from https://dictionary.cambridge.org/dictionary/english/ table 142 Chapter 3. Words and Word Vectors verboranoun. Disambiguatingawordforagivensetofwordsenseshasbeenstudied fordecadesinNLPandiscommonlyknownaswordsensedisambiguation(WSD) [Kelly and Stone, 1975]. However, the word representation problem discussed here ismorechallengingbecauseweusuallydonothaveapre-definedsetofwordsenses inhand. Weinsteadwantacontextualrepresentationmodelthatcangenerateaword representationdependentonitscontext. Thus,itisimportanttotaketheideathatthe meaningofawordmaynotbeconstant. Thismakestheproblemsomewhatdifferent fromwhatwediscussedatthebeginningofthesection,aswenolongerhavealookup tableforwordrepresentations,butamodelthatproducesdifferentrepresentationsofa wordindifferentcontexts. Intheremainingsectionsofthischapter,wefocusonlearningvectorrepresentationsof words from their distributions in language use. We leave the discussion on the contextual modelsforlearningdensewordrepresentationstoChapters4-6. 3.3 Count-based Models We have framed the induction of word meanings as a problem of learning word vectors. In this section, we proceed by assuming",
    "inhand. Weinsteadwantacontextualrepresentationmodelthatcangenerateaword representationdependentonitscontext. Thus,itisimportanttotaketheideathatthe meaningofawordmaynotbeconstant. Thismakestheproblemsomewhatdifferent fromwhatwediscussedatthebeginningofthesection,aswenolongerhavealookup tableforwordrepresentations,butamodelthatproducesdifferentrepresentationsofa wordindifferentcontexts. Intheremainingsectionsofthischapter,wefocusonlearningvectorrepresentationsof words from their distributions in language use. We leave the discussion on the contextual modelsforlearningdensewordrepresentationstoChapters4-6. 3.3 Count-based Models We have framed the induction of word meanings as a problem of learning word vectors. In this section, we proceed by assuming that the meaning of a word is determined by the environmentwherethewordisused. Thisisusuallystatedasthedistributionalhypothesis— wordsaresemanticallysimilariftheyappearinsimilarcontexts[Harris,1954;Firth,1957]. Awordrepresentationlearnedunderthishypothesisisalsocalledthedistributionalword representation or distributional representation11. To ease the reading, however, we will still use the terms word vector and word representation throughout this book. Next, we introduceseveralmethodsformodelingthedistributionofwordsintexts,andthenoffersome refinements. 3.3.1 Co-occurrence Matrices Indistributionalsemantics,wordsarerepresentedwithsemanticmodelsthatconsidervarious aspects of the context. These models differ in how the context of a word is modeled, for example,howlargethecontextisconsidered,howeachoccurrenceofawordiscounted,how thedimensionalityofadistributionisdefined,andsoon. Inthissectionweassume,asinmost modelsusedinNLP,thatwordrepresentationsarelearnedfromacollectionofdocuments. A way to view a document is as a very simple way of decomposing it into a set of unordered words. Then we can think of each occurrence of these words as an independent contextindicator. Inthisway,thedistributionofawordinitscontextcanbedescribedasthe number of times the word co-occurs with the context words. We can do this by building a 11Itshouldbenotedthatdistributionalrepresentationanddistributedrepresentationaretwodifferentconcepts. Adistributionalrepresentationreferstoarepresentationthatdescribesthedistributionoflanguageitemsinlanguage use. Arelatedtermisnon-distributionalrepresentationwhichmeanssomethingthatisobtainedfromlexical databases,suchastheinterpretationofawordinadictionary.Ontheotherhand,adistributedrepresentationrefers toavectorofvariablescorrespondingtosomeunderlyingattributesofalanguageitem.Incontrasttodistributed representation,aone-hotrepresentationjustdescribesthewordsymbol. 3.3 Count-based Models 143 co-occurrencematrixwhereacellcountsthenumberofco-occurrencesofarowitemanda columnitem. Consider,forexample,thefollowingdocuments12: Doc1 Aberryisasmall,pulpy,andoftenediblefruit. Doc2 In botanical terminology, a berry is a simple fruit with seeds and pulp producedfromtheovaryofasingleflower. Doc3 The term \"banana\" is also used as the common name for the plants that producethefruit. Doc4 Bananaseedsarelargeandhardandspikyandliabletocrackteeth. Doc5 Abananaisanelongated,ediblefruit-botanicallyaberry-producedby severalkindsoflargeherbaceousfloweringplantsinthegenusMusa. For each pair of words, we collect the total number of times they co-occur in these documents,leadingtoamatrix,calledtheword-wordco-occurrencematrixorterm-term co-occurrencematrix. Hereisasubsetofthematrixfortheabovedocuments. flowering fruit herbaceous ... often plants seeds berry 1 3 1 ... 1 1 1 terminology 0 1 0 ... 0 0 1 common 0 1 0 ... 0 1 0 teeth 0 0 0 ... 0 0 1 banana 1 2 1 ... 0 2 1 simple 0 1 0 ... 0 0 1 and 0 2 0 ... 1 0 2 Inthematrix,eachrowwordisassociatedwithawordvectorof|V|entries. Thenumbers intheentriesdescribehowoftentherowwordco-occurswithdifferentcontextwords,that is,howagivenwordisdistributedindifferent“contexts”. Inageometricsense,iftwowords havesimilardistributionsinco-occurringwiththesamegroupofcontextwords,thentheangle betweenthewordvectorswouldbesmall13. Forexample,ifwethinkofthesewordsasvectors inavectorspace,berryisclosertobananathanteeth(seeFigure3.4). Thisgeometricintuition isthebasisofmanyrepresentationmodels. MoreexampleswillbegiveninChapters4and5. Aproblemwiththismethodisthatthedistancebetweenwordsisnottakenintoaccount althoughthecorrelationisnotthatstrongwhenthecontextwordisdistant. Asimplesolution is to constrain context words in a window, called the context window or window for short [LundandBurgess,1996]. Forexample,foreachwordinadocument,weonlycountthe-2 and+2wordssurroundingit(i.e.,awindowofsize5). 12ThetextsarefromWikipedia. 13Theanglebetweentwovectorsdoesnothingwiththelengthsofthevectors.Ifthevectorsarenormalizedin someway(e.g.,byvectornorm),similarvectorsmeanthatmostentriesofthetwovectorshavesimilarvalues. 144 Chapter 3. Words and Word Vectors z teeth distance2 berry distance1 banana y x Figure3.4: Wordvectorsinavectorspacethatisbuiltfromthewordco-occurrencestatistics ontheEnglishdatafromWMT2012. Allthevectorsarenormalizedandrepresentedasarrows. Forvisualization,weprojectthesevectorsfromahigh-dimensionalspacetoa3-dimensional spaceviaprincipalcomponentanalysis. Asexpected,berryisclosertobananathantoteeth. Notethatthewordvectorslearnedbythebag-of-wordsmodelinSection3.2isaspecial instance of the co-occurrence matrix. In that example, we only have one document from which we collect context words. For each entry of a word vector, an indicator function is used to mark the presence of the context word. In addition to the indicator and counting functions,thereareotherchoicesforcomputingwordvectorsbyexaminingtheco-occurrence ofwords. Inpractice,thevalueofanentryofawordvectorcanbethoughtofasthedegree ofthecorrespondencebetweenwords. Iftwowordsarecorrelatedwitheachotherinsome context,afeaturefunctionmayassignascorebetweentheminanymanner. Thisscoredoesnot necessarilyhavetobeacount,butcanbeanarbitraryrealnumber. Assuch,theproblemcan bestatedasmeasuringtheassociationstrengthbetweenwords. Itiscommonpracticetodefine suchameasureonthebasisofcorrelationmodels. Instatistics,correlationdescribestowhat extenttwovariablesareassociated,measuredbycorrelationcoefficients. Commoncorrelation coefficients include the Pearson correlation coefficient (Pearson’s r), the Spearman’s rank correlation coefficient (Spearman’s ρ), and so on14. In NLP, a widely used measure is the pointwisemutualinformation(PMI)[ChurchandHanks,1990]. Letaandbbetwowords. ThemathematicalformofPMIisgivenby Pr(a,b) PMI(a,b) = (3.21) Pr(a)Pr(b) 14Some of the correlation coefficients assume certain distributions of the data. For example, the Pearson correlationcoefficientiscalculatedbasedontwovariablesfollowingnormaldistributions. 3.3 Count-based Models 145 where Pr(a,b) is the joint probability of a and b co-occurring, and Pr(a) (or Pr(b)) is the probabilityofa(orb)occurring. Theseprobabilitiescanbesimplyestimatedonthetextsbythe relativefrequencymethod15. Givenawordaandavocabularyofcontextwords{b ,...,b }, 1 |V| thePMI-basedwordvectorofaiswrittenas (cid:104) (cid:105) e(a) = PMI(a,b ) ... PMI(a,b",
    "coefficients assume certain distributions of the data. For example, the Pearson correlationcoefficientiscalculatedbasedontwovariablesfollowingnormaldistributions. 3.3 Count-based Models 145 where Pr(a,b) is the joint probability of a and b co-occurring, and Pr(a) (or Pr(b)) is the probabilityofa(orb)occurring. Theseprobabilitiescanbesimplyestimatedonthetextsbythe relativefrequencymethod15. Givenawordaandavocabularyofcontextwords{b ,...,b }, 1 |V| thePMI-basedwordvectorofaiswrittenas (cid:104) (cid:105) e(a) = PMI(a,b ) ... PMI(a,b ) (3.22) 1 |V| Correlationcoefficientsaregenerallyusedtotestwhethertwovariablesare(linearly)related. So,analternativemethodistodefineanentryofthewordvectorastheoutcomeofatest. For example,theentry(a,b)choosesavalueof1,ifthecorrelationcoefficientbetweenwordsa andbislargerthanathreshold,orthecorrelationofwordsaandbissufficientlysupportedby hypothesistesting. However,modelingwordsasvectorsofcorrelationscoresbetweenwordssomewhatlimits the scope of contextual information one may use. Another idea for word vectorization is to consider each document as a whole and establish the relationship between words and documents. We can do this by using the word-document co-occurrence matrix or term- documentco-occurrencematrix. Forexample,fortheabovementioneddocuments,wecan buildamatrix,likethis Doc1 Doc2 Doc3 Doc4 Doc5 berry 1 1 0 0 1 terminology 0 1 0 0 0 common 0 0 1 0 0 teeth 0 0 0 1 0 banana 0 0 1 1 1 simple 0 1 0 0 0 and 1 1 0 2 0 In the matrix, the value of entry (a,d) is defined to be the number of times the word a occurs in the document d, giving the strength of the relationship between a and d. This is commonlycalledthetermfrequency(TF)ofaind(denotedbytf(a,d)). Also,wecanuse a0-1indicatorfunctiontomarkthepresenceofthewordoccurrence(seeSection3.2). See Table3.1forafewvariationsoftheTFweightingfunction. Asaco-occurrencematrix,eachrowoftheabovematrixisthevectorrepresentationof the row word. In addition, each column is a vector representation of a document. Recall thebag-of-wordsmodelusedinthetextclassificationproblemmentionedinChapter1. The word-documentco-occurrencematrixisbasicallythesamethingasthebag-of-wordsmodel 15AproblemwithPMIisthatthemeasurebecomesunstablewhenthewordsarerare.Forexample,ifavery rarewordhappenstoappearinadocument,thePMIvalueofthiswordandanyotherwordinthisdocumentwould beunreasonablylarge. 146 Chapter 3. Words and Word Vectors Entry Mathematicalform (cid:40) 1 aoccursind Binary tf(a,d)= 0 otherwise Count tf(a,d)=count(a;d) ExponentialCount tf(a,d)=count(a;d)α Log-scaleCount tf(a,d)=log(1+count(a;d)) count(a;d) NormalizedCount(orFrequency) tf(a,d)= (cid:80) a′count(a′;d) Table3.1: Functionsoftheterm-frequencyweightingscheme. count(a;d)countstheoccur- rencesofthewordainthedocumentd. wheretheorderingofwordsisignoredbutthewordcountsmatter. Hereweperformdocument vectorizationviathismodelonacollectionofdocuments. 3.3.2 TF-IDF Themodelingofword-documentassociationsisknowntobeimportantformanyNLPtasks. Animprovementonusingword-documentrelationshipstobuildwordvectorsanddocument vectorssimultaneouslyisthetermfrequency-inversedocumentfrequency(TF-IDF)method. Given a set of documents D, the TF-IDF weighting scheme assigns a score to each word- documentpair(a,d)bytheequation tfidf(a,d,D) = tf(a,d)·idf(a,D) (3.23) where • tf(a,d)isthetermfrequency(seeTable3.1). Whentf(a,d)islarge,thewordaisagood indicator for thedocument d. In contrast, when tf(a,d) is small, the word-document associationisnotthatstrong. • idf(a,D)istheinversedocumentfrequency(IDF).Itisdevelopedbasedonthefact thatcommonwordsacrossdocumentsarelessinformative. Forexample,foracollection of documents on sports, it is likely to see player and players in most documents. In thiscase,thewordsplayer andplayersarelessinterestingindiscriminatingdifferent documentsorcontexts. Letdf(a,D)bethenumberofdocumentsinD containingthe worda. Acommonformofidf(a,D)isgivenby |D| idf(a,D) = log (3.24) df(a,D) Eq. (3.25)wouldpenalizeawordifitmoreoftenappearsinthecollectionofdocuments. 3.3 Count-based Models 147 Similarly,wecanhaveasmoothedversionofidf(a,D),likethis |D| idf(a,D) = log +1 (3.25) df(a,D)+1 HavingtheTF-IDFfeaturefunctioninhand,wecanbuildaword-documentco-occurrence matrixforagivencollectionofdocuments,thatis,thevalueoftheentry(a,d)ofthematrix is tfidf(a,d,D). Then, as described in the last subsection, we can treat a row of the matrix as the vector representation of the row word. Note that, traditionally, the TF-IDF method andword-documentco-occurrencematricesareoftenusedindocumentrepresentation. For example,onecanrepresentaqueryandanumberofdocumentsastheTF-IDF(column)vectors inaninformationretrievalsystem. Thisallowsustolookathowmuchthequerymatcheseach of these documents viavector similarity. However, the vector space models in information retrievalarebeyondthescopeofthischapter,butthereadercanrefertorelatedtextbooksfor greatercoverageofthistopic[Manningetal.,2008;Buttcheretal.,2016]. 3.3.3 Low-Dimensional Models Co-occurrence matrices are often high dimensional. Suppose, for example, that there is a vocabulary of 20,000 unique words and a collection of 10,000,000 documents. Then, a word-documentco-occurrencematrixhas20,000×10,000,000=2×1011 entries. However, if we consider the computational burden of such a model, it would be hard to imagine that a word is represented as a 10,000,000-dimensional vector and a document is represented as a 20,000-dimensional vector. Instead, we expect that the representation of a word (or a document)requiresonlyareasonablysmallnumberoffeatures. Inthissubsection,wediscuss some standard approaches to transforming words (or documents) into lower-dimensional representationsfromtheco-occurrencematrices. Mostoftheseapproacheshavebeenwell studiedintheliteratureandhavebeensuccessfullyappliedinseveraldisciplines[Barber,2012; WrightandMa,2022]. Sowedonotdiveintothemathematicaldetailsbehindthem,butshow howtoapplytheminthecontextoflearningword(ordocument)vectors. 1. Latent Semantic Analysis InNLP,latentsemanticanalysis(LSA)isamethodofseekingthelatentsemanticstructure behindtheword-documentassociations[Deerwesteretal.,1990;Landaueretal.,1998]16. It assumesthateitherwordsordocumentscanberepresentedaslow-dimensionalvectorsthatare distilledfromtheco-occurrencematrix,preservingthepropertyoftheoriginalvectorspace model,e.g.,theanglebetweenvectorsissmallforsimilarwords. Morespecifically,LSAfactorizestheco-occurrencematrixintoamatrixforwordrepre- sentation,amatrixfordocumentrepresentation,andathirdmatrixconnectingthefirsttwo matrices. Mathematically, this can be framed as a singular value decomposition (SVD) process[Stewart,1993]. LetM∈R|V|×|D|",
    "representation of a word (or a document)requiresonlyareasonablysmallnumberoffeatures. Inthissubsection,wediscuss some standard approaches to transforming words (or documents) into lower-dimensional representationsfromtheco-occurrencematrices. Mostoftheseapproacheshavebeenwell studiedintheliteratureandhavebeensuccessfullyappliedinseveraldisciplines[Barber,2012; WrightandMa,2022]. Sowedonotdiveintothemathematicaldetailsbehindthem,butshow howtoapplytheminthecontextoflearningword(ordocument)vectors. 1. Latent Semantic Analysis InNLP,latentsemanticanalysis(LSA)isamethodofseekingthelatentsemanticstructure behindtheword-documentassociations[Deerwesteretal.,1990;Landaueretal.,1998]16. It assumesthateitherwordsordocumentscanberepresentedaslow-dimensionalvectorsthatare distilledfromtheco-occurrencematrix,preservingthepropertyoftheoriginalvectorspace model,e.g.,theanglebetweenvectorsissmallforsimilarwords. Morespecifically,LSAfactorizestheco-occurrencematrixintoamatrixforwordrepre- sentation,amatrixfordocumentrepresentation,andathirdmatrixconnectingthefirsttwo matrices. Mathematically, this can be framed as a singular value decomposition (SVD) process[Stewart,1993]. LetM∈R|V|×|D| beaco-occurrencematrixoveravocabularyV 16Latentsemanticanalysisisalsocalledlatentsemanticindexing(LSI).Thistermismoreoftenusedin informationretrievalandrelatedfields. 148 Chapter 3. Words and Word Vectors andadocumentsetD. TheSVDproducesafactorizationofM,likethis M = PΣQT (3.26) whereP∈R|V|×r,Σ∈Rr×r andQT∈Rr×|D|. Inthisfactorization,therepresentationmodel isisolatedintotwotermsPandQTsothatbothofthemaresemi-unitary(orsemi-orthogonal in our case)17, that is, the columns of either P or Q are orthogonal vectors. Thus, these columnsformanorthogonalbasisofRr,wherer istherankofM. Thismeansthatweusea “minimum”numberofdimensionsofdatatorepresentM. Σisadiagonalmatrix:   σ 0 ... 0 1 0 σ ... 0  2  Σ =   . . . . . . ... . . .   (3.27)   0 0 ... σ r Thediagonalentries{σ ,...,σ }areallnon-negativerealnumbers,andarecalledthesingular 1 r valuesofM. Typically,{σ ,...,σ }arearrangedindescendingorder(i.e.,σ ≥σ ≥...≥σ ). 1 r 1 2 r Thus,SVDisuniqueforthegivenmatrixM. IfwewritePasasequenceofcolumnvectors (callthemleft-singularvectors) (cid:104) (cid:105) P = p ,...,p (3.28) 1 r andQT asasequenceofrowvectors(callthemright-singularvectors)  qT 1 . QT =   . .   (3.29) qT r thenwecanwriteMas r (cid:88) M = σ p qT (3.30) i i i i Forrepresentingwords,wecanthinkofp asthevaluesofafeaturefunctionoverallthe l entriesofthevocabularyV. Then,wedescribeaworda asanr-dimensionalfeaturevector i e inwhichthel-thfeatureisthei-thentryofp . Inotherwords,thevectorrepresentationof i l a is i (cid:104) (cid:105) e = p (i) ... p (i) (3.31) i 1 r 17Anon-squarematrixXissemi-orthogonalifandonlyifXXT=IorXTX=I. 3.3 Count-based Models 149 Similarly,thevectorrepresentationofadocumentd canbewrittenas j (cid:104) (cid:105) h = q (j) ... q (j) (3.32) j 1 r Inthisway,wehavetwoseparaterepresentationmodelsforwordsanddocuments: Pdeals withwordrepresentationsandQdealswithdocumentrepresentations. Thus,wecantakeM asaproductoftheserepresentationmodels,likethis M = PΣQT = sdrow    e σ ... 0 1 1 documents   . . .     . . . ... . . .   (cid:104) hT ... hT (cid:105) (3.33) 1 |D| e 0 ... σ |V| r In practice, the rank r is usually much smaller than |V| and |D|. Thus, we have, for eachword(oreachdocument),anewrepresentationwhosedimensionalityismuchsmaller than the representation contained in the co-occurrence matrix. A further improvement can makeuseofther∗ largestsingularvalues(i.e.,{σ ,...,σ })andthrowawaytherest. Asa 1 r∗ consequence,weonlykeepthefirstr∗ left-singularvectorsandright-singularvectorsinPand Qrespectively. Herer∗<r isahyperparameterspecifyingthenumberofvectorsinPandQ, i.e.,thenumberoffeaturesusedtodescribeawordoradocument. Inthisway,wehaveanew factorizationofMas r∗ (cid:88) M ≈ σ p qT (3.34) i i i i The right hand side of Eq. (3.34) is also known as a low-rank approximation of M. By specifyingr∗,itcanapproximateMwithamatrixhavinganarbitraryrank<r. ThereareanumberofalgorithmsforimplementingtheSVD[ClineandDhillon,2014]. In fact,mostofthemodernimplementationsoftheSVDareefficientandscalable. Onecanuse themasoff-the-shelftoolkitsinNLPapplications. 2. Principal Component Analysis Indataanalysis,principalcomponentanalysis(PCA)isawidely-usedtechniquefordimen- sionreduction. Givenasetofdatapoints,PCAfindsasequenceoforthogonaldirectionsin thecoordinatespacesothatthevarianceofthedatapointsalongthesedirectionsismaximized. Thesedirectionsaretypicallyrepresentedasunitvectors,calledprincipalcomponentload- ingsorprincipalcomponentcoefficients. Asaresult,theyformanewcoordinatespaceto whichwecanmapthegivendatapointsbyanorthogonallineartransformation. Consider a word-document co-occurrence matrix M∈R|V|×|D|, where each row is a |D|-dimensionalwordvectororfeaturevector. ThePCAdefinesalinearmappingfromR|D| toRp,thatis,wetransformeach|D|-dimensionalwordvectortoap-dimensionalwordvector. 150 Chapter 3. Words and Word Vectors Thisisgivenby N = MC (3.35) whereN∈R|V|×p isthemappedwordvectorsoverthevocabularyV,andC∈R|D|×p isthe matrixofthelinearmapping. Then,wecanwriteCasasequenceofcolumnvectors (cid:104) (cid:105) C = c ... c (3.36) 1 p   c (1) i . Eachcolumnvectorc i =  . .   isagroupofprincipalcomponentcoefficients,indi- c (|D|) i catingalinearfunctionthatcombinestheinputfeaturesintoanewfeature. Forexample,ifwe viewMasthevaluesofabunchoffeaturefunctions(say,columnvectors{m ,...,m }), 1 |D| wecanmapMtoanewfeaturespaceintermsofc : i   c (1) i (cid:104) (cid:105) . Mc i = m 1 ... m |D|   . .   c (|D|) i |D| (cid:88) = c (k)m (3.37) i k k=1 Mc (i.e.,thei-thcolumnofN)isacolumnvectorwhereeachentryisthenewfeature i forawordinV. InPCA,wegenerate{c ,...,c }insequencesuchthattheymaximizethe 1 p varianceofthelinearmappinginEq. (3.37). Thus,foreachi∈[1,p],theoptimalprincipal componentcoefficientsaredefinedtobe cˆ = argmaxVar(Mc ) i i ci = argmaxcTSc (3.38) i i ci",
    "= m 1 ... m |D|   . .   c (|D|) i |D| (cid:88) = c (k)m (3.37) i k k=1 Mc (i.e.,thei-thcolumnofN)isacolumnvectorwhereeachentryisthenewfeature i forawordinV. InPCA,wegenerate{c ,...,c }insequencesuchthattheymaximizethe 1 p varianceofthelinearmappinginEq. (3.37). Thus,foreachi∈[1,p],theoptimalprincipal componentcoefficientsaredefinedtobe cˆ = argmaxVar(Mc ) i i ci = argmaxcTSc (3.38) i i ci where Var(Mc ) is the variance of Mc , and S is the covariance matrix of M. For a well- i i definedsolutiontoEq. (3.38),itiscommontoimposeanadditionalconstraintthatc isaunit i vector,i.e.,cTc =1. Then,theproblemcanbeframedas i i cˆ = argmaxcTSc −λ (cTc −1) (3.39) i i i i i i ci whereλ istheLagrangemultiplier. SolvingEq. (3.39)undersuchaconstraintrequirescˆ i i to be an eigenvector of S and λ to be the corresponding eigenvalue [Jolliffe, 2002]. Since i Sisap×psymmetricmatrix, ithasexactlypeigenvectorsandeigenvalues. Then, wecan ordertheseeigenvectorsbytheassociatedeigenvalues,andtaketheorderedeigenvectorsas {cˆ ,...,cˆ }. Inotherwords,cˆ istheeigenvectorofSwiththelargesteigenvalue,cˆ isthe 1 p 1 2 3.3 Count-based Models 151 14 Direction1 12 10 8 Direction2 6 Proj. 4 2 0 0 2 4 6 8 10 12 Figure 3.5: Transforming 2-dimensional data to 1-dimensional data via PCA. There are a number of data points (represented by black circles) on a Euclidean plane. By using PCA, we find a direction (represented by an arrow) such that the variance of the projected data (represented by colored circles) in this direction is maximized. Such a direction can be represented by a unit vector, called principal component coefficients. In this example, the principalcomponentcoefficientsdescribea1-dimensionalcoordinatespace. Wecanmapthe datafromthe2-dimensionalcoordinatespacetothe1-dimensionalcoordinatespacevialinear transformation. Themappeddataiscalledtheprincipalcomponentoftheoriginaldatapoints. eigenvectorofSwiththesecondlargesteigenvalue,andsoon. Typically,Mcˆ iscalledthe i i-thprincipalcomponentofM. AnintuitivewaytothinkaboutPCAistomapdatapointsinaEuclideanspacefromone coordinatesystemtoanother. ForadatasetM,wecanvieweachrowinMasthecoordinates of a data point in a |V|-dimensional coordinate system A. In PCA, we want to represent thesedatapointsinanewp-dimensionalcoordinatesystemB. Thei-thdimensionofthenew coordinatesystemissimplyadirectionrepresentedbyaunitvectorc . Forthei-thcoordinate i of each data point in B, we project the data point in A onto the c line. The optimal c is i i chosenintermsofhowtheseprojecteddatapointsarespreadalongc . Inotherwords,weseek i alinealongwhichwecanbestseparatethedatapoints. Inthisway,wegenerateasequenceof principalcomponentcoefficients, successivelysolvingEq. (3.38). Weillustratetheideaof PCAusinganexampleprojecting2-dimensionaldatato1-dimensionaldatainFigure3.5. In real-world applications, p is commonly set to a number much smaller than |D|, and PCAcansignificantlyreducethenumberofdimensionsusedinrepresentingwords. Notethat PCAisaverygeneralmethodandisfoundtobeusefulinmanydisciplines. Inpractice,M canbeextendedtorepresentobservationsonasetofvariables. ByapplyingPCA,onecan transformtheseobservationsintodatavaluesoffewernewvariables. 152 Chapter 3. Words and Word Vectors 3. Others Inmachinelearning,learninglow-dimensionalmodelsisafundamentalproblem,andhasbeen generalizedinseveraldirections. Forexample,theneuralwordembeddingmodelsdescribed inSections3.4and3.5themselvestendtolearnlow-dimensional,real-valuedwordvectors fromtexts. Herewepresentsomeofthedimensionreductionmethodsonemaycomeacrossin theNLPandmachinelearningliterature. • Topicmodels. Technically,topicmodelsarenotwaysofdimensionreduction,buttools fordescribinghowdocumentsandwordsaregeneratedbasedondistributionsovertopics [Blei,2012]. Forexample,latentDirichletallocation(LDA)modelsthegenerationof adocumentbyusingdocument-topicandtopic-worddistributions[Bleietal.,2003]. As aby-product,weobtainadistributionoverwordsforeachtopic,indicatinghowlikelya wordoccursgivenatopic. Ifwewriteallthesetopic-worddistributionsasamatrix,say a|V|×K matrixwhere|V|isthenumberofwordsandK isthenumberoftopics,then wewillhavesomesortofwordrepresentationsthatareverysimilartothosedescribedin previoussections. K iscommonlysettoa“small”number(e.g.,200). Inthiscase,we havealow-dimensionalmodelforrepresentingwords. AlthoughLDAisnotsopopular inlearningwordrepresentationsinNLPapplications,itoffersawaytorepresentwords asdistributionsoverlatentthematicstructures. • Auto-encoders. Undercompleteauto-encodersareatypeofneuralmodelthatencodes featuresintolow-dimensionalcodessuchthattheinputfeaturescanbereconstructed fromthecodes. Anadvantageofauto-encodersisthattheydonotmakeassumptions on the hidden structures of the features. Thus, auto-encoders can be used to learn to transformanytypeofdataintolow-dimensionalrepresentations. Forexample,inChapter 7wewillseeexamplesofapplyingauto-encoderstolearnsentencerepresentations. For moredetailsaboutauto-encodersthereadercanrefertoChapter2. • Superviseddimensionreduction. Traditionally,dimensionreductionmethods(suchas PCA)areassumedtoworkinanunsupervisedmanner. Whenthebenchmarkdataofthe targettaskisaccessible,itisnaturaltomakeuseofthisinformation. Acommonexample issuperviseddimensionreductionforclassification. Forexample,intheFisher’slinear discriminant and linear discriminant analysis methods, we find a mapping from high-dimensionaldatatosingle-dimensionaldatasothattheseparationoftheclasses associatedwiththedataismaximized. Thisideacanbegeneralizedtomulti-dimensional dataintheCanonicalVariatesmethod[Barber,2012]. • Feature selection. Feature selection refers to a process of selecting a subset of the features used in representing an object and thus reducing the dimensionality of the representation. Feature selection is a wide-ranging topic in machine learning, and manymethodscanbeseenasinstancesoffeatureselection[GuyonandElisseeff,2003; Liu and Motoda, 2012]. The simplest is to frame it as a search problem: we search in the space of feature subsets so that the selected features maximize (or minimize) someobjective. Ingeneral,thedesignoftheobjectivedependsonthetaskwherewe apply the features. This makes feature selection somewhat difficult because one has 3.4 Inducing Word Embeddings from NLMs 153 to consider many factors in such a process, such as the performance measure of the",
    "in the space of feature subsets so that the selected features maximize (or minimize) someobjective. Ingeneral,thedesignoftheobjectivedependsonthetaskwherewe apply the features. This makes feature selection somewhat difficult because one has 3.4 Inducing Word Embeddings from NLMs 153 to consider many factors in such a process, such as the performance measure of the target task, the search efficiency, and the representation of each feature subset. Note thatfeatureselectionisgenerallydiscussedinsupervisedlearningthatrequireslabeled datatocomputelossforoptimization. ThereaderisreferredtoSolorio-Fernándezetal. [2020]’sreviewpaperforunsupervisedfeatureselectionmethods. In statistics, many methods can fall under the dimension reduction framework and are relatedtowhatwediscussedinthissection. Forexample,factoranalysisisamethodsimilar toPCAbecausetheybothseekalinearmappingfromtheinputvariablestoasmallernumber ofnewvariables. Thedifferencebetweenthemisthatfactoranalysisfocusesonmodelingthe commonvarianceofvariables,whilePCAfocusesonmaximizingthevarianceoftheprojected data. Anotherexampleisindependentcomponentanalysis(ICA).UnlikePCA,thegoalof ICAistofindindependentcomponentsthatareadditivelyseparable. Moreexamplescanbe foundinmachinelearningandstatisticstextbooks[McClaveandSincich,2006;Freedman etal.,2007;Barber,2012]. 3.4 Inducing Word Embeddings from NLMs Countingword-wordorword-documentoccurrencesisasimplewaytorepresentwordsby usingtheirdistributionsintexts. Whilethismethodiseffectiveinmanyapplications,itimposes aconstraintonwordrepresentations: theentriesofawordvectorshouldbeabletobeexplained assome“evidence”onhowtheworddistributesindifferentcontexts. Ideally,wewouldliketo representwordsinamoregeneralform,say,areal-valuedvector(callitthewordembedding) withoutconstraintsorassumptionsonhowthemeaningofeachentryofthevectorisdefined. Learningwordvectorswithnoconstraintscomesatacost. Unlikethecount-basedmethods presentedinSection3.3,wedonotuseheuristicsorpriorknowledgetoestimatethevalueof a word vector but wish to induce meaningful word representations directly from data. One ofthedifficultieshereisthatthereisnogoldstandardtoguidethelearningprocessbecause it is simply impossible to manually annotate a real-valued word vector. Thus, we are often interested in treating the learning of word vectors as a part of a well-defined task (call it a background task). The learned word vectors are then a by-product of the learning on the backgroundtask. Acommonexampleistheinductionofwordvectorsfromneurallanguagemodels(NLMs). Recall the NLM described in Chapter 2. Its goal is to build a neural network that predicts the probability of a word given its preceding words [Bengio et al., 2003a]. More formally, letw bethewordwewanttopredict,and{w ,...,w }bethecontextwordswehave i i−n+1 i−1 seen. First, the words {w ,...,w } are transformed to d -dimensional word vectors i−n+1 i−1 e {e ,...,e }throughanembeddinglayer. Assumingw istheone-hotrepresentationof i−n+1 i−1 j wordj (arowvectorofsize|V|),thewordvectore isgivenby j e = w C (3.40) j j whereC∈R|V|×de istheparameteroftheembeddinglayer. Cisoftenknownastheword 154 Chapter 3. Words and Word Vectors embeddingtableinwhichthek-throwistherepresentationofthek-thwordinV. Then,weuseafeed-forwardneuralnetworktocomputetheprobabilitydistributionofthe wordatpositioni. Thisisgivenby Pr(·|w ,...,w ) = F (e ,...,e ) (3.41) i−n+1 i−1 θ i−n+1 i−1 whereF (·)isafeed-forwardneuralnetworkparameterizedbyθ. Typically,theembedding θ layer can be seen as a component of the NLM. Here we use slightly different notation to emphasizethattheNLMisafunctionofbothθ andC,likethis Pr (·|w ,...,w ) = F (w ,...,w ) (3.42) θ,C i−n+1 i−1 θ,C i−n+1 i−1 Fortraining,weoptimizebothθ andCtominimizealossfunction. Apopularmethodis maximumlikelihoodtrainingwhichmaximizesthesumoflog-likelihoodoveralln-gramsin thedata. Givenasequenceofwordsw ...w ,theobjectiveofthetrainingisdefinedtobe18 1 m m (cid:88) (θˆ,C(cid:98)) = argmax logPr θ,C (w i |w i−n+1 ,...,w i−1 ) (3.43) θ,C i=n Having obtained the optimized parameters θˆ and C(cid:98), we can apply F θˆ,C(cid:98) (·) to deal with newn-grams. Moreimportantly,wehavesomewell-trainedwordvectors(i.e.,C(cid:98))thatcanbe usedinsystemsotherthanNLMs. Thisisalsoknownasthepre-trainingofwordvectors. In pre-training,wecandefineF (·)asanysystemthatmakesuseofthewordvectorsC. Thus, θ,C thetaskoflearningCistransformedtothetaskofoptimizingF (·)onthebackgroundtask θ,C (seeFigure3.6foranillustration). Themainadvantageofthismethodisthatwecanreuse existingNLPtaskstotrainthewordvectors. Ariskhereisthatthe“best”wordvectorsfound intrainingF (·)mightnotbewellsuitedforthesystemwherethewordvectorsareinactual θ,C use. Interestingly,inmanysituations,wordvectorsthatarepre-trainedbyNLMsareofgood quality for downstream tasks, or at least provide a good starting point for further tuning of thesewordvectorsinthetargetsystem. 3.5 Word Embedding Models Inprinciplewordvectorscanbelearnedinanymanner. Treatingwordvectorsascomponents ofexistingNLPsystemsisoneoption,buttypicallylackstask-specificconsiderations. Another optionistodevelopmethodsspecificallytailoredtotheproblem. Thetrainingofsuchsystems, therefore,doesnotneedtosatisfytheconstraintsofstandardNLPtasks,makingiteasierto learnwordvectors. 18Thiscanbegeneralizedtoadatasetconsistingofmultiplesequences. 3.5 Word Embedding Models 155 OptimizingCandθ UsingoptimizedC(cid:98) vialanguagemodeling onthenewtask G (·) F (·) θ,C π,C(cid:98) (cid:20) (cid:21) (cid:20) (cid:21) • • optimizedwordvectorsC(cid:98) • • C= • • C(cid:98) = • • • • • • TrainingWordVectors ApplyingWordVectors Figure3.6: Illustrationofpre-trainingwordvectorsinanNLM.TheNLMcanbedenotedas a function F (·) of the word embedding table (i.e., C) and other parameters of the NLM θ,C (i.e., θ). The pre-training of C is essentially a process of training F (·) on a background θ,C task. TheoutcomeistheoptimizedwordvectorsC(cid:98) whicharethenappliedtoanewsystem G (·)thatmightbedifferentfromtheNLM.Inthenewsystem,C(cid:98) isthewordembedding π,C(cid:98) tablelearnedfromtheNLMandπ istheparametersspecializedtoG(·). 3.5.1 Word2Vec Word2Vec is a short name for the models proposed in [Mikolov et al., 2013a;c].",
    "and other parameters of the NLM θ,C (i.e., θ). The pre-training of C is essentially a process of training F (·) on a background θ,C task. TheoutcomeistheoptimizedwordvectorsC(cid:98) whicharethenappliedtoanewsystem G (·)thatmightbedifferentfromtheNLM.Inthenewsystem,C(cid:98) isthewordembedding π,C(cid:98) tablelearnedfromtheNLMandπ istheparametersspecializedtoG(·). 3.5.1 Word2Vec Word2Vec is a short name for the models proposed in [Mikolov et al., 2013a;c]. As with neural language models, the Word2Vec models are based on neural networks. Rather than resortingtothegenerativemodelingofn-grams,theWord2Vecmodelsdescribethelearning ofwordvectorsinalog-linearfashion. Inconsequence,thearchitecturesofthesemodelsare differentfromthoseusedinlanguagemodeling. TherearetwotypesofmodelsinWord2Vec: • Thecontinuousbag-of-wordsmodel(ortheCBOWmodel). TheCBOWmodelisa wordpredictionmodel. Itisusedtopredicthowlikelyawordatpositionioccursgiven the−nand+nwordwindowsaroundit. ThestructureoftheCBOWmodelissimilarto thatoftheneurallanguagemodelintroducedinChapter2(seeFigure3.7(a)). First,we useanembeddinglayertotransformthecontextwordsw ...w andw ...w to i−n i−1 i+1 i+n correspondingwordvectors. Thisisperformedbymultiplyingtheone-hotrepresentation ofeachinputwordw j withtheembeddingtableC∈R|V|×de,asshowninEq. (3.40). Thesewordvectorsarethenaveragedtoproduceasinglerepresentationfortheinput words,givingus   i−1 i+n 1 (cid:88) (cid:88) h =  w j C+ w j C (3.44) 2n j=i−n j=i+1 Notethattheabovedefinesamodelthatcompletelyignorestheorderofinputwords becauseoftheuseofthesumoperation. ThisexplainswhytheCBOWmodeliscalled bag-of-words. TheoutputlayeroftheCBOWmodelisastandardSoftmaxlayerthat 156 Chapter 3. Words and Word Vectors projectshtoaprobabilitydistributionoverthevocabulary y = Softmax(hU+b) (3.45) whereU∈Rde×|V|istheparametermatrixofthelinearmappingandb∈R|V|isthebias term. yisadistributionoverthevocabulary,andPr(w |w ,...,w ,w ,...,w )= i i−n i−1 i+1 i+n y(w ). Eqs. (3.44-3.45)describeaverysimpleneuralnetwork. Anadvantageisthatthe i resultingmodelissmallandefficientascomparedtoNLMs. ThetrainingoftheCBOW modelisregular. Wecanframeitasfindingthemaximumlikelihoodestimationofthe parametersofthemodel. Forsimplicity,letθ denotetheparametersotherthanC(i.e, θ={U,b}). Wehave m−n−1 (cid:88) (θˆ,C(cid:98)) = argmax logPr θ,C (w i |w i−n ,...,w i−1 ,w i+1 ,...,w i+n ) (3.46) θ,C i=n+1 wheremisthelengthofthewordsequence. Aftertraining,wecansimplydropθˆ and useC(cid:98) asawordvectorlook-uptable. • Thecontinuousskip-grammodel(ortheskip-grammodel). Theskip-grammodel is another word prediction model. It models the reverse of the task described in Eqs. (3.44-3.45). Tobemoreprecise,ourobjectiveistopredicteachofthe±ncontextwords givenw . Thisisgenerallyframedasestimatingtheprobabilityofw occurringgiven i j w (i−n≤j ≤i−1 or i+1≤j ≤i+n). Figure 3.7 (b) shows the structure of the i skip-grammodel. Theembeddinglayerdealswithw asusual. Therepresentationofw i i isgivenby h = w C (3.47) i ItisthenpassedtoaSoftmaxlayertopredicttheprobabilityforeachcontextwordw j (assumingj =i+k)19 y = Softmax(hV +b ) (3.48) k k k whereV andb aretheparametersofthemodel(−n≤k≤−1and1≤k≤n). We k k have Pr(w |w ) = Pr(w |w ) j i i+k i = y (w ) (3.49) k i+k Letθ beashortrepresentationof{V }and{b }. Thetrainingproblemcanbedefined k k 19Whenk>0,w j isawordintherightcontextwindowofw i;whenk<0,w j isawordintheleftcontext window. 3.5 Word Embedding Models 157 as m−n−1 (cid:88) (cid:88) (θˆ,C(cid:98)) = argmax logPr θ,C (w i+k |w i ) (3.50) θ,C i=n+1 −n≤k≤−1, 1≤k≤n Bothoftheabovemodelsmakeananalogytoclozetestsbyconsideringonlythepairwise dependencybetweenwords. Adangeristhatifcomplexrelationshipsamongwordsandword orderinformationarerequired,theresultingprobabilitydistributionswillbenotthatprecise comparedtolanguagemodels. Note,however,thatthegoalofthesemodelsisnottoprecisely predictmissingwordsgiventheircontexts,buttolearnwordrepresentationsfromsometask thatcapturesword-wordrelationships. Itisthereforenotsoimportanttocareabouttheword predictionperformanceofthelearnedmodel. Anothermeritofthesemodelsisthattheyhaveverysimple,easy-to-trainarchitectures. Forexample,inbothmodelstherearenohiddenlayersandtheembeddinglayerisdirectly connectedtotheoutputlayer. Thesemodelstructurescanbeseenasinstancesoflog-linear modelinginmachinelearning: theinputvariablesarelinearlytransformedtoafeaturevector (e.g.,Eq. (3.44)),followedbyalog-linearfunction(e.g.,Eq. (3.45)). 3.5.2 GloVe Globalvectors,alsoknownasGloVe,arewordvectorsthatarelearnedbyusingbothglobal statisticsoverthecorpusandlocalmodelsofwordprediction[Penningtonetal.,2014]. The GloVemethodstartswithaword-wordco-occurrencematrix(seeSection3.3),andthenforms aneuralmodelbymakingaseriesofassumptions. Given a word-word co-occurrence matrix M, where each cell M(a,b) = count(a,b) represents the number of co-occurrences of words a ∈ V and b ∈ V, we can obtain the conditionalprobabilityPr(b|a)byusingtheequation count(a,b) Pr(b|a) = (cid:80) count(a,b′) b′ count(a,b) = (3.51) count(a) wherecount(a)isthenumberoftimesthewordaoccursinthecorpus. Let us now see a motivating example of GloVe. Suppose that we want to distinguish betweenwordsairandwater. Itiseasytoobtainhowlikelyoneofthesewordsoccursgivena contextwordinthecorpusviaEq. (3.51). Seethefollowingtableforasmallfractionofthe Pr(b|a)matrixfrom3.8M-sentenceEnglishdatainWMT14. Entry w=fly w=drink w=breath w=live w=flow Pr(air|w) 1.5×10−4 6.2×10−5 2.2×10−4 1.6×10−4 3.6×10−4 Pr(water|w) 1.3×10−5 4.1×10−4 1.8×10−5 1.4×10−4 3.0×10−4 Pr(air|w)/Pr(water|w) 11.54 0.15 12.2 1.14 1.2 158 Chapter 3. Words and Word Vectors Pr(w |w ,w ,w ,w )=y(w ) i i−2 i−1 i+1 i+2 i y=Softmax(hU+b) (cid:16) (cid:17) h= 1 (cid:80)i−1 e + (cid:80)i+2 e 4 j=i−2 j j=i+1 j Embedding e =w C e =w C e =w C e =w C i−2 i−2 i−1 i−1 i+1 i+1 i+2 i+2 w w w w i−2 i−1 i+1 i+2 (a)CBOW Pr(w |w ) Pr(w |w ) Pr(w |w ) Pr(w |w ) i−2 i i−1 i i+1",
    "j j=i+1 j Embedding e =w C e =w C e =w C e =w C i−2 i−2 i−1 i−1 i+1 i+1 i+2 i+2 w w w w i−2 i−1 i+1 i+2 (a)CBOW Pr(w |w ) Pr(w |w ) Pr(w |w ) Pr(w |w ) i−2 i i−1 i i+1 i i+2 i =y (w ) =y (w ) =y (w ) =y (w ) −2 i−2 −1 i−1 1 i+1 2 i+2 y =Softmax( y =Softmax( y =Softmax( y =Softmax( −2 −1 1 2 hV +b ) hV +b ) hV +b ) hV +b ) −2 −2 −1 −1 1 1 2 2 Embedding h=w C i w i (b)Skip-gram Figure 3.7: The CBOW and skip-gram architectures. The CBOW model computes the probability Pr(w |w ,w ,w ,w ) where w is a word in a sequence and i i−2 i−1 i+1 i+2 i {w ,w ,w ,w } are words in the ±2 context windows. The context representa- i−2 i−1 i+1 i+2 tionhisthemeanofthewordvectorsthatareproducedthroughanembeddinglayer. histhen fedintoaSoftmaxlayertooutputadistributionoverthevocabulary(i.e.,y). Theprediction probability of w is Pr(w |w ,w ,w ,w )=y(w ). The skip-gram model is also i i i−2 i−1 i+1 i+2 i basedontheembedding+Softmaxstructure. Itmodelstheprobabilityofeachcontextword w giventhewordw . ThisisachievedbysimplycomputingtheoutputofastandardSoftmax j i layerthattakesthevectorrepresentationofw asinput. BoththeCBOWandskip-grammodels i are trained in a maximum likelihood fashion. The resulting lookup table of the embedding layeristhewordvectors(orembeddings)forthewordsinthevocabulary. Inthistable,Pr(air|w)andPr(water|w)indicatehowwellairandwatercorrelatewith different w. We also compute the probability ratio Pr(air|w)/Pr(water|w) in the last line 3.5 Word Embedding Models 159 of the table. Interestingly, it is found that w can be viewed as a probe word by which Pr(air|w)/Pr(water|w) models the relevance between words. When w is more relevant to air but less relevant to water (e.g., w = fly or w = breath), Pr(air|w)/Pr(water|w) is large. Incontrast,whenw islessrelevanttoairbutmorerelevanttowater(e.g.,w=drink), Pr(air|w)/Pr(water|w)issmall. Whenwisrelevanttobothwords,orirrelevanttothem(e.g., w=liveorw=flow),Pr(air|w)/Pr(water|w)isaround1. Aninsightthatwecangainfromtheaboveexamplesisthatthewordvectorsshouldbe abletointerpretPr(air|w)/Pr(water|w). Asimpleideaistodevelopamodeltoapproximate thisprobabilityratio,say, Pr(a|w) F(e ,e ,e˜ ) = (3.52) a b w Pr(b|w) wheree a ,e b ∈Rde arethevectorrepresentationsofthewordsaandb,ande˜ w ∈Rde isthe vectorrepresentationofthecontextwordw. Notethatthenotationhasdifferentmeaningsfore ande˜. TheformerisawordvectorfromanembeddingtableC,andthelatterisawordvector fromanotherembeddingtableC(cid:101). Theuseoftwoembeddingtableshasseveraladvantages. Themainadvantageisthatcombiningmultiplesetsofparameterscouldmitigatetheoverfitting ofthemodel. Thefinalwordembeddingtabletakestheform C+C(cid:101). 2 TherearemanywaystodefinethefunctionF(·). HerewesimplytreatF(·)asaneural networkparameterizedbyC,C(cid:101) andsomeotherparameters. Consideringthesubtractionnature Pr(a|w) incomparingaandbin ,wecanassumethatF(·)dependsone −e . Furthermore, Pr(b|w) a b wecantakee eT ∈R(ore eT ∈R)tomodeltherelationshipbetweentheworda(orb)and a w b w thecontextwordw. Theseleadtoanewformofthefunction F (cid:0) (e −e )e˜T(cid:1) = Pr(a|w) (3.53) a b w Pr(b|w) where (e −e )e˜T ∈R is the difference in representing words a and b when taking w as a a b w probeword. There are still many solutions to Eq. (3.53), though the input of the function is greatly simplified. For a feasible form of F(·), we further assume that Eq. (3.53) holds when we either exchange the embedding tables C and C(cid:101) (i.e., exchange e and e˜ for a, b and w), or transposetheword-wordco-occurrencematrix(i.e., useMinsteadofM(cid:102)). Tomakeuseof theseassumptions,onewayistoletF(·)beahomomorphismbetweentwosidesofEq. (3.53). Thatis F (cid:0) (e −e )e˜T(cid:1) = F(e a e˜T w ) (3.54) a b w F(e e˜T) b w 160 Chapter 3. Words and Word Vectors ThesolutiontoEq. (3.54)requiresthatF(·)=exp(·),andwehave F(e e˜T) = exp(e e˜T) a w a w = P(a|w) count(a,w) = (3.55) count(a) Rewritingthisequation,wehave e e˜T+logcount(a)−logcount(a,w) = 0 (3.56) a w AproblemwithEq. (3.56)isthatthetermlogcount(a)makesthesolutionnon-exchangeable for M and M(cid:102). To address this, a method is to absorb logcount(a) in some terms that",
    "Chapter 3. Words and Word Vectors ThesolutiontoEq. (3.54)requiresthatF(·)=exp(·),andwehave F(e e˜T) = exp(e e˜T) a w a w = P(a|w) count(a,w) = (3.55) count(a) Rewritingthisequation,wehave e e˜T+logcount(a)−logcount(a,w) = 0 (3.56) a w AproblemwithEq. (3.56)isthatthetermlogcount(a)makesthesolutionnon-exchangeable for M and M(cid:102). To address this, a method is to absorb logcount(a) in some terms that are symmetricforaandw,likethis e e˜T+β +β˜ −logcount(a,w) = 0 (3.57) a w a w where β and β˜ are bias terms that depend on a and w, respectively. The quantity on the a w left-handsideofEq. (3.57)describeshowwelle e˜T+β +β˜ fitstheco-occurrencematrix. a w a w We wish to find some word vectors to enforce this quantity to be close to 1. Then, we can definethesquaredloss,asfollows (cid:16) (cid:17)2 L = e e˜T+β +β˜ −logcount(a,w) (3.58) a,w a w a w Thelossoverallpairsofaandw isgivenby (cid:88) L = γ(count(a,w))·L (3.59) GloVe a,w a,w∈V whereγ(count(a,w))isascalarforL . InPenningtonetal.[2014]’spaper, a,w (cid:16) (cid:17)σ count(a,w)  count(a,w)<count γ(count(a,w)) = countmax max (3.60) 1 otherwise wherecount andσ arehyper-parameters. Typically,σ issettoanumbersmallerthan1. max Assuch,γ(count(a,w))willpenalizetheword-pair(a,w)ifcount(a,w)<count ,that max is,thelossfunctionwillassignsmallerweightstorareword-pairs. Eqs. (3.58-3.59)provideaverysimplewaytolearnwordvectorsandcanbeimplemented byusingstandardneuralnetworkbuildingblocks(e.g.,vectordotproductandsummation). AnimportantpropertyofGloVeisthatthemodele e˜T+β +β˜ −logcount(a,w)isitself a w a w linear. Thetrainingisevenachievedwithouttheneedofcross-entropyloss. Thisdifferentiates GloVe greatly from NLM and word2vec in which expensive normalization of the output is required. Theintuitionhereisthattherelationbetweentwowordscanbemodeledinways otherthanprobability-baseddivergence. Infact,Eq. (3.58)looksmorelikearegressionmodel 3.5 Word Embedding Models 161 thatfitsthedataoflogcount(a,w),thatis,wetendtolearntopredictlogcount(a,w)forany pairof(a,w). Anothernoteabouttheuseofglobaldatabearsrepeating. Theco-occurrencematrixis asourceofinformationthatdescribestheentirecorpus. Animportantconsequenceofusing suchinformationisthatthelearningtaskisframedasfindingwordvectorsthatareglobally optimized. Ofcourse,thisdoesnotmakeGloVeuniquebecausethelearningofmanymodels likeNLMandWord2Vecitselfadmitsasimpleformulationasaglobaloptimizationproblem, e.g.,maximizingthelikelihoodovertheentireinputspace. However,theobjectivesinthose problems are complex, and most of them are in practice trained via online learning, e.g., updatingthemodelparametersonabatchofsampleseachtime. Giventhis,GloVeactually definesamoreefficientglobalmodelascomparedwithNLMandWord2Vec. 3.5.3 Remarks Wehaveseenintheprevioussectionshowwordvectorsarelearnedbyusingseveraldifferent methods. Wenowturntodiscussionsofissuesthatonemightbeinterestedinwhentraining and/orapplyingwordvectors. • Count-basedvsNeuralNetwork-based. Thesimplicityandinterpretabilityofcount- based methods have long been appreciated. The use of the distributional hypothesis greatlysimplifiestheproblem,butmakesastrongassumptionontheinformationsource the word vectors can be learned from, and generally leads to data sparsity due to the curseofdimensionality. Attheotherendofthespectrumislearningwithnoassumptions. In these methods, we remove the constraints on the meaning of each dimension, but treat word vectors as low-dimensional intermediate states of a neural network that is developedtoaccomplishsomeNLPtask. Thisenablesthelearningoffeaturesthatare hardtodescribeinrepresentingaword. Thecomparisonofthetwotypesofmethods herecanfallunderthecomparisonoftwowell-knownlearningparadigms,say,feature engineeringvs. end-to-endlearning. Herewedonotwanttogetboggeddownbythis topic. Itis,however,worthpointingoutthatitdoesnotnecessarilyrestrictwordvectors tocertainforms. Ingeneral,thechoiceofthetypesofwordvectorsdependsoninwhat applicationweapplythemandwhatinterpretationweplaceonthem. Forexample,if wewishtohavesomeinterpretable,easy-to-learnwordrepresentation,inducingword vectorsfromco-occurrencematricesmightbeagoodchoice;ifwewishtohavesome real-valued,low-dimensionalwordvectorsthatwillbeintegratedintoabiggerneural network,deeplearningmethodsmightbeworthatry. Notethat,learningcontinuous word vectors has become more and more common recently, given that the past few yearshavesignificantprogresstowardneuralmodelsofNLP.Also,therehasbeenmuch interestincomparingcount-basedandneuralnetwork-basedmethods,andinexploring relationshipsbetweenthem[LevyandGoldberg,2014b;Baronietal.,2014;Levyand Goldberg,2014c;Schnabeletal.,2015a;Levyetal.,2015;Gladkovaetal.,2016]. • Shallow Models vs Deep Models. While it has become popular to solve the word vectorlearningproblemusingneuralnetworks,themodelstructuresweintroducedin 162 Chapter 3. Words and Word Vectors this chapter are simple. Technically, they all have one or two layers of neurons and are often thought of as instances of shallow models. A similar example is the vLBL word embedding model [Mnih and Kavukcuoglu, 2013]. It models the interaction among words using a two-layer neural network. This model, which does not even involve a Softmax function, is one of the simplest word embedding models subject to our knowledge. Such a simple model, however, still works well in many cases. A benefit of shallow models is that they are efficient and scalable to a large amount of data. Thismakesiteasiertousethemtodealwithmore“difficult”NLPproblems. A goodexampleisthefastTextsystemfortextclassification[Joulinetal.,2017]. Ithasa similararchitecturetotheCBOWmodel(seeSection3.5.1). InfastText,theinputtextis representedasabagofwordvectorsthatareaveragedtoformahiddenrepresentation ofthetext. Thisisfollowedbyanoutputlayerthatmapsthehiddenrepresentationto a distribution over predefined classes. In this way, the classification model and word vectorsaretrainedjointly. Althoughshallowmodelsareremarkablyeffectiveforword vectorlearning,therearedeepermodelsthatonemaybeinterestedinformoremodeling power. As with most multi-layer neural networks, learning word vectors with deep neuralnetworkshasacoupleofbenefits[Telgarsky,2016]. First,byusingadeepmodel, wecanexploitpotentiallybetterhypothesesinalargehypothesisspace. Second,deep modelsintroducemorenon-linearityintomodeling,andthusincreasetheabilityofthe modeltodescribecomplexproblems. Therearemanyexamplesoflearningwordvectors in deep models. The simplest of these might be to simply stack",
    "representedasabagofwordvectorsthatareaveragedtoformahiddenrepresentation ofthetext. Thisisfollowedbyanoutputlayerthatmapsthehiddenrepresentationto a distribution over predefined classes. In this way, the classification model and word vectorsaretrainedjointly. Althoughshallowmodelsareremarkablyeffectiveforword vectorlearning,therearedeepermodelsthatonemaybeinterestedinformoremodeling power. As with most multi-layer neural networks, learning word vectors with deep neuralnetworkshasacoupleofbenefits[Telgarsky,2016]. First,byusingadeepmodel, wecanexploitpotentiallybetterhypothesesinalargehypothesisspace. Second,deep modelsintroducemorenon-linearityintomodeling,andthusincreasetheabilityofthe modeltodescribecomplexproblems. Therearemanyexamplesoflearningwordvectors in deep models. The simplest of these might be to simply stack more layers on the wordembeddinglayerinthosesystems. Thestackedlayerscanbefeed-forwardlayers, recurrent layers, convolutional layers, or some combination of them. More recently, word vectors have been employed and/or trained by very deep and complex systems, achievingstate-of-the-artperformanceonmanyNLPtasks[Radfordetal.,2018;Devlin etal.,2019]. However,strongermodelscomewithaddedcomputationalandtraining challenges. Sothereareseverallinesofresearchonmeetingthesechallenges[Pascanu et al., 2013; Bapna et al., 2018; Wang et al., 2019a; Zhang et al., 2019a; Pham et al., 2019;Lietal.,2020b]. InChapters4-6,wewillseeseveralsuccessfulNLPsystemsthat arebasedonverydeepneuralnetworks. • Training Objectives. The idea of taking word vector representations as parameters ofamodelfitswellwiththelatent-variablemodeling: amodelisparameterizedwith learnablewordvectors,andthevaluesofthesewordvectorsareinferredbymaximizing orminimizingsomeobjectivefunctionoftheentiremodel. Whilesuchalearningprocess isregularinmostsituations,thetrainingobjectivevariessomewhat. Adifficultywith thisisthatthereisnoobviousobjectivefordirectlysignalingthetrainingofwordvectors. Asimplesolutiontothisdifficultyistoresorttowell-definedNLPtasks. Forexample, we can use word vectors to represent the input of an NLP model (such as language modelingandtextclassificationsystems). Hencethewordvectorscanserveasstandard parametersofthemodelandbeoptimizedasusual. Anothersolutionistodevelop“new” trainingtasks. Asingeneralmachinelearningproblems,however,thisisawide-ranging topicandtherearesomanychoicestodesignatrainingobjective. Soageneralmethod 3.6 Evaluating Word Embeddings 163 is to slightly update existing tasks. For example, the training objective of CBOW is essentiallybasedonthegeneralwordpredictionproblem,andhasasimilarformasthat usedinlanguagemodeling. Wewillalsoseeseveralnewtasksthatstemfromlanguage modelinginChapter7. Yetinanothersensethesetrainingtasksdonotdirectlyconcern themselveswiththeissueoflearningwordvectors,butgenerallyofferawaytoinjectit intoawell-designed,efficienttrainingprocedure. Notethat,inwordvectorapplications, we may not assume a supervised learning scenario: the learned word vectors can be usedinvarioussystemsthatwehavenoideaoftheseapplicationsystemsinthetraining stage. This makes the problem more like an unsupervised learning problem because thereisnosupervisioninformationfromthetaskwherethewordvectorsareinactual use. Sometimes,whenthetargetapplicationisaccessible,andthereissomelabeleddata, wecanhavefurthertrainingonthosewordvectorsthathavebeentrainedsomewhere. 3.6 Evaluating Word Embeddings Having obtained the vector representation of words, we need to assess the quality of these vectors. Ideally, we wish to evaluate the word vectors against a gold standard. However, unfortunately,thereisingeneralnosuchgoldstandarddatasincenoonecanannotateavector ofnumbersfordescribingaword. Asimplesolutioninthiscaseistoresorttotheresultof someworkingsysteminwhichthesewordvectorsareinvolved. Typically,therearetwotypes ofevaluationapproaches[Schnabeletal.,2015b]. • ExtrinsicEvaluation(orend-to-endtesting). Wedirectlyincorporatethewordvectors into an NLP system which is easy to evaluate, and see how the performance of the systemisinfluencedbythewordvectors. • IntrinsicEvaluation. Wetesttheabilityofthewordvectorstomodelthegivenaspects ofmorphological,syntactic,andsemanticproblems. Wewillbrieflydescribebelowhowtheseapproachesareappliedtowordvectorevaluation. 3.6.1 Extrinsic Evaluation This approach is often taken in practice since it allows researchers and engineers to glean aquickunderstandingofhowareal-worldsystembehaveswhenchangingpartofit. Since manyNLPsystemsusewordsasinputs,itiscommontoreplacethesymbolicrepresentation of words in these systems with the word vectors. So far, we have seen several systems of thiskind,commonlywithanembeddinglayertransformingtheone-hotrepresentationtothe real-valued vector representation of each input word, see for example the neural language modelinChapter2. Givensuchasystemandasetoflearnedwordvectors,wecanuseitsperformanceasa measureofthequalityofthewordvectors. Consideringthewayweusethewordvectors,there aretwowaystotrainthesystem: • WordVectorsasFixedParameters. Wefixthewordvectors,andtrainotherparameters 164 Chapter 3. Words and Word Vectors ofthesystemasusual. • WordVectorsasInitialParameters. Wetrainalltheparametersinthesamemanner. In this way, the provided word vectors can be seen as initial values of some of the parameters,andwouldbeupdatedduringtraining. Both methods fall under the area of pre-training, and could be extended to cover many problemswherepartofamodeliswelltrainedbeforeseeingthedownstreamtask. Byfixing wordvectors,wesimplifythetrainingprocess,leadingtoaquickevaluationofthewordvectors. Incontrast,treatingthewordvectorsaslearnableparametersmayincreasethedifficultyof training,butcouldlearn“new”wordvectorsthatarebettersuitedfortheworkingsystem. Notethatalthoughextrinsicevaluationisofinteresttopractitioners,theresultsfromthis evaluationarehighlydependentonthesysteminwhichweapplythewordvectors. Because developingadesiredNLPsystemofteninvolvessophisticatedtrainingandtuningprocedures otherthanwordrepresentation,theconclusiondrawnbyexperimentingwithsuchacomplex systemisgreatlyinfluencedbythewaywebuildandusethesystem. Thisisalsothecasefor manyotherNLPproblems. Forexample,atokenizationmethodthatishelpfulforamachine translationsystemmightnotbeagoodchoiceforaninformationretrievalsystem. Therefore, totestthegeneralizabilityofthegivenwordvectors,awidely-usedapproachistocarryout experimentsonavarietyofNLPsystems. 3.6.2 Intrinsic Evaluation Althoughmuchofwordrepresentationresearchinvolvesend-to-endtestsinNLPapplications, italsoinvolvesexaminingtheabilityoftherepresentationtodealwithcertainproblems,such asinterpretingtherelationshipbetweentwowords. Therearemanywaystodesignintrinsic evaluation,eachaddressingaspecificproblem. Inthefollowingwedescribesomeofthese methods. Formorecomprehensivedescriptionsaboutintrinsicevaluation,thereadercanrefer topapersonthissubject[Baronietal.,2014;Bakarov,2018;Rogersetal.,2018]. 1. Semantic Relatedness Modelingtherelatednessbetweenwordsisperhapsthemostpopularmethodtoevaluatethe qualityofwordvectorsinNLP[ReisingerandMooney,2010;Huangetal.,2012;Baronietal., 2014]. Itisfundamentallyaboutcomputingsomedistancebetweenwords(callittheword semanticdistanceorworddistanceforshort). Themotivationisthattheworddistancein a word vector space should agree with the judgments on the word relatedness in our mind [Rubenstein and Goodenough, 1965]. For example, we wish that dog is close to wolf, and peachisfarfromtelevision. Mathematically,therearealotofwaystocalculatethedistance(or angle)betweentwovectors. AsimpleandcommonlyuseddistancemeasureistheEuclidean distance. Also,wecancomputethecosinesimilarityoftwovectorstoobtainascoreinthe interval[−1,1]20. In evaluation, we are given a set of word pairs, each of which is assigned an expected 20Itisoftentousetheabsolutevalueofthecosinescoresothat0indicatestwovectorsinthesamedirectionand 1indicatestwoorthogonalvectors. 3.6 Evaluating Word Embeddings 165 distance by humans. Then, given a pair of words, we compare the expected distance with the distance in the word vector space. The quality of the word vectors is reflected in the differencebetweenthetwodistances.",
    "of word pairs, each of which is assigned an expected 20Itisoftentousetheabsolutevalueofthecosinescoresothat0indicatestwovectorsinthesamedirectionand 1indicatestwoorthogonalvectors. 3.6 Evaluating Word Embeddings 165 distance by humans. Then, given a pair of words, we compare the expected distance with the distance in the word vector space. The quality of the word vectors is reflected in the differencebetweenthetwodistances. However,adifficultyhereisthatthereis,inpractice, no gold-standard distance between words. Even for humans, it is still very difficult to give an exact number to describe how close a word is to another. An alternative method in this caseistocategorizethedistanceintoafewcategoriesorratingscores,suchasanintegerin [1,5] [Reisinger and Mooney, 2010]. This greatly reduces the difficulty in data annotation. Anotherwaytoreducethedifficultyistoletthemodelfindthemostsimilarwordinasmall set of candidates to a given word. Such a method prevents us from predicting an absolute distancebetweenwords. Insteadweonlyneedsomemechanismtoobtaintherelativedistance orsimilaritybetweenwords[Baronietal.,2014]. Judging the relationship between words, however, may result in a highly ambiguous taskbecauseoftheambiguousnatureoflanguageuseandunderstanding. Ingeneral,many factors may affect one’s thoughts on how words are related [Faruqui et al., 2016]. For example,cornandcorneaaresimilarifweconsiderstringoverlapsinthesuffix,buttheyare semanticallydissimilarbecausetheyrefertodifferentmeanings. Theambiguityalsocomes fromthedefinitionofrelatedness. Sometimes,relatednessandsimilarityaretwotermsused interchangeablybuttheymayrefertodifferentconcepts. Forexample,carisrelatedtoroad, butinanothersensecar issimilartovan. Anotherproblemisthatthemeaningofawordis often context-dependent. This makes it more difficult to establish the relationship between wordswithmultipledifferentmeanings(i.e.,polysemy). Broadlyspeaking,thisisaninherent problemwithstatisticwordvectormodelswhereeverywordisassumedtobemappedtoa singlevector. Forcontextualizedmodelingofwordvectors,wewilldescribeinthefollowing chaptersseveralmethodsthatconsiderawordtobedifferentinrepresentationgivendifferent contexts. 2. Word Analogy Wordanalogyisconcernedwithmodelinganalogicalrelationsbetweenpairsofwords. The assumption here is that the relation between words can be captured by performing simple algebraicoperationsonthecorrespondingwordvectors. Awell-knownexampleistheone presentedinMikolovetal.[2013d]’spaper,whereitisfoundthatthewayawordisrelatedto anotherwordcanbedescribedbyvectorsubtraction. Thisleadstoaninterestingresult: ifwe subtractman’swordvectorfromking’swordvector,andaddwoman’swordvectortoit,then wewillobtainawordvectorclosetoqueen’s. Thatis e −e +e ≈ e (3.61) king man woman queen Formally, word analogy is a task of comparing two word pairs (a,a∗) and (b,b∗). An analogycanbemadeifthewayaisrelatedtoa∗ issimilartothewaybisrelatedtob∗. This essentiallyreflectssomesortoflinguisticregularityinwordvectors,whichcanbeexpressed 166 Chapter 3. Words and Word Vectors byusingvectorsubtraction: e −e ≈ e −e (3.62) a∗ a b∗ b The word analogy can be framed as an analogical reasoning task: we try to predict e b∗ usinge ,e ande . Morespecifically,wewishe −e +e tobeclosetoe if(a,a∗)and a a∗ b a∗ a b b∗ (b,b∗) hold similar relations. Also, improvements can be made on such a formulation. For example, we can consider the angle between vectors e −e and e −e , rather than the a∗ a b∗ b differenceine −e +e ande [LevyandGoldberg,2014b]. a∗ a b b∗ Wordanalogyprovidesasimplewaytoexaminethelinearitypropertyofawordvector modelwhichisnottypicallyinvolvedinclassicmethods. Aninterestingpointhereisthatthe recentwordvectormodelsexhibitgoodlinearbehavior,althoughwedonotconsiderthisin modelingand/ortraining. Italsogivesresearchersusefulinsightsintothemodelslearnedby thosemethodsandintopotentialwaysofapplyingthesemodels[LevyandGoldberg,2014b; Linzen,2016;AllenandHospedales,2019]. Ontheotherhand,wordanalogyisnotageneral- purposemethod. Inmanycases,itdoesnotcorrelatewellwiththeperformanceofdownstream systems,andistherebyusedasawaytostudycertainissuesofwordrepresentation. 3. Word Categorization (or Clustering) Another way to see how well the word vectors correlate with our understanding of word meaning is to see how well these vectors can be categorized into meaningful groups. This is often achieved by performing clustering algorithms on the word vectors. We wish that similarwordsaregroupedintothesamecluster,anddissimilarwordsaregroupedintodifferent clusters. Forexample, apple, grape, peach, andorangebelongtothesamegroupofwords because they are all fruits. An advantage of this kind of evaluation is that many clustering algorithmsandwordclusteringbenchmarkshavebeendevelopedandarestraightforwardly applicablehere. Ontheotherhand,asinmostclusteringtasks,therearepracticalissuesthat wehavetodealwith,suchasdeterminingthenumberofclusters. In machine learning, most clustering methods require computing the distance between datapoints. Inthissense,wordclusteringisessentiallybasedonthesameideaofmodeling thewordrelatedness,thoughwedonotneedtojudgethequalityofthedistanceinthiscase. Thisshowssomeintrinsicconnectionsamongdifferentevaluationmethods. However, asa side-effect,wordclusteringinheritsthesameproblemwithrelatedmethods(suchassemantic relatedness). AsdiscussedinSection3.6.2,itisdifficulttodesignagold-standardcriterionto measurehowwellthewordsareclustered,sincewecangroupwordsintoclustersinsomany differentways. 4. Subconscious Evaluation The general idea of subconscious evaluation is to examine the correlation between the use ofwordvectorsandsubconsciousbehaviorsorbrainfunctionswhenonereadstext. Awide varietyofpsycholinguisticphenomenacanbeusedasthetest[MitchellandLapata,2010]. A well-knownmethodisprimingwhichstudieshowapersonrespondstostimuli[Schacterand 3.6 Evaluating Word Embeddings 167 Buckner,1998;TulvingandSchacter,1990;WiggsandMartin,1998]. Forexample,wecan designanexperimenttotestthespeedwithwhichapersonreadsagivenword(callitthetarget word)whenitfollowsanotherword(callittheprimeword)[MeyerandSchvaneveldt,1971; Lund,1995;McNamara,2005]. Ifthetargetwordtisreadmorequicklywhenfollowingaword athanwhenfollowinganotherwordb,thenwewouldsaythattcorrelatesmorewithathanb. Then,wecanusesuchapsychologicalmeasuretojudgethedistanceorsimilaritybetween wordvectors. Toobtainthetimetheparticipanttakesinreading,apopularmethodistoframeit asaself-pacedreadingtask21. Anothermethodistouseeye-trackingtoautomaticallyrecord theinformationoftheeyemovementandposition. Byusingthesetechniques,severalmethods anddatasetshavebeenusedforstudyingavarietyofpsycholinguisticissues[Mitchelland Lapata,2010;Hutchisonetal.,2013;LapesaandEvert,2014;Klerkeetal.,2015;Søgaard, 2016;Augusteetal.,2017]. Inadditiontotrackinghumanbehaviorinreading,wecanmonitorbrainactivitybyusing neurological tests, such as functional magnetic resonance imaging (fMRI) and electroen- cephalography (EEG) [Devereux et al., 2010; Søgaard, 2016; Bhattasali et al., 2020]. For example, it is often hypothesized that,",
    "Embeddings 167 Buckner,1998;TulvingandSchacter,1990;WiggsandMartin,1998]. Forexample,wecan designanexperimenttotestthespeedwithwhichapersonreadsagivenword(callitthetarget word)whenitfollowsanotherword(callittheprimeword)[MeyerandSchvaneveldt,1971; Lund,1995;McNamara,2005]. Ifthetargetwordtisreadmorequicklywhenfollowingaword athanwhenfollowinganotherwordb,thenwewouldsaythattcorrelatesmorewithathanb. Then,wecanusesuchapsychologicalmeasuretojudgethedistanceorsimilaritybetween wordvectors. Toobtainthetimetheparticipanttakesinreading,apopularmethodistoframeit asaself-pacedreadingtask21. Anothermethodistouseeye-trackingtoautomaticallyrecord theinformationoftheeyemovementandposition. Byusingthesetechniques,severalmethods anddatasetshavebeenusedforstudyingavarietyofpsycholinguisticissues[Mitchelland Lapata,2010;Hutchisonetal.,2013;LapesaandEvert,2014;Klerkeetal.,2015;Søgaard, 2016;Augusteetal.,2017]. Inadditiontotrackinghumanbehaviorinreading,wecanmonitorbrainactivitybyusing neurological tests, such as functional magnetic resonance imaging (fMRI) and electroen- cephalography (EEG) [Devereux et al., 2010; Søgaard, 2016; Bhattasali et al., 2020]. For example, it is often hypothesized that, when a person reads and understands words, some activationsoccurinhisorherbrain. Thereforewecanlinkthemeaningofwordswithbrain functions. Ontheotherhand,anobjectionisthattheknowledgeaboutthemechanismbehind theseprocessesisstilllimited,makingitdifficulttocorrelatetheresultsofthesestudieswith real-worldNLPsystems[Baronietal.,2014;Bakarov,2018]. 5. Linguistically Motivated Evaluation Linguistically motivated evaluation is based on an assumption that word vectors learned fromdatashouldexplainlinguisticresources. Oneinterestingapproachtoperformingsuch evaluationistoalignthewordvectorswithsomerepresentationsoftheentriesofadictionary [Tsvetkovetal.,2015;AcsandKornai,2016]. Thequalityofthewordvectorsismeasured in terms of the correlation between these word vectors and the linguistic representations22. Apartfromstandarddictionaries,wecancomparethewordvectorsagainstasemanticnetwork, suchasWordNet. Inthisway,theevaluationwouldbeimprovedifweconsidergraph-based algorithmsonresourcesofthistype[Agirreetal.,2009]. 3.6.3 Visualization Taking word vectors as data points, we can adopt general approaches to visualizing multi- dimensionaldatatolocatedatapointsina2or3-dimensionalmap. Inthisway,wecananalyze patternsencodedinthesewordvectorsandinterpolatetherelationshipbetweenwords. Sincea wordvectorgenerallyhashundredsofdimensionsinpracticalapplications,weneeddimension reductiontechniquestomapitto2or3-dimensionaldataforvisualization. OnemethodisPCA whichseeksalinearmappingfromahigh-dimensionalspacetoalow-dimensionalspace(see 21Inself-pacedreading,thetextissegmentedintowords(orphrases),andtheparticipantisaskedtopressa buttontorequestthedisplayofasegment. 22Alinguisticrepresentationcanbeseenasafeaturevectorthatismanuallybuiltonalinguisticresource(such asadictionary). 168 Chapter 3. Words and Word Vectors Section3.3.3). Anotherwell-knownmethodist-distributedstochasticneighborembedding (t-SNE)[HintonandRoweis,2002;VanderMaatenandHinton,2008]. t-SNEisanon-linear dimensionreductionmethod,andhasbeenwidelyusedinvisualizinghigh-dimensionaldata. Apartfromthese, onecanconsiderthemethodspresentedinSection3.3.3aswellasthose tailoredforvisualizingwordvectors[Zhangetal.,2019b;Liuetal.,2017]. 3.7 Summary In this chapter we discussed two interesting problems in NLP: tokenization and word (or token)representation. First,weintroducedmodelsfordividingasentenceintounitsthatare meaningfuland/orwellsuitedfordownstreamtasks. Second,weintroducedtheideaofword vectormodelswithparticularattentiontolearningbothcount-basedhigh-dimensionalmodels and real-valued low-dimensional models. While most of these models are simple, they are oftenusedincomplexNLPsystemsandformthebasisofmanyadvancedmodels,aswillbe showninthefollowingchapters. Tokenization(orsegmentation)isanimportant“operation”inNLP,commonlyasapre- processingstepformanyapplications[WebsterandKit,1992]. However,theuseoftheterm tokenization is somewhat misleading because it originally refers to a process of dividing a string into substrings and is more often used as a general computer science term. In NLP, tokenizationcandrawonconceptsandresultsfromseveralsub-fields. Onthelinguisticsside, tokenizationishighlyrelatedtotwofundamentalquestions: howwordsarecomposedandhow wordsformsentences. Itisthereforenaturaltousetheoriesandmethodsofmorphologyand syntaxtodefinethebasicunitsofalanguage,leadingtomanyrule-basedtokenizationsystems coveringavarietyoflanguages. Onthemachinelearningside,tokenizationhaslongbeencast asaproblemoflearningtokenboundariesfromdataineitherasupervisedorunsupervised manner[Mielkeetal.,2021]. Acommonapproachistofirstannotatesometokenizedtextwith humanknowledgeaboutwhatbasiclanguageunitsshouldbe,andthenlearntotokenizeon thisannotateddata(seeSection3.1.3). Morerecently,learningtokenizerswithoutlinguistic constraintshasbeenfoundtobepromising(seeSection3.1.4). Sincenaturallanguagesare themselvessetsofcharactersorbytesequences,itisalsopossibletosegmentasentenceinto characters or bytes [Ling et al., 2015; Lee et al., 2017]. The tokenization-free method in generalmayhelpwhenonewantsalanguage-independenttokenizerandasimplerpipelinefor processingthetext. Fromamoremathematicalperspective,tokenizationcanbethoughtofasamappingfrom the input data to a sequence of variables. In this way, the concept of tokenization can be generalizedbyrelaxingtheassumptionthatboththeinputandoutputvariablesareconstrained todiscretevalues. Inrecentimageandspeechprocessingsystems,forexample,researchers trytotransformcontinuousinputdata(suchaspixelsandacousticsignals)intoasequence ofvector-based“tokens”[Schneideretal.,2019;Dosovitskiyetal.,2021]. Someinteresting extensionsoftheseideasareeventotransformimageandspeechdatatoasequenceofindices, leadingtoapproachesbearingacloserrelationtoNLP[Oordetal.,2017;Baevskietal.,2020; Hsuetal.,2021]. 3.7 Summary 169 Giventhattheinputtextisdividedintosmallerpieces,anaturalnextstepistorepresent thesepiecesinsomewaythatcapturestheirunderlyingfeatures. Whilerepresentinglanguage unitsasvectorsofnumbershasbeenthedefactostandardforthedevelopmentofrecentNLP systems,theworkonvectorrepresentationdatesbacktotheveryearlydaysofcomputational linguistics. Accordingtomanypopulartextbooksandpapers[ManningandSchütze,1999; JurafskyandMartin,2008],theideaofusingadistributiontorepresentwordmeaning,also knownasdistributionalsemantics,startedinthe1950swiththeriseofempiricism. Atthe time,mostoftheworkwasinfluencedbyHarris’sdistributionalism[Harris,1954]andrelated work [Firth, 1957; Wittgenstein, 1953]. In parallel, Osgood [1952] proposed to define the meaningofaconceptasapointinamultidimensionalspaceinapsychologicalmanner. All theseideasgreatlyinfluencedthewaylinguisticsandNLPpeoplethinkofwordmeaningin thefollowingdecades. Modernapproachestodistributionalsemanticsappearedinthe1990s,mainlyasaresultof therevivalofempiricisminartificialintelligence[Church,2011]. Mostoftheseweredrivenby thedistributionalhypothesis: wordshavingsimilarmeaningsaremorelikelytooccurinsimilar contexts. Inresponse,anumberofmethodsweredeveloped,differinginthewaythecontexts are modeled. For example, a context can be the words in a context-window, or the words witharelationtothegivenwordinasyntaxtree. ApartfromthosementionedinSection3.3, methodsthatarenotcoveredinthischapterincludehyperspaceanalogueoflanguage(HAL) [LundandBurgess,1996],distributionalmemory[BaroniandLenci,2010],dependency-based semanticspacemodels[PadóandLapata,2007],andsoon. Forcomprehensivedescriptionsof distributionalsemanticsmodels,thereadercanrefertopapersthatsurveythistopic[Lenci, 2018; Mitchell and Lapata, 2010]. Note that most of the above-mentioned work can be thoughtofasinstancesofthevectorspacemodelwhichcandealwithproblemsbeyondlexical semantics. Forexample,incompositionaldistributionalsemantics,themeaningofaphraseor asentencecanberepresentedasavectorobtainedbyperformingsimplealgebraicoperations onthewordvectors[Clarketal.,2008;MitchellandLapata,2010;BlacoeandLapata,2012]. WhiledistributionalmodelshaveattractedattentionintheNLPcommunityformanyyears, wordembeddingmodelsthatlearnlow-dimensional,real-valuedwordvectorsdirectlyfrom textshavebeenapredominantapproachrecently. AsdescribedinSections3.4-3.5,models ofthistypedonotdependonstrongassumptionslikethedistributionalhypothesis,butlearn to represent a word as a vector of hidden attributes (or features) describing the word. The resulting model is an extension of the feature-based semantic model [Markman, 2013]. A recognizeddifferencewithtraditionalfeature-basedmethodsisthatwedonotneedtomanually definethefeatures. Weinsteadtakethesefeaturesasparametersofthemodel,andtrainthem inthewayasincommon(supervised)machinelearningsystems. Formulatingwordrepresentationasanend-to-endlearningproblembringswithitseveral benefits. One of the benefits is that new features can be found because no constraints are placed on how these features are learned and interpreted. On the other hand, as shown in Section3.6.2,thewordvectorsobtainedinthiswayindeedshowsomelinguisticproperties, though the word embedding models are not trained to achieve this. Another benefit is that the word embedding models also fall in the vector space models in NLP, enabling the easy 170 Chapter 3. Words and Word Vectors useofwordvectorsinvariousapplications. Therearealsomanyexamplesofmethodsthat attempttoimprovestandardwordembeddingsystems. Forexample,researchershavetriedto incorporateadditionallinguisticinformationintowordvectors[LevyandGoldberg,2014a; CotterellandSchütze,2015;Tissieretal.,2017],andtolearnuniversalwordvectorsacross multiplelanguages[Klementievetal.,2012;Mikolovetal.,2013b;Ammaretal.,2020;Smith etal.,2017;Artetxeetal.,2017]. WidelyassociatedwithneuralmodelsinNLP,theideaofdistributedrepresentationhas beensuccessfullyappliedtoproblemsbeyondwordrepresentation,e.g.,sentencerepresentation [LeandMikolov,2014;Kalchbrenneretal.,2014;Kirosetal.,2015;Hilletal.,2016;Arora etal.,2017;Linetal.,2017;Conneauetal.,2017b],tree/graph-structurerepresentation[Socher etal.,2011;Perozzietal.,2014;Taietal.,2015;GroverandLeskovec,2016],andsoon. In particular,contextualizedrepresentationsofwords,thoughnotdiscussedinthischapter,are generallyappreciatedformodelingsequentialdata[McCannetal.,2017;Petersetal.,2018; Devlinetal.,2019]. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 4 Recurrent and Convolutional Sequence Models Thewholeismorethanthesumofitsparts. –Aristotle,384-322BC[Ross,1924] Aristotlemightormightnotthinkoflinguisticphenomenawhenhavingthisthought,butitis indeedsomethingwewanttoexpressinthischapter: thereissomethingdifferentinasentence orphrasebesideswords. Ofcourse,wordshavemeanings,alone. However,whentheycome togethertoformasentenceorphrase,themeaningofthewholecouldbemuchmorecomplex anddiverse. Thisleadstothemostbeautifulaspectoflanguagethathumanbeingscanexpress anymeaningusingafinitesetofelements(e.g.,wordsorcharacters). Theinfiniteandnon-compositionalnatureoflanguagemakesitmoredifficulttomodela sequenceofwordsthantomodelindividualwords. Adifficultyisthatawordmayrepeatedly alteritsmeaningindifferentcontexts. Takingtheideaofwordembeddingthatawordcanbe representedasalow-dimensional,real-valuedvector,the“meaning”ofalanguageunitcould becontinuous. Itisthereforepossibletoextendmethodsofdistributedrepresentationfrom",
    "Words and Word Vectors useofwordvectorsinvariousapplications. Therearealsomanyexamplesofmethodsthat attempttoimprovestandardwordembeddingsystems. Forexample,researchershavetriedto incorporateadditionallinguisticinformationintowordvectors[LevyandGoldberg,2014a; CotterellandSchütze,2015;Tissieretal.,2017],andtolearnuniversalwordvectorsacross multiplelanguages[Klementievetal.,2012;Mikolovetal.,2013b;Ammaretal.,2020;Smith etal.,2017;Artetxeetal.,2017]. WidelyassociatedwithneuralmodelsinNLP,theideaofdistributedrepresentationhas beensuccessfullyappliedtoproblemsbeyondwordrepresentation,e.g.,sentencerepresentation [LeandMikolov,2014;Kalchbrenneretal.,2014;Kirosetal.,2015;Hilletal.,2016;Arora etal.,2017;Linetal.,2017;Conneauetal.,2017b],tree/graph-structurerepresentation[Socher etal.,2011;Perozzietal.,2014;Taietal.,2015;GroverandLeskovec,2016],andsoon. In particular,contextualizedrepresentationsofwords,thoughnotdiscussedinthischapter,are generallyappreciatedformodelingsequentialdata[McCannetal.,2017;Petersetal.,2018; Devlinetal.,2019]. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 4 Recurrent and Convolutional Sequence Models Thewholeismorethanthesumofitsparts. –Aristotle,384-322BC[Ross,1924] Aristotlemightormightnotthinkoflinguisticphenomenawhenhavingthisthought,butitis indeedsomethingwewanttoexpressinthischapter: thereissomethingdifferentinasentence orphrasebesideswords. Ofcourse,wordshavemeanings,alone. However,whentheycome togethertoformasentenceorphrase,themeaningofthewholecouldbemuchmorecomplex anddiverse. Thisleadstothemostbeautifulaspectoflanguagethathumanbeingscanexpress anymeaningusingafinitesetofelements(e.g.,wordsorcharacters). Theinfiniteandnon-compositionalnatureoflanguagemakesitmoredifficulttomodela sequenceofwordsthantomodelindividualwords. Adifficultyisthatawordmayrepeatedly alteritsmeaningindifferentcontexts. Takingtheideaofwordembeddingthatawordcanbe representedasalow-dimensional,real-valuedvector,the“meaning”ofalanguageunitcould becontinuous. Itisthereforepossibletoextendmethodsofdistributedrepresentationfrom wordstosequencesofwords. Thisleadsustoexploremodelsinwhichtheprocessofdealing withvariable-lengthwordsequencesisfundamentallycontinuous. Hereweconsiderthegeneralapproachtolearningthedistributedrepresentationofword sequences. Inparticular,weconsiderrecurrentandconvolutionalneuralnetworkswhichhave beenextensivelyusedinmanyfieldsrangingfromspeechprocessingtocomputervision. For naturallanguageinputs,theresultofapplyingthesemodelsisasequence-levelrepresentation oftheinput. Therepresentationcouldbeeitherasinglereal-valuedvector,orasequenceof suchvectors,eachcorrespondingtoacontextualizedrepresentationforaninputwordofthe input sequence. Such a model of representation, that can broadly be called an encoder, is generallyusedwithavarietyofsystemswhoseinputissequentialdata. Wewillseeseveral examplesofitinthischapter. 172 Chapter 4. Recurrent and Convolutional Sequence Models 4.1 Problem Statement FormanyNLPapplications,ourobjectiveistomakepredictionsbasedonaninputsequence. Let us consider again the text classification problem mentioned in Chapter 1. If we obtain a text that may talk about food or not, we want to assign one of the two classes to it (say FoodorNot-food). Todothis,acommonmethodofclassificationistorepresentthetextasa bagoffeatures,denotedasH. Then,aprobabilityisassignedtoeachoftheclassesusinga probabilisticmodelPr(y|H). Thepredictedclassistheonethathasthemaximumprobability yˆ=argmax Pr(y|H). y Whilethisisastandardprocedureforclassification, theunderlyingideacanbeusedto describeageneralproblem. Formally,letw=w ...w beasequenceofwords1. Asequence- 1 m levelNLPsystemcanbeformulatedasafunctionthatmapsthesequencewtosomeoutput y. This can be divided into two steps, called the representation (or encoding) step and the predictionstep. • Representation(orEncoding). Ittransformstheinputsequencewtosome“features” HbyusinganencoderEnc(·): H = Enc(w) (4.1) • Prediction. ApredictorPredict(·)takesHandgeneratesanoutput: y = Predict(H) (4.2) AsimpleformofHisafeaturevector. Forexample,Hcouldbeasetofhuman-designed indicatorfeaturesextractedfromw(asahigh-dimensionalsparserepresentation),orasetof realnumbersindicatingsomelatentfeatures(asalow-dimensionaldenserepresentation). In NLP,anothercommonformofHisasequenceofvectorsinwhicheachvectorh corresponds i toaninputwordw (seeFigure4.1). Inthiscase,h canbeviewedasa“new”representation i i ofbothw anditscontextinw2. Thecorrespondencebetweenh andw enablestherepresen- i i i tationtomakedistinctionsamongdifferentpositionsofthesequence,andmoreimportantly,to varyitsmodelingpowerforvariable-lengthinputs. The form of y is dependent on the problem we intend to deal with. For example, for classificationproblems,yistheindexofaclass(oradistributionofclasses);forregression problems,y isarealnumber;fortranslationproblems,y isasequenceofwordsinanother language, and so on. Note that, in the above model, representation and prediction can be regardedastwoseparateproblems. Agreatadvantageofisolatingrepresentationandprediction isthatwecanusethesameencoderinmanyapplicationswithdifferentpredictors. Thisalso motivatesapromisinglineofresearchinwhichageneral-purposeencoderistrainedonlarge- scaledataandthenusedascomponentsindifferentdownstreamsystems[Petersetal.,2018; 1Although we restrict ourselves to word sequences for discussion, the methods can be used to deal with sequencesofanylanguageunits,e.g.,sub-words,characters,etc. 2Thisarchitecturecanbeextendedtoencodersinwhichtheinputandoutputhavedifferentlengths,say,the inputisw 1 ...wmandtheoutputish 1 ...hn(m̸=n). 4.2 Recurrent Models 173 H (asinglevector) h 1 h 2 ··· h m−1 h m =H Encoder Encoder w w ··· w w w w ··· w w 1 2 m−1 m 1 2 m−1 m (a)Encodingthesequenceasavector (b)Encodingthesequenceasavectorsequence Figure4.1: Representingawordsequenceas(a)avectoror(b)asequenceofvectors. Devlinetal.,2019]. There are many possible forms for Enc(·) and Predict(·). For text classification, for example, one way is to define Enc(·) as a function computing a feature vector using a set ofhand-craftedfeaturetemplates,anddefinePredict(·)asastatisticalclassificationmodel (suchasSVMsandmaximumentropy-basedmodels). AnotherwayistodefineEnc(·)asa multi-layerneuralnetworkthatoutputsareal-valuedvector,anddefinePredict(·)asasimple neuralnetworkthatinvolvesonlyoneSoftmaxlayer. Inthischapterwewillfocusonneural network-based encoders. We will show that such a type of encoder could be applied to a numberofNLPtasksinSection4.5. 4.2 Recurrent Models A study of various sequence models is not easy work. It is convenient, however, to first introduce one of the most common and practical neural models, called recurrent neural networks(RNNs). WewillseelaterthatRNNsareextensivelyusedinsequencemodeling, andthetechniquespresentedherearegenericandapplicabletomanysystems. 4.2.1 An RNN-based Language Model PerhapsthemostpopularuseofsequencemodelsinNLPisestimatingtheprobabilityofa wordsequence,alsoknownaslanguagemodeling. Mathematically,languagemodelingisan instanceofawell-knownprobleminthefieldofstochasticprocesses(orrandomprocesses): theproblemofmodelingtimeseriesdata[Hamilton,1994;Chatfield,2003;Fuller,2009]. As atimeseries,asequenceofwordscanbetreatedasasequenceofdatapointsattimeintervals that are equally spaced. In this sense, the methods we present here are somewhat general, althoughthediscussiononabroaderrangeoftimeseriesproblemsisbeyondthescopeofthis book. Given a sequence of words w ...w , the goal of language modeling is to compute 1 m Pr(w ,...,w ). Thisjointprobabilityistypicallywrittenasaproductofconditionalprobabili- 1 m 174 Chapter 4. Recurrent and Convolutional Sequence Models tiesusingthechainrule: Pr(w ,...,w ) = Pr(w )·Pr(w |w )···Pr(w |w ,...,w ) (4.3) 1 m 1 2 1 m 1 m−1 Inotherwords,theproblemofgeneratingw ...w isthesameastheproblemofgenerating 1 m a word w at a time based on the previous words w ...w . RNN-based language models i+1 1 i representw ...w viaarecurrentunitRNN(·)[Mikolovetal.,2010],likethis 1 i h =",
    ") = Pr(w )·Pr(w |w )···Pr(w |w ,...,w ) (4.3) 1 m 1 2 1 m 1 m−1 Inotherwords,theproblemofgeneratingw ...w isthesameastheproblemofgenerating 1 m a word w at a time based on the previous words w ...w . RNN-based language models i+1 1 i representw ...w viaarecurrentunitRNN(·)[Mikolovetal.,2010],likethis 1 i h = RNN(h ,x ) (4.4) i i−1 i wherex i ∈Rde isthewordvector(orwordembedding)forw i . LetV bethevocabularyfrom whichwecanchooseaword. Ifw ∈R|V| isaone-hotwordrepresentation3,x isgivenby i i multiplyingw i withthewordembeddingtableC∈R|V|×de: x = Embed(w ) i i = w C (4.5) i As shown in Chapter 1, the use of C transforms a |V|-dimensional (and probably high- dimensional)vectortoad -dimensional(andprobablylow-dimensional)vector. NotethatC e isessentiallyalookuptable,withadistincttableentry(i.e.,arow)foreachwordinV. So,the right-handsideofEq. (4.5)isinpracticeafunctionthatselectsarowfromCwiththeword index. Now we go back to Eq. (4.4). The equation is not difficult to understand: the state of the context we have seen so far (i.e., h ) is some representation of the combination of the i currentinput(i.e.,x )andthestateoftheearliercontext(h ). Putanotherway,itcanbe i i−1 thoughtofasaprocessofrepeatedlyaddinginformationofanewwordtoacacheof“history”. AnelegantaspectofthisprocessisthatitcanbeeasilyimplementedbyrunningEq. (4.4)a numberoftimesuntiltheendofthesequence. RNN(·)canbeanyfunctionthattakesh andx ,andproducesanewvectorh . The i−1 i i vanillaRNNhasaform RNN(h ,x ) = ψ(h U+x V) (4.6) i−1 i i−1 i where ψ(·) is an activation function, such as TanH(·) and Sigmoid(·). Together with Eqs. (4.4)and(4.5),wecandefineh asafunctionofh andw i i−1 i h = ψ(h U+w CV) (4.7) i i−1 i whereU∈Rd h ×d h,V∈Rde×d h,andC∈R|V|×de arelearnableparametersofthemodel,and d isahyper-parameterindicatingthenumberofdimensionsofh andh . h i i−1 We now have an encoder that represents the word sequence w ...w as a sequence of 1 m 3Theone-hotrepresentationw iisa|V|-dimensionalvectorinwhichonlyoneentryis1andallotherentries arezeros.Followingthenotationusedthroughoutthisbook,avectorisingeneralrepresentedasavariableinbold text.Herewetreatw iasawordindexandinterchangeablyuseitwiththeone-hotrepresentation. 4.2 Recurrent Models 175 RNN’s outputs H={h ,...,h }. Given that each h encodes the sub-sequence spanning 1 m i fromw tow ,wecanplaceaSoftmaxlayeronh toobtainadistributionofwords: 1 i i y = Softmax(h O+b) (4.8) i+1 i whereO∈Rd h ×|V| andb∈R|V|. Takingthewordindexw i+1 ,wehave Pr(w |w ,...,w ) = y (w ) (4.9) i+1 1 i i+1 i+1 Thus,wehavedevelopedalanguagemodelthatproducesaprobabilityPr(w |w ,...,w ) i+1 1 i ateachstep. Figure4.2showsanillustrationoftheRNN-basedlanguagemodelforanexample sequence. Torunthismodelonawordsequence,wesurelywishtostartwithpredictingw but 1 thisrequiresaprecedingwordw thatistakenastheinput. Asimpleandwidelyapplicable 0 methodforgivinganappropriatestartingstatetoRNNsistoaddabeginningsymbol⟨SOS⟩ tothesequencesothatallsequencesstartwiththesame“word”. Likewise,wecanattachan endsymbol⟨EOS⟩tothesequencetomodelthecompletenessofthesequence. Thisleadstoa newformoftheprobabilityofthesequence Pr(⟨SOS⟩,w ,...,w ,⟨EOS⟩) = Pr(⟨SOS⟩)· 1 m Pr(w |⟨SOS⟩)· 1 Pr(w |⟨SOS⟩,w )· 2 1 ··· Pr(w |⟨SOS⟩,w ,...,w )· m 1 m−1 Pr(⟨EOS⟩|⟨SOS⟩,w ,...,w ) (4.10) 1 m WecansimplyassumePr(⟨SOS⟩)=1. ToobtainPr(⟨SOS⟩,w ,...,w ,⟨EOS⟩),wetake 1 m ⟨SOS⟩w ...w asaninputsequenceandw ...w ⟨EOS⟩astheoutputsequence. 1 m 1 m 4.2.2 Training As a neural network, the RNN-based language model can be trained in a regular way. The trainingproblemhasbeenwelldiscussedinChapter2. So,wedonotgiveafulldescriptionin thischapter,butalittlebitaboutitsbasicideaaswellassomerefinements. RNN-basedlanguagemodelingcanbeframedasanext-step-predictionproblem. Suppose wearegivenacollectionofwordsequencesS. Foreachsequencew=w ...w inS, we 1 |w| haveasequenceofpairsofaninputwordandthecorrespondinggold-standardanswer,like this4 {(w ,w ),(w ,w ),...,(w ,w )} 1 2 2 3 |w|−1 |w| The language model takes the input sequence w ...w and returns a sequence of 1 |w|−1 4Whilethe⟨SOS⟩and⟨EOS⟩tricksaregenerallyconsideredinreal-worldsystems,wedropthe⟨SOS⟩and ⟨EOS⟩symbolsfromnowonforsimplification. 176 Chapter 4. Recurrent and Convolutional Sequence Models Pr(w1|w0) Pr(⟨EOS⟩|w0,···,wm) Pr(·|w 0 ) ··· OutputLayer Softmax Softmax ··· Softmax Softmax Softmax(hiO+b) HiddenLayer h1 h2 hm−1 hm RNN RNN ··· RNN RNN ψ(hi−1U+xi−1V) EmbeddingLayer Embedding Embedding ··· Embedding Embedding wi−1C ··· w 0 (⟨SOS⟩) w 1 w m−1 w m Figure 4.2: Illustration of using an RNN-based language model to calculate Pr(⟨SOS⟩w ...w ⟨EOS⟩). The input is ⟨SOS⟩w ...w , and the output is the proba- 1 m 1 m bility Pr(w |⟨SOS⟩)Pr(w |⟨SOS⟩ w )...Pr(⟨EOS⟩|⟨SOS⟩",
    "··· Embedding Embedding wi−1C ··· w 0 (⟨SOS⟩) w 1 w m−1 w m Figure 4.2: Illustration of using an RNN-based language model to calculate Pr(⟨SOS⟩w ...w ⟨EOS⟩). The input is ⟨SOS⟩w ...w , and the output is the proba- 1 m 1 m bility Pr(w |⟨SOS⟩)Pr(w |⟨SOS⟩ w )...Pr(⟨EOS⟩|⟨SOS⟩ w ...w ). As Pr(⟨SOS⟩) = 1 2 1 1 m 1, the probability of generating the sequence is simply Pr(⟨SOS⟩w ...w ⟨EOS⟩) = 1 m Pr(⟨SOS⟩)Pr(w |⟨SOS⟩)Pr(w |⟨SOS⟩ w )...Pr(⟨EOS⟩|⟨SOS⟩w ...w ). For each input 1 2 1 1 m w ,wefirstrepresentitasawordvectorx viatheembeddinglayer,resultinginasequence i i of word vectors x ...x . The RNN layer maps x ...x to a sequence of hidden states 0 m 0 m h ...h . In this process, we repeat the same thing: an RNN unit takes both h and x 1 m+1 i−1 i and produces a new state h . On top of that, we use the output layer (Softmax) to obtain i Pr(w |⟨SOS⟩w ...w ). i+1 1 i distributionsy ...y . Seethefollowingtableforanillustrationoftheinputsandoutputsof 2 |w| themodel. Step History Input Output Gold- (One-hot) (Distribution) Standard 1 w y w 1 2 2 2 w w y w 1 2 3 3 3 w ,w w y w 1 2 3 4 4 ... ... ... ... ... |w|−2 w ,w ,...,w w y w 1 2 |w|−3 |w|−2 |w|−1 |w|−1 |w|−1 w ,w ,...,w ,w w y w 1 2 |w|−3 |w|−2 |w|−1 |w| |w| AlossfunctionL(y ,w )isdefinedtomeasurehowmany“errors”wewillmakeifweuse i i y insteadoftheone-hotrepresentationw . Acommonchoiceisthecross-entropylosswhich i i computesthedivergenceofadistributionfromanother[Mitchell,1997;Bishop,2006]. 4.2 Recurrent Models 177 Then,thelossovertheentiresetisdefinedtobe |w| (cid:88)(cid:88) L = L(y ,w ) (4.11) i i w∈Si=2 Onceweknowtheloss,thetrainingoftheRNN-basedlanguagemodelcanbeachievedby usinggradientdescent. Asimpleformofthismethodisthedeltarule ∂L θ = θ −lr· (4.12) new old ∂θ where θ stands for the parameters. For the model described in Section 4.2.1, θ includes C, U,V,Oandb. ∂L isthederivativeofthelosswithrespecttotheparameters,callederror ∂θ gradient. Eq. (4.12)canbeunderstoodasaprocessofmovingthecurrentparametersasmallstep inthesteepestdownhilldirection(i.e.,thedirectionof−∂L). Herelr standsforhowfarwe ∂θ moveineachstepofgoingdownhill,alsocalledthelearningrate. Obtaining ∂L oftenrequires ∂θ a back-propagation process that flushes the error gradient from the output to the input. In modernimplementationsofdeeplearningsystems,inwhichneuralnetworksarerepresentedas computationgraphs,back-propagationissimplesinceitisjustaby-productofgraphtraversal and there are many automatic differentiation toolkits to do this. Similar algorithms, called back-propagationthroughtime(BPTT),werealsousedinearliersystems[Werbos,1990]. Forfurtherinformationabouttrainingneuralnetworks,seeChapter2and/ortextbooksonthis subject[Goodfellowetal.,2016;Zhangetal.,2021]. If the input is a long sequence, the application of RNNs would result in a deep neural network. Inthiscase,theuseofthechainruleoforderedderivativesmakeslargeorsmallloss derivativesaccumulate,andtheupdatetotheparametersinEq. (4.12)isconsequentlyvery largeorsmall. Thesearetypicallyknownastheexplodingandvanishinggradientproblems. ThereareseveralmethodstomitigatetheseproblemsforRNNs[Sutskever,2013]. Someof themare • Regularization. Introducing regularization terms (such as the l and l norms on 1 2 parametermatrices)intotrainingcanavoidmodelsinwhichmostoftheparametershave largevalues,andthushelptoavoidexplodinggradients. Similarly,onecanpenalizethe casesinwhichthenormsofthegradientsaretoosmall[Pascanuetal.,2013]. • GradientClipping. Whenthenormofthegradientsistoolarge,itisnaturaltodirectly scaledowntheirmagnitudes. Asimplemethodistoclipthegradientnormintermsof a threshold τ. If the norm ||∂L|| is larger than τ, we can rescale ∂L accordingly, say, ∂θ ∂θ ∂L = τ ·∂L.5 ∂θ ||∂L|| ∂θ ∂θ 5Itisusuallyformulatedasanequation ∂L τ ∂L = · (4.13) ∂θ max(τ,||∂L||) ∂θ ∂θ 178 Chapter 4. Recurrent and Convolutional Sequence Models • TruncatedBack-propagation. Anotherideaistobreakalongsequenceofinput-output pairsintoshorterpieces,andtrainRNNsontheseseparatesub-sequences[Williamsand Peng, 1990; Elman, 1990]. This reduces both the cost of training and the risk of too largeorsmallvaluesinaccumulatingerrorgradients. • ImprovedArchitectures. Itisalsopossibletoredesignthemodeltooverwhelmthe limits of standard RNNs, usually using the memory mechanism. In Section 4.3, we willseeafewexamplesofredesigningtherecurrentunitforaddressingthevanishing gradientproblem. • Initialization and Constraints of Parameters. Initializing the model parameters to a desirable region is generally helpful for optimization, and, sometimes, helpful for preventingverysmallgradients. Analternativemethodistorandomlysetthemodeland onlylearntheparametersoftheoutputlayers[JaegerandHaas,2004]. • Non-saturating Activations.",
    "of too largeorsmallvaluesinaccumulatingerrorgradients. • ImprovedArchitectures. Itisalsopossibletoredesignthemodeltooverwhelmthe limits of standard RNNs, usually using the memory mechanism. In Section 4.3, we willseeafewexamplesofredesigningtherecurrentunitforaddressingthevanishing gradientproblem. • Initialization and Constraints of Parameters. Initializing the model parameters to a desirable region is generally helpful for optimization, and, sometimes, helpful for preventingverysmallgradients. Analternativemethodistorandomlysetthemodeland onlylearntheparametersoftheoutputlayers[JaegerandHaas,2004]. • Non-saturating Activations. Many common activation functions have a compact rangeofoutputs,e.g.,theSigmoidfunctionhasarangeof[0,1]. Theyarealsocalled saturatingactivationfunctions6. Theuseofsaturatingactivationfunctionsoftenleads tothedecayofgradientsoverlayers,i.e.,thevanishinggradient. Itisthereforepromising tousenon-saturatingactivationfunctionsinstead,e.g.,theReLUfunction. • NormalizationofActivations. Saturatingactivationsmayalsoresultingettingstuck in a saturated region of outputs, and we need a large learning rate to escape from localoptimums[IoffeandSzegedy,2015]. Thus,thetrainingwouldbeunstable,and subtlechangesininputsand/ormodelparameterswouldleadtoabigvarianceinmodel behavior. A possible solution is to normalize the activations to reduce the variance, e.g., subtractingthemeanoftheactivationsinagroupofsamples(e.g., samplesina mini-batchoftraining),anddividingbytheirstandardderivation. 4.2.3 Layer Stacking Ifwethinkoftheapplicationofarecurrentunitasafunctionmappingavariablesequencetoa newvariablesequenceofthesamelength,itisnaturaltocomposethisfunctionwithanother functionofthesametype,orevenwithitself. ThismakesitveryeasytoextendRNNstodeep neuralnetworks: allyouneedistostackRNNs. Lethl betheoutputofthel-threcurrentunitinthestackatpositioni. Wecanapplyanew i recurrentunittohl,resultinginanewoutputatlevell+1 i hl+1 = RNN(hl+1,hl) (4.14) i i−1 i wherehl+1 istheoutputofthepreviousstepatlevell+1. TomakeEq. (4.14)well-formed, i−1 wetypicallydefineh0=x . Inotherwords,thestackstartsoffwiththewordvectorx ,thena i i i seriesofRNNoutputs(i.e.,h1,h2,h3,etc). i i i To illustrate, Figure 4.3 (a) shows a stacked RNN for language modeling. We see that 6Anactivationfunctionf(x)isnon-saturatingifandonlyifwhenx→∞(or−∞),f(x)→∞.Anactivation functionissaturatingifitisnotanon-saturatingactivationfunction. 4.2 Recurrent Models 179 ··· ··· ··· ··· Layerl+2 Layerl+2 RNN RNN RNN RNN hl+1 hl+1 Layerl+1 i Layerl+1 i hl+1 hl+1 hl+1 hl+1 i−1 i i−1 i RNN RNN RNN RNN hl hl i i Layerl Layerl RNN RNN RNN RNN ··· ··· ··· ··· positioni−1 positioni positioni−1 positioni (a)A3-layerRNN. (b)A3-layerRNNwithresidualconnections. Figure 4.3: 3-layer RNNs (with and without residual connections). To stack RNN layers, we feed the output of layer l to layer l+1. Thus the output of layer l+1 is given by hl+1=RNN(hl+1,hl). Linesinredcolorstandfortheresidualconnectionswhichdirectly i i−1 i addtheinputofalayertoitsoutput,resultinginhl+1=RNN(hl+1,hl)+hl. i i−1 i i applyingastackofrecurrentunitsisequivalenttocreatingmultiplelayersofRNNssimultane- ously. However,therewouldbeariskofconfusionifwecallanunrolledrecurrentnetwork a layer, as the term layer typically refers to a set of neurons receiving the same inputs in a feed-forwardneuralnetwork. Hereweextendthetermlayertocoveramoregeneralconcept: agroupofneuronsthataretopologicallyplacedonthesamelevel. So,wesaythatthelanguage modelinFigure4.3has3RNNlayers. StackingmultiplelayersofRNNs, webuildamodelwhichisdeeperbutmoredifficult totrain. Thisdifficultyarisesinpartfromthebarriersofpassinginformationthroughmany- layered RNNs. To make the training easier, a widely-used approach is to introduce skip connections or residual connections into a multi-layer neural network [He et al., 2016a]. These connections are intended to leverage an additional path to allow information to skip layers. AsdescribedinChapter2,theformofaresidualneuralnetworkisgivenby yl+1 = F(yl)+yl (4.15) whereyl istheoutputoflayerl. ExtendingthisformulationtoEq. (4.14)leadstomulti-layer RNNswithresidualconnections,givenby hl+1 = RNN(hl+1,hl)+hl (4.16) i i−1 i i 180 Chapter 4. Recurrent and Convolutional Sequence Models The only difference from Eq. (4.14) is that we introduce the identity map of hl to the i right-handsideofEq. (4.16). Thus,theinputhl isdirectlyaccessiblefromlayerl+1. This i greatlysimplifiesthewaythattheinformationflowsthroughtheneuralnetwork,andallows thesystemto“skip”layersinpropagatingerrors. Figure4.3(b)showsa3-layerRNNwith residualconnections. 4.2.4 Bi-directional Models The use of RNNs enables us to formulate the problem of encoding a word sequence as a problem of left-to-right generation of words. One advantage of this approach is that the modelingofcontextwordsarisesnaturally: theoutputofanRNNunitinsomewaydescribes the history words up to that point. This feature makes it very straightforward to model the probabilitydistributionPr(w |w ,...,w ),aswecanuseh asarepresentationofthecontext i+1 1 i i w ...w ,thatis,Pr(w |w ,...,w )=Pr(w |h ). 1 i i+1 1 i i+1 i The left-to-right generation is widely used in sequence generation, such as machine translation. Itcanbeviewedasaninstanceofautoregressiveprocesses(ARprocesses)in whichthestateofavariableisdependentonthestateofthepreviousvariables[Chatfield,2003; Boxetal.,2015]7. However, suchamethodisnottheonlychoiceformodelingsequences. Wedo noteven necessarilyrestrict ourselves tolanguage modelingfor traininga sequence encoder. Thisgivesrisetoaninterestingquestion: howcanwedevelopanencoderofword sequenceswithoutassumptionsregardingthepredictor? Answeringthequestionleadsusto isolatethelearningofthetextencoderfromaspecificNLPtask,andtoregarditasaseparate taskwhoseresultcanbeappliedtomanyothersystems. Amoredetaileddiscussionisnotthe focushereandweleaveittosubsequentchapters. We now present a simple extension of the left-to-right sequence model by returning to RNNs. Notethatinsequencemodelingourdesireissomerepresentationoftheentiresequence. A problem with usual RNNs is",
    "sequence generation, such as machine translation. Itcanbeviewedasaninstanceofautoregressiveprocesses(ARprocesses)in whichthestateofavariableisdependentonthestateofthepreviousvariables[Chatfield,2003; Boxetal.,2015]7. However, suchamethodisnottheonlychoiceformodelingsequences. Wedo noteven necessarilyrestrict ourselves tolanguage modelingfor traininga sequence encoder. Thisgivesrisetoaninterestingquestion: howcanwedevelopanencoderofword sequenceswithoutassumptionsregardingthepredictor? Answeringthequestionleadsusto isolatethelearningofthetextencoderfromaspecificNLPtask,andtoregarditasaseparate taskwhoseresultcanbeappliedtomanyothersystems. Amoredetaileddiscussionisnotthe focushereandweleaveittosubsequentchapters. We now present a simple extension of the left-to-right sequence model by returning to RNNs. Notethatinsequencemodelingourdesireissomerepresentationoftheentiresequence. A problem with usual RNNs is that they are uni-directional models in which the context wordsfollowingw areabsent. Toconsiderboththeleftandrightcontextsofagivenword, i wecaninsteadusebi-directionalmodels. Figure4.4showsanexampleofthebi-directional RNN.Therearetwosub-models: aleft-to-rightRNNandaright-to-leftRNN.Theyhavethe 7Asastochasticprocess,anautoregressiveprocessexpressesavariableattimetbyrelatingittothepastvalues oftheprocessandthecurrentvalueofanerrorprocess[Chatfield,2003]. Formally,atimeseries{z 1 ,...,z T } describesanautoregressiveprocessoforderpifforanyt∈{p+1,...,T} p (cid:88) zt = α i z t−i +ϵt (4.17) i=1 where{α 1 ,...,αp}aretheparametersoftheprocess,andϵtistheerrorattimet.Thisprocessiscalledregressive becauseithasthesameformasthemultiplelinearregressionmodel.Theprefixauto-comesfromthewaywe regresszt: ztisdependentonitspastvaluesinsteadofadditionalindependentvariables. Onewaytointerpret languagemodelinginanautoregressiveprocessperspectiveistosimplytreat{z 1 ,...,z T }asrepresentationsofa sequenceofwords{w 1 ,...,w T }.Thus,wecangainsomeideaofpredictingwtusingpreviouswords{w 1 ,...,wt} byconsideringtheautoregressivepropertyoftheproblem.However,itshouldbenotedthatmostofthesequence generationmodelsusedinNLParenotmathematicallyequivalenttoEq. (4.17),althoughtheyareoftencalled regressivemodels.Forexample,theRNN-basedlanguagemodeldiscussedhereisnotalinearmodel.Rather,it takeslayersofnon-linearitytodescribethecomplexrelationshipsamongwords. 4.3 Memory 181 −→ ←− −→ ←− −→ ←− h =[h ,h ] h =[h ,h ] h =[h ,h ] i−1 i−1 i−1 i i i i+1 i+1 i+1 ←− →− hi hi RNN RNN RNN ··· ··· (right-to-left) (right-to-left) (right-to-left) RNN RNN RNN ··· ··· (left-to-right) (left-to-right) (left-to-right) Embedding Embedding Embedding w w w i−1 i i+1 Figure4.4: Abi-directionalRNNmodel. Givenawordsequence,werunanRNNfromleftto rightandanotherRNNfromrighttoleft. Therefore,ateachpositionweobtainaleft-to-right representationandaright-to-leftrepresentation. Theoutputistheconcatenationofthetwo representationssothatitinvolvesboththeleftandrightcontexts. samearchitecturebutworkinoppositedirections. Foreachinputwordw ,theleft-to-right i →− RNN outputs a vector representing the context {w ,...,w } (denoted by h ), and the right- 1 i i ←− to-leftRNNoutputsavectorrepresentingthecontext{w ,...,w }(denotedby h ). Wecan i m i →− ←− concatenate h and h toobtainabi-directionalrepresentation i i →− ←− h = [h , h ] (4.18) i i i Thus,thebi-directionalRNNhasthesameformofoutputasthatoftheuni-directionalRNN, thatis,asequenceofvectors{h ,..,h }. Unliketheuni-directionalRNN,therepresentation 1 m h heredescribesthecontextonbothsides. i For a stronger model, the bi-directional RNN can be extended to a neural network of multipleRNNlayers. Forexample,wecanrundeepRNNsintwodirectionsandcombinetheir resultsasinEq. (4.18). Suchmodelarchitectureshavebeenextensivelyusedinlanguageand speechprocessingtasks,includingmachinetranslation[Wuetal.,2016],sentimentanalysis [Tang et al., 2015], POS tagging [Huang et al., 2015], speech recognition [Graves et al., 2013a;b],andsoon. 4.3 Memory RNNscanbeappropriateforsequencelearninginwhichwesummarizeateachstepthepast inputsandthenmakesomepredictiononthissummaryofthe“history”. AbenefitofRNNsis thatwecanrepresentahistoryofarbitrarylengthasafixed-sizevector,andupdateitwhen 182 Chapter 4. Recurrent and Convolutional Sequence Models newinformationarrives. Inotherwords,wehaveamemory,thoughnotexplicitlydefined,to storetheinformation. Nextweshowthatsuchamemorymechanismisgeneralandcanbe usedtoimprovesequencemodels. 4.3.1 Memory as A System In psychology, memory is the ability of the mind to retain and recall information. There are many cognitive models of psychology. A well-known model is the multi-store model [AtkinsonandShiffrin,1968]. Itdefinesmemoryasasystemconsistingofthreecomponents: short-livedsensorymemory,short-termmemory,and long-termmemory. Thesensory memoryretainsthesensoryinformationthatisveryquicklyceased,suchasimmediatedata fromthesensesofsightandsmell. Theshort-termmemorystoresinformationforalonger timebutisnotpermanent. Anexampleoftheshort-termmemoryisthatwetrytomemorizea sequenceofdigits(e.g.,aphonenumber)butmayforgetitafterashortwhile. Thelong-term memory is permanent. This also means that the information is retained indefinitely. For example,adultscanrememberdetailsoftheeventsthatoccurredintheirchildhood. Giventhiscategorization,thereappeartobeinterestingconnectionsbetweentheabove modelofmemoryandtheneuralnetworkswediscusshere. Forexample,thestateofarecurrent unitcanbesimplythoughtofasashort-termmemory. Itmaintainsinformationuntilweget tothe endofa sequenceandwouldbereset ifweswitch toanewsequence8. Ontheother hand, the entire language model and associated parameters perform more like a long-term memory: the language model is intended to learn and memorize some useful information about probabilistic word prediction from the text, so that it can be used whenever we want to. Moreover,thereareotherconceptsthatmaystemfrompsychologybutareusedinseveral differentfields. Forexample,codingorencodingisreferredtoashowtheinformationisstored inamemory,durationisreferredtoashowlongtheinformationisstoredinamemory,and capacityisreferredtoashowmuchinformationisstoredinamemory. InmachinelearningandNLP,wecangainanunderstandingofmemorybyconsideringit fromaninformationprocessingpointofview. Broadly,memorycanbeviewedasasystem thatwritesinformationtoa“storage”andreadsitwhenqueried. Ithasthefollowingfunctions. • Encoding. Theinputofthesystemisencodedinaformthatiseasytoprocess. Fortext inputs,thiscanbesimplythoughtofasthesameencodingprocessaswediscussinboth Chapter3andthischapter: awordorasequenceofwordsisrepresentedasafeature vectororasequenceoffeaturevectors. • Update. Giventheencodedinformation,westoreitinthememory. Thisoperationis generallydependentontheorganizationofthememory. Forexample,onecantreata group of encoded items as a datastore with an indexing system. In this case, storing anitemrequiresfindingtherightplacetokeepit. Alternatively,onecanrepresentthe memoryasasinglevectorofnumbers. • Retrieval. Thestoredinformationcanberetrieved. Thistypicallyinvolvesmatching 8Anotherexplanationisthatthestateofarecurrentunitatstepimaycontainlittleinformationaboutveryearly steps. 4.3 Memory 183 each item in the memory against an input query. If the memory is represented in a simplerform,suchasavector,itmaynotbeexplicitlyretrieved,andwereturntheentire memorywhenrequired. These functions can be designed in many different ways, leading to a variety of NLP systems. One simple example is information retrieval [Manning et al., 2008]. A typical informationretrievalsystemindexesalargenumberofdocuments(orotherresources)and allowsuserstosearchforinterestedinformationinthiscollectionofdocuments. Toenable",
    "183 each item in the memory against an input query. If the memory is represented in a simplerform,suchasavector,itmaynotbeexplicitlyretrieved,andwereturntheentire memorywhenrequired. These functions can be designed in many different ways, leading to a variety of NLP systems. One simple example is information retrieval [Manning et al., 2008]. A typical informationretrievalsystemindexesalargenumberofdocuments(orotherresources)and allowsuserstosearchforinterestedinformationinthiscollectionofdocuments. Toenable search,documentsarerepresentedinformsthatareconvenienttouse,forexample,wemay usethebag-of-wordsmodeltocomputethematchingscorebetweenadocumentandaquery, and may use the inverted index to make an efficient mapping of a document to its location inthestorage. Systemsofthistypecoverawiderangeofapplications,includingtranslation memory,dialogue,summarization,documentclassification,andsoon. Anotherdesignchoicemadeformemorysystemsistoconsider,eitherpartiallyorfully, acontinuousformfortheabovecomponents. Onemethodistoencodeeachinputitemasa real-valuedvector(e.g.,awordembedding)butusethesamemodulesofupdateandretrieval asinusualinformationretrieval-likesystems[Westonetal.,2015;Khandelwaletal.,2019]. An alternative method is to adopt differentiable functions for all the steps in building and accessingthememory. Thesemodelsaretypicallyimplementedusingneuralnetworksand trainedusinggradientdescent[Sukhbaataretal.,2015;Gravesetal.,2014;Kumaretal.,2016; Gravesetal.,2016;Milleretal.,2016]. Thisideamotivatesworkonexploringapproachesto couplingneuralnetworkswithmemories,suchasend-to-endmemorynetworksandneural Turingmachines. Notethattheabovemodelsaresometimescalledexternalmemories,as theyareusedasseparatemodulesworkingwithothersystems. Memorycanalsoworkasaninternalorhiddencomponentofasystem. Inthiscase,the memoryistypicallyrebuiltforeachinputsample,andsoitcanberegardedasaninstanceofthe short-termmemory. Therearevariouswaysofusingthistypeofmemorytoimprovesequence models. In the remainder of the section, we will focus on using the memory mechanism in RNNs. In Chapter 5, we will see how the idea of memory is extended to model the correspondencebetweentokensoftwosequences. 4.3.2 Long Short-Term Memory InthevanillaRNNpresentedinSection4.2.1,thesummarizationofthecontextwordswas givenbytheoutputofarecurrentunit. Itimplicitlydefinesamemory,andthusenablesthe predictionbasedonpastinformationforanarbitraryduration. Thememorysimplycombines therepresentationsoftheearlierhistoryw ...w andtheinputatthecurrentstepw ,butdoes 1 i−1 i notconsiderhowmuchinformationfromdifferentstepsshouldbesqueezedintoafixed-length representation. Aproblemwiththismodelisthat,iflong-termdependenciesarerequiredfor prediction,memorymayprovidelittleinformationaboutit,anditmaybehardtolearnthese dependenciesthroughback-propagation[Bengioetal.,1994;Pascanuetal.,2013]. Amore powerfulapproach,therefore,istocomputewhatshouldberetainedateachstep,andtoletthe modellearntodecidewhethertomemorizeorforget. 184 Chapter 4. Recurrent and Convolutional Sequence Models Longshort-termmemory(LSTM)isperhapsthebest-knownvariantofRNNstoaccom- plishtheabovegoal[HochreiterandSchmidhuber,1997]. ThebasicideaofLSTMisthata recurrentunitcanlearntomemorizeusefulthingsandforgetunusefulthingsbymaintainingan explicitmemory[Gersetal.,2000]. Tothisend,thevanillarecurrentunitisreplacedwithan LSTMunitthatismadeupofanoutputvector(callitarecurrentcell),amemoryvector(call itamemorycell),andthreegatestocontroltheinformationflowinsidetheLSTMunit. Asan extensiontoRNNs,anLSTMnetworkdealswithaninputsequenceasusual: itstartswith someinitialstates,andthenrepeatedlytakesaninputandoutputsavector. Akeydifference betweenLSTMnetworksandRNNsisthattheLSTMunitofstepitakesboththerecurrent cellandthememorycellofitspreviousstep. TheformofanLSTMunitisgivenby (h ,c ) = LSTM(h ,c ,x ) (4.19) i i i−1 i−1 i where h i ∈Rd h is the recurrent cell of step i, c i ∈Rd h is the memory cell of step i, and x i ∈Rde istheinputofstepi. GivenLSTM(·),applyingtheLSTMmodelisstraightforward. We simply repeat the call of LSTM(·) for the inputs {x ,...,x } and obtain the outputs 1 m {h ,...,h }. This resembles the way we use vanilla RNNs, making it very easy to extend 1 m LSTMtomulti-layermodels(seeSection4.2.3)andbi-directionalmodels(seeSection4.2.4). WecandivideLSTM(·)intothreesteps. • Step1: Forget. Assumingthatc containstheinformationthatthemodelmemorizes i−1 atstepi−1,weneedtodeterminehowmuchinformationinc isdiscardedinbuilding i−1 c . Todothis,agateisusedtocontroltowhatextentweforgetforeachdimensionof i c . Theforgetgateisdefinedtobe: i−1 f = Sigmoid(h U +x V +b ) (4.20) i i−1 f i f f wheref i ∈[0,1]d h isavectorwiththesamenumberofdimensionsasc i−1 . TheSigmoid functionmapstheinputdatatotherange[0,1]. Thus,anentryoff indicateshowmuch i is preserved for the same entry of c . Taking this further, f ⊙c describes the i−1 i i−1 memorythatisleftoutafterpassingthroughtheforgetgate. SeeFigure4.5(a)foran illustrationoftheforgetgateintheLSTMunit. • Step2: Update. Nextweupdatethememorybyconsideringboththepreviousstate ofthememory(i.e.,c )andtheinputoftheLSTMunit(i.e.,x andh ). Wefirst i−1 i i−1 combinex andh usingasimpleneuralnetwork,likethis i i−1 cˆ = TanH(h U +x V +b ) (4.21) i i−1 c i c c cˆ canbetreatedasthenewinformationweintendtoaddtothememoryatstepi. Again, i weneedawaytocontroltheamountofinformationcomingintothememory. Hencewe defineaninputgateas g = Sigmoid(h U +x V +b ) (4.22) i i−1 g i g g 4.3 Memory 185 This equation is similar to Eq. (4.20) but with different parameters. We then define g ⊙cˆ tobetheactualnewinformationthatweareinterestedin. Takingbothf ⊙c i i i i−1 andg ⊙cˆ ,thememorycellatstepiisgivenby i i c = f ⊙c +g ⊙cˆ (4.23) i i i−1 i i In other words, we forget something old in c and memorize something new in cˆ . i−1 i SeeFigure4.5(b)foranillustrationoftheupdatestep. • Step 3: Output. In the last step we generate the output h based on the memory c . i i Insteadofcopyingc toh ,wefeedc toahyperbolicfunctionandmultiplyitsresult i i i withtheoutputgate. LikeEqs. (4.20)and(4.22),theoutputgateisgivenby o = Sigmoid(h U +x V +b ) (4.24) i i−1 o i o o Then,theoutputoftheLSTMunitisdefinedtobe h = o ⊙TanH(c ) (4.25) i i i SeeFigure4.5(c)foranillustrationoftheoutputstep. The LSTM",
    "generate the output h based on the memory c . i i Insteadofcopyingc toh ,wefeedc toahyperbolicfunctionandmultiplyitsresult i i i withtheoutputgate. LikeEqs. (4.20)and(4.22),theoutputgateisgivenby o = Sigmoid(h U +x V +b ) (4.24) i i−1 o i o o Then,theoutputoftheLSTMunitisdefinedtobe h = o ⊙TanH(c ) (4.25) i i i SeeFigure4.5(c)foranillustrationoftheoutputstep. The LSTM model is parameterized by U f ,U c ,U g ,U o ∈ Rd h ×d h, V f ,V c ,V g ,V o ∈ Rde×d h, and b f ,b c ,b g ,b o ∈Rd h. Compared with vanilla RNNs, additional parameters are introducedherebecauseoftheuseofthreegates. Inpracticeonecanimplementtheminmany differentways,e.g.,usingactivationfunctionsotherthanSigmoid(·)andTanH(·),removing the bias terms b , b , b , and b , and so on. Training LSTM models follows the standard f c g o paradigmoftrainingRNN-basedmodels. Forexample,wecanbuildanLSTM-basedlanguage modelandtrainitbyusingthemethodspresentedinSection4.2.2. 4.3.3 Gated Recurrent Units Above,wesawtheimportantroleplayedbythegateunitsandthememorycell. Ingeneralthe useoftheseneuralnetworksmakesthemodelcomputationallymoreexpensive. Analternative to LSTM in a cheap case, namely gated recurrent units (GRUs), uses a simplified model structurewithfewergatefunctions[Choetal.,2014;Chungetal.,2014]. UnlikeLSTM,a GRUdoesnothaveamemorycellso,asanRNNunit,ittakesboththepreviousstatevector h andthecurrentinputvectorx ,andproducesthecurrentstatevectorh . i−1 i i InGRUs,therearetwogateunits: theresetgateandtheupdategate. Theresetgate,as thenamesuggests,isusedtoreset(orrescale)thestateoftheGRU(i.e.,h ). Followingthe i−1 gatefunctionsusedinLSTM,theresetgateisdefinedtobe r = Sigmoid(h U +x V +b ) (4.26) i i−1 r i r r wherer i ∈[0,1]d h isavectorofscalars,eachdimensiondescribinghowmuchinformationin thecorrespondingdimensionofh isretained. Thus,wehavearepresentationofretained i−1 186 Chapter 4. Recurrent and Convolutional Sequence Models c i−1 × f ⊙c + i i−1 Gate♯1 ForgetGate × TanH + f i Gate♯1 Gate♯2 FFN Gate♯3 h i−1 x i (a)Theforgetgate. c i−1 × + c i Gate♯1 ForgetGate × g i ⊙cˆ i TanH + Gate♯2 InputGate g i cˆ i Gate♯1 Gate♯2 FFN Gate♯3 h i−1 x i (b)Theinputgate. h i c c i−1 × + i Gate♯1 ForgetGate × TanH Tan ⊙ H(ci) × Gate♯2 InputGate oi Gate♯1 Gate♯2 FFN Gate♯3 Gate♯3 OutputGate h h i i−1 x i (c)Theoutputgate. Figure4.5: ThearchitectureoftheLSTMunit. Atstepi,ittakestheinputx ,andthenupdates i boththememorycell(c →c )andtherecurrentcell(h →h ). Thisprocessinvolves i−1 i i−1 i threegates: theforgetgatecontrols how muchinformationinc isretained atstepi, the i−1 inputgatecontrolshowmuchinformationinc andx isretainedatstepi,andtheoutput i−1 i gatecontrolshowmuchinformationinc isusedtoformh . i i information υ = r ⊙h (4.27) i−1 i i−1 Takingboththeretainedinformationυ andthecurrentinputx ,anewstatevectorisdefined i−1 i 4.4 Convolutional Models 187 tobe hˆ = TanH(υ U +x V +b ) (4.28) i i−1 h i h h Theupdategateisthengivenby u = Sigmoid(h U +x V +b ) (4.29) i i−1 u i u u u can be thought of as a coefficient vector which could be used to control the trade-off in i choosingthenewstatevectorhˆ ortheoldstatevectorh . Finally,theoutputoftheGRUis i i−1 definedasalinearinterpolationofhˆ andh i i−1 h = u ⊙hˆ +(1−u )⊙h (4.30) i i i i i−1 Figure 4.6 shows how the information flows in a GRU unit. The parameters here are U r ,U h ,U u ∈ Rd h ×d h, V r ,V h ,V u ∈ Rde×d h, and b r ,b h ,b u ∈ Rd h. Therefore, the GRU model is smaller than the LSTM model because of the use of fewer gate units. Note that removing the memory cell makes GRUs more efficient. In this case, the role of memory is implicitlyplayedbyGRU’soutputh ,andwemaintainitbymemorizingmore“important” i information. 4.4 Convolutional Models Inthissectionwedescribeanothertypeofmodelforsequencemodeling,calledconvolutional neural networks (CNNs). Our description is mostly standard, but not a full introduction",
    "the LSTM model because of the use of fewer gate units. Note that removing the memory cell makes GRUs more efficient. In this case, the role of memory is implicitlyplayedbyGRU’soutputh ,andwemaintainitbymemorizingmore“important” i information. 4.4 Convolutional Models Inthissectionwedescribeanothertypeofmodelforsequencemodeling,calledconvolutional neural networks (CNNs). Our description is mostly standard, but not a full introduction to thenumerousvariantsofCNNsandcutting-edgetechniques. Inparticular,wefocusonusing CNNstodealwithsequentialdataandpresentingsomerefinements. 4.4.1 Convolution CNNsfeaturetheirshared-weightarchitecturesbywhichakernelorfilterslidesovertheinput dataandproducesamapoffeatures. Theideaisthatthefilteronlyreceivessignalsfroma restrictedregionofdataatatime(callitthereceptivefield),andcomputestheweightedsum of these input signals. To illustrate this, we follow the convention that a filter in CNNs is generallyusedtodealwith2Ddata. Considera3×3datamatrix   1 9 7 A = 3 1 2  (4.31)   0 1 −1 anda2×2filterwithaweightmatrix (cid:34) (cid:35) 2 0 W = (4.32) 2 2 188 Chapter 4. Recurrent and Convolutional Sequence Models × FFN hˆ υ i i−1 Gate♯1 : ResetGate r i Gate♯1 h i−1 x i (a)Theresetgate. ui⊙hˆ i h i × FFN × + 1−ui Gate♯1 : ResetGate 1− × (1−ui) u ⊙hi−1 i Gate♯1 Gate♯2 Gate♯2 : UpdateGate h i−1 x i (b)Theupdategate. Figure4.6: ThearchitectureoftheGRU.UnliketheLSTMunit,theGRUdoesnotinvolve a memory cell, and thus follows the same input and output forms of a standard RNN unit. There are two gates in the GRU. The reset gate controls how much information in h is i−1 retainedatstepi. Theretainedinformationisthentakentofuzewiththeinputx ,generating i thecandidateoutputhˆ . Theupdategateseeksabalancebetweenhˆ andh incomputing i i i−1 thefinaloutputoftheGRU. Wecanapplythefiltertoevery2×2sub-matrixofA(therearefour2×2sub-matriceshere), and compute the sum of the 2×2 entries weighted by W. For example, consider the 2×2 sub-matrixintheupperleftcornerofA. Theoutputofthefilterisgivenby (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 9 1 9 2 0 Conv( ,W) = Conv( , ) 3 1 3 1 2 2 = 1×2+9×0+3×2+1×2 = 10 (4.33) Conv(·)definesaconvolutionoperationthatsumstheentriesoftheelement-wiseproduct ofthetwomatrices. Theconvolutionoperationcanbeextendedtocovertheentireinputmatrix 4.4 Convolutional Models 189 byslidingthefilteroverit,asfollows   1 9 7 Conv(A,W) = Conv(3 1 2 ,W)   0 1 −1  (cid:34) (cid:35) (cid:34) (cid:35)  1 9 9 7 Conv( ,W) Conv( ,W)  3 1 1 2    =  (cid:34) (cid:35) (cid:34) (cid:35)   3 1 1 2  Conv( ,W) Conv( ,W) 0 1 1 −1 (cid:34) (cid:35) 10 24 = (4.34) 8 2 (cid:34) (cid:35) 10 24 Theoutput isalsocalledthefeaturemapforthefilterWonA. Sometimes, 8 2 theconvolutionoperationConv(A,W)iswrittenasA∗W wherethesymbol∗standsfor theconvolutionproduct.9 NowletusconsideramoregeneraldescriptionofconvolutioninCNNs. SupposethatA isamulti-dimensionaldataarray. Afilterdefinesawindow(orreceptivefield)onA. Wecan movethewindowonAindifferentdirections. Thisresultsinasetofdataarrays,denotedby Ω. Eachdataarraya ∈ΩisformedbytheelementsfromthecorrespondingregionofA. For p (cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35) 1 9 9 7 3 1 example,therearefoursub-matricesinEq. (4.34)): a = ,a = ,a = , 1 2 3 3 1 1 2 0 1 (cid:34) (cid:35) 1 2 anda = . Also,wesupposethefilterisparameterizedbyaweightarrayWwiththe 4 1 −1 samesizeofa,i.e.,|a |=|W|. TheresultofapplyingthefiltertoAisanarrayoffeatures p (cid:104) (cid:105) Conv(A,W) = v ... v (4.37) 1 |Ω| 9Inmathematicalanalysis,giventwointegrablefunctionsf(·)andg(·),convolutiondefinesanewintegrable functionf∗g(·)todescribetheintegraloff(·)weightedbyreflected,shiftedg(·).Moreformally,theconvolution forcontinuousfunctionsisdefinedas (cid:90) f∗g(x) = f(y)g(x−y)dy (4.35) R wheref(y)isthefunctionthatweareconcernedwith,andg(x−y)istheweightfunctionwhichistranslatedby reflectingg(y)alongthey-axisandthenshiftingitbyx.Aspecialcaseisthatxandyarebothintegers.Inthis case,wecandefinef∗g(·)as (cid:88) f∗g(x) = f(y)g(x−y) (4.36) y whichisthebasicformofEq.(4.33).InCNNs,x,yandx−ycanbeseenasindicesofitemsindataarrays.f(y) isadataitemintheinputarray,andg(x−y)isthecorrespondingweightinthefilter. ByusingEq. (4.36),we calculatethevalueoftheitemindexedbyxintheoutputarrayf∗g(x)(i.e.,thefeaturemap). 190 Chapter 4. Recurrent and Convolutional Sequence Models Eachfeaturev isgivenby p v = Conv(a ,W) p p = a ·W p |W| (cid:88) = a (k)·W(k) (4.38) p k=1 wherea (k)andW(k)arethek-thelementsofa andW,respectively. Notethatthearray p p (cid:104) (cid:105) v ... v canbeorganizedintodifferentshapes,suchasamatrixora3Dtensor,though 1 |Ω| theyareessentiallythesamethingfromthedatastorageviewpoint. Forexample,for2Dinput dataanda2Dfilter,thefeaturemapisamatrixlikeEq. (4.34). Furthermore,weneedtoconsidertwothingstomakethemodelpractical. First,weneed tospecifythestrideofeachmoveofthefilteroverA. Intheaboveexample,wesimplyuse stride=1. Bychoosingalargerstride,wecancompressAintoasmallernumberoffeatures. Second, in some situations, to ensure that the feature map has a desired size, we can add dummyelements(orpaddings)aroundtheinputdata. Acommonmethodofpaddingistoset zerostotheelementsoutsidetheinputregion. Forexample,considera2×2datamatrix. (cid:34) (cid:35) 1 9",
    "k=1 wherea (k)andW(k)arethek-thelementsofa andW,respectively. Notethatthearray p p (cid:104) (cid:105) v ... v canbeorganizedintodifferentshapes,suchasamatrixora3Dtensor,though 1 |Ω| theyareessentiallythesamethingfromthedatastorageviewpoint. Forexample,for2Dinput dataanda2Dfilter,thefeaturemapisamatrixlikeEq. (4.34). Furthermore,weneedtoconsidertwothingstomakethemodelpractical. First,weneed tospecifythestrideofeachmoveofthefilteroverA. Intheaboveexample,wesimplyuse stride=1. Bychoosingalargerstride,wecancompressAintoasmallernumberoffeatures. Second, in some situations, to ensure that the feature map has a desired size, we can add dummyelements(orpaddings)aroundtheinputdata. Acommonmethodofpaddingistoset zerostotheelementsoutsidetheinputregion. Forexample,considera2×2datamatrix. (cid:34) (cid:35) 1 9 A = (4.39) 7 3 Wecanaddzero-valuedentriesaroundittoobtaina4×4matrix,likethis   0 0 0 0 0 1 9 0 A =   (4.40) padding 0 7 3 0   0 0 0 0 UsingthesamefilterasinEq. (4.33)withstride=1,wehavea3×3featuremap   2 20 18 Conv (A ,W) = 14 22 24 (4.41) stride=1 padding   0 14 6 Ifstride=2,thenwewouldhaveafeaturemapwiththesamesizeoftheinputdata (cid:34) (cid:35) 2 18 Conv (A ,W) = (4.42) stride=2 padding 0 6 4.4.2 CNNs for Sequence Modeling Followingtheformulationintheprevioussections,weassumethattheinputofasequence modelisavectorsequencex ...x andtheoutputisanothervectorsequenceh ...h . For 1 m 1 m example,wecanthinkofx 1 ...x m asamatrixX∈Rm×de inwhichthei-throwvectoristhe 4.4 Convolutional Models 191 h6 h5 h4 h3 h2 h1 Filter1 Filter2 0 padding 0 x6 x5 Features 0 x1 x2 x In 3 x pu 4 t Sequence 0 Figure 4.7: Two filters applied to a sequence of word vectors. The inputinvolves ten word vectors(wordsx ...x andtwopaddingwordsoneachofthetwoendsofthesequence). Each 1 6 word vector has 6 dimensions, and so, the input is a 10×6 matrix. Filter 1 has a receptive fieldofsize3×6. Byslidingitovertheinputmatrix,weobtainasequenceofoutputs,each correspondingtoaposition(i.e.,asequenceof6outputs). Similarly,weapplyfilter2tothe inputsequenceandobtainanothersequenceofoutputs. Thetwooutputsequencesarethen organizedasa2×6matrixinwhichthei-throwvectorish . i representationofthei-thwordofthesequence. It is straightforward to perform convolution on X. Since x is just a set of unordered i features,itisnotnecessarytoslideafilteroverdifferentfeatures. Hencewecanuseareceptive field of size r×d , and consider all the dimensions of x in the convolution operation. In e i practicalapplications,theremightbemultiplefiltersforrepresentingtheinputsindifferent aspects. Forexample,onecanuseafilterwithalargereceptivefieldtoinvolvemorecontexts inmodeling,anduseafilterwithasmallreceptivefieldtoconcentratemoreonlocalfeatures. SeeFigure4.7fortwofiltersthatareusedtodealwithasequence. To distribute features to {h ,...,h }, we can associate each application of a filter to a 1 m positionofthesequence. Toensuretheinputandoutputsequencesareofthesamelength,a paddingvectorisaddedtoeachendofthesequence. Thefollowingshowstheinputandoutput 192 Chapter 4. Recurrent and Convolutional Sequence Models ofaCNNforanexamplesequence. Position Input ReceptiveField Output 0 x (=0) N/A N/A 0 1 x {x ,x ,x } h 1 0 1 2 1 2 x {x ,x ,x } h 2 1 2 3 2 3 x {x ,x ,x } h 3 2 3 4 3 4 x {x ,x ,x } h 4 3 4 5 4 5 x (=0) N/A N/A 5 Anactivationfunctionistypicallyusedtointroducesomenon-linearitytothefinaloutput. In this way, we build a standard convolutional layer which can be viewed as a sequence of fullyconnectedneural networks, each taking inputsfroma fixed-sizewindow. Forthei-th position,theoutputoftheconvolutionallayerisgivenby v = ψ(Conv(a ,W)) (4.43) i i wherea istheinputsinthereceptivefield10,andWistheparametersofthefilter. Insituations i involving multiple filters (say d h filters), we have a set of parameters {W(1),...,W(d h )}, a set of activation functions {ψ(1),...,ψ(d h )}, and a set of inputs {a (1) ,...,a (d h ) }. Each tuple i i (W(k),ψ(k),a (k) )givesanoutputby i v (k) = ψ(k)(Conv(a (k) ,W(k))) (4.44) i i (k) Notethatv issimplyanentryofh . Thus,h canbewrittenas i i i (cid:104) (cid:105) h = v (1) ... v (d h ) (4.45) i i i ManyCNN-basedsystemsofpracticalinterestcomprisetwoormoreconvolutionallayers. Thesimplestwaytoachievethisislayerstacking,asinmulti-layerRNNs(seeSection4.2.3). Thatis,wetreattheoutputofaconvolutionallayerastheinputofthefollowinglayer. See Figure4.8foranexampleofaCNNinvolvingthreeconvolutionallayers. Oneofthebenefits ofmulti-layerCNNsisalargerscopeforrepresentation. AsseenfromFigure4.8,aneuronin layer1connectsthreeinputvectors,whileaneuroninlayer3connects,thoughnotdirectly, seven input vectors. Since neurons of the higher-level layers receive and process signals fromalargerspanofthesequence,theyareexpectedtoproduceahigher-levelrepresentation of the sequence and to be able to deal with more difficult problems, such as long-distance dependencies.",
    "(1) ... v (d h ) (4.45) i i i ManyCNN-basedsystemsofpracticalinterestcomprisetwoormoreconvolutionallayers. Thesimplestwaytoachievethisislayerstacking,asinmulti-layerRNNs(seeSection4.2.3). Thatis,wetreattheoutputofaconvolutionallayerastheinputofthefollowinglayer. See Figure4.8foranexampleofaCNNinvolvingthreeconvolutionallayers. Oneofthebenefits ofmulti-layerCNNsisalargerscopeforrepresentation. AsseenfromFigure4.8,aneuronin layer1connectsthreeinputvectors,whileaneuroninlayer3connects,thoughnotdirectly, seven input vectors. Since neurons of the higher-level layers receive and process signals fromalargerspanofthesequence,theyareexpectedtoproduceahigher-levelrepresentation of the sequence and to be able to deal with more difficult problems, such as long-distance dependencies. Insomeapplications,weneedafixed-length,low-dimensionalrepresentationoftheentire sequence. Acommonwayistoaddapoolinglayertomerge{h ,...,h }intoasinglevector. 1 m Forexample,wecanselectthemaximumvalue(i.e.,max-pooling)oraveragethevalues(i.e., 10Foranr×dereceptivefield,a iisdefinedtobe{x ⌈i−r⌉ ,...,x ⌈i+r−1⌉ }or{x ⌊i−r⌋ ,...,x ⌊i+r−1⌋ }. 2 2 2 2 4.4 Convolutional Models 193 ScopeforRepresentation Layer3 CNN CNN CNN CNN CNN Layer2 0 CNN CNN CNN CNN CNN 0 Layer1 0 CNN CNN CNN CNN CNN 0 Input 0 x 1 x 2 x 3 x 4 x 5 0 Padding Padding Figure 4.8: A CNN with 3 convolutional layers (stride=1 and r=3). Each layer takes a sequenceofvectorsandproducesanothersequenceofvectors. Inthisprocessafiltermoves overtheinputandperformstheconvolutionoperationineachmove. Inlayer1,thereceptive fieldofthefilterisaregionofthreeinputitems(seegreenshadows). Thehigheralayeris, thelargerreceptivefieldafilterhas. Forexample,inlayer3,anapplicationofthefiltercanat mostcovertheentireinputsequence(seeorangeshadows). max-pooling) along each dimension. This is a generic method in machine learning and is applicabletomostofthesequencemodelsdiscussedinthisbook. 4.4.3 Handling Positional Information OneinterestingpropertyofCNNsistheirabilitytobalancecomplexityandefficiency. This isachievedbyrestrictingfullconnectivitytoonlyasmallregionoftheinputdata. Thisalso leads us to describe a convolution layer using the same mathematical form of a layer in a fully-connectedneuralnetwork: theoutputofaneuronissometransformationoftheweighted sumoftheinputnumbers. Despitethesimplicityinherentinmodeling,aproblemwithsuch models is that the order of inputs is completely ignored. An interesting point, however, is that,ifwerestrictourselvestosequencemodeling,thisshouldnotbeaproblembecausethe outputofthemodelisitselfasequence. Itseemsreasonabletoassumethattheoutputsequence preservestheorderinginformationoftheinputsequence. Ontheotherhand,applyingCNNs tosequentialdatadoesnotguaranteeaone-to-onemappingbetweentheinputandoutputitems. Technically, h is not simply a representation of x . It instead encodes a window of inputs i i centeredatx . This,inturn,makestheproblemverycomplicated,sinceitisdifficulttowork i outfromh howthoseinputsareordered. i 194 Chapter 4. Recurrent and Convolutional Sequence Models Explicitly modeling word orders is very important in NLP, and has been extensively studied in tasks like machine translation [Lopez, 2008; Koehn, 2010]. For neural network- basedmodels,onemayaddresstheproblembyresortingtoorder-sensitivemodelarchitectures likeRNNs. Amorepopularapproachinrecentsystemsistodevelopapositionalencoding sub-modelandincorporateitintoexistingsequencemodels[Gehringetal.,2017b;Vaswani etal.,2017;Shawetal.,2018;Dufteretal.,2022]. Formally,wesaythattheinputatpositioni isacombinationoftheoriginalinputx andthepositionalencodingofi(denotedbyPE(·)): i xp = Merge(x ,PE(i)) (4.46) i i where Merge(·) combines x and PE(i) in some way. The use of positional encoding is i straightforward: all you need is to replace {x ,...,x } with {xp ,...,xp } in a sequence 1 m 1 m model. So,thisapproachismodel-free. Inthissubsection,wepresentseveralversionsofPE(·)andwaystocombinethemwith x . NotethatthefollowingdiscussionisnotspecifictoCNNs. Weconsideritherebecause i positionalencodingisusefulformodelsthatareinsensitivetotheorderofinputs,andCNNs areagoodexampletoseehowitisused[Gehringetal.,2017a;b]. InChapter6,wewillseean applicationofpositionalencodinginTransformerwhichisastate-of-the-artneuralmodelin manyareas. 1. Offset-based Positional Encoding Thesimplestwaytodescribeapositioniistojustleaveitasitis. Thiscanbeformulatedas the“distance”fromareferencepoint PE(i) = i−i (4.47) 0 wherei isanintegerindicatingwherewestartcounting. Ifi =0,PE(i)=igivesthenormal 0 0 waytodefineaposition. NotethatPE(i)couldbeanegativenumberifi >i. Inthissense, 0 PE(i)isnotarealdistancebutitisfinewithconsideringitasafeatureinamachinelearning system. Todesignanon-negativemeasure,theright-handsideofEq. (4.47)canbedefinedas anabsolutevalue PE(i) = |i−i | (4.48) 0 Treating positions as simple integers leads to unbounded, discrete positional encoding. A more desirable method might be to use a continuous representation in a range of values, becauseitallowsthesystemtoworkwithinasamplespacethatissmoothandeasytooptimize. A simple way to do this is normalization. For example, dividing i−i by some maximum 0 value,weobtainanormalizedversionoftheoffset-basedencoding i−i 0 PE(i) = (4.49) i −i max 0 Forexample,wecanseti =themaximumpossiblelengthofthesequenceandi =0, max 0 4.4 Convolutional Models 195 sothatPE(i)choosesitsvaluein[0,1]. Anothercommonchoiceistoseti =m(i.e.,the max lengthoftheinputsequence)anddefinePE(i)asaratiowhosevaluevariesasmchanges. To make use of these scalar positions, it is straightforward to enrich the original input vectorsbyaddingnewdimensions,providedtheycanbeviewedasnewfeatures. Thus,xp is i givenby xp = [x ,PE(i)] (4.50) i i where[·]standsfortheconcatenationoperation. 2. Sinusoidal Positional Encoding Thenextobviousstepistorepresentpositionsasvectorsinsteadofscalars. Althoughvector- izingtherepresentationsofpositionssoundscomplicated,asimpleideaistouseacarrying systemwhichdescribeshowanaturalnumberisexpressedbyapolynomialwithrespecttoa base[Kernes,2021]. Forexample,icanbewrittenas k (cid:88)max i = a(i,k)bk (4.51) k=0 wherea(i,k)isthek-thdigit,k +1isthemaximumnumberofdigits,andbisthebaseof max thesystem. Thecarryingoccurswhena(i,k)reachesb: weincreasea(i,k+1)by1androll backa(i,k)to0. Inthiswaywecanchangea(i,k)withaperiodofbk,thatis,a(i,0)changes withaperiodofb0,a(i,1)changeswithaperiodofb1,a(i,2)changeswithaperiodofb2,and soon. Usingthissystem,icanberepresentedasavector (cid:104) (cid:105) PE(i) = a(i,0) a(i,1) ... a(i,k ) (4.52) max (cid:104) (cid:105) For example, when b=2, PE(11)= 1 1 0 1 . However, in",
    "Encoding Thenextobviousstepistorepresentpositionsasvectorsinsteadofscalars. Althoughvector- izingtherepresentationsofpositionssoundscomplicated,asimpleideaistouseacarrying systemwhichdescribeshowanaturalnumberisexpressedbyapolynomialwithrespecttoa base[Kernes,2021]. Forexample,icanbewrittenas k (cid:88)max i = a(i,k)bk (4.51) k=0 wherea(i,k)isthek-thdigit,k +1isthemaximumnumberofdigits,andbisthebaseof max thesystem. Thecarryingoccurswhena(i,k)reachesb: weincreasea(i,k+1)by1androll backa(i,k)to0. Inthiswaywecanchangea(i,k)withaperiodofbk,thatis,a(i,0)changes withaperiodofb0,a(i,1)changeswithaperiodofb1,a(i,2)changeswithaperiodofb2,and soon. Usingthissystem,icanberepresentedasavector (cid:104) (cid:105) PE(i) = a(i,0) a(i,1) ... a(i,k ) (4.52) max (cid:104) (cid:105) For example, when b=2, PE(11)= 1 1 0 1 . However, in Eq. (4.52), PE(i) is still a discrete function. As discussed throughout this book, we may want a continuous vector representation that can describe intermediate states between discrete events. Considering a(i,k) as a periodic function, a common choice is the sine function. Thus a(i,k) can be re-defined,asfollows a(i,k) = sin(i·ω ) (4.53) k Thisfunctionhasanamplitudeof1andaperiodof 2π. Usingananalogousformofperiodsto ω k thatusedinEq. (4.51),wedefineω as k 1 ω = (4.54) k (b model )k/d model where b > 0 and d > 0 are hyper-parameters of the model. Obviously, we have model model 2π < 2π <...< 2π . ω0 ω1 ω kmax 196 Chapter 4. Recurrent and Convolutional Sequence Models Figure4.9: AheatmapofthepositionalembeddingmodelofEqs. (6.14)and(6.15)(b = model 10,000andd =512). Considerapositioni(i.e.,thei-throw),thenmoveanotherposition model j fromiupwardsordownwards. Intuitively,wheniandj arecloser,thecorrespondingrow vectorsaremoresimilar. Bycontrast, whenj movesawayfromi, thesimilarityisnotthat obvious. This property helps explain the idea behind the positional embedding model: the “distance”betweentwopositionsisimplicitlymodeledbycomparingtheirmulti-dimensional representations. Similarly,wecandefinea(i,k)viathecosinefunction a(i,k) = cos(i·ω ) (4.55) k TakingbothEqs. (4.53)and(4.55),wecreateanewrepresentationofi,asfollows (cid:104) (cid:105) PE(i) = sin(i·ω ) cos(i·ω ) ... sin(i·ω ) cos(i·ω ) (4.56) 0 0 kmax kmax Vaswanietal.[2017]instantiatedtheaboveformbysettingb =10,000. LetPE(i,k) model be the k-th dimension of PE(i). Vaswani et al. [2017]’s version of positional encoding is writtenas 1 PE(i,2k) = sin(i· ) (4.57) 100002k/d model 1 PE(i,2k+1) = cos(i· ) (4.58) 100002k/d model Choosing b =10,000 is empirical. One can adjust it for specific tasks. Figure 4.9 model plotsthepositionalencodingfordifferentpositions. Weseethat,whenk becomeslarger,the changeofthecolorfollowsalargerperiod. Note that Eqs. (6.14) and (6.15) have a useful property that PE(i+µ) can be easily 4.4 Convolutional Models 197 expressedbyalinearfunctionofPE(i)foragivenoffsetµ11 PE(i+µ,2k) = PE(i,2k)·PE(µ,2k+1)+ PE(i,2k+1)·PE(µ,2k) (4.61) PE(i+µ,2k+1) = PE(i,2k+1)·PE(µ,2k+1)+ PE(i,2k)·PE(µ,2k) (4.62) Theresultingbenefitisthattheencodingcansomewhatmodelrelativepositions. Thatis,the stateatpositioni+µcanbedescribedbystartingwithiandthenappendingitwiththeoffset µ. When applying the sinusoidal positional encoding, one way is to use Eq. (4.50) to concatenatex andPE(i). InVaswanietal.[2017]’swork,theyinsteadassumePE(i)tobea i vectorofthesamesizeasx (i.e.,|PE(i)|=|x |=d ),andaddPE(i)tox ,likethis i i e i xp = x +PE(i) (4.63) i i This sinusoidal additive model has been the basis of many positional encoding approaches [Dehghanietal.,2018;Likhomanenkoetal.,2021;Suetal.,2021]. 3. Learnable Positional Encoding TheresultofsinusoidalpositionalencodingisalookuptableC PE ∈Rmmax×de (wherem max isthemaximumsequencelengthwecanchoose)   PE(1) C =  ...  (4.64) PE   PE(m ) max Inthistable,eachrowvectorPE(i)correspondstotheembeddingofapositioni. These vectors,asdescribedabove,arecomputedbasedonsomeassumptionsandheuristicalgorithms. Analternativeapproachistotreatvectorsofpositionsasparametersofthemodelandlearn themasusual. Inthiscase,bothwordembeddingsandpositionembeddingscanbetrainedin thesamemanner. SeeChapters2and3formoreinformationaboutlearningwordembeddings inneurallanguagemodels. One last note on positional encoding. What we have shown in this section can broadly becharacterizedasanabsolutepositionalencodingparadigm: apositionisdescribedbyits locationinacoordinatesystem. Anotherconceptthatisworthexploringisrelativepositional encoding[Shawetal.,2018]. Forexample,wecanextendEq. (4.48)todefinethedistance 11Onecanderivethesebytaking sin(α+β) = sin(α)·cos(β)+cos(α)·sin(β) (4.59) cos(α+β) = cos(α)·cos(β)−sin(α)·sin(β) (4.60) 198 Chapter 4. Recurrent and Convolutional Sequence Models betweentwopositionsiandj PE(i,j) = |i−j| (4.65) In this case, the positional encoding is no longer an attribute of the i-th input but some representationofthedistancerelativetoareferencepositionj. Infact,mostofthemethods forrelativepositionalencodingarevariantsonathemeinwhichpositionsaredescribedby theirpair-wiserelationships. Thisformsthebasisofseveralmodelsofthistypeaswewillsee inChapter6. 4.5 Examples Bothrecurrentandconvolutionalmodelshavebeensuccessfullyusedinnumerousapplications. Herewediscussafewoftheinterestingexamples. Whilethesemodelsaremostlybasic,they formthefoundationsofmanystate-of-the-artsystems. 4.5.1 Text Classification To illustrate how sequence models could be used, we first consider the text classification probleminwhichweassignoneofsomepre-definedclassestoatext. Itcanbeextendedto coverabroadrangeofproblemsinNLP,includingclassifyingnewstexts,flaggingsentiment sentences,identifyingspamemails,detectingfakecomments,andsoon. IntextclassificationweareinterestedinselectingthebestclassfromasetC,givenaword sequencew ...w : 1 m cˆ = argmax Score(c,w ...w ) (4.66) 1 m c∈C Here Score(c,w ...w ) measures how well a class c is predicted for the input",
    "Classification To illustrate how sequence models could be used, we first consider the text classification probleminwhichweassignoneofsomepre-definedclassestoatext. Itcanbeextendedto coverabroadrangeofproblemsinNLP,includingclassifyingnewstexts,flaggingsentiment sentences,identifyingspamemails,detectingfakecomments,andsoon. IntextclassificationweareinterestedinselectingthebestclassfromasetC,givenaword sequencew ...w : 1 m cˆ = argmax Score(c,w ...w ) (4.66) 1 m c∈C Here Score(c,w ...w ) measures how well a class c is predicted for the input sequence 1 m w ...w . Here we map the sequence of words to the sequence of word vectors (or word 1 m embeddings),thatis,w ...w →x ...x . AssumingScore(·)isaprobabilisticfunctionthat 1 m 1 m describesthedistributionoftheclasses,wecanreformulatetheproblemas cˆ = argmax Pr(c|w ...w ) 1 m c∈C = argmax Pr(c|x ...x ) (4.67) 1 m c∈C The central issue here is the modeling of Pr(c|x ...x ). We define Pr(c|x ...x ) by 1 m 1 m followingthegeneralencoder+predictorframework,asfollows • Theinputx 1 ...x m isrepresentedasafeaturevectorH∈Rd h byusinganencoder H = Encoder(x ...x ) (4.68) 1 m 4.5 Examples 199 Pr(c|x ...x ) 1 m Predictor Softmax(H·U +b ) c c FeaturevectorH ··· Pooling Pooling Pooling Filter1 Filter2 Filter3 ··· x 1 x 2 x 3 Wordvectors x 4 x 5 x 6 Figure 4.10: A CNN-based text classifier [Kim, 2014]. The input is a sequence of word vectors. Aconvolutionallayerinvolvingmultiplefiltersisusedtoextractfeaturesindifferent dimensions. Apoolinglayerisusedtoreducethenumberoffeaturesforrepresentingtheinput text,leadingtoalow-dimensionalfeaturevectorH. ThepredictionconditionsonHandis madebyusingastandardSoftmaxlayer. • HisfedtoastandardSoftmaxlayertopredicttheclassdistribution Pr(·|H) = Softmax(H·U +b ) (4.69) c c whereU c ∈Rd h ×|C| andb c ∈R|C| aremodelparameters. Encoder(·)isexactlythesamethingwediscussedintheprecedingsections. Thereare, therefore,manyencodingmodelsthatareapplicablehere. Forexample,considertheCNN- based encoder presented in Kim [2014]’s work. Kim [2014]’s model is based on a single convolution layer involving d filters. The application of a filter produces a set of features, h each being associated with a position of the sequence. Since we want a single vector for representingtheentiresequence,apoolinglayerisaddedsothatthenumberoffeaturesfor eachfilterisreducedtoone. Then,foranyc,theprobabilityPr(c|H)canbecomputedtrivially accordingtoEq. (4.69). SeeFigure4.10foranillustrationofthisclassifier. Totrainsuchamodel,wejustneedtooptimizeitonsomeloss,and,asmentionedseveral 200 Chapter 4. Recurrent and Convolutional Sequence Models timesinthisbook, oneofthemostcommonmethodsistominimizethecross-entropyloss usinggradientdescent. Also,wecanuseregularizationtoimprovethetrainingofCNNs. More detailsaboutthesetechniquescanbefoundinChapter2. Notethatwhilethemodeldescribedhereisquite“simple”,itisamongthemosteffective models known for text classification. There are, of course, improvements to this kind of classifier. ExamplesofsuchsystemsincludedeepCNNs[Conneauetal.,2017c],character- basedCNNs[SantosandGatti,2014;Zhangetal.,2015],recurrentCNNs[Laietal.,2015], andsoon. 4.5.2 End-to-End Speech Recognition Speech recognition is a task of taking a sequence of acoustic signals and mapping it to a sequenceofwordsorcharacters(callitatranscription)[Reddy,1976;RabinerandJuang, 1993]. Sincetheoriginalinputisanacousticwaveformoverthetimedomain,itiscommonto transformitintoasequenceofwaveformfragments(callthemframes). Typically,aframe is represented as a feature vector, denoted by x . This is achieved by using either feature i functionsinsignalprocessing[DavisandMermelstein,1980;Picone,1993;Campbell,1997] orlearnableembeddings[Chorowskietal.,2019;Schneideretal.,2019]. Regardingtheoutput, speechrecognitionsystemsgenerallydonotproducewords. Instead,theyproducesequences oftranscriptionunits(ortranscriptionlabels),e.g.,phonemes,characters,sub-words,etc. Inthissubsectionweassumethattheoutputofaspeechrecognitionsystemisasequenceof Englishletters,denotedbyy ...y ∈Vn. ThealphabetV consistsofnormalEnglishletters 1 n y y (a−z),numbers(0−9),spaces(⟨sp⟩),periods(⟨pe⟩),andotherpunctuationmarks. Aswith mostmodernspeechrecognitionsystems,weaddablanksymbolϵtothealphabetinorderto indicatethenulloutput. Thegoalhereistofindastringyˆ ...yˆ foragiveninputsequencex ...x ,sothat 1 n 1 m yˆ ...yˆ = argmaxPr(y ...y |x ...x ) (4.70) 1 n 1 n 1 m y1...yn This model is relatively difficult compared to the classification model described in the previoussubsection,astheoutputcanbeanarbitrarystring,ratherthanaclassinapredefined class set. The string generation problem leads to two difficulties. First, we need some mechanismtomodelPr(y ...y |x ...x )foranexponentiallylargenumberofpairsofinput 1 n 1 m and output sequences. Second, in practice y ...y is often much shorter than x ...x (i.e., 1 n 1 m n<m),andsoweneedsomemechanismtoalignalongsequencetoashortone. However, wedonotneedtoconsiderthesedifficultiesinthestageofrepresentingtheinputsequence, andcanstillencodetheinputsequencex ...x inthesamewayasothersequencemodels. 1 m Specifically,werepresentx ...x inthefollowingform 1 m h ...h = Encoder(x ...x ) (4.71) 1 m 1 m TheencodercanbeRNNs,CNNs,ormoreadvancedmodels(suchasTransformer). Here we consider the encoder architecture used in Graves et al. [2013b] and Graves and Jaitly 4.5 Examples 201 [2014]’swork. Itisamulti-layerbi-directionalLSTMmodel. Weskipthedetailsofthismodel withoutlossofcontinuity,asthereadermaybealreadyfamiliarwithitinSections4.2.3,4.2.4 and4.3.2. HavingobtainedthesequencerepresentationH=h ...h ,asoftmaxlayerisusedtomap 1 m eachh i ∈Rd h",
    "Specifically,werepresentx ...x inthefollowingform 1 m h ...h = Encoder(x ...x ) (4.71) 1 m 1 m TheencodercanbeRNNs,CNNs,ormoreadvancedmodels(suchasTransformer). Here we consider the encoder architecture used in Graves et al. [2013b] and Graves and Jaitly 4.5 Examples 201 [2014]’swork. Itisamulti-layerbi-directionalLSTMmodel. Weskipthedetailsofthismodel withoutlossofcontinuity,asthereadermaybealreadyfamiliarwithitinSections4.2.3,4.2.4 and4.3.2. HavingobtainedthesequencerepresentationH=h ...h ,asoftmaxlayerisusedtomap 1 m eachh i ∈Rd h toadistributionoftranscriptionlabels,givenby Pr(·|h ) = Softmax(h ·U +b ) i i s s whereU s ∈Rd h ×|Vy| andb s ∈R|Vy| aremodelparameters. Pr(·|h i )∈R|Vy| isaprobability distributionoverV ,andtheprobabilityoftranscriptionlabell atpositioniissimplyPr(l |h ). y i i i Wecanthenwritetheprobabilityofalabelsequenceintheform m (cid:89) Pr(l ...l |H) = Pr(l |h ) (4.72) 1 m k i k=1 Thisformulationlookssimple. Wecanappealtotheargmaxoperationtofindthemost probable label sequence as usual. However, l ...l cannot be straightforwardly used as a 1 m systemoutput,becauseitoftencontainsmanyduplicateandblanksymbols. To“post-process” l ...l ,wefirstmergethesub-sequenceoflabelstoasinglelabelwhentheyarethesame,and 1 m thenremovetheblanksymbols. Forexample,consideralabelsequence s s ϵ e e ϵ ϵ e ⟨sp⟩ y ϵ o o u Bymerging“ss”,“ee”,and“oo”,wehave s ϵ e ϵ ϵ e ⟨sp⟩ y ϵ o u Then,weremoveallϵandobtain s e e ⟨sp⟩ y o u Theabovesequenceiswhatwewouldcallatranscription. Obviously,differentlabelse- quencescancorrespondtothesametranscription. LetB(y ...y )bethesetoflabelsequences 1 n correspondingtoy ...y 12. Wenowturntothefollowingformofthetranscriptionprobability 1 n (seeFigure4.11foranillustration) (cid:88) Pr(y ...y |x ...x ) = Pr(l ...l |H) (4.73) 1 n 1 m 1 m l1...lm∈B(y1...yn) A problem with this model is that the number of the sequences in B(y ...y ) grows 1 n exponentially with n (and m). Fortunately, there are very efficient methods for comput- (cid:80) ing Pr(l ...l |H). See [Graves et al., 2006] for a dynamic programming l1...lm∈B(y1...yn) 1 m 12B(y 1 ...yn)maycontainlabelsequencesofarbitrarylengths. However,ifwerestrictinputtothesequence x 1 ...xm,theneachsequenceinB(y 1 ...yn)isoflengthm. 202 Chapter 4. Recurrent and Convolutional Sequence Models Gold-standard s e e ⟨sp⟩ y o u Transcription s e e ⟨sp⟩ y o u LabelSequence s s ε e e ε ε e ⟨sp⟩ y ε o o u Softmax HiddenStates h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 h 9 h 10 h 11 h 12 h 13 h 14 Bi-directionalLSTMEncoder Spectrum x x x x x x x x x x x x x x 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Features Speech Figure 4.11: An end-to-end speech recognition architecture. The input of the system is a sequence of acoustic signals that are represented as a sequence of feature vectors (i.e.., x ...x ). These feature vectors are taken by a bi-directional LSTM encoder. The output 1 14 oftheencoderisasequenceofcontextualizedrepresentations(i.e., h ...h )whichisthen 1 14 fedintoasoftmaxlayerforgeneratingadistributionoflabelsateachposition. Wecanthen drawasequenceoflabelsfromthesedistributions. Wemapeachlabelsequencetoaformof final output by eliminating duplicate symbols and blank symbols. An output of the system correspondstoanumberoflabelsequences,andtheprobabilityoftheoutputisthesumofthe probabilitiesoftheselabelsequences. algorithmforsolvingthisproblem. Note that Eq. (4.73) is also known as a form of connectionist temporal classification (CTC)[Gravesetal.,2006]. Itisoneofthemostwidelyusedmethodsfortrainingend-to-end speechrecognitionandspeechtranslationsystems. OneofthemeritsofCTCisthatitallows ustoalignanylabelsequencetoatranscriptioninaverysimpleandefficientway. Itiseasyto makeuseofCTCintrainingaspeechrecognitionsystem. Supposethereisasetofpairsof inputsequenceandtranscription,denotedbyS. Acommontrainingobjectiveistomaximize thelikelihoodofthesetranscriptionsgiventhecorrespondinginputs,writtenas (cid:88) θˆ = argmax logPr(y ...y |x ...x ;θ) (4.74) 1 n 1 m θ (y1...yn,x1...xm)∈S wherePr(y ...y |x ...x ;θ)istheprobabilitycomputedviaEq. (4.73),andθistheparameters 1 n 1 m 4.5 Examples 203 ofthemodel. When testing on new data, we search for an optimal transcription as in Eq. (4.70). Thisprocess,alsoknownasdecoding,generallyinvolvesoptimizedsearchalgorithmsand pruningtechniques. Forexample,wecanusethe1-bestlabelsequenceinsteadofallpossible label sequences to obtain an approximation to Eq. (4.73), that is, Pr(y ...y |x",
    "1 m θ (y1...yn,x1...xm)∈S wherePr(y ...y |x ...x ;θ)istheprobabilitycomputedviaEq. (4.73),andθistheparameters 1 n 1 m 4.5 Examples 203 ofthemodel. When testing on new data, we search for an optimal transcription as in Eq. (4.70). Thisprocess,alsoknownasdecoding,generallyinvolvesoptimizedsearchalgorithmsand pruningtechniques. Forexample,wecanusethe1-bestlabelsequenceinsteadofallpossible label sequences to obtain an approximation to Eq. (4.73), that is, Pr(y ...y |x ...x ) = 1 n 1 m maxPr(l ...l |H). This leads to an efficient decoding method, called Viterbi decoding, 1 m whichhasbeenextensivelyusedinspeechrecognitionandmachinetranslation[Lopez,2008]. Formoredetailsaboutthedecodingofsequencegeneration,wereferthereadertoChapter5. 4.5.3 Sequence Labeling with LSTM and Graphical Models Sequencelabelingisaconceptuallystraightforwardapproachtoclassifyingdatainsequence. InNLP,ithaspenetratedmanysub-areaslikewordsegmentation,part-of-speechtagging,and chunking. LearninginthesemodelsconsistsofsimplypredictingalabelinalabelsetV at y eachpositionofasequence. Ideally,wewishtoperformasequenceoflabelingactionsbased ontheentireinput,givenby yˆ ...yˆ = argmaxPr(y ...y |x ...x ) (4.75) 1 m 1 m 1 m y1...ym where x ...x is an input sequence (such as a sequence of word vectors), and y ...y is a 1 m 1 m labelsequenceinwhicheachlabely correspondstoaninputitemx . i i Aswehaveseeninthischapter,Eq. (4.75)perfectlyfitstheformofthesequencemodeling problem. Asafirststepweuseanencodertomapx ...x toasequenceofcontextualized 1 m representations,asfollows h ...h = Encoder(x ...x ) (4.76) 1 m 1 m We define Encoder(·) as a bi-directional LSTM model because it involves a memory mechanismformodelinglong-rangedependenciesinbothleftandrightcontexts. Hence,the architectureoftheencoderisthesameasthatusedintheprecedingsubsection. h ...h canthenbetakentobetheinputofausualsequencelabelingsystem(seeFigure 1 m 4.12). ThesequencelabelingproblemhasbeendiscussedinChapter1,andmanymodelsare applicabletoit. Thesimplestistheonethatinvolvesaclassifier(suchasmaximumentropy andSVM-basedmodels)forpredictingalabeldistributionforeachh . Aproblemwiththese i modelsisthatthepredictionsaremadeindependently. Amorepowerfulapproachistouse graphical models to consider dependencies among predicted labels. For example, hidden Markovmodels(HMMs)describehowasequenceofobservations(i.e.,x ...x )isgenerated 1 m givenasequenceofvariables(i.e.,y ...y ). ThekeyideaistorewritePr(y ...y |x ...x ) 1 m 1 m 1 m using the Bayes’ rule and approximate Pr(y ...y |x ...x ) by a product of simple factors. 1 m 1 m However, these models require probability density functions of continuous variables (e.g., Pr(x |y )) which are difficult to estimate. This differentiates the use of HMMs in neural i i modelsgreatlyfromthatinconventionalmodelswhereallstatesandobservationsarediscrete 204 Chapter 4. Recurrent and Convolutional Sequence Models ... y y3 m − LabelSequence y 1 1 y2 y m ... CRFNetwork HiddenStates h 1 h 2 h 3 ... h m−1 h m Bi-directionalLSTMEncoder InputSequence x 1 x 2 x 3 ... x m−1 x m Figure4.12: TheBiLSTM+graphicalmodelarchitectureforsequencelabeling. Theencoder is a standard bi-directional LSTM model. Given a sequence of input feature vectors (i.e., x ...x ),itproducesanewsequenceoffeaturevectorsformappingtheinputtocontextualized 1 m representations(i.e.,h ...h ). ACRFnetworkisplacedonh ...h topredictadistribution 1 m 1 m oflabelsequences. Theoptimallabelsequenceistheonethathasthemaximumprobability. variables13. HMMsandtheirdescendantscanbeviewedasinstancesofgenerativemodels. Another typeofmodelthathasbeencommonlyusedtosolvesequencelabelingproblemsisdiscrimina- tivemodels. Onesuchmodelisconditionalrandomfields(CRFs)[Laffertyetal.,2001]. The CRFmodelfeaturesitsabilitytodirectlymodelthejointprobabilityoftheentireinputand 13InHMMs,asequenceofvariablescanbeviewedasapathoftransitingoversomestateswhosevaluesare chosenfromapre-definedset. Ineachtransitionfromonestatetoanother, somethingisobserved(callitan observation).Bymakingsomeassumptions,wecanapproximatePr(y 1 ...ym|x 1 ...xm)inthefollowingfashion Pr(y 1 ...ym|x 1 ...xm) = Pr(y 1 ...ym)·Pr(x 1 ...xm|y 1 ...ym) Pr(x 1 ...xm) (cid:81)m Pr(y |y )· (cid:81)m Pr(x |y ) ≈ i=1 i i−1 i=1 i i Pr(x 1 ...xm) (cid:81)m Pr(y |y )Pr(x |y ) = i=1 i i−1 i i (4.77) Pr(x 1 ...xm) wherePr(y i |y i−1 )isthetransitionprobabilityofmovingfromy i−1toy i(wheni=1,wedefinePr(y i |y i−1 )= Pr(y 1 |y 0 )=Pr(y 1 )),andPr(x i |y i )istheemissionprobabilityofobservingx igiveny i.Asthedenominator Pr(x 1 ...xm)isaconstantnumberfordifferenty 1 ...ym,itcanbedroppedintheargmaxoperationofEq.(4.75), asfollows m (cid:89) yˆ 1 ...yˆm = argmax Pr(y i |y i−1 )Pr(x i |y i ) (4.78) y1...ym i=1 ToestimatePr(x i |y i ),apossiblesolutionistotakePr(x i |y i )= Pr(yi P |x r( i y ) i P ) r(xi) ,anduseaneuralnetworkto computePr(y i |x i ). 4.5 Examples 205 labelsequences,andtoallowustomakeuseofavarietyoffeaturestodothis. Consider,for example,thelinear-chainCRF[SuttonandMcCallum,2012]. ItdefinesPr(y ...y |h ...h ) 1 m 1 m inthefollowingform Pr(y ...y ,h ...h ) 1",
    "y1...ym i=1 ToestimatePr(x i |y i ),apossiblesolutionistotakePr(x i |y i )= Pr(yi P |x r( i y ) i P ) r(xi) ,anduseaneuralnetworkto computePr(y i |x i ). 4.5 Examples 205 labelsequences,andtoallowustomakeuseofavarietyoffeaturestodothis. Consider,for example,thelinear-chainCRF[SuttonandMcCallum,2012]. ItdefinesPr(y ...y |h ...h ) 1 m 1 m inthefollowingform Pr(y ...y ,h ...h ) 1 m 1 m Pr(y ...y |h ...h ) = 1 m 1 m Pr(h ...h ) 1 m exp(Score(y ...y ,h ...h )) 1 m 1 m = (4.79) Z(h ...h ) 1 m whereZ(h ...h )isanormalizationfactor,andhastheform 1 m (cid:88) ′ ′ Z(h ...h ) = exp(Score(y ...y ,h ...h )) (4.80) 1 m 1 m 1 m y′...y′ 1 m Score(·) is a score for weighting the sequence pair (y ...y ,h ...h ). It is given by 1 m 1 m summingoverthevaluesofasetoffeaturefunctions{f (·),...,f (·)},likethis 1 J m J (cid:88)(cid:88) Score(y ...y ,h ...h ) = f (y ,y ,h ) (4.81) 1 m 1 m j i i−1 i i=1j=1 The outer loop of the summation corresponds to a visit to each position i. Given i, each functionf (·)takesthecurrentlabely ,thepreviouslabely andthecurrentinputvectorh , j i i−1 i andthenreturnsthevalueofafeature. Thismodeliscalledlinear-chainbecauseitrepresentsy ...y asachainstructurewhere 1 m eachnodey ,alongwithanobservedvariableh ,onlyconnectstoitsprecedingnodey and i i i−1 itsfollowingnodey 14,likethis i+1 y — ··· — y — y — y — ··· — y 1 i−1 i i+1 m | | | | | h ··· h h h ··· h 1 i−1 i i+1 m InCRFs,itisassumedthatthevariablesinthegraphisonlydependentonitsneighboring variables. Therefore,f (y ,y ,h )canbedefinedaccordingtohowy isconnected. There j i i−1 i i aregenerallytwotypesoffeatures. • Transition-likeFeatures. Thistypeoffeaturesmodelstheconnectionbetweenconsec- utivelabels(y ,y ),givenby i−1 i f (y ,y ,h ) = u(y ,y ) (4.82) 1 i i−1 i i−1 i whereu(y ,y )isanentryofaweightmatrixu,indexedby(y ,y ). i−1 i i−1 i • Emission-likeFeatures. Thesecondtypeoffeaturesmodelstheconnectionbetweena 14CRFscanbroadlybecategorizedasatypeofundirectedgraphicalmodels.Theydefineagraphoveraset ofobservedvariablesandasetofunobservedvariables.Thesevariablesareconnectedinsomewaythatformsa graph. 206 Chapter 4. Recurrent and Convolutional Sequence Models labely andtheassociatedinputx ,givenby i i f (y ,y ,h ) = g (y ) (4.83) 2 i i−1 i i i whereg i (y i )istheentryy i ofavectorg i ∈R|Vy|. Thevectorg i representstheweights ofassociatingh witheachlabelintheform i g = h ·v (4.84) i i wherev∈Rd h ×|Vy| isaweightmatrix. Tosimplifynotation,weusey (ory )todenotetheone-hotrepresentationforalabel15. i i−1 Then,substitutingtheabovefeaturefunctionsintoEq. (4.81)allowsthescoringfunctiontobe writtenintheform m (cid:88) Score(y ...y ,h ...h ) = u(y ,y )+g (y ) 1 m 1 m i−1 i i i i=1 m (cid:88) = y ·u·yT+h ·v·yT i−1 i i i i=1 m (cid:88) = (y ·u+h ·v)·yT (4.85) i−1 i i i=1 Theright-handsideoftheequationonlyinvolvessimplealgebraicoperationsonvectors and matrices, allowing viewing this model as a normal neural network. In this way, it is convenienttoimplementthesequencelabelingsystemwithvariousneuralnetworktoolkits. We just need to stack a CRF network on an encoder network and learn the entire network asusual. Forexample, onecantrainthissystembymaximumlikelihood, andoptimizethe lossfunctionbygradientdescent. Notethat,aswithotherchainorlattice-basedmodels,the CRF network can be efficient because there are dynamic programming algorithms, called theforward-backwardmethods,forcomputingbothScore(y ...y ,h ...h )andZ(h ...h ). 1 m 1 m 1 m Werefertheinterestedreadertorelatedpapersformoredetaileddiscussions[Laffertyetal., 2001;SuttonandMcCallum,2012]. Oneadvantageofmarryingtheworldsofdistributedrepresentationandsequencelabeling isthatwedonotneedtospecifyanyfeaturetemplatesasinconventionalapproaches. Instead, themodelisfreetolearnfeaturesthatdescribewhateverinputsequencesaremosteffectiveat optimizingsomeobjectiveforsequencelabeling. Suchanarchitecturehasbeenusedasthe backbonemodelforseveralstate-of-the-artsystems[Huangetal.,2015;Lampleetal.,2016; MaandHovy,2016;Lietal.,2020c]. 15Inthiscase,y i ∈R|Vy|,althoughitisoriginallyusedasascalar. 4.6 Summary 207 4.5.4 Hybrid Models for Language Modeling Aswehavealreadynoted,manysequencemodelingproblemscanbedealtwithbyeitherRNN- basedorCNN-basedmodels. Eachofthesetwotypesofmodelshasitsowncharacteristics: RNNsareoriginallydesignedfordealingwithvariable-lengthtemporaldata,andCNNsare moreeffectiveininterpretinglocalinformationinrestrictedregionsofinput. Hereweconsider ahybridapproachtolanguagemodelingforobtainingthebenefitsofboth. RecallfromSection4.2.1thataneurallanguagemodelislearnedtopredictaprobability distributionoveravocabulary,givensomerepresentationofthehistorywords. Letw ...w 1 m beawordsequencetowhichwewanttoassignaprobability. First,werepresenteachword w asawordvectorx . Then,anRNNmodeltakesawordvectoratatimeandoutputsthe i i probability Pr(w |w",
    "1 m 1 m Werefertheinterestedreadertorelatedpapersformoredetaileddiscussions[Laffertyetal., 2001;SuttonandMcCallum,2012]. Oneadvantageofmarryingtheworldsofdistributedrepresentationandsequencelabeling isthatwedonotneedtospecifyanyfeaturetemplatesasinconventionalapproaches. Instead, themodelisfreetolearnfeaturesthatdescribewhateverinputsequencesaremosteffectiveat optimizingsomeobjectiveforsequencelabeling. Suchanarchitecturehasbeenusedasthe backbonemodelforseveralstate-of-the-artsystems[Huangetal.,2015;Lampleetal.,2016; MaandHovy,2016;Lietal.,2020c]. 15Inthiscase,y i ∈R|Vy|,althoughitisoriginallyusedasascalar. 4.6 Summary 207 4.5.4 Hybrid Models for Language Modeling Aswehavealreadynoted,manysequencemodelingproblemscanbedealtwithbyeitherRNN- basedorCNN-basedmodels. Eachofthesetwotypesofmodelshasitsowncharacteristics: RNNsareoriginallydesignedfordealingwithvariable-lengthtemporaldata,andCNNsare moreeffectiveininterpretinglocalinformationinrestrictedregionsofinput. Hereweconsider ahybridapproachtolanguagemodelingforobtainingthebenefitsofboth. RecallfromSection4.2.1thataneurallanguagemodelislearnedtopredictaprobability distributionoveravocabulary,givensomerepresentationofthehistorywords. Letw ...w 1 m beawordsequencetowhichwewanttoassignaprobability. First,werepresenteachword w asawordvectorx . Then,anRNNmodeltakesawordvectoratatimeandoutputsthe i i probability Pr(w |w ...w ) = Pr(w |x ...x ) i+1 1 i i+1 1 i = Pr(w |h ) (4.86) i+1 i whereh isthestateoftherecurrentunitatstepi. i The process of converting words from symbols to continuous representations plays an importantroleinthismodel. Whileitiscommonforpractitionerstoobtainx fromaword i embedding table, this approach treats each word as a whole and simply ignores its internal structure. In consequence, it might be difficult to learn distinct vectors for rare words in languageswithlargevocabularies[Bojanowskietal.,2017]. Hereweconsideradifferentwayofrepresentingwords. Theideaissimple: anadditional neuralnetworkisusedtoembedwords[Lingetal.,2015;Kimetal.,2016]. Supposeevery word w can be expressed as a sequence of characters. We represent these characters as i real-valuedvectorse ...e viaacharacterembeddingtable. FollowingKimetal.[2016]’s i,1 i,leni work,wecanuseaCNNtorepresente ...e asawordvectorinthefollowingform i,1 i,leni x = CNN(e ...e ,W) i i,1 i,leni = Pooling(TanH(Conv(e ...e ,W))) (4.87) i,1 i,leni where Conv(·,W) is a convolutional layer with parameters W, TanH(·) is a hyperbolic tangentfunction,andPooling(·)isapoolinglayer. Figure4.13showsthearchitectureofthemodel. Weseethatthereisahierarchicalstructure behindthismodel,thatis,charactersformaword,andwordsformasentenceorphrase. On a practical side, in many NLP tasks it is quite natural to consider the hierarchical nature of language. Wewillseeafewexamplesofmakinguseoftherelationshipsbetweendifferent levelsoflanguagerepresentationsinlaterchapters. 4.6 Summary Thischapterhasintroducedtherecurrentandconvolutionalneuralapproachestomodeling sequencesofwords. Ononehand, recurrentneuralnetworksaredesignedfordealingwith 208 Chapter 4. Recurrent and Convolutional Sequence Models Pr(·|w ...w ) 1 i RNN RNN RNN RNN WordVectors x 1 x 2 x 3 x 4 wordw 3 Character-aware WordRepresentation CNN CNN CNN Character e3,1 e3,2 e3,3 e3,4 e3,5 Vectors Figure4.13: Alanguagemodelwithcharacter-awarewordrepresentations[Kimetal.,2016]. Asalanguagemodel,thegoalofthismodelistocomputetheprobabilityPr(w |w ...w ) i+1 1 i foreachi. Werepresenteachwordw asareal-valuedvectorx . Thisvectoristheoutputofa i i CNNthattakesasequenceofcharacterscorrespondingtothisword. Then,thesequenceofthe wordvectorsx ...x isusedastheinputtoanRNN+Softmaxmodel. Themodeloutputs 1 m ateachpositioniadistributionofwords,wheretheentryw describesPr(w |w ...w ). i+1 i+1 1 i Thishierarchicalstructureprovidesamulti-scaleapproachtolanguagemodeling: asentence ismodeledbyconsideringwords,andawordismodeledbyconsideringcharacters. sequentialdata,andhavebroadapplicabilityinNLP.Toimprovethemodelingpowerofthese models,thememorymechanismisgenerallyused. Inparticular,wehaveintroducedLSTM andGRUwhicharetwopopulartypesofmodelsindealingwithlongsequenceproblems. On theotherhand,whileconvolutionalneuralnetworksarecommonlyusedtoprocessvisiondata, theyarestraightforwardlyapplicabletosequencemodeling. Wehaveseenthatallthesemodels can be used in several language and speech processing tasks, including text classification, speechrecognition,sequencelabeling,andlanguagemodeling. Therootsofmodelingsequencesoflanguageunitscanbetracedback toearlyworkin severaldifferentfields. Forexample, theprocessofgeneratingasequenceofwordscanbe describedasaMarkovchainwherethepredictionofawordonlydependsonalimitednumber of previous words [Markov, 1913]. This idea motivates the n-gram methods for sequence modeling [Shannon, 1948a], as well as hidden Markov models which later appeared and becamepopularinmodelingsequencesofpairsofobservedandunobservedvariables[Baum andPetrie,1966;Baumetal.,1970]. Thesemodelsandtheirvariantslaythefoundationsof many successful NLP systems in past decades [Manning and Schütze, 1999; Jurafsky and 4.6 Summary 209 Martin,2008]. The idea of using neural networks in sequence modeling has also been investigated for sometime. Oneexampletoseehowneuralnetworksaredevelopedandappliedtosequence modeling is speech recognition [Lippmann, 1989]. Most of the studies in the early days of this research area try to either combine neural networks with existing models [Bourlard andWellekens,1990;BourlardandMorgan,1993;TrentinandGori,2001],oraddresssub- problems of speech recognition [Tank and Hopfield, 1987; Waibel et al., 1989; Lang et al., 1990;Bengio,1991]. However,scalingneuralnetworksupinsizewaschallengingbecause trainingdeepneuralnetworksrequiresalotofcomputationresourcesanddata. Thefieldhad longbeendominatedbyapproachesbasedonhiddenMarkovmodelsandGaussianmixture models(GMMs),withapipelineofseveralmodulesthatrequirecarefultuning. Ontheother hand,whilefullyneuralapproacheswerenotstate-of-the-artduringthattime,researcherswere awareoftheirpotentialinlearningrepresentationsofacousticinputsandfreeingthemfrom hand-craftedfeatures[LeCunandBengio,1995]. Adramaticshiftfromconventionalpipelinedapproachestoend-to-endapproachescomes withtherevivalofneuralnetworksinthe2000s[Hannunetal.,2014;Gravesetal.,2013b]. Theshiftissoinfluentialthatabroadsetoffieldscomestogetherlikeneverbefore,e.g.,in computervisionandspeechprocessing,thepasttenyearshave,meanwhile,witnessedgreat performancegainsbroughtbyverydeepneuralnetworksandend-to-endlearning[Hintonetal., 2006;Gravesetal.,2013b;Heetal.,2016a;Krizhevskyetal.,2017]. InNLP,theparadigm shiftstartswiththeworkonwordembeddings[Mikolovetal.,2013a;Penningtonetal.,2014], andcontinuesasmorepowerfulsequencerepresentationmodelsaredeveloped[Vaswanietal., 2017]. Asimpleapproachtosequencemodeling,thoughnotdiscussedindepthinthischapter, iscompositionalmodels[Janssen,2012]. Forexample,wecanusethebag-of-wordsmodel to sum or average word vectors of a sequence. Despite the simple architectures of these approaches, they achieve satisfactory results in many tasks, providing strong baselines for furtherresearchonmoreadvancedmodels[Conneauetal.,2018]. Asthenextstep,applying recurrentandconvolutionalneuralmodelstosequencemodelingisstraightforward. Thisis notsurprisingbecausethesemodelsarefairlywellstudiedinotherfields[Liptonetal.,2015; Lietal.,2021d;Khanetal.,2020]. Inparticular,theLSTMmodeliswellsuitedtodealwith longsequencesandthusofgreatinteresttoNLPresearchers[Sundermeyeretal.,2012;Huang etal.,2015;Wuetal.,2016]. However,wearealwaysontheway. Learningsequencemodels isoneofthemostactiveresearchfieldswithnoendinsight. Therearemanymodelsthatare basedonnewarchitecturesandshowstrongerperformanceinvarioustasks. Morediscussions onsomeofthesemodelscanbefoundinChapters6,7and8. Notethatthetermsequencemodelingiscurrentlyusedinmanydifferentways,referring todifferenttasks. Inmanycasesitismorecommontousethetermsencodingandencoder toemphasizetheprocessofmappingasequenceofsymbolstoacontinuousrepresentation. Asdiscussedintheprevioussections,abenefitofviewingencodingasanindividualtaskis thatwecanlearnageneralrepresentationmodelthatisnotdependentonwhereweapplyit. It opens the door to a wide range of pre-trained encoders for learning to represent various typesofdata,suchastext[Petersetal.,2018;Devlinetal.,2019],speech[Oordetal.,2018; 210 Chapter",
    "satisfactory results in many tasks, providing strong baselines for furtherresearchonmoreadvancedmodels[Conneauetal.,2018]. Asthenextstep,applying recurrentandconvolutionalneuralmodelstosequencemodelingisstraightforward. Thisis notsurprisingbecausethesemodelsarefairlywellstudiedinotherfields[Liptonetal.,2015; Lietal.,2021d;Khanetal.,2020]. Inparticular,theLSTMmodeliswellsuitedtodealwith longsequencesandthusofgreatinteresttoNLPresearchers[Sundermeyeretal.,2012;Huang etal.,2015;Wuetal.,2016]. However,wearealwaysontheway. Learningsequencemodels isoneofthemostactiveresearchfieldswithnoendinsight. Therearemanymodelsthatare basedonnewarchitecturesandshowstrongerperformanceinvarioustasks. Morediscussions onsomeofthesemodelscanbefoundinChapters6,7and8. Notethatthetermsequencemodelingiscurrentlyusedinmanydifferentways,referring todifferenttasks. Inmanycasesitismorecommontousethetermsencodingandencoder toemphasizetheprocessofmappingasequenceofsymbolstoacontinuousrepresentation. Asdiscussedintheprevioussections,abenefitofviewingencodingasanindividualtaskis thatwecanlearnageneralrepresentationmodelthatisnotdependentonwhereweapplyit. It opens the door to a wide range of pre-trained encoders for learning to represent various typesofdata,suchastext[Petersetal.,2018;Devlinetal.,2019],speech[Oordetal.,2018; 210 Chapter 4. Recurrent and Convolutional Sequence Models Hsuetal.,2021;Chenetal.,2022], vision[ChenandHe,2021;Baoetal.,2021;Heetal., 2022],andcombinationsofthem[Chuangetal.,2020;Lietal.,2021c;Kimetal.,2021]. A closelyrelatedconcepttotextencodingistextembeddingorsentenceembedding[Conneau etal.,2017a;Ceretal.,2018]. Thesecanbebroadlyconsideredthesamething. Ingeneral, an embedding model in NLP means a process of transforming the input text into a single low-dimensionalvectorratherthanproducingsequencesofcontextualizedvectors[Kirosetal., 2015;Hilletal.,2016]. InmanyNLPproblems,systemsarenotnecessarilysequentialontheirinputand/oroutput. For example, in text classification, a system may take tree-structured input and produces a label [Tai et al., 2015; Yang et al., 2016]. In this case we need some mechanism to encode hierarchicalstructures. Analternativeapproachistoconverttreestosequences(orlinearized trees) so that we can directly make use of sequence models to handle non-sequential data [Vinyalsetal.,2015]. Thisisagreatideabecauseitopensupthepossibilityofdevelopinga universallyapplicableencodertorepresentvarioustypesofdataiftheinputoftheencodercan belinearizedinsomeway. Forexample,byrepresentinganimageasasequenceofpatches, sequence models can be directly applied to image classification, achieving state-of-the-art resultsonseveraltasks[Chenetal.,2020a;Dosovitskiyetal.,2021]. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 5 Sequence-to-Sequence Models 天下万物之理，无独必有对。 According to the Principle of Heaven and Earth and all things, nothing existsinisolationbuteverythingnecessarilyhasitsopposite. –《近思录》 Reflectionsonthingsathand 朱熹/XiZhu(AD1130-1200) 吕祖谦/ZuqianLv(AD1137-1181) translatedbyChang[1967] Intheworldoflanguage,thingsoftencomeinpairs. Ifthereisaquestion,therewouldbean answer;ifthereisaChinesetext,therewouldbeanEnglishtranslation;ifthereisasentence, there would be a parse of it according to some syntax. Many NLP systems are designed to modelthecorrespondencebetweenthesepairs,i.e.,oneofthetwoistakenasinputandthe otheristakenasoutput. Theseproblemscanbeexpressedinaformthatwehaveencountered severaltimes,likethis yˆ = argmaxPr(y|x) (5.1) y wherexisaninputvariable,y isanoutputvariable,andPr(y|x)isamodelthatestimateshow likelyy wouldbethetrueoutputgivenx. This chapter is more interested in a particular family of problems where both x and y are sequences of words, called sequence-to-sequence (or seq2seq) problems. Unlike classificationproblemswheretheoutputyˆisselectedfromafixedsetofclasses,sequence-to- sequenceproblemsrequireproducinganoutputfromanexponentiallylargersetofsequences. Obtaining yˆ in this case turns out to be a much more complex problem than the case of classification,becauseweneedmorepowerfulmodelstodescribePr(y|x)andmoreefficient searchalgorithmstosolveEq. (5.1). 212 Chapter 5. Sequence-to-Sequence Models This chapter will discuss the well-known encoder-decoder architecture for sequence- to-sequencemodeling. Also, thischapterwilldiscusstheattentionmechanismwhichisan improvementonthisarchitecture. Bothofthesemodelslaythefoundationofdiscussionsof severalstate-of-the-artmodelsinthefollowingchapters. Furthermore,thischapterwilldiscuss thesearchproblemwhichplaysanimportantroleinsequencegenerationandrelatedproblems. 5.1 Sequence-to-Sequence Problems Wechoosemachinetranslationasanillustrativeexamplethroughoutthischapter,becauseit isnowoneofthemostpopularsequence-to-sequencetasks. Weusex=x ...x todenote 1 m a sequence of words in one language (call it a source-side sequence or source sequence), andusey=y ...y todenoteasequenceofwordsinanotherlanguage(callitatarget-side 1 n sequenceortargetsequence). WecanwriteEq. (5.1)usingthenewnotation,asfollows yˆ = argmaxPr(y|x) y = argmaxPr(y ...y |x ...x ) (5.2) 1 n 1 m y1...yn As discussed in Chapter 1 and in [Brown et al., 1993], this formulation implies three fundamentalissues. • Modeling. First,weneedtodefinetheformofPr(y|x). Inthischapterweshowthat Pr(y|x)canbecomputedusingasingleneuralnetworkbasedontheencoder-decoder architectureandtheattentionmechanism. Notethatsometimeswejustneedamodelfor discriminating“good”from“bad”targetsequences. Inthiscase,itisnotnecessaryto requirethemodeltomakeprobabilitysense,andwecantakeadiscriminantfunction instead. • Training. Then,weneedtolearnparametersofthemodelPr(y|x)givensometraining data. As Pr(y|x) is expressed as a neural network, we can train it in a regular way: weoptimizesomelossbygradientdescent. SeeChapter3forcommonapproachesto trainingneuralnetworks. Wewillalsodiscusstechniquesthataretailoredforspecific tasksinthisandthefollowingchapters. • Search(orDecoding). Oncewehavelearnedamodel,wewillobtainyˆ bysearchingfor thetargetsequencethatmaximizesPr(y|x). Thisisacomputationalchallengebecause thenumberofcandidatesequencesgrowswiththemaximumlengthofthesequences and the size of the vocabulary. In Section 5.4, we will discuss efficient and effective searchmethodsforsequence-to-sequenceproblems,particularlyformachinetranslation. Many NLP problems that fit the form of Eq. (5.2) can fall into sequence-to-sequence problems,andtheresearchontheseproblemsislargelymotivatedbydiscussionsoftheabove issues. Table 5.1 shows common examples of sequence-to-sequence problems taken from theliterature. Whenthetarget-sideisatext,theproblemscanbroadlybecategorizedasthe text generation problems, although a general text generation system does not require the 5.2 The Encoder-Decoder Architecture 213 Task Source Target MachineTranslation Text Translation inOneLanguage inAnotherLanguage QuestionAnswering Question Answer DialogueSystems Text/SpeechforConversation Response Summarization LongText SummariesoftheText TextSimplification Text SimplerText TextStyleTransfer Text SameContent inOneStyle inAnotherStyle GrammarCorrection TextwithErrors CorrectedText SpeechRecognition Speech Transcription SpeechSynthesis Text Speech SpeechTranslation Speech Translation inOneLanguage inAnotherLanguage Table5.1: Examplesofsequence-to-sequenceproblems. source-side to be sequential. In addition to language and speech processing, sequence-to- sequence problems can be generalized to cases where the input",
    "Response Summarization LongText SummariesoftheText TextSimplification Text SimplerText TextStyleTransfer Text SameContent inOneStyle inAnotherStyle GrammarCorrection TextwithErrors CorrectedText SpeechRecognition Speech Transcription SpeechSynthesis Text Speech SpeechTranslation Speech Translation inOneLanguage inAnotherLanguage Table5.1: Examplesofsequence-to-sequenceproblems. source-side to be sequential. In addition to language and speech processing, sequence-to- sequence problems can be generalized to cases where the input and/or output of a system arenotnaturallysequential. Forexample,image-to-textgeneration(orimagecaptioning) and text-to-image generation systems both involve dealing with images that are typically representedas2Ddata. Byrepresentingimagesassequencesinsomeway(suchassequences ofpatches),sequence-to-sequencemodelsaredirectlyapplicabletothesetasks. Historically,mostsystemsinthesetasksweredevelopedsomewhatindependently,resulting indifferentarchitectures,features,andtrainingmethodsfordifferenttasks. However,asshown in this chapter, when we represent these models as neural networks and train them in an end-to-endfashion,thereappearstobea“universal”paradigmforalltheseproblems. Thisisa bigchangefortheAIcommunitybecausemanyresearchfieldscometogetherandsystemscan besharedacrossthem. Wecangainsomeinsightintothecommonnatureofabroadvariety ofproblems,thoughtherearemanytask-specificconsiderationsinpractice. Inthefollowing sections,wewilldiscusssomeofthecommonthreadsamongsequence-to-sequencemodels. 5.2 The Encoder-Decoder Architecture In this section we discuss the encoder-decoder architecture and a simple neural machine translationmodelbasedonthisarchitecture. 5.2.1 Encoding and Decoding Fromasupervisedlearningviewpoint,wewouldideallyliketolearnamodelfromanumber of sequence pairs such that any source-side sequence can be mapped to the corresponding target-sidesequence. However,learningthemappingbetweensequencesofdiscretevariables 214 Chapter 5. Sequence-to-Sequence Models istypicallyaproblemoflearningfromhigh-dimensionaldata. Itinevitablysuffersfromthe curseofdimensionality,makingthemodelingandtrainingdifficult. One approach to learning such a mapping is to divide the problem into “simpler” sub- problems. We assume that there is a low-dimensional representation shared by x and y, denoted by H. Then, the mapping x→y can be achieved by mapping x to H and then to y. Formally,givenasource-sidesequencex,wemapittotherepresentationHbyusingan encodingsystem(callitanencoder) H = Encode(x) (5.3) Then, we map H to the target-side sequence y by using a decoding system (call it a decoder)1 y = Decode(H) (5.4) Thisarchitecture,alsoknownastheencoder-decoderarchitecture,iswidelyusedinrecent sequence-to-sequence systems (see Figure 5.1 for an illustration). It is easy to see that the formofEq. (5.3)isthesameasthoseofthesequencemodelsmentionedinChapter4,andso therearemanyencodingmodelstochoosefrom,suchasbi-directionalLSTM.Thegoalofthe decoderistoproducea“best”target-sidesequencegiventherepresentationofthesource-side sequence. Likeclassificationmodels,thepredictionismadebyfirstproducingadistribution overallpossiblesequences,andthenselectingtheonewiththemaximumprobability. Assuch, wecanre-defineDecode(·)asaprobabilityfunction Pr(·|H) = Decode(H) = Decode(Encode(x)) (5.5) Inotherwords,givenatarget-sidesequencey,thedecoderassignsitaprobability Pr(y|x) = Pr(y|H) (5.6) Then,theoptimalsequenceyˆ isobtainedbyperformingargmax Pr(y|x)asinEq. (5.2). In y manysystemsbasedontheencoder-decoderarchitecture,bothEncode(·)andDecode(·)are modelsconstructedfromneuralnetworks. Thus,wecantreatthesequence-to-sequencemodel 1Itisimportanttodistinguishbetweentheconceptofdecoding(ordecoder)usedinconventionalsequence-to- sequencesystemsandthatusedintheencoder-decoderarchitecture.Thetwoareoftenconfused,thoughtheyare differentsomehow.Inmanymachinetranslationorspeechrecognitionsystems,decodinghasthesamemeaningas translationortranscription,thatis,werecovertheoptimalyfromx. AspointedoutinEq. (5.2),thisprocess involvesasearchoverallcandidatey. Therefore,theconventionaluseofdecodinginthesesystemsistorefer toasearchprocess(i.e.,theargmaxoperationinEq.(5.2))[Koehn,2010].Bycontrast,intheencoder-decoder architecturedecodingmeansaprocessofrecoveringthetarget-sidesequenceyfromtheintermediaterepresentation H.Itisallaboutmodelingratherthansearching.Itisalsoworthnotingthat,whilethetermdecoding(ordecoder) isusedindifferentways,itcanbethoughtofasaprocessofmappinganencodedmessagebacktotheoriginal messageinacommunicationsystemasdefinedininformationtheory[Shannon,1948b].Inthissense,thedecoding processesinthesesystemsdothesamethingasthewordsoundslike:convertsomethingtoitsoriginalform. 5.2 The Encoder-Decoder Architecture 215 Target-sidesequence: y=y ...y 1 n Decoder (y=Decode(H)) .2 -1 6 5 .7 -2 Representation: H Encoder (H=Encode(x)) Source-sidesequence: x=x ...x 1 m Figure5.1: Theencoder-decoderarchitecture. Inthecaseofsequence-to-sequenceproblems, ittransformsasource-sidesequencex=x ...x toatarget-sidesequencey=y ...y . This 1 m 1 n procedureinvolvestwosteps: xisfirstencodedasarepresentationH,andthisrepresentation isthendecodedtoy. asasingleneuralnetworkandtrainitasusual,providedtheentiremodelissomecombination ofEncode(·)andDecode(·). Toapplytheencoder-decoderarchitecturetoareal-worldtask,weneedtomakeanumber of design choices, such as the forms of H, Encode(·) and Decode(·). As a very simple example, consider the task of regenerating an input word. We can define Encode(·) as a feed-forwardneuralnetworkthattakesaword(inone-hotrepresentation)andoutputsaword vector. Inthisway,Hisadistributedrepresentationoftheword. Then,wedefineDecode(·) asanotherfeed-forwardneuralnetworkthattakesthewordvectorandgeneratesadistribution overthevocabulary. Fortraining,wewishtolearnasystemthatassignsthelargestprobability to the input word. As discussed in Chapter 2, we can call this an auto-encoder which is a specialinstanceoftheencoder-decoderarchitecture. 5.2.2 Example: Neural Machine Translation Nextweillustratetheapplicationoftheencoder-decoderarchitectureusingaworkingexample —neuralmachinetranslation(NMT).Weconsiderawell-knownNMTmodelwhichuses RNNoritsvariantsforbuildingboththeencoderanddecoder[Choetal.,2014;Sutskeveretal., 2014]. TheencoderoftheNMTmodelisastandardRNN-basedencoder. AstheRNN-based sequencemodelhasbeendiscussedindetailinChapter4,wejustgiveabriefreviewofthis modelhere. Supposethatthesource-sidevocabularyisV andeachsource-sidewordx is x j representedasaone-hotvectorinR|Vx|. Then,x j istransformedintoah s -dimensionalvector 216 Chapter 5. Sequence-to-Sequence Models (orwordembedding) xe = Embed (x ) (5.7) j s j where Embed (·) is the word embedding function. More details about word embedding s modelscanbefoundinChapter3. TheRNNmodeltakesthesequenceofthewordvectorsxe...xe andproducesasequence 1 m ofRNNstatevectorsh 1 ...h m . AnRNNstatevectorh j ∈Rd h isdefinedtobe h = RNN(h ,xe) (5.8) j j−1 j HereRNN(·)isanRNNunitthatsummarisestheinformationuptopositionj bycombining thepreviousstateh andthecurrentinputxe insomeway. Then,thelaststateh canbe j−1 j m treatedasarepresentationoftheinputsequencex ...x ,andwecanuseh astheoutputof 1 m m theencoder,writtenas h = Encode(x ...x ) (5.9) m 1 m Figure 5.2 (a-b) shows an illustration of the encoding process. Note that the model describedabovejustinvolvesasingle-layerRNN.Inpracticalsystems,thisframeworkcanbe easilyextendedtoincludemultiplelayersandmorepowerfulrecurrentunits(suchasLSTM units). The decoder of the NMT model is a",
    "thepreviousstateh andthecurrentinputxe insomeway. Then,thelaststateh canbe j−1 j m treatedasarepresentationoftheinputsequencex ...x ,andwecanuseh astheoutputof 1 m m theencoder,writtenas h = Encode(x ...x ) (5.9) m 1 m Figure 5.2 (a-b) shows an illustration of the encoding process. Note that the model describedabovejustinvolvesasingle-layerRNN.Inpracticalsystems,thisframeworkcanbe easilyextendedtoincludemultiplelayersandmorepowerfulrecurrentunits(suchasLSTM units). The decoder of the NMT model is a standard RNN-based language model, that is, we predict the next word y given all previous words y ...y . To incorporate the source-side i+1 1 i informationintotranslation,asimpleandstraightforwardmethodistotreath astheinitial m state of the target-side RNN. Let ye ∈Rds be the word vector of the start symbol ⟨SOS⟩ 0 (denotedbyy ). ThecorrespondingRNNstateisgivenby 0 s = RNN(h ,ye) (5.10) 0 m 0 HereRNN(·)hasthesameformastherecurrentunitusedintheencoder,butwithdifferent parameters. Fori>0,thestatevectors i ∈Rds isgivenintheform s = RNN(s ,ye) (5.11) i i−1 i Then,s isfedintoaSoftmaxlayertoproduceadistributionoverthetarget-sidevocabulary i V . TheoutputoftheSoftmaxlayerisgivenby y Pr(·|y ...y ,x ...x ) = Pr(·|s ) 1 i 1 m i = Softmax(s U +b ) (5.12) i y y whereU y ∈Rds×|Vy|andb y ∈R|Vy|aretheparametersoftheSoftmaxlayer. Pr(y i+1 |y 1 ...y i ,x 1 ...x m ) canbeseenastheprobabilityofpredictingwordy byconditioningonboththetranslated i+1 5.2 The Encoder-Decoder Architecture 217 h1 h1 h2 hm(H) ... RNN RNN RNN RNN xe xe xe xe 1 1 2 m (a)Theencodingprocessstarts (b)TheencoderrepresentsxasH Pr(y1|y0,x) ··· Pr(yn+1|y0...yn,x) Soft. Soft. Soft. s0 s0 s1 sn ... RNN RNN RNN RNN ye ye ye ye 0 0 1 n h1 h2 hm(H) h1 h2 hm(H) ... ... RNN RNN RNN RNN RNN RNN xe xe xe xe xe xe 1 2 m 1 2 m (c)Thedecodertakestherepresentationofx (d)Thedecoderpredictsthetarget-sidewords Figure5.2: TheencodinganddecodingstepsforanRNN-basedNMTsystem. Theencoderis astandardRNN.Theencodingprocessstartswiththefirstsource-sidewordandendsupwith thelastsource-sideword. ThelaststateoftheRNNistakentobetherepresentationofthe entiresource-sidesequence(i.e.,H=h ). ThedecoderisanotherRNN.Atthefirststep,it m takesHfromtheencoder. Afterrepresenting(ye...he,H)ass atpositioni,asoftmaxlayer 0 i i isbuilttopredictthenextwordy . i+1 wordsy ...y andthesource-sidesequencex ...x . SeeFigure5.2(c-d)foranillustrationof 1 i 1 m thewordpredictionsofadecoder. Armed with this model of word prediction, we turn to a form that is frequently used in papersonNMT,likethis Pr(y|x) = Pr(y y|x) 0 = Pr(y y ...y |x ...x ) 0 1 n 1 m = Pr(y |x ...x )Pr(y ...y |y ,x ...x ) 0 1 m 1 n 0 1 m n−1 (cid:89) = Pr(y |y ...y ,x ...x ) (5.13) i+1 0 i 1 m i=0 218 Chapter 5. Sequence-to-Sequence Models Sometimes,thisequationisalsowritteninanequivalentform n (cid:89) Pr(y|x) = Pr(y |y ...y ,x ...x ) (5.14) i 0 i−1 1 m i=1 Hereweassumethatyalwaysstartswithy (i.e.,⟨SOS⟩)andsoPr(y |x ...x )=1. In 0 0 1 m manypracticalsystems,itisalsocommontoassumethatyendswithaspecialsymbol⟨EOS⟩. Therefore,wecanmodifythisequationtoinvolve⟨SOS⟩and⟨EOS⟩onboththesourceand target-sides,asfollows Pr(y yy |x xx ) = Pr(y y ...y y |x x ...x x ) 0 n+1 0 m+1 0 1 n n+1 0 1 m m+1 = Pr(y |x ...x )· 0 0 m+1 Pr(y ...y y |y ,x ...x ) 1 n n+1 0 0 m+1 n (cid:89) = Pr(y |y ...y ,x ...x ) (5.15) i+1 0 i 0 m+1 i=0 wherex =y =⟨SOS⟩,x =y =⟨EOS⟩,andPr(y |x x ...x x )=1. 0 0 m+1 n+1 0 0 1 m m+1 SincePr(y|x)canbeexpressedasaneuralnetwork,trainingthismodelisstraightforward. AsdescribedinChapter4,RNN-basedlanguagemodelsaretrainedbyusingthecross-entropy lossandgradientdescent. NMTcanusethissamemethodfortrainingmodelparameters. Once wehaveobtainedtheoptimizedmodel,wecanthenuseittotranslatenewsentences. Finding thebesttranslationforanygivensource-sidesentenceisastandardsearchproblem. Wewill discussitinSection5.4. 5.3 The Attention Mechanism TheNMTmodeldiscussedintheprevioussectionwasbasedonafixed-lengthrepresentationof thesource-sidesequence. Whilethismodeliseasytoimplement,inmanypracticalapplications it is unsatisfactory because a fixed-length vector might not be sufficient for representing a variable-lengthsequence, especiallywhenthesequenceislong. Thissystemwilltherefore needsomemechanismtocoupletheencoderandthedecoderinafine-grainedmanner. Inthis sectionwediscusstheattentionmechanismbywhichasystemcanlearn,foreachwordofthe target-sidesequence,anadaptiverepresentationthatfocusesmoreonimportantpartsofthe source-sidesequence. In fact, the discussion here is related to the",
    "m m+1 SincePr(y|x)canbeexpressedasaneuralnetwork,trainingthismodelisstraightforward. AsdescribedinChapter4,RNN-basedlanguagemodelsaretrainedbyusingthecross-entropy lossandgradientdescent. NMTcanusethissamemethodfortrainingmodelparameters. Once wehaveobtainedtheoptimizedmodel,wecanthenuseittotranslatenewsentences. Finding thebesttranslationforanygivensource-sidesentenceisastandardsearchproblem. Wewill discussitinSection5.4. 5.3 The Attention Mechanism TheNMTmodeldiscussedintheprevioussectionwasbasedonafixed-lengthrepresentationof thesource-sidesequence. Whilethismodeliseasytoimplement,inmanypracticalapplications it is unsatisfactory because a fixed-length vector might not be sufficient for representing a variable-lengthsequence, especiallywhenthesequenceislong. Thissystemwilltherefore needsomemechanismtocoupletheencoderandthedecoderinafine-grainedmanner. Inthis sectionwediscusstheattentionmechanismbywhichasystemcanlearn,foreachwordofthe target-sidesequence,anadaptiverepresentationthatfocusesmoreonimportantpartsofthe source-sidesequence. In fact, the discussion here is related to the attention models in psychology because translationisitselfacognitiveprocess[Sternberg,1996;Neisser,2014]. Thekeyideabehind this type of model is natural: attention is generally concentrated on specific parts of the data when we process something. This forms the basis of many state-of-the-art sequence- to-sequence models, and the attention mechanism has been the de facto standard for the developmentofthesesystems. 5.3 The Attention Mechanism 219 ... ... Pr(y1|−)Pr(y2|−) Pr(yn+1|−) Pr(y1|−)Pr(y2|−) Pr(yn+1|−) Decoder Decode..r. ye ye ... ye ye ye ... ye 0 1 n 0 1 n Attention h h ... h h h ... h 1 2 m 1 2 m Encoder Encoder xe xe ... xe xe xe ... xe 1 2 m 1 2 m (a)AnNMTsystemwithoutattention (b)AnNMTsystemwithattention Figure5.3: NMTarchitectureswithout(left)andwith(right)theattentionmodel. Whenthe attention model is not involved, a fixed-length representation is considered for generating the entire target-side sequence. By contrast, when the attention model is involved, a new representationiscomputedspecificallyforeachtarget-sidestatesothatthedecodercanlearn toconcentrateondifferentpartsofthesource-sidesequenceforpredictingatarget-sideword. 5.3.1 A Basic Model RecallthatintheNMTmodeloftheprevioussection,theencoderrepresentsasource-side wordsequenceash ...h ,andthedecoderrepresentsatarget-sidewordsequenceass ...s . 1 m 1 n Theattentionmechanismaddressesthequestionofhowarepresentationcanbelearnedfrom h ...h sothatthisrepresentationcanexplainthesource-sidesequencewellforagiventarget 1 m states 2. Fromaninformationprocessingperspective,solongasweignorethemeaningsof i h ...h and s in NMT, attention can be thought of as a generic process of processing the 1 m i inputinformationh ...h byconsideringhoweachh isrelatedtotheinterests . Figure5.3 1 m j i comparesNMTarchitectureswithandwithouttheattentionmechanism. More formally, an attention model produces a linear combination of {h ,...,h } in the 1 m form m (cid:88) c = α ·h (5.16) i i,j j j=1 whereα istheattentionweightthatdescribeshowmuchthemodelshouldrelyonh when i,j j 2Followingtheconventioninmachinetranslation[Brownetal.,1993],weusejtorepresentapositioninthe source-sidesequence,anduseitorepresentapositioninthetarget-sidesequence. 220 Chapter 5. Sequence-to-Sequence Models computingc fors . Sometimesc isalsocalledacontextvector. i i i Acommonapproachtocomputingattentionweightsistonormalizealignmentscoresin thefollowingform α = Softmax(a(s ,h )) i,j i j exp(a(s ,h )) i j = (5.17) (cid:80)m exp(a(s ,h )) j′=1 i j′ Herethealignmentscorea(s ,h )measureshowstrongh isrelatedtos . Ingeneral,a(s ,h ) i j j i i j canbedefinedinseveraldifferentways[Gravesetal.,2014;Bahdanauetal.,2014;Luong etal.,2015]. Acomprehensivelistofthesefunctionscanbefoundinsurveypapersonthis subject[Chaudharietal.,2021]. Hereweintroducesomeofthecommonones. • Dot-product Attention. One of the simplest methods is to measure the similarity betweenh ands . Thus,wecancalculatethedot-productofthetwovectors,asfollows j i a(s ,h ) = s hT i j i j d (cid:88)h = s (k)·h (k) (5.18) i j k=1 Avariantofthismodel,calledscaleddot-productattention,addsascalarfactor 1 to β theright-handsideofEq. (5.18),asfollows s hT i j a(s ,h ) = (5.19) i j β Wewillseeanexampleofthismodellaterinthissection. • CosineAttention. Anothercommonlyusedsimilaritymeasureinvectoralgebraisthe cosineoftheanglebetweentwovectors,givenby a(s ,h ) = cos(s ,h ) i j i j s hT i j = (5.20) ∥s ∥ ·∥h ∥ i 2 j 2 1 where∥a∥ 2 =(a·a)2 istheEuclideannormofthevectora. • WeightedDot-productAttention. Thisattentionmodelinvolvesalinearmappingof theinputvectorsbeforeperformingthedot-productoperation,givenby a(s ,h ) = s W hT (5.21) i j i a j whereW a ∈Rd h ×d h istheparametermatrixofthelinearmapping. Boththisapproach andthedot-productattentionapproacharealsocalledmultiplicativeattention[Ruder, 2017]. 5.3 The Attention Mechanism 221 • AdditiveAttention. Inadditiveattention,theentriesofthetwovectorsaresummedin someway. Awidely-usedformisgivenbyBahdanauetal.[2014] a(s ,h ) = vTTanH(s W +h W ) (5.22) i j a i s j h where W h ,W s ∈Rd h ×da and v a ∈Rda are parameters. TanH(s i W s +h j W h ) pro- ducesad -dimensionalvectorwhereeachentryisatransformedweightedsumofthe a",
    "221 • AdditiveAttention. Inadditiveattention,theentriesofthetwovectorsaresummedin someway. Awidely-usedformisgivenbyBahdanauetal.[2014] a(s ,h ) = vTTanH(s W +h W ) (5.22) i j a i s j h where W h ,W s ∈Rd h ×da and v a ∈Rda are parameters. TanH(s i W s +h j W h ) pro- ducesad -dimensionalvectorwhereeachentryisatransformedweightedsumofthe a entriesofh ands . Itisfollowedbyadot-productwithanotherweightvectorv . j i a NowletusreturntoEqs. (5.16-5.17)andrethinktheroleofattentionweights. Eq. (5.17) informallydefinesa“distribution”overh ...h ,writtenas 1 m Pr(h |s ) = α (5.23) j i i,j Ifweconsiderharandomvariablethattakesavaluefrom{h ,...,h },thenα canbe 1 m i,j thoughtofastheprobabilityofh=h ,conditionedons ,andEq. (5.16)canberewrittenas j i m (cid:88) c = Pr(h |s )·h i j i j j=1 = E (h) (5.24) h∼Pr(h|si) Inotherwords,c canbeviewedasanexpectedrepresentationofthesource-sidesequence i given the target-side state s , that is, the expectation of {h ,...,h } under the distribution i 1 m Pr(h |s ). Thisprovidesageneralframeworkfordescribingthewaythedecoderreceivesthe j i informationfromtheencoder: thedecoderisareceiverthatdetermineshowmuchinformation isacceptedfromeachsender. Forexample,intheNMTmodeloftheprevioussection,there is only one sender h , and so the receiver receives all the information the sender sends. m By contrast, in the NMT model armed with the attention mechanism, there are m senders {h ,...,h }andthereceiverreceivesinformationaccordingtoadistributionofpreferences 1 m forthesenders. Itisstraightforwardtointroducetheattentionmodelintotheprocessofwordprediction. We modify our treatment of s so as to make use of both the source-side and target-side i information at each decoding step. We slightly modify the definition of s to include the i contextvectorcorrespondingtothepreviousstates ,asfollows i−1 s = RNN(s ,c ,ye) (5.25) i i−1 i−1 i ComparedwiththemodelofEq. (5.11),themodelofEq. (5.25)takesc asanadditional i−1 input. Therefore,thismodelconsidersboththerepresentationofthetarget-sidewordsy ...y 1 i−1 (asencodedins andye)andtherepresentationoftheentiresource-sidesequencex ...x i−1 i 1 m (asencodedinc ). Then,thedistributionoftargetwordsatpositionicanbeconditionedon i−1 s asusual i Pr(·|y ...y ,x ...x ) = Pr(·|s ) (5.26) 1 i 1 m i 222 Chapter 5. Sequence-to-Sequence Models compute Pr(y |y ...y ,x ...x ) i+1 1 i 1 m (i.e.,Pr(y |s )) i+1 i DecoderState DecoderState atstepi atstepi+1 s i−1 s s i−1 i c i−1 ye i c = (cid:80)m α h i−1 j=1 i−1,j j weightα i−1,j of connectings i−1andh j ...... ααii−−11,,11 ααii−−11,,22 ααii−−11,,33 ααii−−11,,mm h h h ... h 1 2 3 m Encoder Figure 5.4: An attention model for NMT. Suppose we have obtained the representations {h ,...,h }andthedecoderstates uptothispoint. Wewishtoobtainthedecoderstateat 1 m i−1 thenextstep. Tothisend,wefirstcomputeattentionweightsbynormalizingsomeattention scoresbetweens and{h ,...,h },andthencomputeacontextvectorc bysumming i−1 1 m i−1 over{h ,...,h }withtheattentionweights. Anewdecoderstates iscreatedbytakingthe 1 m i contextvectorc ,thepreviousstates ,andthewordrepresentationye. s willbeusedas i−1 i−1 i i aconditionforpredictingadistributionofwordsatstepi+1. wherePr(·|s )isgenerallyaSoftmaxlayer. ThisprocessisillustratedinFigure5.4. i WenowhaveamodelforcomputingPr(y |y ...y ,x ...x ). Abriefoutlineofthekey i+1 1 i 1 m stepsofthismodelisgivenby 1. Encodethesource-sidesequenceash ...h whereh =RNN(h ,xe). 1 m j j−1 j 2. Repeatthefollowingprocedurefromi=1ton−1. 5.3 The Attention Mechanism 223 a. Computethealignmentscorea(s ,h )foreachj. i−1 j b. Computetheattentionweights{α ,...,α } i−1,1 i−1,m whereα = exp(a(si−1,hj)) . i−1,j (cid:80)m j′=1 exp(a(si−1,h j′)) c. Computethecontextvectorc = (cid:80)m α ·h . i−1 j=1 i−1,j j d. Computethetarget-sidestates =RNN(s ,c ,ye). i i−1 i−1 i e. Computethedistributionoftarget-sidewordsPr(·|s ). i f. ComputePr(y |y ...y ,x ...x )=Pr(y |s )foragivenwordy (asin i+1 1 i 1 m i+1 i i+1 training),orselectthemostlikelywordyˆ =argmax Pr(y |y ...y ,x ...x ) i+1 yi+1 i+1 1 i 1 m (asintesting). Inreal-worldsystems,thisbasicmodelcanbemodifiedtobetterpredictthetarget-side words. Forexample, wecanintroducefusionlayerstocombines , c , andye beforethe i i−1 i Softmaxlayersothatwehaveadeepermodelforprediction[Bahdanauetal.,2014]. Another commonlyusedapproachistostackmultipleRNNlayersonthetarget-side. Inthiscase,one canperformattentionineithereachlayerofthestack[Wuetal.,2016]orthetop-mostlayer ofthestack[Luongetal.,2015].",
    "|y ...y ,x ...x )=Pr(y |s )foragivenwordy (asin i+1 1 i 1 m i+1 i i+1 training),orselectthemostlikelywordyˆ =argmax Pr(y |y ...y ,x ...x ) i+1 yi+1 i+1 1 i 1 m (asintesting). Inreal-worldsystems,thisbasicmodelcanbemodifiedtobetterpredictthetarget-side words. Forexample, wecanintroducefusionlayerstocombines , c , andye beforethe i i−1 i Softmaxlayersothatwehaveadeepermodelforprediction[Bahdanauetal.,2014]. Another commonlyusedapproachistostackmultipleRNNlayersonthetarget-side. Inthiscase,one canperformattentionineithereachlayerofthestack[Wuetal.,2016]orthetop-mostlayer ofthestack[Luongetal.,2015]. SeeSection5.3.5formoreinformationaboutmulti-layer approachestoattention. 5.3.2 The QKV Attention Because the attention mechanism is such a powerful approach, many variants have been developed. Perhapsthemostwidelyusedapproachistoreframetheattentionproblemasone of matching a query in a set of key-value pairs. It lays the foundation for the well-known sequencemodel—Transformer[Vaswanietal.,2017]. Hereweassumethatthereareanumberofkey-valuepairs{(k ,v ),...,(k ,v )}anda 1 1 m m queryq. Thegoalofthequery-key-valueattention(orQKVattention)modelistoobtaina valuebyconsideringthecorrespondencebetweenthequeryandthekeys. Thisisastandard searchingproblemindatabasesystemsinwhichinformationisreturnedinitsoriginalform or a new form when it matches the query. In the QKV attention, the result of searching is notasinglevaluein{v ,...,v }butinsteadacombinationofthesevalues. Thisisthekey 1 m differenceofthisattentionmodelcomparedwiththeconventionalmodelsofsearching. Formally,theresultoftheQKVattentionisdefinedtobe m (cid:88) c = α v (5.27) j j j=1 where qkT j α = Softmax( ) (5.28) j β istheattentionweight. Itturnsoutthattheabovemodelhaspreciselythesamegeneralform asthemodeldescribedintheprevioussubsection,andccanbesimplyviewedasacontext 224 Chapter 5. Sequence-to-Sequence Models vector. WhilethebasicformoftheQKVattentionisnotsomething“new”,itcanhandleavariety ofproblemsbygivingq,k andv appropriatemeanings. Hereweconsideramoregeneral j j case where there are n queries {q ,...,q } and n output vectors {c ,...,c }. To simplify 1 n 1 n notation,weuseQtodenoteamatrixwherethei-throwvectorisq ,likethis i   q 1 . Q =   . .   (5.29) q n       k v c 1 1 1 . . . Likewise,wecandefineK=  . .   ,V=  . .   ,andC=  . .   . Then,theattentionmodel k v c m m n canbeformulatedas QKT C = Softmax( )V (5.30) β Figure5.5showsanillustrationofthisequation. NotethatSoftmax(QKT )computesa β matrixofattentionweights   α ... α 1,1 1,m QKT . . Softmax( β ) =   . . . .   (5.31) α ... α n,1 n,m (cid:104) (cid:105) wherearowvector α ... α representsadistributionover{v ,...,v }. Wecanthen i,1 i,m 1 m expandEq. (5.30)foreasyunderstandingofthemodel   c 1 . C =   . .   c n (cid:80)m α v  j=1 1,j j . =   . .   (cid:80)m α v j=1 n,j j    α ... α v 1,1 1,m 1 . . . =   . . . .     . .   (5.32) α ... α v n,1 n,m m In sequence-to-sequence modeling, Q, K and V can be defined in several different ways. Todescribethecorrespondencebetweenthesource-sideandtarget-sidesequences,one 5.3 The Attention Mechanism 225 Queries(e.g.,{s ,...,s }) Keys(e.g.,{h ,...,h }) Values(e.g.,{h ,...,h }) 1 n 1 m 1 m q 1 q 2 q 3 k 1 k 2 k 3 v 1 v 2 v 3  q 1   k1   v 1  Q= q 2  K= k2  V= v 2  q 3 k3 v 3 QKT q 1 q 2 × kT kT kT = 1 2 3 q 3 q3kT 3 α (cid:16) (cid:17) Softmax = β α3,3 ReturnedValuesC v c 1 1 × v 2 = c 2 c i = (cid:80)3 j=1 α i,j v j v c 3 3 Figure5.5: TheQKVattentionmodelforbatchesofqueries(Q),keys(K),andvalues(V). The figureshowsadirectimplementationoftheformulaC=Softmax(QKT )V. Softmax(QKT ) β",
    "kT kT kT = 1 2 3 q 3 q3kT 3 α (cid:16) (cid:17) Softmax = β α3,3 ReturnedValuesC v c 1 1 × v 2 = c 2 c i = (cid:80)3 j=1 α i,j v j v c 3 3 Figure5.5: TheQKVattentionmodelforbatchesofqueries(Q),keys(K),andvalues(V). The figureshowsadirectimplementationoftheformulaC=Softmax(QKT )V. Softmax(QKT ) β β computestheattentionweightsbynormalizingascaleddot-productofQandKT. Thisresults inamatrixαinwhicharowvectordescribesweightsofdifferentvalues. Bymultiplyingα withV,weobtainasequenceofnewvalues,eachexpressingaweightedsumoftheoriginal values. approach,calledencoder-decoderattention,istosimplyassumethat   s 1 . Q =   . .   (5.33) s n 226 Chapter 5. Sequence-to-Sequence Models and   h 1 . K = V =   . .   (5.34) h m In this case, C is a sequence of new representations of the source-side sequence given the representations of the target-side sequence. As with the model described in the previous subsection,eachc ∈Ccanbeusedtopredictthewordy . i i+1 In addition to applying the model to sequence-to-sequence problems, another type of approachistoregarditasasequencemodel,thatis,weusetheQKVattentiontorepresenta sequenceinonelanguage. Inthiscase,theQKVattentionisalsocalledself-attentionwhich formsthebasisofthewell-knownTransformermodel[Vaswanietal.,2017]. Consider,for example,thesequenceofstatesh ...h . Theself-attentionmodelassumesthat 1 m   h 1 . Q = K = V =   . .   (5.35) h m Then,theoutputofthemodelisasequenceofrepresentationsc ...c . c isarepresentation 1 m j whichconsidersthecorrelationsbetweenh andanyotherelementoftheinputsequence. We j willseeamoredetaileddiscussiononthismodelinChapter6. 5.3.3 Multi-head Attention Multi-head attention is an interesting extension to the above models. The key idea is to performattentionin differentsub-spacesofrepresentationssimultaneouslyrather than ina singlespaceofrepresentations. Toillustrate,considerastandardattentionmodelthattakes sequencesofsource-sideandtarget-sidestatesandoutputsasequenceofnewstates,writtenas c ...c = Att(h ...h ,s ...s ) (5.36) 1 n 1 m 1 n where h j ,s i ,c i ∈Rd h, and Att(·) is the attention function. We can map h j into τ vectors [1] [τ] {h ,...,h }viathefollowinglineartransformations j j [1] [1] h = h W (5.37) j j h . . . [τ] [τ] h = h W (5.38) j j h whereh [ j 1] ,...,h j [τ] ∈Rd τ h ,andW h [1] ,...,W h [τ] ∈Rd h × d τ h . [1] [τ] Similarly,wecanmaps intoτ vectors{s ,...,s }. Wethendefineτ featuresub-spaces i i i inwhichtheattentionfunctionisperformedindependently. Forthek-thfeaturesub-space,we 5.3 The Attention Mechanism 227 have c [k] ...c[k] = Att(h [k] ...h[k],s [k] ...s[k]) (5.39) 1 n 1 m 1 n Theoutputofthemodelisasequenceofd -dimensionalvectors,eachofwhichisobtained h byconcatenatingthevectorsthatareproducedinallthesefeaturesub-spaces,followedbya lineartransformation. Thisprocedureisgivenby [1] [τ] c = [c ,...,c ]W (5.40) 1 1 1 c ... c = [c[1],...,c[τ]]W (5.41) n n n c whereW c ∈Rd h ×d h. Following the notation used in the previous subsection, we can express a sequence of       h s c 1 1 1 . . . vectorsasamatrix,say,H=  . .   ∈Rm×d h,S=  . .   ∈Rn×d h,andC=  . .   ∈Rn×d h. h s c m n n Usingthisnotation,werewriteEq. (5.36)as C = Att(H,S) (5.42) Togiveaformaldefinitionofmulti-headattention,wefirstintroducethesplitandmerge functions. Thesplitfunctiondivideseachrowvectorofamatrixintoanumberofsub-vectors, resultingina3Dtensor. Forexample,splittingam×d matrixAwithτ producesaτ×m×d h h τ tensor3 A = Split(A,τ) (5.43) heads Themergefunctionhasareverseformofthesplitfunction. Givenaτ×n×d h tensor(say τ A ),itmergeseachgroupofτ d h-dimensionalsub-arraysintheform heads τ A = Merge(A ,τ) (5.44) merge heads Thustheformofmulti-headattentionisgivenby C = C W merge c = Merge(C ,τ)W heads c = Merge(Att(H ,S ),τ)W (5.45) heads heads c H = Split(HW ,τ) (5.46) heads h S = Split(SW ,τ) (5.47) heads s 3Aa×b×ctensorcanbetreatedasanarrayofamatriceswhoseshapesareb×c. 228 Chapter 5. Sequence-to-Sequence Models C∈Rn×d h",
    "τ A = Merge(A ,τ) (5.44) merge heads Thustheformofmulti-headattentionisgivenby C = C W merge c = Merge(C ,τ)W heads c = Merge(Att(H ,S ),τ)W (5.45) heads heads c H = Split(HW ,τ) (5.46) heads h S = Split(SW ,τ) (5.47) heads s 3Aa×b×ctensorcanbetreatedasanarrayofamatriceswhoseshapesareb×c. 228 Chapter 5. Sequence-to-Sequence Models C∈Rn×d h Merge&Project Merge(C ,τ)W heads c C heads ∈R3×n× d 3 h Att(H ,S ) heads heads H S heads heads ∈R3×m× d 3 h ∈R3×n× d 3 h Project&Split Project&Split Split(HW ,τ) Split(HW ,τ) h s H∈Rm×d h S∈Rn×d h Figure 5.6: An attention model with τ =3 heads. First, we transform the input matrices intomulti-headrepresentations,i.e.,3DtensorsH heads ∈R3×m× d 3 h andS heads ∈R3×n× d 3 h . These tensors are then taken by an attention model. The output of this model is a tensor C heads ∈R3×n× d 3 h . WethenmergetheheadsofC heads ,followedbyalineartransformation. Finally,weobtainnvectorsofsized ,representedbyann×d matrix. h h whereW h ,W s ∈Rd h ×d h aretheparameters. Split(HW h ,τ)implementstheprojectionsof Eqs. (5.37-5.38)forallh . Likewise,wecanhavethemeaningofSplit(HW ,τ). Notethat j h hereAtt(·)isextendedtodealwithmulti-headinputs. SeeFigure5.6foranillustrationofthis model. Multi-headattentionisaverygeneralapproachthatcanbeextendedtomanymodels. Asa simpleexampleofthisextension,considertheQKVattentionmodeldiscussedintheprevious subsection. LetAtt QKV (Q,K,V)betheattentionfunction,andQ∈Rd k,K∈Rd k,V∈Rdv 5.3 The Attention Mechanism 229 betheinputs. Themulti-headQKVattentionmodelisgivenby C = Merge(Att (Q ,K ,V ))W (5.48) QKV heads heads heads c Q = Split(QW ,τ) (5.49) heads q K = Split(KW ,τ) (5.50) heads k V = Split(VW ,τ) (5.51) heads v whereW q ∈Rd k ×d k,W k ∈Rd k ×d k,W v ∈Rdv×dv,W c ∈Rdv×dv arethemodelparameters. Oneadvantageofmulti-headattentionisthatthefeaturesub-spaceswilleachdescribea differentperspectiveofattention(callitanattentionheadorheadforshort). Therefore,the concatenationoftheoutputsovertheseheadsrepresentsanensembleofattentionmodelsthat dealwithdifferentpartsofthedata. Thisissimilartolearningagroupofmodelsindependently andcombiningthemtoformastrongermodel. Thistypeofmachinelearningapproachhas beenproventobeusefulinmanyproblems[OpitzandMaclin,1999;Zhou,2012b]. Notethat the multi-head attention models discussed here are parameterized by the linear projections ontheinputandoutputspaces. Theuseoftheselinearprojectionsisgenerallyhelpfulasthe modelsbecomedeeperandcandescribemorecomplexproblems. Fromanarchitecturedesignperspective,multi-headattentionfallsintoabroadclassof neural networks — those involving a number of branches of layer stacks for dealing with the same input (call them multi-branch neural networks). However, unlike conventional approaches,whichrequiredifferentmodelarchitecturesfordifferentbranches,themulti-head attentionapproachisbasedonasinglemodelforalltheheads. Asaresult,suchsystemsare veryefficientinpracticebecausetheattentionprocedurecanruninparallelovertheseheads. 5.3.4 Hierarchical Attention In many cases the underlying structure of an NLP problem is hierarchical. For example, documentsmayhaveamulti-levelstructure: adocumentismadeupofsentences,asentenceis madeupofwords,andawordismadeupofcharacters. Itisthereforedesirabletomodifythe attentionmodelstotakeintoaccountthehierarchicalnatureofthisdata[Yangetal.,2016]. To illustrate, we consider a simple problem where the source-side has a 2-level tree structure. Supposethesource-sidesequenceisaconcatenationofanumberofsub-sequences {u¯ ,...,u¯ }. Eachu¯ yieldsasequenceofwords 1 T t u¯ = x ...x (5.52) t p(t,1) p(t,|u¯t|) wherep(t,i)isthepositionofthei-thwordofu¯ intheentiresource-sidesequencex ...x . t 1 m Then,thesequencex ...x canbewrittenasacompositionofT sub-sequences: 1 m x ...x =x ...x x ...x ... x ...x (5.53) 1 m p(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯ T|) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) u¯1 u¯2 u¯ T 230 Chapter 5. Sequence-to-Sequence Models Similarly,theencoderoutputh ...h canbewrittenas 1 m h ...h = h ...h h ...h ... h ...h (5.54) 1 m p(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯ T|) On the target-side, we assume that there are two sequences of state vectors: one for placing the standard representations of the target-side sequence (i.e., s ...s ) and one for 1 n placinghigher-levelrepresentationsofs ...s . Letϕ(i)denotethepositioninthehigher-level 1 n sequenceofs ,ands¯ denotethecorrespondingstatevector. Foreachi,wethushaveapair i ϕ(i) ofstatevectorss and¯s . Ingeneral,therelationshipbetweens and¯s comesfromthe i ϕ(i) i ϕ(i) hierarchicalstructureoftheproblem. Forexample,s istherepresentationofaword,and¯s i ϕ(i) istherepresentationofthesentencethewordbelongsto4. Asbefore, ourgoalistoobtainacontextvectorc foreachtarget-sidepositioni. Here i we still take c to be a weighted sum of {h ,...,h }, as in Eq. (5.16). All that remains is i 1",
    "sequenceofs ,ands¯ denotethecorrespondingstatevector. Foreachi,wethushaveapair i ϕ(i) ofstatevectorss and¯s . Ingeneral,therelationshipbetweens and¯s comesfromthe i ϕ(i) i ϕ(i) hierarchicalstructureoftheproblem. Forexample,s istherepresentationofaword,and¯s i ϕ(i) istherepresentationofthesentencethewordbelongsto4. Asbefore, ourgoalistoobtainacontextvectorc foreachtarget-sidepositioni. Here i we still take c to be a weighted sum of {h ,...,h }, as in Eq. (5.16). All that remains is i 1 m tospecifytheattentionweightforeachh . Asafirststepweattends toeachu . Thisisa j i t standardprocedure. Wejustneedtoruntheattentionmodelonh ...h insteadof p(t,1) p(t,|u¯t|) h ...h ,givenby 1 m h¯ = Att(h ...h ,s ) t p(t,1) p(t,|u¯t|) i |u¯t| (cid:88) = π h (5.55) i,k,t p(t,k) k=1 whereπ istheattentionweightrestrictedtou . h¯ isarepresentationofu ,andsowehave i,k,t t t t anewsequenceofrepresentationsh¯ ...h¯ . 1 T Then, we run the attention model on h¯ ...h¯ to perform a second round of attention. 1 T Thisisdonebyattendings toh¯ ...h¯ . Theoutputisacontextvectorforthehierarchical ϕ(i) 1 T attentionmodel,givenby c = Att(h¯ ...h¯ ,s ) i 1 T ϕ(i) T (cid:88) = γ h¯ (5.56) i,t t t=1 whereγ istheweightofattendings toh¯ . SubstitutingEq. (5.55)intoEq. (5.56),wecan i,t ϕ(i) t writec as i T |u¯t| (cid:88)(cid:88) c = γ π h i i,t i,k,t p(t,k) t=1k=1 m (cid:88) = α h (5.57) i,j j j=1 Whilethenotationinthissubsectionisabitcomplicated,theformoftheresultingmodel 4Ifthea-thsentencecoverswordsfrompositionbtoc,thenϕ(b)=ϕ(b+1)=...=ϕ(c)=a. 5.3 The Attention Mechanism 231 c i 2ndattention Att(·) h¯ 1 ... h¯ t ... h¯ T s ϕ(i) 1stattention Att(·) h ... h ... h ... h s p(1,1) p(t,1) p(t,|u¯t|) p(T,|u¯ T|) i sub-sequenceu¯ t Figure 5.7: A 2-level hierarchical attention model. The input sequence h ...h is made 1 m up of T sub-sequences. For each sub-sequence u¯ , an attention model is used to produce t a context vector h¯ by considering the target-side state (i.e., s ) and the representations of t i thesub-sequence(i.e.,h ...h ). TheresultofrunningthisprocedureontheT sub- p(t,1) p(t,|u¯t|) sequences is T level-1 representations h¯ ...h¯ . They are then taken by a second attention 1 T modeltoconsidertheattentionbetweentheserepresentationsandahigher-leveltarget-side state s . This results in the context vector c which describes the attention between the ϕ(i) i target-sidestates andtheentiresource-sidesequenceh ...h . i 1 m is simple. We still combine {h ,...,h } in a linear manner but with new weights [Maruf 1 m etal.,2019]. Computingα describesagenerativeprocessinwhichwefirstdeterminethe i,j weightofeachsub-sequenceandthendeterminetheweightofeachwordinasub-sequence,as illustratedinFigure5.7. Seebelowforanalignmentamongdifferenttypesofattentionweight. sequence h ... h h ... h ...... h ... h 1 |u1| |u1|+1 |u1|+|u2| (cid:80)T t= − 1 1|ut|+1 m weight(α) α ... α α ... α ...... α ... α i,1 i,|u1| i,|u1|+1 i,|u1|+|u2| i,(cid:80) t T = − 1 1|ut|+1 i,m sequence h ... h h ... h ...... h ... h p(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯T|) weight(γ) γ ... γ γ ... γ ...... γ ... γ i,1 i,1 i,2 i,2 i,T i,T weight(π) π ... π π ... π ...... π ... π i,1,1 i,|u¯1|,1 i,1,2 i,|u¯2|,2 i,1,T i,|u¯T|,T 232 Chapter 5. Sequence-to-Sequence Models 5.3.5 Multi-layer Attention So far wehave consideredthe case ofsingle-layer attention— theoutput of theattention modelsiswrittenasalinearcombinationofthesource-siderepresentations. Nowweextendit inanaturalwaytomulti-layerattentioninwhichthesingle-layerattentionprocedurerunsa numberoftimesforforminga“deeper”attentionmodel. Todothis,amulti-layerneuralnetworkiscreatedonthetarget-side. Themodelarchitecture isregular. Westackanumberofattentionlayers,eachinteractingwiththesource-sidesequence andfeedingitsoutputtothenextlayer. Inanattentionlayer,weperformattentionasusual. Forthel-thlayerinthestack,thissteptakesthesource-sidesequence(denotedbyh ...h )as 1 m wellastheoutputofthepreviouslayer(denotedbysl−1...sl−1),andproducesasequenceof 1 n vectorsby cl...cl = Att(h ...h ,sl−1...sl−1) (5.58) 1 n 1 m 1 n whereAtt(·)couldbeanyattentionfunctiondescribedinthischapter. Then,wecreateanotherneuralnetworkf(·)togivemoremodelingpowertothemodel. Theoutputoftheattentionlayeristhusdefinedtobe sl...sl = f(cl...cl ,sl−1...sl−1) (5.59) 1 n 1 n 1 n f(·)canbedesignedinmanyways[Sukhbaataretal.,2015;Wuetal.,2016;Vaswanietal.,",
    "attention— theoutput of theattention modelsiswrittenasalinearcombinationofthesource-siderepresentations. Nowweextendit inanaturalwaytomulti-layerattentioninwhichthesingle-layerattentionprocedurerunsa numberoftimesforforminga“deeper”attentionmodel. Todothis,amulti-layerneuralnetworkiscreatedonthetarget-side. Themodelarchitecture isregular. Westackanumberofattentionlayers,eachinteractingwiththesource-sidesequence andfeedingitsoutputtothenextlayer. Inanattentionlayer,weperformattentionasusual. Forthel-thlayerinthestack,thissteptakesthesource-sidesequence(denotedbyh ...h )as 1 m wellastheoutputofthepreviouslayer(denotedbysl−1...sl−1),andproducesasequenceof 1 n vectorsby cl...cl = Att(h ...h ,sl−1...sl−1) (5.58) 1 n 1 m 1 n whereAtt(·)couldbeanyattentionfunctiondescribedinthischapter. Then,wecreateanotherneuralnetworkf(·)togivemoremodelingpowertothemodel. Theoutputoftheattentionlayeristhusdefinedtobe sl...sl = f(cl...cl ,sl−1...sl−1) (5.59) 1 n 1 n 1 n f(·)canbedesignedinmanyways[Sukhbaataretal.,2015;Wuetal.,2016;Vaswanietal., 2017]. A popular choice is to define f(·) as a feed-forward neural network with a residual connection,givenby f(cl...cl ,sl−1...sl−1) = FFN(cl...cl ) + sl−1...sl−1 (5.60) 1 n 1 n 1 n 1 n Substitutingforthevectorscl...cl ,usingEq. (5.58),theoutputoflayeriiswritteninthe 1 n form sl...sl = FFN(Att(h ...h ,sl−1...sl−1)) + sl−1...sl−1 (5.61) 1 n 1 m 1 n 1 n As with the models in the previous subsections, it is convenient to use a more compact notation by expressing a sequence of vectors as a matrix. Thus this model can be given in anotherform Sl = FFN(Att(H,Sl−1))+Sl−1 (5.62) HereFFN(·)isgenerallyamulti-layerneuralnetworkwithnon-linearactivationfunctions. The identity mapping (i.e., +Sl−1) creates a direct path from the input to the output of the layer,makingiteasiertotrainadeepneuralnetwork. Figure5.8showsthearchitectureofthismodel. Theattentionmodelstartswiththeinitial 5.3 The Attention Mechanism 233 S2 FFN(·) C 2 Layer2 Att(·) H S1 FFN(·) C 1 Layer1 Att(·) H H S0=S Figure 5.8: A 2-layer attention model. These layers take the same “key-value” pairs (i.e., H) but each takes a different “query” (i.e., Sl). The attention model is standard: context vectors Cl are generated by taking both H and Sl. A feed-forward neural network is built to transform Cl, followed by an addition of Sl. So this model can be formulated as Sl = FFN(Att(H,Sl−1))+Sl−1. Sl isthenusedinthenextlayerasthequery,thatis,layerl+1 takesHandSl,andrepeatstheaboveprocess. Theoutputofthelastlayercanbeviewedasa deeperrepresentationofHforS.   s 1 . representation of the target-side sequence, that is, S0 =S=  . .   . If there are L attention s n layers,thenthefinaloutputwillbeSL. 5.3.6 Remarks Aboveweconsideredabasicattentionmodelandaseriesofrefinements. Theliteratureon attention and related topics contains a wide range of methods for modeling how a system concentrates on different parts of the input, as well as a wide range of applications of such 234 Chapter 5. Sequence-to-Sequence Models models. Thissubsectionprovidesdiscussionsonsomeoftheinterestingissues. 1. Alignment vs Attention Attention is related to a long line of research on alignment approaches to modeling the correspondencebetweentwogroupsoflanguageunits. InNLP,alignmentisaverygeneral concept that is used to refer to several problems. For example, most statistical machine translation systems are trained on bilingual texts which are annotated with word-to-word alignment[Koehnetal.,2003;Chiang,2005]. Wordalignmentmodelsarethusdevelopedto generatelinksbetweenwordsintwosentences[Vogeletal.,1996;OchandNey,2003;Taskar etal.,2005;Dyeretal.,2013]. Whiletheoutputsofthesesystemsarediscretevariables,the underlyingmodelsaremostlyprobabilisticandcontinuous. Therefore, thecorrespondence betweenwordalignmentandtheattentionmodelsdiscussedhereisapparentbecausetheyare bothlearnedtoassignaweighttoeachpairofwords. Notethatdespitethesimilaritybetweenalignmentandattentionproblems,theirgoalsare different. In most cases word alignment models are used as individual systems to produce alignmentresultsfordownstreamsystems,whereasattentionmodelsaretypicallytreatedas componentsofbiggersystemsanddonotworkalone(seeFigure5.9foracomparisonofthese models). This makes them fit different types of sequence-to-sequence systems in practice: wordalignmentisonestepofapipelinedsystem,andattentionissomeintermediatestatesofa neuralnetwork. Nevertheless,wordalignmentandattentionaretworelatedproblems,andcanhelpeach otherinsomecases. Forexample,onewaytoseehowanattentionmodelbehavesistoinduce wordalignmentsfromitandmeasurethequalityofthesewordalignments[Tuetal.,2016;Li etal.,2019;Gargetal.,2019]. Also,systemsequippedwiththeattentionmechanismcanbe guidedbyexternalwordalignmentresources[Mietal.,2016b;Liuetal.,2016b]. 2. Introducing Priors AsdiscussedinSection5.3.1,theattentionmodelsimplicitlydefineanattentiondistribution over{h ,...,h }bywhichwecancomputeaweightedsumoftheserepresentations. This 1 m distributionisexpressedintermsofthealignmentweightsandislearnedaspartofamodel. Inadditiontolearningtheattentiondistributioninanend-to-endfashion,wecanalsodefine itbasedonourknowledgeabouthowweconcentrateondifferentpartsofasequencewhen readingit. Oneapproachistodirectlyimposesomestructureonthedistribution. Asimpleexample is that weconsider only a span ofthe sequence for attention anddiscard the rest. Let [ρ − i D,ρ +D]bea2D+1wordwindowcenteredatpositionρ ofthesource-sidesequence. We i i canconnects onlytoh ...h andobtainalocalcontextvectorinthefollowingform i ρi−D ρi+D c = Att(h ...h ,s ) (5.63) i ρi−D ρi+D i This approach is also called local attention. The problem of determining ρ is similar i tothereorderingprobleminmachinetranslation. Fortranslationbetweenlanguageswith 5.3 The Attention Mechanism 235 mean , while the statio h n asmade ne w progre in ss",
    "andobtainalocalcontextvectorinthefollowingform i ρi−D ρi+D c = Att(h ...h ,s ) (5.63) i ρi−D ρi+D i This approach is also called local attention. The problem of determining ρ is similar i tothereorderingprobleminmachinetranslation. Fortranslationbetweenlanguageswith 5.3 The Attention Mechanism 235 mean , while the statio h n asmade ne w progre in ss co mp c re o h n e s n t . r s u iv c e tion 同时 , 场 站 全面 建设 也 取得 了 新 成绩 . (a)Aheatmapofwordalignments mean , while the statio h n asmade ne w progre in ss co mp c re o h n e s n t . r s u iv c e tion 同时 , 场 站 全面 建设 也 取得 了 新 成绩 . (b)Aheatmapofattentionweights Figure5.9: HeatmapsofawordalignmentmodelandanattentionmodelforapairofChinese and English sentences. The heat maps are represented as shaded grids in which each cell describesthecorrespondenceofapairofChineseandEnglishwords. Thiscorrespondence isshownonacolorscalerangingfromwhitedenotingaweightof0topurebluedenotinga weightof1. 236 Chapter 5. Sequence-to-Sequence Models similarwordorders,itiscommontoassumethatthetranslationismonotonicandρ islinear i withrespecttoi[Koehn,2004],e.g.,ρ =⌊mi⌋or⌈mi⌉. Analternativemethodistousea i n n neuralnetworktopredictarelativepositioninthesource-sidesequence(denotedbyr )[Luong i etal.,2015]. ρ canthenbedefinedtobe⌊mr ⌋or⌈mr ⌉. i i i In another thread of research, a new distribution is derived by combining the original attentiondistributionandsomepriordistribution. Thesimplestsuchdistributiontakestheform oflinearinterpolation P(cid:102)r(h j |s i ) = η·Pr(h j |s i )+(1−η)·Prior (5.64) where Prior is the prior, and η is the interpolation coefficient. When η=1, it is astandard attentionmodel. Bycontrast,whenη=0,theattentioniscompletelydependentontheprior [Youetal.,2020]. The prior can be chosen in many ways. A simple choice is to assume Prior to be a GaussiandistributionGaussian(µ,σ2). Thismakesthemodelwellexplained: theattention is concentrated on some point of the sequence and decreases its strength as we spread the attentionfromthispoint. TodeterminethemeanandvarianceoftheGaussiandistribution,we canusethesametechniquedescribedabove,say,wedeveloptwoneuralnetworkstocompute themrespectively. Theinterpolationcanalsobeconsideredanintermediatestepofcomputingtheattention distribution. For example, consider the QKV attention discussed in Section 5.3.2. The interpolationcanbeplacedonthequery-keydot-product[Yangetal.,2018a;Guoetal.,2019]. Tothisend,wecanmodifyEq.(5.28)inthefollowingform qkT j α = Softmax( +ηPrior) j β s hT i j = Softmax( +ηPrior) (5.65) β As qkT j (or sihT j )isnotconstrainedin[0,1],Priorisre-scaledbyahyper-parameterη. β β Sometimes, priors arise in the context where the knowledge of attention comes from externalsources. Asdiscussedabove,incorporatingwordalignmentsintoattentionmodelsis oneofthesimplestwaystodothis. Theideacanbeextendedtomakeuseofmorestructural information, such as fertility and coverage [Cohn et al., 2016; Feng et al., 2016; Tu et al., 2016], or more task-specific constraints, such as monotonic alignments between input and outputsequences[Raffeletal.,2017;ChiuandRaffel,2018]. Also,aswithsyntacticmachine translationsystems,parsetreescanbeusedtobiastheprocessofattentionasanauxiliaryinput. For example, dependency trees are a widely used source of information in modeling word correspondenceforeithersequence-to-sequence[Chenetal.,2018a]orsequencemodeling problems[Zhangetal.,2020c;Nguyenetal.,2020;Xuetal.,2021b]. Since attention models can be computationally expensive in large-scale applications, researchers have also developed efficient attention models by introducing more inductive 5.3 The Attention Mechanism 237 biasesintomodeldesign[Tayetal.,2020b]. Thislineofresearchcanbroadlybecategorized intoefficientmethodsforNLP.InChapter6wewillpresentadiscussion. 3. Attention in Memory Networks Aswellasbeingofgreatinterestinsequence-to-sequencesystems,theattentionmechanism is extensively used in memory-based neural models [Sukhbaatar et al., 2015; Graves et al., 2014;Kumaretal.,2016]. AsdiscussedinChapter4,amemorysystemmaintainsacollection ofdataitemsandallowsuserstoretainandretrieveinformation. Givenaquery,itcomputes, insomeway,thematchbetweenthequeryandthekeyofeachdataitem. Thisprocedureis also called addressing [Graves et al., 2014]. Such addressing is typically implemented by first representing the query and the data item as real-valued vectors, and then calculating a weightbyconsideringsomesimilaritybetweenthetwovectors. Theresultoftheretrievalisa weightedsumofallthedataitems. ThisformalismfitsperfectlywiththemodeloftheQKV attentiondiscussedinSection5.3.2. Providedtheattentionmechanismandthememorymechanismarecorrelated,thoughnot fromapsychologyperspective,wecanviewattentionasaprocessofretrievinginformationin amemory(i.e., {h ,...,h })foragivenquery(i.e., s ). Thuswecaninterpretasequence- 1 m i to-sequence system with the attention mechanism as follows. On the source-side, we store information in a memory represented as a sequence of vectors h ...h . Then, we retrieve 1 m fromthismemorytorecoverstepbystepthesource-sideinformationonthetarget-side. 4. Beyond Sequence-to-Sequence Problems Whilewerestrictourdiscussiontotheproblemoftransformationfromonesequencetoanother sequence in this section, the general approach",
    "s ). Thuswecaninterpretasequence- 1 m i to-sequence system with the attention mechanism as follows. On the source-side, we store information in a memory represented as a sequence of vectors h ...h . Then, we retrieve 1 m fromthismemorytorecoverstepbystepthesource-sideinformationonthetarget-side. 4. Beyond Sequence-to-Sequence Problems Whilewerestrictourdiscussiontotheproblemoftransformationfromonesequencetoanother sequence in this section, the general approach of attention can be used to deal with other problems. AsmentionedinSection5.3.2,andwillbediscussedinChapter6,awell-known variantofthisapproachisself-attention. Inself-attention,theQKVattentionmodelisused asasequencemodel,andwehaveonlyonesequenceofvariablesasinput. Asaresult,the outputsof thisattentionmodelcan betreatedas newrepresentations oftheinput sequence. Self-attention provides a general approach to modeling the interactions and dependencies between input variables, and so can be applied to a variety of problems. For example, we canconcatenateh ...h ands ...s asanewsequenceh ...h s ...s ,andrunthismodelon 1 m 1 n 1 m 1 n thesequence. Inthisway,self-attentioniseasilyextendedtoasequence-to-sequencemodel [LampleandConneau,2019;Raffeletal.,2020]. Suchanapproachevenworkswhenh ...h 1 m ands ...s representdifferenttypesofdata. Forexample, wecanuseh ...h torepresent 1 n 1 m atextanduses ...s torepresentanimage. Then,wehaveamulti-modalmodelthatfuses 1 n textualandvisualrepresentationsbyperformingself-attentiononthem[Chenetal.,2020c]. Anotherapproachtojointrepresentationlearningofsequencesistobuildmultipleattention modelssothateachsequencecanlearnfromothersequences. Anexampleofsuchmodels isco-attention,whichhasbeenwidelyusedinmulti-modallanguageprocessing[Luetal., 2016]. Forexample,forthepurposesofvisualquestionanswering(VQA),wewishtofigure out which parts of the image are related to a word of the question and to figure out which wordsofthequestionarerelatedtoagivenpartoftheimage. Todothiswewillbuildtwo 238 Chapter 5. Sequence-to-Sequence Models attention models: one for image-to-text attention, and one for text-to-image attention. The outputsofbothmodelscanbethoughtofasjointrepresentationsfortheimageandtext,and thuscanbeusedasfeaturesforanswerprediction. Theattentionmodelsdiscussedinthissectionareorder-independentforinput. Thisisan issuefordealingwithsequentialdata,andcanbeaddressedbyencodingorderinformation in the inputs themselves (see Chapters 4 and 6). On the other hand, the simplicity of this formulationmakesthesemodelsgeneral: theinputdataofthemodelsneedsnottobesequential. Asaresult,theattentionmodelscanbedirectlyappliedtomorecomplexdata,suchasgraphs [Velicˇkovic´ etal.,2018;Leeetal.,2019]. 5.4 Search Searchisafundamentalissueinartificialintelligence,andplaysanimportantroleinmany NLPsystems. Thesearchproblemisacomputationalchallengeherebecausethenumberof hypothesesinthesearchspaceincreasesexponentiallywiththelengthofthesequenceand the size of the vocabulary on the target-side. Exhaustive search in this case is simply slow. Therefore, real-world systems often involve search algorithms or heuristics to ensure that optimalorsub-optimalsolutionscanbefoundinanacceptabletime. Formanypracticalsequence-to-sequenceapplications,thesearchproblem,alsocalledthe inferenceproblemsometimes,canbeformulatedasthefollowingequation yˆ = argmaxScore(x,y) (5.66) y∈Ω whereScore(x,y)isamodelthatmeasuresthegoodnessofygivenx. ThisequationtakesaslightlydifferentformfromthatofEq. (5.2). First,weuseScore(x,y) insteadofPr(y|x)asthegoodnessfunction. Whileatypicalapproachtotrainingsequence-to- sequencemodelsistomaximizePr(y|x)(orPr(x,y)),weoftenneedtoconsidertask-specific problemswhenperforminginferenceontestdata,forexampletheproblemoflengthbias. Itis thereforecommontoinvolveotherterms,aswellasPr(y|x),todefinetheobjectivefunction forsearch(seeSection5.4.1). AseconddifferencebetweenEq. (5.66)andEq. (5.2)arises fromtheformofthesearchspacewhichisconstrainedtoΩ. Ingeneral,Ωisaprunedsearch spaceandcontainsarelativelysmallnumberofhypotheses. Acommonwaytoachievethis isthroughtheuseofpruningtechniquesandadvancedsearchalgorithms(seeSection5.4.2). Inthissectionweconsidersolutionstotheseproblemsandsomeoftherefinements. These methodsarelargelymotivatedbythedevelopmentofmachinetranslation,butthediscussions herearegeneralandcanbeconsideredinmosttextgenerationproblems. 5.4.1 The Length Problem RecallfromSection5.2.2thattheprobabilityofthetarget-sidesequenceygiventhesource- sidesequencexcanbewrittenasaproductofprobabilitiesofeachwordy givenboththe i generated words y ...y and x. Here we re-express Eq. (5.14) using simpler notation, as 0 i−1 5.4 Search 239 follows n (cid:89) Pr(y|x) = Pr(y |y ,x) (5.67) i <i i=1 wherey denotesthesequencey ...y . Thiscanbeequivalentlyexpressedintermsoflog <i 0 i−1 probabilities n (cid:88) logPr(y|x) = logPr(y |y ,x) (5.68) i <i i=1 Such a simple form of modeling has clear advantages as practical models for NLP, but unfortunately,thisleadstoapreferenceforshortertarget-sidesequencesoverlongertarget-side sequences. SoitseemsimplausibletosimplytakeScore(x,y)=Pr(y|x)orlogPr(y|x)in search because the result is very probably a short sequence, say, a sequence of one or two words. Thisproblemisadirectconsequenceoftheinductivebiasoftheabovemodel. Froma supervisedlearningperspective,anotherreasonforthisisthatteacherforcingisusedtotrain themodel: onlyaground-truthtarget-sidesequenceisconsideredintraining,andthemodelis forcedtooutputthisground-truth. Bycontrast,whenapplyingthismodeltotestdata,weneed toexploreabigsetofyofdifferentlengths,andtocomparedifferentyintermsofafunction thatisdifferentfromtheonelearnedonthetrainingdata. Thisproblemcanbeaddressedthroughatechniquecalledlengthreward,whichgives bonusestolongersequencesbyaddingatermtoScore(x,y)[Heetal.,2016c]. Asdiscussed inChapter3,acommonlyusedformoflengthrewardisgivenby Score(x,y) = logPr(y|x)+λ·n (5.69) Herethelengthrewardtermisthelengthofy(i.e.,n=|y|),weightedbytheparameterλ>0. Improvements on this approach involve replacing n with an estimated sequence length by usingalengthpredictionmodel. Forexample,wecanboundtherewardinthefollowingform [Huangetal.,2017b;Yangetal.,2018b] Score(x,y) = logPr(y|x)+λ·max(l ,n) (5.70) p where l is a predicted length, and is generally defined to be a scaled length of x, that is, p l =scalar ·m. p p IfwesubstitutethelogprobabilitylogPr(y|x)givenbyEq. (5.68)intoEq. (5.69), we obtain n (cid:88) Score(x,y) = logPr(y |y ,x)+λ·n i <i i=1 n (cid:88) = [logPr(y |y ,x)+λ] (5.71) i <i i=1 240 Chapter 5. Sequence-to-Sequence Models Thus,wecaninterpretthelengthrewardtermasarewardtoeachwordy . Suchamethodhas i been widely used in statistical machine translation (SMT) systems in which the rewards aretreatedasfeaturesofalog-linearmodel[Koehnetal.,2003;Chiang,2007]. Tofindan appropriatevalueofλ,wecaneitheruseminimumerrorratetraining[Och,2003],following theconventioninSMT,orusegradient-basedmethodsasinneuralnetwork-basedsystems [MurrayandChiang,2018]. Asecondapproachtobiasingsearchtowardslongersequences,calledlengthnormaliza- tion,istodividelogPr(y|x)byalengthcorrectionterm,writteninthefollowingform logPr(y|x) Score(x,y) = (5.72) n correct Asimpleexampleofthismodelistodefinethelengthcorrectiontermasthesequencelength [Jeanetal.,2015],likethis n = n correct = |y| (5.73) Inthiscase, logPr(y|x) = (cid:80)n i=1 logPr(yi|y<i,x)",
    "240 Chapter 5. Sequence-to-Sequence Models Thus,wecaninterpretthelengthrewardtermasarewardtoeachwordy . Suchamethodhas i been widely used in statistical machine translation (SMT) systems in which the rewards aretreatedasfeaturesofalog-linearmodel[Koehnetal.,2003;Chiang,2007]. Tofindan appropriatevalueofλ,wecaneitheruseminimumerrorratetraining[Och,2003],following theconventioninSMT,orusegradient-basedmethodsasinneuralnetwork-basedsystems [MurrayandChiang,2018]. Asecondapproachtobiasingsearchtowardslongersequences,calledlengthnormaliza- tion,istodividelogPr(y|x)byalengthcorrectionterm,writteninthefollowingform logPr(y|x) Score(x,y) = (5.72) n correct Asimpleexampleofthismodelistodefinethelengthcorrectiontermasthesequencelength [Jeanetal.,2015],likethis n = n correct = |y| (5.73) Inthiscase, logPr(y|x) = (cid:80)n i=1 logPr(yi|y<i,x) canbeviewedasthelog-scalegeometricmean n n oftheprobabilities{Pr(y |y ,x)}5. i <i Another example is the one used in the GNMT system [Wu et al., 2016]. It takes the exponentialoftheshifted,re-scaledn,asfollows (5+n)α n = (5.76) correct (5+1)α wherethepowerαisahyper-parameterandcanbedeterminedempiricallyonatuningset. To comparedifferentmethods,Table5.2showsalistofscoringfunctionsforlengthrewardand lengthnormalization. Inmachinetranslation,thelengthproblemisalsocloselyrelatedtothecoverageproblem whichhasbeendiscussedextensivelyinSMT.Whentranslatingasource-sidesequence,we wishtoknowhowmanytimeseachwordistranslated. Then,wewillsaythatover-translation occurs(i.e.,alongertranslation)ifsomewordsaretranslatedtoomanytimes,andthatunder- translation occurs (i.e., a shorter translation) if some words are not sufficiently translated. Traditionally,thecoverageofasource-sidesequenceisdescribedintermsofanm-dimensional 5Suppose{a 1 ,...,an}arenvariables.Since exp (cid:18)(cid:80)n i=1 n loga i (cid:19) = (cid:32) (cid:89) n a i (cid:33) n 1 (5.74) i=1 wehave (cid:80)n i=1 n loga i = log (cid:32) (cid:89) n a i (cid:33) n 1 (5.75) i=1 5.4 Search 241 Method FormofScore(x,y) NoReward/Normalization Score(x,y)=logPr(y|x) LengthReward Score(x,y)=logPr(y|x)+λ·n BoundedLengthReward Score(x,y)=logPr(y|x)+λ·max(l ,n) p logPr(y|x) LengthNormalization(Basic) Score(x,y)= n logPr(y|x) LengthNormalization(GNMT) Score(x,y)= (5+n)α/(5+1)α Table 5.2: Scoring functions for length reward and length normalization. m=|x|, n=|y|, andl =scalar ·m. λandαareparameters. p p (cid:104) (cid:105) vector υ ... υ ,calledthecoveragevector. υ describestowhatextentthesource-side 1 m j wordx istranslated. InSMTsystemsυ isabinaryvariable: 0denotesuntranslated,and j j 1denotestranslated. However,NMTsystemshavenosuchsymboliccoveragemechanism. Instead,theyhavemodelsthatcomputetheattentionweightsbetweenx andallthetarget-side j words. Therefore,onewaytodefinewhatwemeanbythecoverageofawordistoconsider howstrongx connectstothetarget-sidewords. Todothis,weextendυ tobeacontinuous j j variable,givenby n (cid:88) υ = α (5.77) j i,j i=1 υ canthusbeviewedasthe“numberoftimes”x istranslated,say,υ =0meansthatx is j j j j nottranslatedatall,andυ =1meansthatx iscountedonlyonceintranslation. Considerthe j j exampleinFigure5.9. Forthesource-sideword建设,thecorrespondingattentionweightsare shownbelow. 0.23 0.14 0.20 0.22 0.22 0.21 0.10 0.29 0.15 0.30 0.60 0.09 elihwnaem , eht noitats sah edam wen ssergorp ni evisneherpmoc noitcurtsnoc . 建设 (j =5) υ = (cid:80)12 α =2.75 5 i=1 i,5 Wewillsaythat建设istranslated2.75times. Itispossibletomakeuseof{υ ,...,υ }to 1 m definehowmuchthesource-sidesequenceiscoveredintranslation. Asimplewaytodothis istodevelopacoveragescorecp(x,y)bycombining{υ ,...,υ }. Forexample,theGNMT 1 m systemdefinescp(x,y)inthefollowingform m (cid:88) cp(x,y) = β log(min(υ ,1)) (5.78) j j=1 242 Chapter 5. Sequence-to-Sequence Models whereβ isaweightforthecoveragemodel. Theunderlyingideaisthatwhenυ ≥1theword j x isassumedtobeadequatelytranslated;whenυ <1thewordx isassumedtobelackof j j j translation. Thuscp(x,y)penalizeshypothesesinwhichsomeofthesource-sidewordsmiss partsofthetranslations. AnimprovementtothisformisgivenbyLietal.[2018] m (cid:88) cp(x,y) = β log(max(υ ,γ)) (5.79) j j=1 where γ is the hyper-parameter for truncation, giving a tolerance for under-translation. A similarformwasproposedin[ChorowskiandJaitly,2017] m (cid:88) cp(x,y) = β 1(υ >γ) (5.80) j j=1 Itjustcountsthenumberoftimesυ isgreaterthanγ. j cp(x,y)canbeeasilyintroducedintosearchbyaddingittoScore(x,y). Forexample,the GHKM-stylescoringfunctionisdefinedtobe logPr(y|x) Score(x,y) = +cp(x,y) (5.81) (5+n)α/(5+1)α Inpractice,modifyingScore(x,y)isnottheonlywaytoaddressthelengthproblemin search. An alternative approach is to have architecture changes for modeling the problem [Tu et al., 2016; Mi et al., 2016a; Sankaran et al., 2016; See et al., 2017; Malaviya et al., 2018]. Note that, sometimes the length of the target-side sequence has been specified or predicted in some way. In these cases, we can either develop models not dependent on the auto-regressiveassumption[Guetal.,2018],ordeveloplength-controllabletextgeneration systemsforinterestingapplications[Rushetal.,2015;Kikuchietal.,2016]. 5.4.2 Pruning and Beam Search Therearemanywaystodefineasearchspace. Asageneralconceptincomputerscience,a searchspaceisoftenreferredtoasthedomainoftheproblemthatissearched. Forsequence-to- sequenceproblems,wecanthinkofahypothesisasamappingfromasource-sidesequencex toatarget-sidesequencey,andcanthinkofasearchspaceasacollectionofsuchhypotheses6. Wecanimplementasearchprogrambyorganizinghypothesesinanunderstandableway so that we can look at the search space for the problem. Recall that in Eqs. (5.67-5.68) we assign a probability of y given x by using a left-to-right factorization. A typical search systemmaintainsasetofhypotheses(orpartialhypotheses)andbuildsupthesehypotheses fromlefttoright7. ThesearchprocedurebeginswithaninitialhypothesissetZ containing 0 6Hereweuse(x,y)todenoteahypothesis.Whentherearemultiplemappingsfromxtoy,ahypothesiscan berepresentedas(x,y,d)whereddenotesthemapping.Forexample,ifwetransformxtoywithasynchronous grammar,theremightbemultiplederivationsofgrammarrulestodothis. 7Ahypothesisiscalledpartialwhenthecorrespondingtarget-sidesequencedoesnotendwith⟨EOS⟩,i.e.,an incompletetarget-sidesequence.Inthissectionweusethetermshypothesisandpartialhypothesisinterchangeably 5.4 Search 243 onlyonehypothesisz whosetarget-sideisy byconsideringy =⟨SOS⟩isthestartsymbol 0 0 0 for all target-side sequences. Then, we extend this hypothesis set over a",
    "Eqs. (5.67-5.68) we assign a probability of y given x by using a left-to-right factorization. A typical search systemmaintainsasetofhypotheses(orpartialhypotheses)andbuildsupthesehypotheses fromlefttoright7. ThesearchprocedurebeginswithaninitialhypothesissetZ containing 0 6Hereweuse(x,y)todenoteahypothesis.Whentherearemultiplemappingsfromxtoy,ahypothesiscan berepresentedas(x,y,d)whereddenotesthemapping.Forexample,ifwetransformxtoywithasynchronous grammar,theremightbemultiplederivationsofgrammarrulestodothis. 7Ahypothesisiscalledpartialwhenthecorrespondingtarget-sidesequencedoesnotendwith⟨EOS⟩,i.e.,an incompletetarget-sidesequence.Inthissectionweusethetermshypothesisandpartialhypothesisinterchangeably 5.4 Search 243 onlyonehypothesisz whosetarget-sideisy byconsideringy =⟨SOS⟩isthestartsymbol 0 0 0 for all target-side sequences. Then, we extend this hypothesis set over a number of search steps. SupposewehaveasequenceofhypothesissetsZ ...Z wheren isthemaximum 0 nmax max numberofsearchsteps. Atstepi,wewishtoextendeachhypothesisbyaddinganewwordv k drawnfromthevocabularyV . Letz.srcbethesource-sideofz andz.tgtbethetarget-side y ofz. Clearly,wehavez.src=xforanyz. Givenahypothesisz ∈Z ,wecanextendit cur i−1 to|V |hypotheses{z1 ,...,z |Vy| },givenby y next next {z1 ,...,z |Vy| } = Extend(z ,V ) next next cur y (cid:91) = Extend(z ,v ) (5.82) cur k v k ∈Vy HereExtend(z ,v )isafunctionthatextendstheinputhypothesisz withawordv ∈V . cur k cur k y Thetarget-sideofaresultinghypothesisistheconcatenationofz .tgtandv ,writtenas8, cur k zk .tgt = z .tgt◦v (5.83) next cur k These new hypotheses {z1 ,...,z |Vy| } are then added to Z . Figure 5.10 illustrates a next next i fewstepsinthishypothesisextensionprocess. Weseethatallthehypothesescaneasilybe representedasatreestructure. HereZ correspondstoasetofthenodesatleveliofthesearch i tree,andwesimplyhave |Z | = |V|·|Z | (5.84) i i−1 Inotherwords,thesizeofZ growsexponentiallywiththenumberofsteps,say,|Z |=|V|i. i i EachhypothesiszisassociatedwithalogprobabilitylogPr(z.tgt|z.src). logPr(z.tgt|z.src) simplytakestheformofEq. (5.68),andcanbedefinedinarecursivefashion logPr(zk .tgt|zk .src) = logPr(z .tgt|z .src)+ next next cur cur logPr(v |z .tgt,z .src) (5.85) k cur cur Asanexample,supposezk .tgt=y ...y . TheformofEq. (5.85)becomesclearfromthe next 0 i+1 followingrewriting logPr(y ...y |x) = logPr(y ...y |x) +logPr(y |y ...y ,x) 0 i+1 0 i i+1 0 i (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) logPr(z n k ext .tgt|z n k ext .src) logPr(zcur.tgt|zcur.src) logPr(v k |zcur.tgt,zcur.src) i (cid:88) = logPr(y |y ,x)+logPr(y |y ...y ,x) k <k i+1 0 i k=1 i+1 (cid:88) = logPr(y |y ,x) (5.86) k <k k=1 becausetheirformsarethesame. 8Weusea◦btodenotetheconcatenationoftwostringsaandb. 244 Chapter 5. Sequence-to-Sequence Models . . . ⟨SOS⟩A◦tail s: −1.3×1012 . . . ⟨SOS⟩◦“ ⟨SOS⟩A◦taxi s: −1.6×107 s: −2.7×1012 ⟨SOS⟩◦A ⟨SOS⟩A◦team s: −3.2×104 s: −5.0×1010 z 0 ⟨SOS⟩ ⟨SOS⟩◦An ⟨SOS⟩A◦text s: 0 s: −6.1×105 s: −1.0×109 ⟨SOS⟩◦Because ⟨SOS⟩A◦tiger s: −7.7×106 s: −3.2×1012 . . . ⟨SOS⟩◦If s: −3.9×106 . . . Step0 Step1 Step2 (Z ={z }) (Z ) (Z ) 0 0 1 2 Figure5.10: Illustrationofhypothesisextensioninfirst3steps. Each(partial)hypothesisis represented as a box in which we show the corresponding target-side sequence and model score. Each search step is associated with a hypothesis set Z . We start with a hypothesis i z ∈Z denotingthestartsymbol⟨SOS⟩. Instepi, weextendeveryhypothesisinZ by 0 0 i−1 tryingtoappendeverywordfromavocabularyV (seewordsinred). Thisoperationwillresult in|V|·|Z |hypotheses,formingthehypothesissetZ . Thehypothesisextensionprocedure i−1 i representsabreadth-firstsearchalgorithm: wecreateallthenodes(orsearchstates)atdepth i−1beforemovingtodepthi. Atreestructureiscreatedalongwiththisprocedure,andaleaf nodeofthetreecantracethesearchpathbacktotherootnode. Giventhisprobability,wecanthencomputez.score=Score(z.src,z.tgt),asinSection 5.4.1. Thisenablesustocomparedifferenthypothesesintermsofz.score. Ifahypothesisends withthesymbol⟨EOS⟩,itiscalledcompleteandisnotextendedanymore. Onceahypothesis 5.4 Search 245 iscomplete,itisaddedtoamax-heap9. Wecandumpthehypotheseswithmaximummodel scoresfromtheheap. Ingeneral,thesearchprocedurewillstopifwefindacertainnumber ofcompletehypotheses. Forexample,wecanstopsearchingwhentheheapisfull(seemore discussionslaterinthissubsection). Theresultingsearchalgorithmisdescribedbelow. Algorithm: ASimpleBreadth-firstSearchAlgorithm SimpleSearch(x) //Searchforthebesthypothesisgiventhesource-sidesequencex 1. CreateaHeapwithsize elements heap 2. Z ={z }wherez .src=xandz .tgt=y 0 0 0 0 0 3. Foreachstepi=1ton max 4. Foreachhypothesisz ∈Z cur i−1 5. Foreachwordv ∈V k y 6. z =Extend(z ,v ,x) next cur k 7. Ifz .tgtendswith⟨EOS⟩,then next 8. Addz toHeap next 9. Else 10. Addz toZ next i 11. IfHeapisfulland/orotherstoppingcriteriaaremet,then 12. Breakalltheloops 13. returnHeap.Pop() Extend(z ,v ,src) cur k //Createanewhypothesisbyappendinganewwordv tothetarget-sideofz k cur 1. Createanewhypothesisz next 2. z .src=src next 3. z .tgt=z .tgt◦v next cur k 4. z .prob=z .prob+logPr(v |z .tgt,z .src) //seeEq. (5.85) next cur k cur cur 5.",
    "Else 10. Addz toZ next i 11. IfHeapisfulland/orotherstoppingcriteriaaremet,then 12. Breakalltheloops 13. returnHeap.Pop() Extend(z ,v ,src) cur k //Createanewhypothesisbyappendinganewwordv tothetarget-sideofz k cur 1. Createanewhypothesisz next 2. z .src=src next 3. z .tgt=z .tgt◦v next cur k 4. z .prob=z .prob+logPr(v |z .tgt,z .src) //seeEq. (5.85) next cur k cur cur 5. z .score=score(z .src,z .tgt) //seeSection5.4.1 next next next 6. Returnz next Ifthehypothesisheaphasaninfinitecapacity(size =∞),thisalgorithmwillperform heap anexhaustivesearchoveraspaceofallhypotheseswhosetarget-sidelengthsareupton , max resultinginatmost1+|V y |+|V y |2+···+|V y |nmax = |Vy|n |V m y a |− x+ 1 1−1 completehypotheses. This isanextremelyhugesearchspacewhichiscomputationallyintractableinpractice10. Therefore, inpracticalsystemsitiscommontoprunethesearchspaceinordertomakethesearchtractable. Inlaterpartsofthissubsectionwewillintroducetwopopularsearchalgorithms,bothadopting pruningforefficientsearch. 9Givenamax-heapa,weusea.Pop()todenoteafunctionpoppingthetop-1itemofa,andusea.PopAll()to denoteafunctionpoppingalltheitemsofa. 10Consider,forexample,avocabularysizeof20,000(|Vy|=20,000)andalengthlimitof20(nmax=20). |Vy|nmax+1−1 wouldbe1.05×1086. |Vy|−1 246 Chapter 5. Sequence-to-Sequence Models 1. Greedy Search The greedy strategy is one of the most common concepts that one learns in textbooks on algorithms. Itisbasedonaheuristicthatthegloballyoptimalsolutioncanbeapproximatedby makinglocallyoptimaldecisions. Althoughsuchanapproximationcanonlyobtainalocally optimalsolution,thisissufficientformanypracticalapplicationsanditslowcomputational complexityisaclearadvantage. Applying the greedy strategy to the search problem here is straightforward. In each extensiongivenstepi,weonlyconsiderthebesthypothesisuptoi. Tobemoreprecise,for anyZ ,weonlykeepthehypothesiswiththehighestmodelscoreanddiscardtherest. The i outputofeachstepofthegreedysearchisgivenby z = argmax z .score (5.87) best next znext∈Extend(Zi−1,Vy) HerethefunctionExtend(Z ,V )hasthesamemeaningasthatinEq. (5.82),butoperates i−1 y onasetofhypotheses,thatis, (cid:91) Extend(Z ,V ) = Extend(z,V ) (5.88) i−1 y y z∈Zi−1 Then,Z isdefinedtobe i Z = {z } (5.89) i best Agreedysearchalgorithmforsequence-to-sequenceproblemsisdescribedbelow. Algorithm: AGreedySearchAlgorithm GreedySearch(x) //Searchforthe“best”hypothesisinagreedymanner 1. Createahypothesisz best 2. Z ={z }wherez .src=xandz .tgt=y 0 0 0 0 0 3. Foreachstepi=1ton max 4. z .score=−∞ best 5. Foreachhypothesisz ∈Z cur i−1 6. Foreachwordv ∈V k y 7. z =Extend(z ,v ,x) next cur k 8. Ifz .score<z .score,then best next 9. z =z best next 10. Ifz .tgtendswith⟨EOS⟩and/orotherstoppingcriteriaaremet,then best 11. Breaktheloop 12. Z ={z } i best 13. Returnz best Ineachstepofsearch,wehaveonlyoneactivehypothesistoextend(i.e.,|Z |=1)and i−1 5.4 Search 247 thereforeneed|V|extensionsfromwhichweselectthebestoneforthenextstepofsearch. ThetotalnumberoftimesExtend(z ,v )iscalledis|V|·n . ProvidedExtend(z ,v ) cur k max cur k isafixed-costfunction,thetimecomplexityofthealgorithmislinearwithrespectto|V|and n . max 2. Beam Search Beamsearchisanaturalextensionoftheabove1-bestgreedysearchalgorithm. Itisbased on the greedy heuristics as well, and is thus a type of greedy algorithm. The idea of beam searchistokeepateachstepanumberofthemostpromisinghypothesesratherthanthe1-best hypothesis. Abeamisadatastructurethatstoresthebesthypotheseswehavegeneratedso far. Thenumberofhypothesesinabeamisapredeterminedparameter,calledbeamwidthor beamsize. HerewecansimplyviewZ asabeam,writtenas i Z = {z1 ,...,zsize beam} (5.90) i best best wheresize isthebeamsize. z1 isthebesthypothesisintheextensionExtend(Z ,V ) beam best i−1 y (seeEq. (5.87)),z2 isthe2ndbesthypothesisinExtend(Z ,V ),andsoon. best i−1 y Thefollowingpseudo-codedescribesabeamsearchalgorithmforsequence-to-sequence problems. Algorithm: ABeamSearchAlgorithm BeamSearch(x) //Searchforthe“best”hypothesisbyconsideringanumberofbestcandidates //ineachstep 1. CreateaHeapwithsize elements heap 2. Z ={z }wherez .src=xandz .tgt=y 0 0 0 0 0 3. Foreachstepi=1ton max 4. CreateaheapBeamwithsize elements beam 5. Foreachhypothesisz ∈Z cur i−1 6. Foreachwordv ∈V k y 7. z =Extend(z ,v ,x) next cur k 8. Ifz .tgtendswith⟨EOS⟩,then next 9. Addz toHeap next 10. Else 11. UpdateBeam(Beam,z ) next 12. IfHeapisfulland/orotherstoppingcriteriaaremet,then 13. Breakalltheloops 14. Z =Beam.PopAll() i 15. ReturnHeap.Pop() UpdateBeam(Beam,z ) next //UpdateBeamwithanewly-generatedhypothesisz next 248 Chapter 5. Sequence-to-Sequence Models 1. Addz toBeama next aBeamisamax-heapwithsize beam elements.So,ifznext.scoreislowerthanalltheelementsinthe heap,theheapwillbeleftunchanged.Inotherwords,Beamonlystorestop-size besthypothesesand beam ignorestherest. ThefunctionUpdateBeam(Beam,z )isadirectimplementationofhistogramprun- next ing. Note that this general-purpose framework provides a simple way to implement other pruningmethods,andonecanmodifyUpdateBeam(Beam,z )asneeded. Forexample, next analternativemethod,calledthresholdpruning,retainsthehypotheseswhosedifferencesin modelscoreswiththebesthypothesisinBeamarebelowathresholdθ ,say,wediscard beam z inUpdateBeam(Beam,z )if next next z .score < z .score−θ (5.91) next best beam wherez isthebesthypothesisinBeam. Alternatively,wecanconsiderarelativethreshold best method[FreitagandAl-Onaizan,2017],givenby z .score < z .score·θ (5.92) next best beam Figure5.11showsacomparisonofexhaustivesearch,(1-best)greedysearchandbeam search. Atoneextreme,theoptimalsolutionisguaranteed,butanexponentiallylargenumber ofsearchstatesarevisited. Attheotherextreme,onlytheminimumnumberofsearchstatesare visited,butthesolutionissub-optimal. Bycontrast,beamsearchmakesatrade-offbetweenthe twomethods. Alargerbeamsizemeansmoresearcheffortandahigherpossibilityoffinding theoptimum,whileasmallerbeamsizemeansfastersearchandahigherriskofmissingthe optimum. Itisalsopossibletouseavariablebeamsizetomakeabettertrade-offduringsearch [Buckmanetal.,2016;PostandVilar,2018;Kulikovetal.,2019]. Animportantproblemrelatedtothesesearchalgorithmsistheproblemofsearcherrors. Ingeneral,searcherrorscanbedefinedinseveraldifferentways. Herewesaythatasearch erroroccursifthesearchresultisnotthesameasthatofexhaustivesearch. Commonsense tellsusthatfewersearcherrorsarehelpfulforfinding“better”results. Thus,weoftenwishto haveamoredesirabletarget-sidesequencebyenlargingthebeamsize. However,thisisnot thecaseforsomesequence-to-sequencesystems. Forexample,asearchwithalargerbeam sizemayleadtoalowertranslationqualityforneuralmachinetranslationsystems[Koehnand Knowles,2017]. Thisinspiresveryinterestingstudiesonthedeteriorationissueoflargebeam search[Ottetal.,2018b;Yangetal.,2018b;StahlbergandByrne,2019]. 3. Stopping Criteria",
    ".score−θ (5.91) next best beam wherez isthebesthypothesisinBeam. Alternatively,wecanconsiderarelativethreshold best method[FreitagandAl-Onaizan,2017],givenby z .score < z .score·θ (5.92) next best beam Figure5.11showsacomparisonofexhaustivesearch,(1-best)greedysearchandbeam search. Atoneextreme,theoptimalsolutionisguaranteed,butanexponentiallylargenumber ofsearchstatesarevisited. Attheotherextreme,onlytheminimumnumberofsearchstatesare visited,butthesolutionissub-optimal. Bycontrast,beamsearchmakesatrade-offbetweenthe twomethods. Alargerbeamsizemeansmoresearcheffortandahigherpossibilityoffinding theoptimum,whileasmallerbeamsizemeansfastersearchandahigherriskofmissingthe optimum. Itisalsopossibletouseavariablebeamsizetomakeabettertrade-offduringsearch [Buckmanetal.,2016;PostandVilar,2018;Kulikovetal.,2019]. Animportantproblemrelatedtothesesearchalgorithmsistheproblemofsearcherrors. Ingeneral,searcherrorscanbedefinedinseveraldifferentways. Herewesaythatasearch erroroccursifthesearchresultisnotthesameasthatofexhaustivesearch. Commonsense tellsusthatfewersearcherrorsarehelpfulforfinding“better”results. Thus,weoftenwishto haveamoredesirabletarget-sidesequencebyenlargingthebeamsize. However,thisisnot thecaseforsomesequence-to-sequencesystems. Forexample,asearchwithalargerbeam sizemayleadtoalowertranslationqualityforneuralmachinetranslationsystems[Koehnand Knowles,2017]. Thisinspiresveryinterestingstudiesonthedeteriorationissueoflargebeam search[Ottetal.,2018b;Yangetal.,2018b;StahlbergandByrne,2019]. 3. Stopping Criteria Althoughthetimecomplexitiesoftheabovealgorithmsareboundedbythemaximumnumber ofsearchsteps(i.e.,n ),itisimportanttohavemoreefficientalgorithmstostopsearchingas max earlyaspossible,especiallyforlatency-sensitiveapplications. Thistypicallyrequiresheuristics todesignadditionalcriteriaforstoppingthesearchprocedureattheappropriatepoint. Some ofthesestoppingcriteriaare: 5.4 Search 249 S S S te te te p p p 3 3 3 S S S te te te p p p 2 2 2 S S S te te te p p p 1 1 1 (a)ExhaustedSearch (b)(1-best)GreedySearch (c)BeamSearch Figure5.11: Acomparisonofexhaustivesearch,(1-best)greedysearchandbeamsearch. Balls representsearchstatesorpartialhypotheses. Exhaustedsearchexploresallsearchstatesinthe searchspace. Bycontrast,greedysearchkeepsonlythe1-bestpathofsearchstatesandprunes awaytherest. Beamsearchisatrade-offbetweenthemandkeepsthemostpromisingsearch statesinabeamineachstep. • If a given number of complete hypotheses are created, then we stop searching. For example,inthebeamsearchalgorithmdescribedinthissubsection,thesearchprogram terminates when we have size complete hypotheses. Another way to implement heap this idea is to shrink the beam as the number of complete hypotheses increases. In Bahdanauetal.[2014]’ssystem,onceanewcompletehypothesisiscreated,thebeam size decreases by 1. Therefore, the search program will terminate if the beam size is reducedto0. • Ifeveryhypothesisatstepihasascorelowerthanthatofthebestcompletehypothesisin Heapbysomemargin,thenwestopsearching. Supposez isthebesthypothesis bestinall we have generated so far (i.e., z =Heap.Pop()). If every hypothesis z at bestinall next stepisatisfies z .score−z .score ≥ θ (5.93) bestinall next all then we will finish the search process at this step. Here θ is a parameter. One all can specify it with an appropriate value through multiple tries. A simple choice is θ =0,whichisemployedinsomeofthepopularsequence-to-sequencesystems[Ott all etal.,2019]. Undersomecircumstances,suchanearly-stopstrategycanguaranteethe 250 Chapter 5. Sequence-to-Sequence Models optimalityofsearch[Huangetal.,2017b;Yangetal.,2018b]. • Ifeveryhypothesisatstepihasascorelowerthanthatofthelastcompletehypothesisin Heapbysomemargin,thenwestopsearching. Thisisaweakconditionforearly-stop. • Ifthetoprankedhypothesesatstepiareallcompletehypotheses,thenwestopsearching. Thisisamoreaggressiveversionofearly-stop. Forexample, inKleinetal.[2017]’s system, the search program terminates at step i if the top-1 hypothesis is a complete hypothesis. • If the search program consumes a certain amount of computing resources, such as a certainnumberoffloating-pointinstructionsandacertainamountofwallclocktime, then we stop searching. In applications where computer performance is limited and latency plays an important role, we will often be interested in this kind of stopping criterion. Sometimes,thesearchalgorithmwillnotfindanycompletehypothesisuntilhittingthe lengthlimitn . Asapracticalmatteritmightbeeasyinthiscasetoforcethebestpartial max hypothesistobecompletebyadding⟨EOS⟩toitsend. Notethatchoosingappropriatestoppingcriteriareflectsatrade-offbetweenfastcomputa- tionandaccuratepredictionatinferencetime(callitthespeed-accuracytrade-off). Whileit isnotalwaysthecasethatmoretimeasearchprogramtakescouldresultinbetterresultsfora sequence-to-sequencesystem,wewouldalwayswanttoknowhowclosewecangettoabetter solutiontotheproblembysearchingthroughalargerregionofthesearchspace. Adiscussion ofaccuratesearchalgorithmscanbefoundinSection5.4.4. 5.4.3 Online Search So far in our general discussion of sequence-to-sequence problems, we have assumed that allthesource-sidewordscometogetherasawholeandcanbeaccessedintheentiresearch process. However,insomepracticalapplications,theinputsarereceivedinorder,andwewish tomakepredictionsconditionedonsomeoftheobservedinputs. Anexampleofthisisonline automaticspeechrecognitioninwhichthesystemcontinuallytakesnewacousticsignalsand atthesametimeoutputsthecorrespondingtranscriptionunits. Intuitively,wemightthinkofthegenerationofthei-thtarget-sidewordasaproblemof mappingaprefixofthesource-sidesequencetothetarget-sidevocabulary. Wecanformulate thisbyintroducingafunctiong(i)whichdenotesthemaximumlengthoftheprefixofxwe useingeneratingy . Thus,theprobabilityofy giventheentiresequencexandthepreviously i i generatedwordsy canbeapproximatedby <i Pr(y |y ,x) ≈ Pr(y |y ,x ) (5.94) i <i i <i ≤g(i) wherex denotesthesub-sequencex ...x . Then,thelogprobabilityofthetarget-side ≤g(i) 1 g(i) 5.4 Search 251 sequenceygiventhesource-sidesequencexiswrittenas n (cid:88) logPr(y|x) = logPr(y |y ,x) i <i i=1 n (cid:88) ≈ logPr(y |y ,x ) (5.95) i <i ≤g(i) i=1 Thisequationframesasequence-to-sequenceproblemasaprefix-to-prefixproblem,thatis, theprefixy isonlydependentontheprefixx . Inferenceforthismodelissimple. For ≤i ≤g(i) eachi,thesearchsystemwaitsuntilallg(i)source-sidewordsarereceived,andthenextends thehypothesesasusual. Thiscanbedonebyreusingthealgorithmsdescribedintheprevious subsection. Forexample,wecanmodifythebeamsearchalgorithmandobtainthefollowing onlinesearchalgorithm. Algorithm: AnOnlineBeamSearchAlgorithm OnlineBeamSearch(x,g(·)) //Onlinesearchinwhichthesearchisoperatedonceanadequatenumberofinput //wordsarereceived. Ineachsearchstep,anumberofthemostpromisingcandidates //areconsidered. 1. CreateaHeapwithsize elements heap 2. Z ={z }wherez .tgt=y 0 0 0 0 3. j =0 4. i=1 5. input=ϕ 6. Whilei≤n do max 7. Ifj <g(i),then //readawordfromtheinputstream 8. input=input◦x j 9. Else //makeapredictionatstepi 10. //wheng(i)inputwordsareobserved(storedininput) 11. CreateaheapBeamwithsize elements beam 12. Foreachhypothesisz ∈Z cur i−1 13. Foreachwordv ∈V k y 14. z =Extend(z ,v ,input) next cur k 15. Ifinputequalsxandz .tgtendswith⟨EOS⟩,then next 16. Addz toHeap next 17. Else 18. UpdateBeam(Beam,z ) next 19. IfHeapisfulland/orotherstoppingcriteriaaremet,then 20. Breakalltheloops 21. OutputPartial(Beam) 22. Z",
    "j 9. Else //makeapredictionatstepi 10. //wheng(i)inputwordsareobserved(storedininput) 11. CreateaheapBeamwithsize elements beam 12. Foreachhypothesisz ∈Z cur i−1 13. Foreachwordv ∈V k y 14. z =Extend(z ,v ,input) next cur k 15. Ifinputequalsxandz .tgtendswith⟨EOS⟩,then next 16. Addz toHeap next 17. Else 18. UpdateBeam(Beam,z ) next 19. IfHeapisfulland/orotherstoppingcriteriaaremet,then 20. Breakalltheloops 21. OutputPartial(Beam) 22. Z =Beam.PopAll() i 252 Chapter 5. Sequence-to-Sequence Models 23. i++ 24. ReturnHeap.Pop() OutputPartial(Beam) //Outputapartialresult 1. DisplaythebesthypothesisinBeam Anadvantageofthissystemisthattheoutputatstepiisimmediateoncewehaveseen x . Thisresultsinanonlinesequence-to-sequencesysteminwhichinputwordsarrive ≤g(i) inacontinuousstreamandpredictionscanbemadejustaftera“sufficient”numberofinput wordsareseen. While the search problem here seems simple, much remains to be done to define g(i). Clearly,g(i)isamonotonicallynon-decreasingfunction. Asasimpleexample,wecandefine g(i)=mforanyi. Thiswillmaketheabovealgorithmpreciselythesameasthestandardbeam searchalgorithmthatworkswithacompleteinputsequence. Bycontrast,inonlinesequence- to-sequencetasks,wewantg(i)tobeassmallaspossible,andsowecanstartcomputationas earlyaspossibleininference. Thesimplestcaseoftheseisthattheinputandoutputsequences aresynchronousinsomeway. Forexample,anautomaticspeechrecognitionsystemassigns eachspectralframeatranscriptionunit. Inthiscase,wehaveasimplecorrespondencebetween inputs and outputs: m=n (i.e., |x|=|y|), and x corresponds to y . Then, we can simply i i defineg(i)=i,inotherwords,eachtimeanewinputarrives,wemakeaprediction. Amorecomplicatedcaseisonlinesequence-to-sequenceproblemswithreordering,such assimultaneoustranslation,inwhichatarget-sidewordmaydependonsource-sidewords with long-range dependencies. A simple way to address this is to delay the predictions for anumberofsteps. Forexample,thewait-k methodforceseachpredictiontolagbehindthe inputsbyk words[Maetal.,2019]. Moreformally,thewait-k versionofthefunctiong(i)is definedtobe g(i) = min(m,k+i−1) (5.96) Here k is a hyper-parameter that controls how large a source-side context is considered in predictingtarget-sidewords. Whenk=∞,itisthesameasthestandardsearchmethodsfor sequence-to-sequenceinference. Insimultaneoustranslationandrelatedtasks,resultsarein generalsatisfactorybyusingasmallvalueofk. Acomparisonofdifferentg(i)isshownin Figure5.12. In some applications of online sequence-to-sequence problems, we may know when to performsearchandwhentoreadinputs. Forexample,ininteractivemachinetranslation [Casacuberta et al., 2009], the translation of a partial input sequence is triggered by some behaviors of users (such as the action of pressing buttons). In this case, we do not need to defineg(i),butviewitasaninputvariableofthemodel. Note that while one can directly employ pre-trained sequence-level models for online inference,developingsuchsystemsoftenrequiresadditionaltrainingeffort. Amoreprincipled approachtoonlinesequence-to-sequencemodelingistomodelthetransformationfromxto yasasequenceofactions[GrissomIIetal.,2014;ChoandEsipova,2016;Guetal.,2017; 5.4 Search 253 1 2 3 4 5 6 = = = = = = i i i i i i j=6 j=5 StandardSeq2Seq g(i)=m j=4 1-to-1Monotonic j=3 g(i)=i (m=n) Transduction j=2 j=1 Wait-k g(i)=min(m,k+i−1) j=0 (a)Visualizationofg(i). StandardSeq2Seq x x x x x x 1 2 3 4 5 6 y y y y y y y 0 1 2 3 4 5 6 1-to-1MonotonicTransduction x x x x x x 1 2 3 4 5 6 y y y y y y y 0 1 2 3 4 5 6 Wait-k(k=3) x x x x x x 1 2 3 4 5 6 y y y y y y y 0 1 2 3 4 5 6 (b)Actionsequences. Figure5.12: Visualization(top)andactionsequences(bottom)ofdifferentg(i)forapairof sequences(x=x ...x ,y=y ...y ). Inanactionsequence,acircledx standsfortheaction 1 6 1 6 j of reading a source-side word (x ), and a circled y stands for the action of predicting the j i probabilityofy givenx andy . Arrowsherestandfordependenciesbetweenwords. i ≤g(i) <i Becausey denotesthestartsymbol⟨SOS⟩,itcouldbegeneratedwithoutdependencieson 0 anywords. Zhengetal.,2019]. Forexample,anactioncanbeeitherapredictoperationthatperforms searchatthecurrentstep,orareadoperationthatacceptsanewinputword. Then,wecan framethetaskofdesigningthefunctiong(i)aslearningapolicytodeterminewhichactionis 254 Chapter 5. Sequence-to-Sequence Models takengivenasource-sideprefixx andatarget-sideprefixy . Andsequence-to-sequence ≤j <i models can be trained on the states of these action sequences so that they can make better predictionsconditionedonpartoftheinput. However,adiscussionoftrainingonlinesequence- to-sequence models lies outside the scope of this section. We refer the reader to the above papersformoredetailsonthesemethods. 5.4.4 Exact Search Fromaformalpointofview,wewouldideallyliketodevelopasystemwithnosearcherrors. Althoughapproximatesearchalgorithmshavebeenusedsuccessfullyinmanyapplications,itis importanttostudymodelerrorsofthesesystems,andthustofocusontheprobleminprinciple, notjustinpractice. Sodevelopingexactsearchalgorithmsforsequence-to-sequencemodels haslongbeenaninterestingtopicinNLPresearch. However,thesearchproblemforasimple word-basedmachinetranslationsystemwithn-gramlanguagemodelshasbeenfoundtobe anNP-hardproblem[Knight,1999]. Muchofearlierresearchformulatedthesearchproblem asclassicalcombinatorialoptimizationproblems,suchasthelinearprogrammingproblem andthetravelingsalesmanproblem,andemployedthecorrespondingsolvers[Germannetal., 2004;Zaslavskiyetal.,2009]. Additionalresearcheffortsexploredexactsearchalgorithmsfor statisticalmachinetranslationsystemsbyusingtheLagrangianrelaxationtechnique[Chang andCollins,2011;RushandCollins,2012]andfinite-stateautomata[deGispertetal.,2010; Allauzenetal.,2014]. Unlike these methods, which are more or less dependent on the integration of n-gram languagemodelsintosequence-to-sequencemodels,themodelsdescribedinthischaptertake asimplerform. Webeginwithabasicmodelinwhichthescoringfunctionscore(x,y)isthe log probability logPr(y|x). Eq. (5.68) tells us that logPr(y|x) can be written as a sum of word-levellogprobabilities,andlogPr(y|x)becomessmallerasmoretarget-sidewordsare generated(i.e.,alargern)11. Inotherwords,logPr(y|x)isamonotonicdecreasingfunction withrespecttothetarget-sidelengthn: foranyi,wehave logPr(y |x) =",
    "anNP-hardproblem[Knight,1999]. Muchofearlierresearchformulatedthesearchproblem asclassicalcombinatorialoptimizationproblems,suchasthelinearprogrammingproblem andthetravelingsalesmanproblem,andemployedthecorrespondingsolvers[Germannetal., 2004;Zaslavskiyetal.,2009]. Additionalresearcheffortsexploredexactsearchalgorithmsfor statisticalmachinetranslationsystemsbyusingtheLagrangianrelaxationtechnique[Chang andCollins,2011;RushandCollins,2012]andfinite-stateautomata[deGispertetal.,2010; Allauzenetal.,2014]. Unlike these methods, which are more or less dependent on the integration of n-gram languagemodelsintosequence-to-sequencemodels,themodelsdescribedinthischaptertake asimplerform. Webeginwithabasicmodelinwhichthescoringfunctionscore(x,y)isthe log probability logPr(y|x). Eq. (5.68) tells us that logPr(y|x) can be written as a sum of word-levellogprobabilities,andlogPr(y|x)becomessmallerasmoretarget-sidewordsare generated(i.e.,alargern)11. Inotherwords,logPr(y|x)isamonotonicdecreasingfunction withrespecttothetarget-sidelengthn: foranyi,wehave logPr(y |x) = logPr(y |x)+logPr(y |y ,x) ≤i <i i <i ≤ logPr(y |x) (5.97) <i Thisisalsocalledthemonotonicityofthescoringfunction. Then,bymakinguseofthemonotonicnatureofmodelscores,wecandevelopaheuristic toruleouthypothesesthatwouldneverbethebest. Letz betheglobalbestcomplete bestinall hypothesiswehavefound. Ifanewhypothesishasamodelscorelowerthanz .score, bestinall thenwewillnotneedtoextendit. Thuswecanexplorearegionthatissignificantlysmaller thantheoriginalsearchspace,withoutlossofoptimality. Notethatz .scorecontinues bestinall to become larger in search. It will be more difficult to find a better hypothesis and more hypotheseswillbeprunedawayasthesearchprocessproceeds. Seethepseudo-codebelow foranexactsearchalgorithmofthesequence-to-sequencemodelofEq. (5.68). 11Consider logPr(y|x) = (cid:80)n i=1 logPr(y i |y <i ,x). Since logPr(y i |y <i ,x) has a non-positive value, logPr(y|x)willbesmallerorunchangedifngrows. 5.4 Search 255 Algorithm: AnExactSearchAlgorithm ExactSearch(x) //Searchforthe“best”hypothesisbymakinguseofthemonotonicityofthe //scoringfunction(score(x,y)=logPr(y|x)). 1. Createapriorityqueue(max-heap)Queue 2. Createahypothesisz withz .score=−∞ best best 3. WhileQueueisnotemptydo 4. z =Queue.Pop() cur 5. If|z .tgt|>n ,then cur max 6. skipz andcontinuetheloop cur 7. Foreachwordv ∈V k y 8. z =Extend(z ,v ,x) next cur k 9. bound=z .score //alowerboundonmodelscores best 10. Ifbound<z .score,then//admissiblepruning next 11. Ifz .tgtendswith⟨EOS⟩,then next 12. z =z best next 13. bound=z .score next 14. Else 15. Addz toQueue next 16. Returnz best Thisisageneralalgorithmforexactsearch,anditssearchefficiencyisgreatlyinfluenced bythedesignofthepriorityqueue[Meisteretal.,2020]. Forexample,wecanviewscore(x,y) asthepriorityofeachhypothesisinthepriorityqueue,asinamax-heap12. Then,theresulting algorithmperformsaprocedureofbreadth-first-likesearch,sinceahypothesiswithashorter target-sidesequenceismorelikelytohaveahighermodelscoreandtobeatop-rankeditem inthepriorityqueue. Forefficientsearch,however,wewishtofindcompletehypothesesas early as possible, such that more unpromising hypotheses can be thrown away in the early stageofsearch. Todothis,wecanbiasthepriorityofahypothesistowardsalongertarget-side sequence. Thisprovidesadepth-firstsearchalgorithmwhichismorelikelytofindcomplete hypothesesinashortertime[StahlbergandByrne,2019]. Whiletheexactsearchalgorithmbecomesapparentbyconsideringthemonotonicityof Pr(y|x), in practical systems, as discussed in Section 5.4.1, score(x,y) often has a more complexforminvolvinglengthrewardornormalization,andsothemonotonicpropertydoes nothold. Fortunately,theassumptionofmonotonicitycanbedroppedattheexpenseofslightly relaxingthelowerboundonmodelscoresforpruning. Herewedefineboundtobethelowest model score that a hypothesis should have so that it can at best be extended to an equally good hypothesis with z . For example, consider a simple word reward model described best in Eq. (5.69): Score(x,y)=logPr(y|x)+λ·n. For a hypothesis z , there are at most next 12Wecanimplementapriorityqueueusingamax-heap. 256 Chapter 5. Sequence-to-Sequence Models n −|z .tgt|wordswecanpredicttoobtainacompletehypothesis. Supposeallthese max next n −|z .tgt|wordsarepredictedwithaprobabilityof1. Then,themodelscoreofthe max next resultinghypothesis(denotedbyz )willbegivenby new n (cid:88)max z .score = z .score+ (log1+λ) new next i=|znext.tgt|+1 = z .score+λ·(n −|z .tgt|) (5.98) next max next Usingthisresult,wecandefineboundas bound = z .score−λ·(n −|z .tgt|) (5.99) best max next Analternativewaytoderivethelowerboundistosimplyconsidern timesofword max reward,givenby bound = z .score−λ·n (5.100) best max Thisisalooselowerboundandleadstolesspruning. Inthecaseoflengthnormalization,wecandothisinasimilarway. Forexample,consider logPr(y|x) the length normalization model Score(x,y)= , as in Eqs. (5.72-5.73). A lower n boundonadmissiblemodelscoresisgivenby Pr(z .tgt|x) next bound = (5.101) n max In practice, such a lower bound can be defined in several different ways to guarantee the optimalityofsearch,dependingonwhichmodelandsearchstrategyareusedinthesequence- to-sequencesystems[Huangetal.,2017b;StahlbergandByrne,2019]. Wecaneasilyapplytheselowerboundstotheaboveexactsearchalgorithmbyreplacing line9withEq. (5.99)or(5.101). Asasideeffect,thesearchwillexploremorehypothesesand thusbemuchslower. 5.4.5 Differentiable Search Wehaveaddressedthesearchproblemthroughtheintroductionofheuristicsearchalgorithms inwhichwetrytominimizethescoringfunctiononasetofsequencesofdiscretevariables. An alternative possibility is to relax these discrete variables to continuous variables and to formulatetheproblemusingtheframeworkofcontinuousoptimization[Hoangetal.,2017; Kumar et al., 2021]. While we try to use a consistent notation throughout this book, it is convenientheretointroducesomenewnotationthatisslightlydifferentfromthatadoptedin thepreviouschapters. Wewilluseavectoryw∈{0,1}|Vy|todenotetheone-hotrepresentation i ofy . SupposetheoutputatstepiisadistributionoverV ,denotedbyPr(·|y ,x). Then,we i y <i 5.4 Search 257 canwritethelogprobabilityofy atstepiasadotproductoftwovectors,likethis i logPr(y |y ,x) = yw·logPr(·|y ,x) i <i i <i = yw·logPr(·|yw...yw ,x) (5.102) i 0 i−1 wherey =y ...y isrepresentedasasequenceofone-hotvectorsyw...yw . Asdiscussed <i 0 i−1 0 i−1 inChapter3,theright-handsideoftheaboveequationmeanstheselectionoftheentryy of i thevectorlogPr(·|y ,x)(orlogPr(·|yw...yw ,x)). <i 0 i−1 Usingthisnotation,wecanwritelogPr(y|x)as n (cid:88) logPr(y|x) = logPr(y |y ,x) i <i i=1 n (cid:88) =",
    "|y ,x) = yw·logPr(·|y ,x) i <i i <i = yw·logPr(·|yw...yw ,x) (5.102) i 0 i−1 wherey =y ...y isrepresentedasasequenceofone-hotvectorsyw...yw . Asdiscussed <i 0 i−1 0 i−1 inChapter3,theright-handsideoftheaboveequationmeanstheselectionoftheentryy of i thevectorlogPr(·|y ,x)(orlogPr(·|yw...yw ,x)). <i 0 i−1 Usingthisnotation,wecanwritelogPr(y|x)as n (cid:88) logPr(y|x) = logPr(y |y ,x) i <i i=1 n (cid:88) = yw·logPr(·|yw...yw ,x) (5.103) i 0 i−1 i=1 Provided we use logPr(y|x) as the objective function (i.e., score(x,y)=logPr(y|x)), thesearchproblemcanbeformulatedas n (cid:88) yˆw...yˆw = argmax yw·logPr(·|yw...yw ,x) (5.104) 0 n i 0 i−1 yw...yw 1 n i=1 Thisisequivalenttothestandardformforinferenceofsequence-to-sequencemodels,givenby yˆ = yˆ ...yˆ 0 n = argmaxPr(y ...y |x) (5.105) 0 n y0...yn GivenEq. (5.104),wecannowrelaxeachone-hotvectortoareal-valuedvectorwitha constraintthatthesumofallitsentriesisequalto1,thatis, yw ∈ +R|Vy| (5.106) i s.t. ||yw|| = 1 (5.107) i 1 Inthisway,yw canbeinformallytreatedasa|V |-dimensionalembeddingofy ,though i y i it has much more dimensions than the usual embeddings used in NLP. Now yw does not i correspondtoaspecificwordinthevocabulary,butdescribesadistributionoverthevocabulary. InHoangetal.[2017]’swork,yw·logPr(·|yw...yw ,x)iscalledtheexpectedembedding i 0 i−1 underthedistributionlogPr(·|yw...yw ,x). Whatisinterestingaboutthisformulationisthat 0 i−1 Eq. (5.104) in fact defines a “new” task in which we try to maximize a sum of continuous variables(i.e.,asumofnexpectedembeddings). We can solve Eq. (5.104) by using the off-the-shelf toolkits in optimization. Since we have a constraint that yw is a variable in a simplex13, it is straightforward to apply general i 13Simplexisatermusedingeometry.InaEuclideanspace,ak-simplexisak-dimensionalpolytopedescribed 258 Chapter 5. Sequence-to-Sequence Models constrainedoptimizationalgorithmstothisproblem. Analternativewayistousealgorithms thataredesignedtosolvetheoptimizationproblemwithsimplexconstraints. Thedetailsof thesealgorithmscanbefoundinmanybooksonoptimization. A third choice of solving Eq. (5.104) is to formulate the constraints in the objective functionexplicitlyandtousegradientdescentmethodstooptimizethisfunction. Forexample, Hoangetal.[2017]modifyEq. (5.104)andobtainanewformforoptimization n (cid:88) yˆw...yˆw = argmax Softmax(yw)·logPr(·|yw...yw ,x) (5.111) 0 n i 0 i−1 yw...yw 1 n i=1 Hereweremovethesimplexconstraintfromyw,andimposeitonanewoutputthatisproduced i byaSoftmaxfunction. Once we have obtained the optimal sequence yˆw...yˆw, we need to map each yw to a 0 n i uniqueword. Asimplemethodistotakethewordcorrespondingtotheentryofyw withthe i largestvalue. However,thismaybreaktheoptimalityofthesolutionbecausethecondition yw...yw is changed when these variables are discretized. A more practical method is to 0 i−1 perform optimization to predict the next word given a prefix, say, we fix yw...yw to the 0 i−1 one-hotrepresentationsoftheoptimalprefix,andmaximize (cid:80)n yw·logPr(·|yw...yw ,x). k=i k 0 k−1 Then,weselectthebestwordatpositioniandmoveontothenextposition. So far we have assumed that the search objective is derived from the log probability logPr(y|x)andthelengthoftheoutputisgiveninadvance. Tohaveasearchoversequences withdifferentlengths,wecanrepeattheaboveoptimizationprocedureforeveryn∈[1,n ], max andselectthesequencewiththemaximumscore. Thisalsomakesiteasytointroducelength normalization and reward into search. We can ignore the length bias issue in each search with a fixed n, and add the length models after optimization, that is, we leave the search objectiveunchanged,but,inthefinalstep,weselectthebestsequenceinasetofcandidates withdifferentnintermsofscore(x,y). 5.4.6 Hypothesis Diversity Multipleoutputsareoftenrequiredwhenonewantstorescoretheseoutputsand/orinteract with the system. One of the most widely used methods is to use beam search to generate a numberoftop-rankedhypotheses. Forexample,wecansimplyviewtheelementsofHeap asthek-besthypothesesinbeamsearch(seeSection5.4.2). However,thisapproachsuffers fromtheproblemthatthereisoftenlittledifferenceamongthehypothesesinthebeam,and byasetofk+1independentpoints{p 0 ,p 1 ,...,p k }.Thispolytopeisdefinedasasetofpoints P k-simplex = {a 0 ·p 0 +a 1 ·p 1 +...+a k ·p k } (5.108) where k (cid:88) a i = 1 (5.109) i=0 a i ≥ 0 foranyi∈[0,k] (5.110) 5.4 Search 259 Rank Output 1 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor anewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek for“ThePriceIsRight”whenBobBarker,91,showeduptorunhisoldshow. 2 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor anewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek for\"ThePriceIsRight\"whenBobBarkershoweduptorunhisoldshow. 3 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor anewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek for\"ThePriceIsRight\"whenBobBarker,91,showeduptoruntheshow. 4 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor anewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek for\"ThePriceIsRight\"whenBobBarker,91,showeduptorunhisshow. Table5.3: 4-bestoutputsofatextsummarizationsystemonasampleintheCNN/DailyMail dataset(beamsize=4). Weseethatthesetextsdifferonlybyafewwords. it is difficult to figure out which one is better though more options are available to users. Table 5.3 shows the 4-best outputs of a text summarization system. We see that these texts arefairlysimilartoeachother. Onereasonforthisphenomenonisthatdiversehypotheses, though probably with",
    "It’sbeenabusyweek for\"ThePriceIsRight\"whenBobBarker,91,showeduptoruntheshow. 4 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor anewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek for\"ThePriceIsRight\"whenBobBarker,91,showeduptorunhisshow. Table5.3: 4-bestoutputsofatextsummarizationsystemonasampleintheCNN/DailyMail dataset(beamsize=4). Weseethatthesetextsdifferonlybyafewwords. it is difficult to figure out which one is better though more options are available to users. Table 5.3 shows the 4-best outputs of a text summarization system. We see that these texts arefairlysimilartoeachother. Onereasonforthisphenomenonisthatdiversehypotheses, though probably with high model scores when completed, will be pruned away if they are low-rankedinsomestagesofbeamsearch. Fromamodelingperspective,wecaninterpretthis asaproblemwiththelocallynormalizedmodelsthatweusehere: everypredictionismade onanintermediatestepofsearch,andthereisnowayforthefollowingstepstoescapeifthe prefixisfixed[MurrayandChiang,2018]. Oneapproachtoimprovingthehypothesisdiversityistogivepenaltiestocaseswherethe hypothesesinthebeamarelessdiverse[LiandJurafsky,2016;Vijayakumaretal.,2018]. A simpleexampleofsuchobjectivefunctionsisgivenby score (x,y) = score(x,y)−λ·dp (5.112) d Itcombinestheoriginalmodelscorescore(x,y)andadiversitypenaltydp. dpcanbedefined inafewdifferentways. Anideaistopenalizehypothesesthatarecloseinthesearchtree. For example,onecandefinedpastherankofahypothesisinthesetofitssiblingsthatareextended from the same parent hypothesis, and so the beam can spread its members over a larger regionofthespaceofhypotheses[LiandJurafsky,2016]. Anotherwaytointroducediversity measuresistoconsiderthedifferencesbetweenthetarget-sidesequencesofthehypotheses inthebeam. Forexample,wecandefinedpastheaveragestringsimilaritybetweenagiven hypothesisandotherhypothesesinthebeam[Xiaoetal.,2013]. The above idea can also be expressed as constraints imposed on the search procedure. For example, we can constrain the beam to include only the hypotheses that are rooted at 260 Chapter 5. Sequence-to-Sequence Models differentparentsinthelaststep[Boulanger-Lewandowskietal.,2013]. Moreprecisely,for eachhypothesisz ∈Z ,weseekthebestnext-stephypothesisby cur i−1 zˆ = argmax Pr(z .tgt|x) (5.113) next next znext∈Extend(zcur,Vy) Thehypothesiszˆ isthenaddedtoZ . Notethatthisisessentiallyasub-spacemethodthat next i dividesaspaceofhypothesesintosub-spacesofhypotheses,andcollectsresultsoverthese sub-spaces. Anintuitionbehindthismethodisthatdifferentsub-spacescandescribedifferent aspectsoftheproblem,andsowecanhavediversesolutions. Anotherapproachtoaddressingthediversityissueistoperturbbeamsearchbyintroducing randomly generated hypotheses into the beam [Holtzman et al., 2020a; Wiher et al., 2022]. Onecommonwaytodothisistochoosesomerandomwordsforextendingahypothesis,and to add the extended hypotheses to the beam. In general, these words can be sampled from the distribution Pr(·|y ,x) over the entire vocabulary or its subset. Randomness can also <i beaddedtotheinputsofasystemattesttime. Forexample,onecanexpressaninputword asalinearcombinationofitsoriginalembeddingandtheembeddingofawordofarandom sequence drawn from the training data [Li et al., 2021b]. In problems having many local minima,addingrandom“noise”tosearchproceduresisgenerallyhelpful,aswecanexplore morediversehypothesesandpreventthesystemsfromgettingstuckincertainregionsofthe searchspace. Insteadofperformingsearchusingasinglesystem,wecanusemultiplesystemstoobtain diversehypotheses. Thesesystemscanbebuiltoneitherdifferentarchitecturesordifferent hidden structures/configurations [He et al., 2018; Shen et al., 2019; Wu et al., 2020a; Sun etal.,2020a]. Althoughmethodsofthistypedonotfallunderthesearchframeworkthatwe havebeendiscussing,combiningtheresultsfrommultiplesystemsisgenerallyhelpful. The followingsectionwillpresentadiscussiononthisissue. 5.4.7 Combining Multiple Models From a machine learning point of view, ensembling are methods for addressing modeling issues,notsearchissues. Inthissubsection,wediscussthesemethodsbecausetheirimplemen- tationstypicallyrequiremodificationstothesearchmodules,andwecangainsomeinsight intotheresultingsystembyviewingitfromthesearchperspective. In machine learning, ensemble methods aim to make better predictions by combining predictions of a number of constituent systems or component systems. The problem of combiningmultiplesystemshasbeendiscussedextensivelyintimeswhenstatisticalmodels emerged in NLP, and is sometimes called system combination methods for emphasizing its practical use. For sequence-to-sequence models discussed here, a widely used form of systemcombinationisanaverageofpredictions[Sutskeveretal.,2014]. SupposewehaveK sequence-to-sequencemodelsthathavebeentrained. Thelogprobabilityofthetarget-side wordy givenitsleftcontexty andthesource-sidesequencexcanbedefinedbyusingthe i <i 5.4 Search 261 geometricaverage K 1 (cid:88) logPr(y |y ,x) = logPr (y |y ,x) (5.114) i <i k i <i K k=1 oralternativelybyusingthearithmeticaverage K 1 (cid:88) logPr(y |y ,x) = log Pr (y |y ,x) (5.115) i <i k i <i K k=1 wherePr (y |y ,x)istheoutputofthek-thcomponentsystem. Theseformsaresosimple k i <i thatonecanimplementthemforanysequence-to-sequencemodelswithoutsignificantmodifi- cationstoexistingsystems,andtheyhavebeenusedasthebasisofmanysuccessfulsystems invariousevaluationtasks[Barraultetal.,2020;Akhbardehetal.,2021]. A problem with prediction averaging is that all the component systems are required to followthesamebasicformofmodeling(seeEq. (5.68))andweneedtohaveaccesstothe probabilities{Pr (y |y ,x)}. Whenwehaveonlyasetofblack-boxsystemsinhand,we k i <i needtoperformsequenceensembling. Acommonideaistovotefromtheensembleofthe sequencesproducedbythecomponentsystems. Forexample,oneofthesimplestwaystodo thisishypothesisselection[HildebrandandVogel,2008],inwhichwesimplyselectthe“best” sequencefromtheensembleusingsomecriterion. Analternativewayofsequenceensembling istoregenerateanewsequencedifferingfromanyoftheoriginalsequences[Matusovetal., 2006;Rostietal.,2007]. Thistypicallyrequiresamodelthatrepresentsthesequencesintoa compactrepresentation(suchasalattice),aswellasanadditionalsearchpassbywhichwe canfindthebestoutputinthisnewrepresentationofhypotheses(suchaslatticesearchand rescoring)[Deorasetal.,2011;Stahlbergetal.,2016;Khayrallahetal.,2017]. Notethattheensemblingofsequence-to-sequencemodelsisrelatedtothediversityissue discussedintheprevioussubsection. Itisoftenthoughtthatcomponentsystemsneedtobe diverse for a better ensembling result, and so we need to build these systems in some way thatwecanmakethemdifferent[Sutskeveretal.,2014;Zhouetal.,2017]. Oneofthemost popularmethodsischeckpointensembling. Ittakesanumberofcopiesofamodelatdifferent checkpointsduringtraining,andcombinesthesemodelcopiesviapredictionaveraging. This methodcanbeusefulforalleviatingtheoverfittingprobleminpractice. Also,differentmodels canbecreatedfromabasemodelunderdifferentsettings. Forexample,wecanbuildmodels with different numbers of parameters on the basis of a backbone model. A more general approachistotakemodelsbasedondifferentarchitectures,althoughthisisattheexpenseof moredevelopmenteffort. Anotherwaytoviewsequenceensemblingisthatitprovidesatwo-passsearchscheme. Inthefirstpassofsearch,multiplesystemsareusedtoperforminferenceindividually. Each ofthesesystemshasitsownbiasformodelingandsearch,andexploresdifferentregionsof thesearchspace. Ahypothesisexploredbyonesystemmightnotbeseenandevaluatedby 262 Chapter 5. Sequence-to-Sequence Models anothersystem. Theresultofthispassisadiverseensembleofhypothesesthatare“optimal” fromsomeperspectives. Inthesecondpassofsearch,weusethisensembletodefineanew spaceofhypotheses,anduseafine-grainedmodeltosearchforthefinalresult. 5.4.8 More Search Objectives In this subsection, we consider more objective functions that can be applied to the",
    "different numbers of parameters on the basis of a backbone model. A more general approachistotakemodelsbasedondifferentarchitectures,althoughthisisattheexpenseof moredevelopmenteffort. Anotherwaytoviewsequenceensemblingisthatitprovidesatwo-passsearchscheme. Inthefirstpassofsearch,multiplesystemsareusedtoperforminferenceindividually. Each ofthesesystemshasitsownbiasformodelingandsearch,andexploresdifferentregionsof thesearchspace. Ahypothesisexploredbyonesystemmightnotbeseenandevaluatedby 262 Chapter 5. Sequence-to-Sequence Models anothersystem. Theresultofthispassisadiverseensembleofhypothesesthatare“optimal” fromsomeperspectives. Inthesecondpassofsearch,weusethisensembletodefineanew spaceofhypotheses,anduseafine-grainedmodeltosearchforthefinalresult. 5.4.8 More Search Objectives In this subsection, we consider more objective functions that can be applied to the search problem. 1. Search with Future Scores Mostofthealgorithmsdescribedinthissubsectioncanbeviewedassomeoptimizationsof best-firstsearchalgorithms[Meisteretal.,2020]. Asanotherexampleofbest-firstsearch,A* searchiswidelyconsideredtobeagoodsolutiontothegeneralsearchproblem. VanillaA* searchrequiresthatallstatesofsearcharesortedineverysearchstep,whichisintractablein ourproblems. Wethereforestillconsiderbeamsearchandgreedysearchforourdiscussion, butuseanA*search-likeobjectivefunctioninstead. Specifically,givenasearchstate(x,y ), ≤i theA*search-likeobjectivefunctioncanbedefinedas score (x,y ) = g(x,y )+h(x,y ) (5.116) A* ≤i ≤i ≤i Hereg(x,y )istherewardofthepathfromthestartstateto(x,y ),andh(x,y )isthe ≤i ≤i ≤i estimatedrewardofthe“optimal”pathfrom(x,y )tothefinalgoal. Becauseg(x,y )and ≤i ≤i h(x,y )canhavearbitraryforms,thisframeworkisverygeneral. Forexample,ifwedefine ≤i g(x,y ) = score(x,y ) (5.117) ≤i ≤i h(x,y ) = 0 (5.118) ≤i thenscore (x,y)isexactlythesameastheobjectivefunctionsdiscussedpreviously. A* Tomakefulluseofthisformulation,itseemsnaturaltoseekafunctionoffuturereward orfuturecost. Ideally,wewouldlikeh(x,y )tobeabletocomputehowmuchadditional ≤i rewardwecanobtainifweextend(x,y )tothebestcompletehypothesis. Thisis,however, ≤i intractablebecauseweneedtoexploreallthehypothesesextendedfrom(x,y )andfindthe ≤i bestone. Itiscommonpracticetouseacomputationallycheapermodelanalogoustothereal futurerewardmodel. Conventionalapproachesrelyonheuristicstodefineh(x,y )[Koehn ≤i etal.,2007],suchasestimatingtheweightsofthewordsthatcouldbefurthergenerated. These heuristicscanbegeneralizedtotheknowledgeofthemodeldesignofsequence-to-sequence systems[Heetal.,2017;Zhengetal.,2018]. Amoregeneralapproachistouseavalue-based treatment of the problem [Ren et al., 2017; Li et al., 2017a; Leblond et al., 2021]. We can develop a policy that learns to predict the distribution of y given x and y , and a value i <i function for this policy that learns to predict future rewards. Eq. (5.116) can therefore be interpretedasalinearcombinationofthepolicyscoreof(x,y )andthecorrespondingvalue. ≤i Suchatreatmentofsearchobjectivesfallsintotheframeworkofvalue-basedsearch,andhas beensuccessfullyemployedinreinforcementlearning[Silveretal.,2017]. 5.4 Search 263 2. Search with Language Models For a long time, language models played an important role in text generation tasks. For example, statisticalmachinetranslationsystemsandautomaticspeechrecognitionsystems typically rely on large n-gram language models to produce fluent texts. While modern sequence-to-sequence models are not required to have separate language models, applying themtosequence-to-sequencesearchstillmakesintuitivesenseformachinetranslationand relatedproblems. Followingtheconventionthatalanguagemodelcanbetreatedasafeatureofalog-linear (orlinear)model[OchandNey,2002],thelanguagemodel-augmentedobjectivecanbedefined as score (x,y) = logPr(y|x)+λ·logPr(y) (5.119) lm Thisformulationdoesnotinvolvelengthrewardandnormalizationterms,buteitherofthem can be easily used as an additional feature of the model. In general, the language model Pr(y)istrainedsolelyontarget-sidesequences,enablingtheuseoflarge-scalemonolingual datainsequence-to-sequencemodels[Gulcehreetal.,2017]. Interestingly,ithasbeenfound thatcurrentsequence-to-sequencemodelsarestronglanguagemodelsthemselvesiftheyare trainedsufficiently,andabetterwaytomakeuseoftarget-sidedatamightbetouseittocreate syntheticdata,calleddataaugmentation. Anexampleofthisisbacktranslationinwhichwe useabackwardtranslationsystemtotranslatetarget-sidesentencestosource-sidesentences, andthenusethissyntheticbilingualdataasadditionaldatafortrainingaforwardtranslation system[Sennrichetal.,2016a;Edunovetal.,2018]. Inmanytasks,suchasimplemethodcan achievesignificantimprovementsintranslationquality,butthisresultquestionsthenecessity ofusingadditionallanguagemodelsinneuralmachinetranslation. NotethatthemodelofEq. (5.119)dependsonourchoiceforthecoefficientλ. Formachine translation,weareusuallyinterestedinapositivevalueofλsothatoursystemcanproduce more fluent texts. By contrast, a negative value of λ means that we want some output that is less frequent. For example, if λ=−1, then Eq. (5.119) can be written as the point-wise mutualinformationofxandy score (x,y) = logPr(y|x)−logPr(y) lm Pr(x,y) = log (5.120) Pr(x)·Pr(y) This scoring function has been shown to be useful for generating more diverse outputs for neuralconversationsystems[Lietal.,2016]. 3. Minimum Bayes Risk Search So far, our discussion of search objectives has focused on the use of the decision rule of choosingthehighestscorehypothesis,calledmaximumaposteriori(MAP)search14. An 14Instatistics,MAPisamethodforinferenceoftheparametersofastatisticalmodel.Supposewehaveamodel thatdescribesthedistributionofavariablexandthemodelisparameterizedbyθ.MAPseekstheoptimalvalueof 264 Chapter 5. Sequence-to-Sequence Models assumptionbehindthismethodisthattheposteriorprobabilityPr(y|x)(orthemodelscore score(x,y))correlateswiththetruequalityofoutputs. Inpractice,thisassumptionleadsto several useful properties, e.g., the search system is easy to implement, and the objective of searchisconsistentwiththatoftraining. However,therearesomeshortcomingswithMAP search,whichcausesresearcherstoconsidermorepowerfulmethods. OneproblemwithMAP search is that the objective does not reflect the way one evaluates the system. The metrics used in end-to-end evaluation of a system may have very different forms from Pr(y|x). A secondproblemisthatMAPisjustaspecialcaseoftheBayesiantreatmentofdetermining posterior probabilities. It provides a point estimate of θ with no uncertainty measure, and issometimesoverconfident. Insomeapplications,sequence-to-sequencemodelsspreadtoo muchprobabilitymassacrossmanydifferenthypotheses[Ottetal.,2018a],andMAPmaynot describethemajorportionofthedistribution. HereweconsiderminimumBayesrisk(MBR)searchthatprovideswaystointroduce evaluationmeasuresintosearch,aswellaswaystomakeuseofthedistributionsoverhypothe- ses. TheMBRmethodassumesariskfunctiononapairofsequences,denotedbyR(y,y ). It r computesthecostofreplacingy withy intermsofsomeevaluationmetric. Forexample, r wecandefinetheriskscoretobe1−BLEUformachinetranslation. Then,theriskforyon asetofsequencesΩisgivenbytheexpectationofR(y,y )withrespecttothedistribution r Pr(y |x) r Risk(y) = E R(y,y ) yr∼Pr(yr|x) r (cid:88) = R(y,y )·Pr(y |x) (5.124) r r yr∈Ω However,thesummationoverallpossibletarget-sidesequencesiscomputationallyinfeasi- ble. WethereforedefineΩtobethek-bestoutputsorsampledoutputsofasystem[Eikema andAziz,2020],denotedbyΩ . Then,wetakescore(x,y)=−Risk(y)andobtainthe system θbymaximizingtheprobabilityofθgivenx,writtenas θˆ = argmaxPr(θ|x) (5.121) MAP θ θˆ isalsocalledthemodeoftheposteriordistributionofθ. FortheMAPsearchproblemhere,wesimply MAP denoteθbyyandseekthemodeofPr(y|x). AsaBayesianmethod,wecanre-expresstheaboveequationusingtheBayes’rule Pr(x|θ)·Pr(θ)",
    "computesthecostofreplacingy withy intermsofsomeevaluationmetric. Forexample, r wecandefinetheriskscoretobe1−BLEUformachinetranslation. Then,theriskforyon asetofsequencesΩisgivenbytheexpectationofR(y,y )withrespecttothedistribution r Pr(y |x) r Risk(y) = E R(y,y ) yr∼Pr(yr|x) r (cid:88) = R(y,y )·Pr(y |x) (5.124) r r yr∈Ω However,thesummationoverallpossibletarget-sidesequencesiscomputationallyinfeasi- ble. WethereforedefineΩtobethek-bestoutputsorsampledoutputsofasystem[Eikema andAziz,2020],denotedbyΩ . Then,wetakescore(x,y)=−Risk(y)andobtainthe system θbymaximizingtheprobabilityofθgivenx,writtenas θˆ = argmaxPr(θ|x) (5.121) MAP θ θˆ isalsocalledthemodeoftheposteriordistributionofθ. FortheMAPsearchproblemhere,wesimply MAP denoteθbyyandseekthemodeofPr(y|x). AsaBayesianmethod,wecanre-expresstheaboveequationusingtheBayes’rule Pr(x|θ)·Pr(θ) θˆ = argmax MAP Pr(x) θ = argmaxPr(x|θ)·Pr(θ) (5.122) θ whereθistreatedasavariablehavingapriordistributionPr(θ). Bycontrast,MLEdirectlymaximizesthelikelihoodfunctionPr(x|θ) θˆ = argmaxPr(x|θ) (5.123) MLE θ Thus,theMAPresultcanbeviewedasanestimationofθthatconsidersbothMLEofxgivenθandthepriorofθ. NotethatMAPandMLEwillbeequivalentifPr(θ)isauniformdistribution. 5.5 Summary 265 followingobjectiveforMBRsearch yˆ = argmax−Risk(y) y (cid:88) = argmin R(y,y )·Pr(y |x) (5.125) r r y yr∈Ωsystem This model is very general and applies to a wide range of NLP problems in which one needstosearchforanoptimalhypothesisinalargesetofcandidates[Goodman,1996b;Goel and Byrne, 2000; Kumar and Byrne, 2004b]. It allows for flexible forms of risk functions, for instance having various factors considered in evaluating hypotheses. MBR search has recentlybeenofinteresttoNLPresearchersastheyarefoundtobeeffectiveineliminatingthe biasescausedbyMAPsearch[MüllerandSennrich,2021;Freitagetal.,2022]. Inadditionto providingaformulationofsearchobjectives,MBRmethodscanbeusedfortrainingsequence- to-sequencemodels,andarethoughttobesolutionstothediscrepancyissuebetweenobjectives oftrainingandevaluation[Shenetal.,2016]. 5.5 Summary Inthischapter,weattemptedtoprovideanoverviewofsequence-to-sequencemodelingwhich can serve as the basis for many NLP systems. Sequence-to-sequence modeling is a very rich area of research, and has been widely discussed in different disciplines, even beyond NLP. This chapter is not a review of all the literature on this subject (this would be a big project),butfocusesonsomeofthecoremethodsandideas. Westartedwithanintroduction ofsequence-to-sequenceproblems,aswellastheencoder-decoderarchitecturewhichlaysthe foundationsformostofthestate-of-the-artsequence-to-sequencesystems. Asanillustrationof theapplicationofthisarchitecture,weconsideredtheproblemofneuralmachinetranslation, andbuiltasimpleneuralmachinetranslationmodelusingthebasicknowledgewehavelearned sofar. Wealsopresentedtheattentionmechanismandaseriesofrefinements. Ifwelookback to the past few years, we will find that exploring attention models is the next natural step in developing sequence-to-sequence models. While these models are well known for their applicationandimpressiveperformanceinmachinetranslation,theyhavedominatedtheNLP community. Thereisalsogreatinterestinattentionmodelsinsomeothersub-fieldsofAI,such ascomputervision[BorjiandItti,2012;Xuetal.,2015;Jaderbergetal.,2015]andspeech processing[Chorowskietal.,2015;Chanetal.,2016;Bahdanauetal.,2016]. Theresultis thatthepastfewyearswereanexcitingtimeforpeopleintheseareas. Sequence-to-sequencemodelsaresosuccessfulthatwetrytoputeverythinginthesame pocket. Notonlyhavewedevelopedpowerfulsequence-to-sequencemodelstodealwithvery generalproblems,butcurrentresearchisforcedtobeunifying. AnexampleisthatTransformer, aself-attention-basedsequence-to-sequencemodel,hasbecomeoneofthefundamentalmodels formanytasksrangingoverdifferenttypesofdata,fromtextualtovisualandacousticdata. It canevenbeextendedtodealwithmultimodalproblemswhicharesometimesmorechallenging. 266 Chapter 5. Sequence-to-Sequence Models Thismakesthingsmoreinterestingandexciting: animprovementtoonemodelcanbeused to improve systems in a variety of tasks. And we are seeing a significant change in our research paradigm in which the NLP and machine learning fields are marrying and results in NLP research are becoming more influential. However, on the other side of the coin is thatwearemakingmuchroomforsomeoftheproblemsbutleavinglessroomfortheothers. In recent NLP conferences, we can see many, many papers talking about how to train big sequence-to-sequencemodelsandapplythemtodifferenttextgenerationtasks,buttherearea relativelysmallnumberofpapersonparsing. Therehavealwaysbeendebatesonthisoverthe pastfewdecades,forexample,whatandhowmuchpriorknowledgedoweneedtobuildan NLPsystem? [Church,2011;See,2018]Gettinginvolvedinsuchdebatesissimplybeyondthe discussionsinthischapter. Fortunately,NLPresearchpromisestocontinuetobediverseand active,andwecanalwayshearandlearnfrombothsidesofthedebates. Forexample,there areinterestingfindingsthattheneuralsequencemodelscanlearnsomelinguisticproperties fromdata,andlinguisticstructurescanhelpsystemdesign. InChapter6,wewillseeafew examples. The“bias”ofresearchfocusalsoexistsonthemachinelearningsideofproblem-solving. Forexample,forsequence-to-sequenceproblemsdiscussedhere,recentyearshavewitnessed adrasticincreaseofinterestinmodeldesignandtrainingmethods,butonlyarelativelysmall group of people discuss the search problem. While search is a classical problem in AI and playsanimportantroleinpracticalsystems[RussellandNorvig,2010],itisevennotdiscussed inrecenttutorialsandsurveysinNLP.Thismotivatesustowriteasectiononthissubjectso thatwecanhaveamorecompletepictureoftheproblem. However,ourgeneraldiscussion doesnotcoverallaspectsofthesearchproblem. Atopicweleftoutisefficiency[Birchetal., 2018;Heafieldetal.,2021]. Whilethischapterincludessomediscussionsontheefficiency issue,suchasstoppingcriteriaofsearchalgorithms,efficientmethodsareawide-rangingtopic andaregenerallydependentonmodelarchitectures. Amoredetaileddiscussionofthemcan be found in Chapter 6. Another topic that one may be interested in is constrained search inwhichconstraintsareimposedonthesearchprocess[HokampandLiu,2017;Anderson etal.,2017]. Ingeneral,theseconstraintscomefromourpriorknowledgeorinteractionswith users. Forexample,constrainedsearchhasbeenusedtoenforcetermtranslationconstraints onmachinetranslation[Hasleretal.,2018;PostandVilar,2018]. One last note on limitations of this chapter. The formulation of the general sequence- to-sequence problem described here is based on the left-to-right factorization of Pr(y|x), resultinginanautoregressivemodel. Onelimitationofthisformulationisthateachprediction atsomestepdependsonlyontheprecedingwords,andsothemodelcannotaccesstheright context. To make use of the right context of a word, a simple approach is to build another modelthatperformsright-to-leftgeneration. Theleft-to-rightandright-to-leftmodelscanthen becombinedtogenerateabetteroutputsequence[Liuetal.,2016a;Hoangetal.,2017;Zhang etal.,2018b;2020a]. Analternativeapproachisgivenbynon-autoregressivegenerationor non-autoregressivedecodinginwhichtheconstraintofautoregressivegenerationisremoved andeachwordpredictionisconditionedontheglobalcontext[Guetal.,2018;Ghazvininejad etal.,2019;Leeetal.,2020]. Anicepropertyofnon-autoregressivegenerationisthepossibility 5.5 Summary 267 ofsystemspeed-up,sinceallthewordsinasequencecanbegeneratedinparallelandwecan dothisefficientlyusingGPUs. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 6 Transformers So far we have discussed several basic models for solving sequence-to-sequence problems. We now explore a new class of models which are based on a powerful architecture, called Transformer. TransformersdifferinseveralwaysfromthemodelsgiveninChapters4and 5. First, they do not depend on recurrent or convolutional neural networks for modeling sequences of words, but use only attention mechanisms and feed-forward neural networks. Second,theuseofself-attentioninTransformersmakesiteasiertodealwithglobalcontexts anddependenciesamongwords. Third,Transformersareveryflexiblearchitecturesandcan be easily modified to accommodate",
    "explore a new class of models which are based on a powerful architecture, called Transformer. TransformersdifferinseveralwaysfromthemodelsgiveninChapters4and 5. First, they do not depend on recurrent or convolutional neural networks for modeling sequences of words, but use only attention mechanisms and feed-forward neural networks. Second,theuseofself-attentioninTransformersmakesiteasiertodealwithglobalcontexts anddependenciesamongwords. Third,Transformersareveryflexiblearchitecturesandcan be easily modified to accommodate different tasks. The past few years have seen the rise ofTransformersnotonlyinNLPbutalsoinseveralotherfields. AsTransformersandtheir variants continue to mature, these models are playing an increasingly important role in the researchandapplicationofartificialintelligence. Inthischapter,wewilldiscussthecoreideasofTransformers. Wewillbeginourdiscussion by looking at the standard Transformer architecture. Then we will look at some notable developments,suchasimprovementstothebasicarchitectureandefficientmethods. Wewill also present several applications in which Transformer models have been extensively used. However,thediscussionofTransformerisawide-rangingtopic,andtherehavemany,many relatedpapers. Thischapterisnotintendedtoprovideacomprehensivesurveyoftheliterature butacollectionofselectedtopicsthatNLPpeoplemaybeinterestedin. 6.1 The Basic Model HereweconsiderthemodelpresentedinVaswanietal.[2017]’swork. Westartbyconsidering theTransformerarchitectureanddiscussthedetailsofthesub-modelssubsequently. 6.1.1 The Transformer Architecture Figure6.1showsthestandardTransformermodelwhichfollowsthegeneralencoder-decoder framework. A Transformer encoder comprises a number of stacked encoding layers (or encodingblocks). Eachencodinglayerhastwodifferentsub-layers(orsub-blocks),called theself-attentionsub-layerandthefeed-forwardneuralnetwork(FFN)sub-layer. Suppose 270 Chapter 6. Transformers we have a source-side sequence x=x ...x and a target-side sequence y =y ...y . The 1 m 1 n inputofanencodinglayerisasequenceofmvectorsh ...h ,eachhavingd dimensions 1 m model (or d dimensions for simplicity). We follow the notation adopted in the previous chapters, usingH∈Rm×d todenotetheseinputvectors1. Theself-attentionsub-layerfirstperformsa self-attentionoperationAtt (·)onHtogenerateanoutputC: self C = Att (H) (6.1) self HereCisofthesamesizeasH,andcanthusbeviewedasanewrepresentationoftheinputs. Then,aresidualconnectionandalayernormalizationunitareaddedtotheoutputsothatthe resultingmodeliseasiertooptimize. TheoriginalTransformermodelemploysthepost-normstructurewherearesidualcon- nectioniscreatedbeforelayernormalizationisperformed,likethis H = LNorm(C+H) (6.2) self where the addition of H denotes the residual connection, and LNorm(·) denotes the layer normalization function. Substituting Eq. (6.1) into Eq. (6.2), we obtain the form of the self-attentionsub-layer Layer (H) = H self self = LNorm(Att (H)+H) (6.3) self ThedefinitionsofLNorm(·)andAtt (·)havebeengiveninChapters2and5,andwewill self alsodiscussthemlaterinthesection. TheFFNsub-layertakesH andoutputsanewrepresentationH ∈Rm×d. Ithasthe self ffn same form as the self-attention sub-layer, with the attention function replaced by the FFN function,givenby Layer (H ) = H ffn self ffn = LNorm(FFN(H )+H ) (6.4) self self Here FFN(·) could be any feed-forward neural networks with non-linear activation func- tions. The most common structure of FFN(·) is a two-layer network involving two linear transformationsandaReLUactivationfunctionbetweenthem. Fordeepmodels,wecanstacktheaboveneuralnetworks. LetHl betheoutputoflayer l. Then, we can express Hl as a function of Hl−1. We write this as a composition of two h  1 1Providedh j ∈Rdisarowvector,wehaveH=  . . .   . hm 6.1 The Basic Model 271 Pr(·|y ,x ...x ) 0 1 m . . . . . . Pr(·|y ...y ,x ...x ) 0 n−1 1 m Softmax(SLWo) Decoder Add&LayerNorm Feed-ForwardNetwork Encoder Layer ffn (·) ×L Add&LayerNorm Add&LayerNorm Feed-ForwardNetwork Encoder-DecoderAttention L× Layer (·) Layer (·) ffn cross Add&LayerNorm Add&LayerNorm Self-Attention Self-Attention Layer (·) Layer (·) self self Word Position Word Position x ...x y y ...y 1 m 0 1 n−1 Figure6.1: TheTransformerarchitecture[Vaswanietal.,2017]. ThereareLstackedlayerson eachoftheencoderanddecodersides. Anencodinglayercomprisesaself-attentionsub-layer and an FFN sub-layer. Both of these sub-layers share the same structure which involves a corefunction(eitherLayer (·)orLayer (·)),followedbyaresidualconnectionandalayer self ffn normalizationunit. Eachdecodinglayerhasasimilararchitecturewiththeencodinglayers,but withanadditionalencoder-decoderattentionsub-layersandwichedbetweentheself-attention andFFNsub-layers. Aswithmostsequence-to-sequencemodels,Transformertakesx ...x 1 m and y ...y for predicting y . The representation of an input word comprises a sum of a 0 i−1 i wordembeddingandapositionalembedding. Thedistributions{Pr(·|y ...y ,x ...x )}are 0 i−1 1 m generatedinsequencebyaSoftmaxlayer,whichoperatesonalineartransformationofthe outputfromthelastdecodinglayer. sub-layers Hl = Layer (Hl ) (6.5) ffn self Hl = Layer (Hl−1) (6.6) self self 272 Chapter 6. Transformers IfthereareLencodinglayers,thenHL willbetheoutputoftheencoder. Inthiscase,HL can beviewedasarepresentationoftheinputsequencethatislearnedbytheTransformerencoder. H0",
    "of an input word comprises a sum of a 0 i−1 i wordembeddingandapositionalembedding. Thedistributions{Pr(·|y ...y ,x ...x )}are 0 i−1 1 m generatedinsequencebyaSoftmaxlayer,whichoperatesonalineartransformationofthe outputfromthelastdecodinglayer. sub-layers Hl = Layer (Hl ) (6.5) ffn self Hl = Layer (Hl−1) (6.6) self self 272 Chapter 6. Transformers IfthereareLencodinglayers,thenHL willbetheoutputoftheencoder. Inthiscase,HL can beviewedasarepresentationoftheinputsequencethatislearnedbytheTransformerencoder. H0 denotestheinputoftheencoder. Inrecurrentandconvolutionalmodels,H0 cansimplybe wordembeddingsoftheinputsequence. Transformertakesadifferentwayofrepresenting theinputwords,andencodesthepositionalinformationexplicitly. InSection6.1.2wewill discusstheembeddingmodelusedinTransformers. TheTransformerdecoderhasasimilarstructureastheTransformerencoder. Itcomprises Lstackeddecodinglayers(ordecodingblocks). LetSl betheoutputofthel-thdecoding layer. Wecanformulateadecodinglayerbyusingthefollowingequations Sl = Layer (Sl ) (6.7) ffn cross Sl = Layer (HL,Sl−1) (6.8) cross cross self Sl = Layer (Sl−1) (6.9) self self Heretherearethreedecodersub-layers. Theself-attentionandFFNsub-layersarethesame as those used in the encoder. Layer (·) denotes a cross attention sub-layer (or encoder- cross decodersub-layer)whichmodelsthetransformationfromthesource-sidetothetarget-side. In Section6.1.6wewillseethatLayer (·)canbeimplementedusingthesamefunctionas cross Layer (·). self TheTransformerdecoderoutputsadistributionoveravocabularyV ateachtarget-side y position. Thisisachievedbyusingasoftmaxlayerthatnormalizesalineartransformationof SL todistributionsoftarget-sidewords. Todothis,wemapSL toann×|V |matrixOby y O = SL·W (6.10) o whereW o ∈Rd×|Vy| istheparametermatrixofthelineartransformation. Then,theoutputoftheTransformerdecoderisgivenintheform   Pr(·|y ,x) 0 .   . .   = Softmax(O) Pr(·|y ...y ,x) 0 n−1   Softmax(o ) 1 . =   . .   (6.11) Softmax(o ) n whereo denotesthei-throwvectorofO,andy denotesthestartsymbol⟨SOS⟩. Underthis i 0 model,theprobabilityofygivenxcanbedefinedasusual, n (cid:88) logPr(y|x) = logPr(y |y ...y ,x) (6.12) i 0 i−1 i=1 Thisequationresemblesthegeneralformoflanguagemodeling: wepredictthewordat 6.1 The Basic Model 273 timeigivenallofthewordsuptotimei−1. Therefore,theinputoftheTransformerdecoder isshiftedonewordleft,thatis,theinputisy ...y andtheoutputisy ...y . 0 n−1 1 n TheTransformerarchitecturediscussedabovehasseveralvariantswhichhavebeensuc- cessfully used in different fields of NLP. For example, we can use a Transformer encoder torepresenttexts(callittheencoder-onlyarchitecture),canuseaTransformerdecoderto generatetexts(callitthedecoder-onlyarchitecture),andcanuseastandardencoder-decoder Transformermodeltotransformaninputsequencetoanoutputsequence. Intherestofthis chapter,mostofthediscussionisindependentoftheparticularchoiceofapplication,andwill bemostlyfocusedontheencoder-decoderarchitecture. InSection6.5,wewillseeapplications oftheencoder-onlyanddecoder-onlyarchitectures. 6.1.2 Positional Encoding Intheiroriginalform,bothFFNsandattentionmodelsusedinTransformerignoreanimportant property of sequence modeling, which is that the order of the words plays a crucial role in expressingthemeaningofasequence. Thismeansthattheencoderanddecoderareinsensitive tothepositionalinformationoftheinputwords. Asimpleapproachtoovercomingthisproblem istoaddpositionalencodingtotherepresentationofeachwordofthesequence. Moreformally, awordx canberepresentedasad-dimensionalvector j xp = x +PE(j) (6.13) j j Herex ∈Rdistheembeddingofthewordwhichcanbeobtainedbyusingthewordembedding j models,asdescribedChapter3. PE(j)∈Rd istherepresentationofthepositionj. Vanilla Transformeremploysthesinusoidalpositionalencodingmodelswhichwewriteintheform 1 PE(i,2k) = sin(i· ) (6.14) 100002k/d 1 PE(i,2k+1) = cos(i· ) (6.15) 100002k/d wherePE(i,k)denotesthek-thentryofPE(i). Theideaofpositionalencodingistodistin- guishdifferentpositionsusingcontinuoussystems. Hereweusethesineandcosinefunctions with different frequencies. The interested reader can refer to Chapter 4 to see that such a methodcanbeinterpretedasacarryingsystem. Becausetheencodingisbasedonindividual positions, it is also called absolute positional encoding. In Section 6.3.1 we will see an improvementtothismethod. Oncewehavetheaboveembeddingresult,xp ...xp istakenastheinputtotheTrans- 1 m formerencoder,thatis,   xp 1 . H 0 =   . .   (6.16) xp m Similarly,wecanalsodefinetheinputonthedecoderside. 274 Chapter 6. Transformers (a)RNN (b)CNN(r=3) (c)Self-attention Figure6.2: Informationflowsinrecurrent,convolutionalandself-attentionmodels,shownas arrowlinesbetweenpositions. 6.1.3 Multi-head Self-attention The use of self-attention is perhaps one of the most significant advances in sequence-to- sequencemodels. Itattemptstolearnandmakeuseofdirectinteractionsbetweeneachpair ofinputs. Fromarepresentationlearningperspective,self-attentionmodelsassumethatthe learnedrepresentationatpositioni(denotedbyc )isaweightedsumoftheinputsoverthe i sequence. Theoutputc isthusgivenby i m (cid:88) c = α h (6.17) i i,j j j=1 whereα indicateshowstrongtheinputh iscorrelatedwiththeinputh . Wethuscanview i,j i j c asarepresentationoftheglobalcontextatpositioni. α canbedefinedindifferentwaysif i i,j oneconsidersdifferentattentionmodels. Hereweusethescaleddot-productattentionfunction tocomputeα ,asfollows i,j α = Softmax(h hT/β) i,j i j exp(h hT/β) i j = (6.18) (cid:80)m exp(h hT/β) k=1 i k √ whereβ isascalingfactorandissetto d. Compared with conventional recurrent and convolutional models, an advantage of self- attentionmodelsisthattheyshortenthecomputational“distance”betweentwoinputs. Figure 6.2 illustrates the information flow in these models. We see that, given the input at posi- tion i, self-attention models can directly access any other input. By contrast, recurrent and convolutionalmodelsmightneedtwoormorejumpstoseethewholesequence. We can have a more general view of self-attention by using the QKV attention model.   q 1 . SupposewehaveasequenceofκqueriesQ=  .",
    "flow in these models. We see that, given the input at posi- tion i, self-attention models can directly access any other input. By contrast, recurrent and convolutionalmodelsmightneedtwoormorejumpstoseethewholesequence. We can have a more general view of self-attention by using the QKV attention model.   q 1 . SupposewehaveasequenceofκqueriesQ=  . .   ,andasequenceofψkey-valuepairs(K= q κ     k v 1 1 . .   . .   ,V=  . .   ). Theoutputofthemodelisasequenceofvectors,eachcorrespondingto k v ψ ψ 6.1 The Basic Model 275 aquery. TheformoftheQKVattentionisgivenby QKT Att (Q,K,V) = Softmax( √ )V (6.19) qkv d WecanwritetheoutputoftheQKVattentionmodelasasequenceofrowvectors   c 1 . C =   . .   c κ = Att (Q,K,V) (6.20) qkv Toapplythisequationtoself-attention,wesimplyhave Hq = HWq (6.21) Hk = HWk (6.22) Hv = HWv (6.23) whereWq,Wk,Wv ∈Rd×d representslineartransformationsofH. ByconsideringEq. (6.1),wethenobtain C = Att (H) self = Att (Hq,Hk,Hv) qkv Hq[Hk]T = Softmax( √ )Hv (6.24) d Hq[Hk]T HereSoftmax( √ )isanm×mmatrixinwhicheachrowrepresentsadistributionover d {h ,...,h },thatis 1 m (cid:104) (cid:105) rowi = α ... α (6.25) i,1 i,m Wecanimprovetheaboveself-attentionmodelbyusingatechniquecalledmulti-head attention. This method can be motivated from the perspective of learning from multiple lower-dimensionalfeaturesub-spaces,whichprojectsafeaturevectorontomultiplesub-spaces andlearnsfeaturemappingsonindividualsub-spaces. Specifically,weprojectthewholeof theinputspaceintoτ sub-spaces(callthemheads), forexample,wetransformH∈Rm×d intoτ matricesofsizem×d,denotedby{Hhead,...,Hhead}. Theattentionmodelisthenrun τ 1 τ τ times,eachtimeonahead. Finally,theoutputsofthesemodelrunsareconcatenated,and transformedbyalinearprojection. Thisprocedurecanbeexpressedby C = Merge(Chead,...,Chead)W (6.26) 1 τ c (6.27) 276 Chapter 6. Transformers Foreachheadh, Hq[Hk]T Chead = Softmax( h√ h )Hv (6.28) h h d Hq = HWq (6.29) h h Hk = HWk (6.30) h h Hv = HWv (6.31) h h Here Merge(·) is the concatenation function, and Att (·) is the attention function de- QKV scribed in Eq. (6.20). W h q,W h k,W h v ∈Rd× τ d are the parameters of the projections from a d-dimensionalspacetoa d-dimensionalspaceforthequeries,keys,andvalues. Thus,Hq ,Hk, τ h h Hv, andChead areallm×d matrices. Merge(Chead,...,Chead)producesanm×dmatrix. h h τ 1 τ ItisthentransformedbyalinearmappingW ∈Rd×d,leadingtothefinalresultC∈Rd×d. c Whilethenotationhereseemssomewhattedious,itisconvenienttoimplementmulti-head modelsusingvariousdeeplearningtoolkits. AcommonmethodinTransformer-basedsystems istostoreinputsfromalltheheadsindatastructurescalledtensors,sothatwecanmakeuse ofparallelcomputingresourcestohaveefficientsystems. Amoregeneraldiscussionofthe QKVattentionandmulti-headattentionmodelscanbefoundinChapter5. 6.1.4 Layer Normalization Layer normalization provides a simple and effective means to make the training of neural networks more stable by standardizing the activations of the hidden layers in a layer-wise manner. As introduced in Ba et al. [2016]’s work, given a layer’s output h∈Rd, the layer normalizationmethodcomputesastandardizedoutputLNorm(h)∈Rd by h−µ LNorm(h) = g⊙ +b (6.32) σ+ϵ Hereµ∈Rd andσ∈Rd arethemeanandstandardderivationoftheactivations. Leth bethe k k-thdimensionofh. µandσ aregivenby d 1 (cid:88) µ = · h (6.33) k d k=1 (cid:118) (cid:117) d (cid:117)1 (cid:88) σ = (cid:116) · (h −µ)2 (6.34) k d k=1 Hereg∈Rd andb∈Rd aretherescalingandbiasterms. Theycanbetreatedasparameters oflayernormalization,whosevaluesaretobelearnedtogetherwithotherparametersofthe Transformermodel. Theadditionofϵtoσ isusedforthepurposeofnumericalstability. In general,ϵischosentobeasmallnumber. We illustrate the layer normalization method for the hidden states of an encoder in the 6.1 The Basic Model 277 followingexample(assumethatm=4,d=3,g=1,b=0,andϵ=0.1).     h 1 1 2 µ=1.3, σ=0.5 1−1.3 1−1.3 2−1.3 1 0.5+0.1 0.5+0.1 0.5+0.1 h 0.9 0.9 0 µ=0.6, σ=0.4 0.9−0.6 0.9−0.6 0−0.6  2  =⇒ 0.4+0.1 0.4+0.1 0.4+0.1 h 0.7 0.8 0 µ=0.5, σ=0.4 0.7−0.5 0.8−0.5 0−0.5  3  0.4+0.1 0.4+0.1 0.4+0.1 h 3 1 7 µ=3.7, σ=2.5 3−3.7 1−3.7 7−3.7 4 2.5+0.1 2.5+0.1 2.5+0.1 As discussed in Section 6.1.1, the layer normalization unit in each sub-layer is used to standardizetheoutputofaresidualblock.",
    "2  =⇒ 0.4+0.1 0.4+0.1 0.4+0.1 h 0.7 0.8 0 µ=0.5, σ=0.4 0.7−0.5 0.8−0.5 0−0.5  3  0.4+0.1 0.4+0.1 0.4+0.1 h 3 1 7 µ=3.7, σ=2.5 3−3.7 1−3.7 7−3.7 4 2.5+0.1 2.5+0.1 2.5+0.1 As discussed in Section 6.1.1, the layer normalization unit in each sub-layer is used to standardizetheoutputofaresidualblock. Herewedescribeamoregeneralformulationfor this structure. Suppose that F(·) is a neural network we want to run. Then, the post-norm structureofF(·)isgivenby H = LNorm(F(H )+H ) (6.35) out in in whereH andH aretheinputandoutputofthismodel. Clearly,Eq. (6.4)isaninstance in output ofthisequation. Analternativeapproachtointroducinglayernormalizationandresidualconnectionsinto modelingistoexecutetheLNorm(·)functionrightaftertheF(·)function,andtoestablishan identitymappingfromtheinputtotheoutputoftheentiresub-layer. Thisstructure,knownas thepre-normstructure,canbeexpressedintheform H = LNorm(F(H ))+H (6.36) out in in Bothpost-normandpre-normTransformermodelsarewidelyusedinNLPsystems. See Figure 6.3 for a comparison of these two structures. In general, residual connections are consideredaneffectivemeanstomakethetrainingofmulti-layerneuralnetworkseasier. In thissense,pre-normTransformerseemspromisingbecauseitfollowstheconventionthata residualconnectioniscreatedtobypassthewholenetworkandthattheidentitymappingfrom theinputtotheoutputleadstoeasieroptimizationofdeepmodels. However,byconsidering the expressive power of a model, there may be modeling advantages in using post-norm Transformer because it does not so much rely on residual connections and enforces more sophisticatedmodelingforrepresentationlearning. InSection6.3.2,wewillseeadiscussion onthisissue. 6.1.5 Feed-forward Neural Networks The use of FFNs in Transformer is inspired in part by the fact that complex outputs can be formed by transforming the inputs through nonlinearities. While the self-attention model itself has some nonlinearity (in Softmax(·)), a more common way to do this is to consider additionallayerswithnon-linearactivationfunctionsandlineartransformations. Givenaninput H ∈Rm×d and an output H ∈Rm×d, the H =FFN(H ) function in Transformer in out out in 278 Chapter 6. Transformers . . . H F(·) LNorm(·) H in out Add&LayerNorm (a)Post-norm CoreFunction F(·) H LNorm(·) F(·) H in out . . (b)Pre-norm . Figure6.3: Thepost-normandpre-normstructures. F(·)=corefunction,LNorm(·)=layer normalization,and⊕=residualconnection. hasthefollowingform H = H W +b (6.37) out hidden f f H = ReLU(H W +b ) (6.38) hidden in h h whereH hidden ∈Rm×d ffn isthehiddenstates,andW h ∈Rd×d ffn,b h ∈Rd ffn,W f ∈Rd ffn ×d andb ∈Rd aretheparameters. Thisisatwo-layerFFNinwhichthefirstlayer(orhidden f layer)introducesanonlinearitythroughReLU(·)2 andthesecondlayerinvolvesonlyalinear transformation. ItiscommonpracticeinTransformertousealargersizeofthehiddenlayer. Forexample,acommonchoiceisd =4d,thatis,thesizeofeachhiddenrepresentationis4 ffn timesaslargeastheinput. Note that using a wide FFN sub-layer has been proven to be of great practical value in manystate-of-the-artsystems. However,aconsequenceofthisisthatthemodelisoccupied bytheparametersoftheFFN.Table6.1showsparameternumbersandtimecomplexitiesfor differentmodulesofastandardTransformersystem. WeseethatFFNsdominatethemodel sizewhend islarge,thoughtheyarenotthemosttimeconsumingcomponents. Inthecase ffn ofverybigTransformmodels,wethereforewishtoaddressthisproblemforbuildingefficient systems. 6.1.6 Attention Models on the Decoder Side A decoder layer involves two attention sub-layers, the first of which is a self-attention sub- layer,andthesecondisacross-attentionsub-layer. Thesesub-layersarebasedoneitherthe post-normorthepre-normstructure,butdifferbydesignsoftheattentionfunctions. Consider, forexample,thepost-normstructure,describedinEq. (6.35). Wecandefinethecross-attention 2ReLU(x)=max{0,x}. 6.1 The Basic Model 279 Sub-model #ofParameters TimeComplexity × Multi-headSelf-attention 4d2 O(m2·d) L Encoder Feed-forwardNetwork 2d·d +d+d O(m·d·d ) L ffn ffn ffn LayerNormalization 2d O(d) 2L Multi-headSelf-attention 4d2 O(n2·d) L Multi-headCross-attention 4d2 O(m·n·d) L Decoder Feed-forwardNetwork 2d·d +d+d O(n·d·d ) L ffn ffn ffn LayerNormalization 2d O(d) 3L Table6.1: NumbersofparametersandtimecomplexitiesofdifferentTransformermodules underdifferentsetups. m=source-sequencelength,n=target-sequencelength,d=default number of dimensions of a hidden layer, d = number of dimensions of the FFN hidden ffn layer,τ =numberofheadsintheattentionmodels,andL=numberofencodingordecoding layers. Thecolumn×meansthenumberoftimesasub-modelisappliedontheencoderor decoderside. Thetimecomplexitiesareestimatedbycountingthenumberofmultiplicationof floating-pointnumbers. andself-attentionsub-layersforadecodinglayertobe S = Layer (H ,S ) cross cross enc self = LNorm(Att (H ,S )+S ) (6.39) cross enc self self S = Layer (S) self self = LNorm(Att (S)+S) (6.40) self whereS∈Rn×d istheinputoftheself-attentionsub-layer,S ∈Rn×d andS ∈Rn×d cross self aretheoutputsofthesub-layers,andH ∈Rm×d istheoutputoftheencoder3. enc As with conventional attention models, cross-attention is primarily used to model the correspondencebetweenthesource-sideandtarget-sidesequences. TheAtt (·)function cross isbasedontheQKVattentionmodelwhichgeneratestheresultofqueryingacollectionof key-valuepairs. Morespecifically,wedefinethequeries,keysandvaluesaslinearmappings ofS andH ,asfollows self enc Sq = S Wq (6.41) self self cross",
    "self self = LNorm(Att (S)+S) (6.40) self whereS∈Rn×d istheinputoftheself-attentionsub-layer,S ∈Rn×d andS ∈Rn×d cross self aretheoutputsofthesub-layers,andH ∈Rm×d istheoutputoftheencoder3. enc As with conventional attention models, cross-attention is primarily used to model the correspondencebetweenthesource-sideandtarget-sidesequences. TheAtt (·)function cross isbasedontheQKVattentionmodelwhichgeneratestheresultofqueryingacollectionof key-valuepairs. Morespecifically,wedefinethequeries,keysandvaluesaslinearmappings ofS andH ,asfollows self enc Sq = S Wq (6.41) self self cross Hk = H Wk (6.42) enc enc enc Hv = H Wv (6.43) enc enc enc whereWq ,Wk ,Wv ∈Rd×d aretheparametersofthemappings. Inotherwords,the cross enc enc queriesaredefinedbasedonS ,andthekeysandvaluesaredefinedbasedonH . self enc 3ForanencoderhavingLencoderlayers,Henc=HL. 280 Chapter 6. Transformers i−3 i−2 i−1 i i+1 i−3 i−2 i−1 i i+1 (a)Encoder-sideSelf-attention (b)Decoder-sideSelf-attention Figure6.4: Self-attentionontheencoderanddecodersides. Eachlineconnectsaninputand anoutputoftheself-attentionmodel,indicatingadependencyofanoutputstateonaninput state. Forencoderself-attention,theoutputatanypositioniscomputedbyhavingaccesstothe entiresequence. Bycontrast,fordecoderself-attention,theoutputatpositioniiscomputedby seeingonlyinputsatpositionsuptoi. Att (·)isthendefinedas cross Att (H ,S ) = Att (Sq ,Hk ,Hv ) cross enc self qkv self enc enc Sq [Hk ]T = Softmax( self√ enc )Hv (6.44) enc d TheAtt (·)functionhasasimilarformasAtt (·),withlinearmappingsofStaken self cross asthequeries,keys,andvalues,likethis Att (S) = Att (Sq,Sk,Sv) self qkv Sq[Sk]T = Softmax( √ +M)Sv (6.45) d whereSq=SWq ,Sk=SWk ,andSv=SWv arelinearmappingsofSwithparameters dec dec dec Wq ,Wk ,Wv ∈Rd×d. dec dec dec This form is similar to that of Eq. (6.20). A difference compared to self-attention on the encoder side, however, is that the model here needs to follow the rule of left-to-right generation(seeFigure6.4). Thatis,givenatarget-sidewordatthepositioni,wecanseeonly thetarget-sidewordsintheleftcontexty ...y . Todothis,weaddamaskingvariableM 1 i−1 Sq[Sk]T Sq[Sk]T totheunnormalizedweightmatrix √ +M. BothMand √ +Mareofsizen×n, d d andsoalowervalueofanentryofMmeansalargerbiastowardsloweralignmentscoresfor Sq[Sk]T thecorrespondingentryof √ +M. Inordertoavoidaccesstotherightcontextgiveni, d Misdefinedtobe (cid:40) 0 i≤k M(i,k) = (6.46) −∞ i>k whereM(i,k)indicatesabiastermforthealignmentscorebetweenpositionsiandk. Below 6.1 The Basic Model 281 weshowanexampleofhowthemaskingvariableisapplied(assumen=4). Sq[Sk]T Softmax( √ +M) d     2 0.1 1 1 0 −∞ −∞ −∞  0 0.9 0.9 0.9 0 0 −∞ −∞ = Softmax( + ) 0.2 0.8 0.7 2  0 0 0 −∞     0.3 1 0.3 3 0 0 0 0   2 −∞ −∞ −∞  0 0.9 −∞ −∞ = Softmax( ) 0.2 0.8 0.7 −∞   0.3 1 0.3 3   1 0 0 0 0.3 0.7 0 0  =   (6.47) 0.2 0.4 0.4 0    0.05 0.1 0.05 0.8 As noted in Section 6.1.3, it is easy to improve these models by using the multi-head attention mechanism. Also, since decoders are typically the most time-consuming part of practicalsystems,thebulkofthecomputationaleffortinrunningthesesystemsisverymuch concernedwiththeefficiencyoftheattentionmodulesonthedecoderside. 6.1.7 Training and Inference Transformerscanbetrainedandusedinaregularway. Forexample,wecantrainaTransformer model by performing gradient descent to minimize some loss function on the training data (seeChapter2),andtestthetrainedmodelbyperformingbeamsearchontheunseendata(see Chapter5). Belowwepresentsomeofthetechniquesthataretypicallyusedinthetraining andinferenceofTransformermodels. • LearningRateScheduling. Asstandardneuralnetworks,Transformerscanbedirectly trainedusingback-propagation. Thetrainingprocessisgenerallyiteratedmanytimesto makethemodelsfitthetrainingdatawell. Ineachtrainingstep,weupdatetheweightsof theneuralnetworksbymovingthemasmallstepinthedirectionofnegativegradientsof errors. Therearemanywaystodesigntheupdateruleoftraining. Apopularchoiceisto usetheAdamoptimizationmethod[KingmaandBa,2014]. Toadjustthelearningrate duringtraining,Vaswanietal.[2017]presentalearningrateschedulingstrategywhich increases the learning rate linearly for a number of steps and then decay it gradually. Theydesignalearningrateoftheform lr = lr ·min (cid:8) n−0.5, n ·(n )−1.5(cid:9) (6.48) 0 step step warmup where lr denotes the initial learning rate, and n denotes the number of training 0 step stepswehaveexecuted,andn denotesthenumberofwarmupsteps. Inthefirst warmup 282 Chapter 6. Transformers n steps, the learning rate lr grows larger as training proceeds. It reaches the warmup highestvalueatthepointofn =n ,andthendecreasesasaninversesquare step warmup rootfunction(i.e.,lr ·n−0.5). 0 step • BatchingandPadding. Tomakeatrade-offbetweenglobaloptimizationandtraining convergency,itiscommontoupdatetheweightseachtimeonarelativelysmallcollection ofsamples,calledaminibatchofsamples. Therefore,wecanconsiderabatchversion offorwardandbackwardcomputationprocessesinwhichthewholeminibatchisused togethertoobtainthegradientinformation. Oneadvantageofbatchingisthatitallows thesystemtomakeuseofefficienttensoroperationstodealwithmultiplesequences inasinglerun. Thisrequiresthatalltheinputsequencesinaminibatcharestoredina singlememoryblock,sothattheycanbereadinandprocessedtogether. Toillustrate thisidea,consideraminimatchcontainingfoursampleswhosesource-sidesare A B C D E F",
    "warmup 282 Chapter 6. Transformers n steps, the learning rate lr grows larger as training proceeds. It reaches the warmup highestvalueatthepointofn =n ,andthendecreasesasaninversesquare step warmup rootfunction(i.e.,lr ·n−0.5). 0 step • BatchingandPadding. Tomakeatrade-offbetweenglobaloptimizationandtraining convergency,itiscommontoupdatetheweightseachtimeonarelativelysmallcollection ofsamples,calledaminibatchofsamples. Therefore,wecanconsiderabatchversion offorwardandbackwardcomputationprocessesinwhichthewholeminibatchisused togethertoobtainthegradientinformation. Oneadvantageofbatchingisthatitallows thesystemtomakeuseofefficienttensoroperationstodealwithmultiplesequences inasinglerun. Thisrequiresthatalltheinputsequencesinaminibatcharestoredina singlememoryblock,sothattheycanbereadinandprocessedtogether. Toillustrate thisidea,consideraminimatchcontainingfoursampleswhosesource-sidesare A B C D E F M N R S T W X Y Z Wecanstorethesesequencesina4×6continuousblockwhereeach“row”representsa sequence,likethis A B C D E F M N □ □ □ □ R S T □ □ □ W X Y Z □ □ Here padding words □ are inserted between sequences, so that these sequences are aligned in the memory. Typically, we do not want padding to affect the operation of thesystem,andsowecansimplydefine□asazerovector(callitzeropadding). On theotherhand,insomecasesweareinterestedinusingpaddingtodescribesomething thatisnotcoveredbytheinputsequences. Forexample,wecanreplacepaddingwords with the words in the left (or right) context of a sequence, though this may require modificationstothesystemtoensurethatthenewlyaddedcontextwordsdonotcause additionalcontenttoappearintheoutput. • SearchandCaching. Attesttime,weneedtosearchthespaceofcandidatehypotheses (orcandidatetarget-sidesequences)toidentifythehypothesis(ortarget-sidesequence) withthehighestscore. yˆ = argmax score(x,y) (6.49) y wherescore(x,y)isthemodelscoreofthetarget-sidesequenceygiventhesource-side sequence x. While there are many search algorithms to achieve this, most of them shareasimilarstructure: thesearchprogramoperatesbyextendingcandidatetarget-side 6.2 Syntax-aware Models 283 y y y y y y 1 1 2 1 2 3 Cache Cache ... ... ... ... ... ... ... ... ... ... ... ... y y y y y y 0 0 1 0 1 2 (a)decodingstep1 (b)decodingstep2 (c)decodingstep3 Figure6.5: IllustrationofthecachingmechanisminTransformerdecoders. Rectanglesindicate thestatesofdecodinglayersorsub-layers. Atstepi,allthestatesatpreviousstepsarestored inacache(seedottedboxes),andweonlyneedtocomputethestatesforthisstep(seeblue rectanglesandarrows). Then,weaddthenewlygeneratedstatestothecache,andmoveonto stepi+1. sequencesinapoolatatime. Inthisway,theresultingalgorithmcanbeviewedasa left-to-rightgenerationprocedure. Foramoredetaileddiscussionofsearchalgorithms andmodelscoresofgeneralsequence-to-sequencemodels,seeChapter5. Notethatall ofthedesignsofscore(x,y),nomatterhowcomplex,arebasedoncomputingPr(y|x). BecausetheattentionmodelsusedinTransformerrequirecomputingthedot-productof eachpairoftheinputvectorsofalayer,thetimecomplexityofthesearchalgorithmisa quadraticfunctionofthelengthofy. Itisthereforenotefficienttorepeatedlycompute theoutputsoftheattentionmodelsforpositionsthathavebeendealtwith. Thisproblem can be addressed by caching the states of each layer for words we have seen. Figure 6.5 illustrates the use of the caching mechanism in a search step. All the states for positions<iaremaintainedandeasilyaccessedinacache. Atpositioni,allweneedis tocomputethestatesforthenewlyaddedword,andthentoupdatethecache. 6.2 Syntax-aware Models AlthoughTransformerissimplyadeeplearningmodelthatdoesnotmakeuseofanylinguistic structure or assumption, it may be necessary to incorporate our prior knowledge into such systems. This is in part because NLP researchers have long believed that a higher level of abstractionofdataisneededtodevelopidealNLPsystems,andtherehavebeenmanysystems thatusestructureaspriors. However,structureisawide-rangingtopicandthereareseveral typesofstructureonemayrefertoSee[2018]’swork. Forexample,theinductivebiasesused inourmodeldesigncanbethoughtofassomestructuralprior,whileNLPmodelscanalso learntheunderlyingstructureofproblemsbythemselves. Inthissubsectionwewilldiscuss 284 Chapter 6. Transformers some of these issues. We will focus on the methods of introducing linguistic structure into Transformermodels. AsTransformercanbeappliedtomanyNLPtasks,whichdiffermuchin theirinputandoutputformats,wewillprimarilydiscussmodificationstoTransformerencoders (callthemsyntax-awareTransformerencoders). Ourdiscussion,however,isgeneral,and themethodscanbeeasilyextendedtoTransformerdecoders. 6.2.1 Syntax-aware Input and Output One of the simplest methods of incorporating structure into NLP systems is to modify the input sequence, leaving the system unchanged. As a simple example, consider a sentence whereeachwordx isassignedasetofκsyntacticlabels{tag1,...,tagκ}(e.g.,POSlabels j j j anddependencylabels). Wecanwritethesesymbolstogethertodefineanew“word” x /tag1/.../tagκ j j j Then,theembeddingofthiswordisgivenby xp = e(x /tag1/.../tagκ)+PE(j) (6.50) j j j j wheree(x /tag1/.../tagκ)∈Rdistheembeddingofx /tag1/.../tagκ. Sincex /tag1/.../tagκ j j j j j j j j j isacomplexsymbol,wedecomposethelearningproblemofe(x /tag1/.../tagκ)intoeasier j j j problems. Forexample,wecandevelopκembeddingmodels,eachproducinganembedding given a tag. Then, we write e(x /tag1/.../tagκ) as a sum of the word embedding and tag j j j embeddings e(x /tag1/.../tagκ) = x +e(tag1)+...+e(tagκ) (6.51) j j j j j j where{e(tag1),...,e(tagκ)}aretheembeddingsofthetags. Alternatively,wecancombine j j theseembeddingsviaaneuralnetworkintheform e(x /tag1/.../tagκ) = FFN (x ,e(tag1),...,e(tagκ)) (6.52) j j j embed j j j whereFFN (·)isafeed-forwardneuralnetworkthathasonelayerortwo. embed Wecandothesamethingforsentencesonthedecodersideaswell,andtreaty /tag1/.../tagκ i i i asasyntax-augmentedword. However,thismayleadtoamuchlargertarget-sidevocabulary andposesacomputationalchallengefortrainingandinference. Anotherformthatiscommonlyusedtorepresentasentenceissyntaxtree. Inlinguistics,the syntaxofasentencecanbeinterpretedinmanydifferentways,resultinginvariousgrammars and the corresponding tree (or graph)-based representations. While these representations differintheirsyntacticforms,ageneralapproachtousetheminsequencemodelingistree linearization. Considerthefollowingsentenceannotatedwithaconstituency-basedparsetree 6.2 Syntax-aware Models 285 S NP VP . PRP VBZ ADJP ! It ’s JJ interesting Wecanwritethistreestructureasasequenceofwords,syntacticlabelsandbracketsviaatree traversalalgorithm,asfollows (S (NP (PRP It ) )",
    "(·)isafeed-forwardneuralnetworkthathasonelayerortwo. embed Wecandothesamethingforsentencesonthedecodersideaswell,andtreaty /tag1/.../tagκ i i i asasyntax-augmentedword. However,thismayleadtoamuchlargertarget-sidevocabulary andposesacomputationalchallengefortrainingandinference. Anotherformthatiscommonlyusedtorepresentasentenceissyntaxtree. Inlinguistics,the syntaxofasentencecanbeinterpretedinmanydifferentways,resultinginvariousgrammars and the corresponding tree (or graph)-based representations. While these representations differintheirsyntacticforms,ageneralapproachtousetheminsequencemodelingistree linearization. Considerthefollowingsentenceannotatedwithaconstituency-basedparsetree 6.2 Syntax-aware Models 285 S NP VP . PRP VBZ ADJP ! It ’s JJ interesting Wecanwritethistreestructureasasequenceofwords,syntacticlabelsandbracketsviaatree traversalalgorithm,asfollows (S (NP (PRP It ) ) (VP (VBZ ’s ) (ADJP (JJ PRP NP VBZ interesting ) ) ) (. ! ) ) JJ ADJP VP . S Thissequenceofsyntactictokenscanbeusedasaninputtothesystem,thatis,eachtoken isrepresentedbywordandpositionalembeddings,andthenthesumoftheseembeddingsis treatedasaregularinputoftheencoder. Anexampleoftheuseoflinearizedtreesistree-to- stringmachinetranslationinwhichasyntaxtreeinonelanguageistranslatedintoastringin anotherlanguage[Lietal.,2017b;CurreyandHeafield,2018]. Linearizedtreescanalsobe usedfortreegeneration. Forexample,wecanframeparsingtasksassequence-to-sequence problemstomapaninputtexttoasequentialrepresentationofitscorrespondingsyntaxtree [Vinyals et al., 2015; Choe and Charniak, 2016]. See Figure 6.6 for illustrations of these models. ItshouldbenotedthatthemethodsdescribedherearenotspecifictoTransformerbut couldbeappliedtomanymodels,suchasRNN-basedmodels. 6.2.2 Syntax-aware Attention Models ForTransformermodels,italsomakessensetomakeuseofsyntaxtreestoguidetheprocess oflearningsequencerepresentations. Intheprevioussectionwesawhowrepresentationsofa sequencecanbecomputedbyrelatingdifferentpositionswithinthatsequence. Thisallowsus toimposesomestructureontheserelationswhicharerepresentedbydistributionsofattention weightsoverallthepositions. Todothisweusetheencoderself-attentionwithanadditive mask Hq[Hk]T AttSyn (H) = Softmax( √ +M)Hv (6.53) self d oralternativelywithamultiplicativemask Hq[Hk]T AttSyn (H) = Softmax( √ ⊙M)Hv (6.54) self d whereM∈Rm×misamatrixofmaskingvariablesinwhichalargervalueofM(i,j)indicates 286 Chapter 6. Transformers 很 好 ！ Encoder Decoder (ADJP (JJ Great ) JJ (. ! ). ) ADJP ⟨SOS⟩ 很 好 (a)Tree-to-StringMachineTranslation (ADJP (JJ Great ) JJ (. ! ). ) ADJP Encoder Decoder Great ! ⟨SOS⟩ (ADJP (JJ Great ) JJ (. ! ). (b)ConstituencyParsing Figure 6.6: Illustration of tree linearization on either the encoder or decoder side. For tree- to-string machine translation, the encoder takes sequential representation of an input parse tree,andthedecoderoutputsthecorrespondingtranslation. Forparsing,theencodertakesa sentence,andthedecoderoutputsthecorrespondingsyntaxtree. a stronger syntactic correlation between positions i and j. In the following description we chooseEq. (6.54)asthebasicform. OnecommonwaytodesignMistoprojectsyntacticrelationsoftheinputtreestructure intoconstraintsoverthesequence. Hereweconsiderconstituencyparsetreesanddependency parsetreesforillustration. Generally,twotypesofmaskingmethodsareemployed. • 0-1Masking. ThismethodassignsM(i,j)avalueof1ifthewordsatpositionsiandj areconsideredsyntacticallycorrelatedandavalueof0otherwise[Zhangetal.,2020c; Bai et al., 2021]. To model the relation between two words in a syntax tree, we can considerthedistancebetweentheircorrespondingnodes. Oneofthesimplestformsis givenby (cid:40) 1 ω(i,j)≤ω max M(i,j) = (6.55) 0 otherwise where ω(i,j) is the length of the shortest path between the nodes of the words at positions i and j. For example, given a dependency parse tree, ω(i,j) is the number 6.2 Syntax-aware Models 287 of dependency edges in the path between the two words. For a constituency parse tree,allthewordsareleafnodes,andsoω(i,j)givesatreedistancebetweenthetwo leavesinthesamebranchofthetree. ω isaparameterusedtocontrolthemaximum max distancebetweentwonodesthatcanbeconsideredsyntacticallycorrelated. Forexample, assuming that there is a dependency parse tree and ω =1, Eq. (6.55) enforces a max constraint that the attention score between positions i and j is computed only if they haveaparent-dependentrelation4. • Soft Masking. Instead of treating M as a hard constraint, we can use it as a soft constraint that scales the attention weight between positions i and j in terms of the degreetowhichthecorrespondingwordsarecorrelated. Anideaistoreducetheattention weightasω(i,j)becomeslarger. Averysimplemethodtodothisistotransformω(i,j) insomewaythatM(i,j)holdsanegativecorrelationrelationshipwithω(i,j)andits valuefallsintotheinterval[0,1] M(i,j) = DNorm(ω(i,j)) (6.56) There are several alternative designs for DNorm(·). For example, one can compute a standardized score of −ω(i,j) by subtracting its mean and dividing by its standard deviation [Chen et al., 2018a], or can normalize 1/ω(i,j) over all possible j in the sequence[Xuetal.,2021b]. Incaseswhereparserscanoutputascorebetweenpositionsi andj,itisalsopossibletousethisscoretocomputeM(i,j). Forexample,adependency parsercanproducetheprobabilityofthewordatpositionibeingtheparentoftheword atpositionj [Strubelletal.,2018]. WecanthenwriteM(i,j)as M(i,j) = Pr (i|j) (6.57) parent oralternatively M(i,j) = max{Pr (i|j),Pr (j|i)} (6.58) parent parent where Pr (i|j) and Pr (j|i) are the probabilities given by the parser. See parent parent Figure6.7foranexampleofinducingasoftmaskingvariablefromadependencyparse tree. 6.2.3 Multi-branch Models Introducing syntax into NLP systems is not easy. This is partially because automatic parse treesmayhaveerrors,andpartiallybecausetheuseofsyntaxmayleadtostrongassumptionof theunderlyingstructureofasentence. Ratherthancombiningsyntacticandwordinformation 4For multiplicative masks, M(i,j)=0 does not mean that the attention weight between j",
    "Pr (i|j) and Pr (j|i) are the probabilities given by the parser. See parent parent Figure6.7foranexampleofinducingasoftmaskingvariablefromadependencyparse tree. 6.2.3 Multi-branch Models Introducing syntax into NLP systems is not easy. This is partially because automatic parse treesmayhaveerrors,andpartiallybecausetheuseofsyntaxmayleadtostrongassumptionof theunderlyingstructureofasentence. Ratherthancombiningsyntacticandwordinformation 4For multiplicative masks, M(i,j)=0 does not mean that the attention weight between j and i is zero becausetheSoftmaxfunctiondoesnotgiveazerooutputforadimensionwhosecorrespondinginputisofazero value. Amethodto“mask”anentryofSoftmax(H√HT )istouseanadditivemaskandsetM(i,j)=−∞if d ω(i,j)>ωmax. 288 Chapter 6. Transformers ul T h e c o n c er w t a s w o n d erf ! The concert was The concert was wonderful ! wonderful ! (a)DependencyParseTree (b)MaskM(darkercolormeanslargervalue) Figure6.7: Priorsinducedfromadependencyparsetree. TherowiofthematrixMrepresents adistributionthatdescribeshowmuchweightwecangivetoM(i,j)intermsofthesyntactic distancebetweeniandj. intoone“big”model,itmaybemoreflexibleandeffectivetobuildonemodeltoencodesyntax andadifferentonetoencodewordsequences. Onewaytoachievethisisthroughtheuseof multipleneuralnetworks(calledbranchesorpaths),eachdealingwithonetypeofinput. The outputsofthesebranchesarethencombinedtoproduceanoutput[Xieetal.,2017;Fanetal., 2020;Linetal.,2022b]. Variousmethodshavethereforebeenusedtocombinedifferenttypes ofinputforneuralmodelslikeTransformer. Onecommonly-usedapproachistobuildtwoseparateencoders,inwhichonemodelis trainedtoencodethesyntacticinput(denotedbyt),andtheotheristrainedtoencodetheusual input(denotedbyx). Figure6.8(a)illustratesthismulti-encoderarchitecture. Thesyntactic encoderEncode (t)isbasedonmodelspresentedinSections6.2.1and6.2.2,andthetext syn encoderEncode (x)isastandardTransformerencoder. Therepresentationsgeneratedby text theseencodersarethenfedintothecombinationmodelasinput,andcombinedintoahybrid representation,givenby H = Combine(H ,H ) hybrid syn text = Combine(Encode (t),Encode (x)) (6.59) syn text ThereareseveraldesignsforCombine(·),dependingonwhatkindofproblemsweapply the encoders to. For example, if we want to develop a text classifier, Combine(·) can be a simplepoolingnetwork. Formorecomplicatedtasks,suchasmachinetranslation,Combine(·) canbeaTransformerencoderaswell,andwecanfuseinformationfromdifferentsourcesby performingself-attentionon[H ,H ]. syn text Whilewerestrictattentiontosyntacticmodelsinthissection,thegeneralmulti-encoder architecturecanbeusedinmanyproblemswhereinputsfromadditionalsourcesarerequired. For example, one can use one encoder to represent a sentence, and use another encoder to 6.2 Syntax-aware Models 289 )·( edocnE nys )·( edocnE txet ... ... Combine(·) t x x (a)Multi-encoder 1hcnarB 2hcnarB ... t t Head 1 x (b)Multi-branchasaSub-model (c)Multi-headAttention Figure6.8: Multi-brancharchitectures. Therearetwoinputs: asentence(denotedbyx)and thesyntaxtreeofthesentence(denotedbyt). Inthemulti-encoderarchitecture(seesub-figure (a)), two encoders are constructed to encode x and t, respectively. A combination model then takes the outputs of the encoders and produces a combined representation of x and t. Theideaofmulti-branchnetworkscanbeusedfordesigningsub-modelsoftheencoder. A simpleexampleisthatwecreatemultiplepathsinparallelforsomelayersoftheencoder(see sub-figure (b)). Another example is multi-head attention (see sub-figure (c)) where we use differentheadstolearndifferentrepresentations. representtheprevioussentenceinthesamedocument. Wethushaveacontext-awaremodelby combiningthetwoencoders[Voitaetal.,2018;Lietal.,2020a]. Furthermore,thearchitectures oftheencodersdonotneedtoberestrictedtoTransformer,andwecanchoosedifferentmodels fordifferentbranches. Forexample,asawidely-used2-branchencodingarchitecture,wecan useaCNN-basedencodertomodellocalcontext,andaTransformerencodertomodelglobal context[Wuetal.,2020b]. Sub-modelsofaTransformermodelcanalsobemulti-branchneuralnetworks. SeeFigure 6.8(b)foranexampleinvolvingtwoself-attentionbranches. Oneisthestandardself-attention networkAtt (H). Theotheristhesyntax-awareself-attentionnetworkAttSyn (H). The self self outputoftheself-attentionmodelisalinearcombinationoftheoutputsofthesetwobranches [Xuetal.,2021b],givenby H = α·Att (H)+(1−α)·AttSyn (H) (6.60) self self self whereαisacoefficientofcombination. H canbeusedasusualbytakingalayernormal- self izationfunctionandaddingaresidualconnection,andsotheoverallarchitectureisthesame asstandardTransformermodels. Multi-headattentionnetworkscanalsobeviewedasformsofmulti-branchmodels. There- fore,wecanprovideguidancefromsyntaxtoonlysomeoftheheadswhilekeepingtherest unchanged[Strubelletal.,2018]. ThisapproachisillustratedinFigure6.8(c)whereonlyone 290 Chapter 6. Transformers headoftheself-attentionsub-layermakesuseofsyntaxtreesforcomputingattentionweights. 6.2.4 Multi-scale Models In linguistics, syntax studies how sentences are built up by smaller constituents. Different levelsoftheseconstituentsareingeneralorganizedinahierarchicalstructure,calledsyntactic hierarchy. Itisthereforepossibletousemultiplelevelsofsyntacticconstituentstoexplain the same sentence, for example, words explain how the sentence is constructed from small meaningfulunits,andphrasesexplainhowthesentenceisconstructedfromlargerlinguistic units. Multi-scaleTransformersleveragevaryingabstractionlevelsofdatatorepresentasentence usingdiversefeaturescales. Acommonapproachistowriteasentenceinmultipledifferent formsandthentocombinethemusingamulti-branchnetwork[Haoetal.,2019]. Forexample, considerasentence Theoldestbeer-makingfacilitywasdiscoveredinChina. Wecantokenizeitintoasequenceofwords,denotedby x =Theoldestbeer-makingfacilitywasdiscoveredinChina. words Alternatively,wecanwriteitasasequenceofphrasesbyusingaparser,denotedby x =[Theoldestbeer-makingfacility] [wasdiscoveredinChina] [.] phrases NP VP . Thesimplestwaytobuildamulti-scalemodelistoencodex andx usingtwo words phrases separateTransformerencoders. Then,theoutputsoftheseencodersarecombinedinsomeway. ThisleadstothesameformasEq. (6.59),andwecanviewthismodelasaninstanceofthe generalmulti-encoderarchitecture. Bothx andx canbeviewedassequencesoftokens,forexample,x has words phrases words nine word-based tokens, and x has three phrase-based tokens5. However, involving phrases all possible phrases will result in a huge vocabulary. We therefore need some method to represent eachphrase asan embeddingin a cheapway. By treatingphrase embeddingas a sequencemodelingproblem,itisstraightforwardtolearnsub-sequencerepresentationssimply byconsideringthesequencemodelsdescribedinthepreviouschaptersandthischapter. Now wehaveatwo-stagelearningprocess. Inthefirststage,welearntheembeddingsofinputunits ondifferentscalesusingseparatemodels. Inthesecondstage,welearntoencodesequences ondifferentscalesusingamulti-branchmodel. Moregenerally,wedonotneedtorestrictourselvestolinguisticallymeaningfulunitsin multi-scalerepresentationlearning. Forexample,wecanlearnsub-wordsegmentationsfrom dataandrepresentaninputsentenceasasequenceofsub-words. Thisresultsinahierarchical 5x comprisesthreetokensTheoldestbeer-makingfacility,wasdiscoveredinChina,and.. phrases 6.2 Syntax-aware Models 291 representationofthesentence,forexample,sub-words→words→phrases. Whilethelearned sub-wordsmaynothavelinguisticmeanings,theyprovideanewinsightintomodelingwords and phrases, as well as a new scale of features. Also, we do not need to develop multiple encoders for multi-scale modeling. An alternative approach is to take representations on differentscalesinthemulti-headself-attentionattentionmodules, whichmakesiteasierto modeltheinteractionsamongdifferentscales[Guoetal.,2020;Lietal.,2022b]. Aproblemwiththeapproachesdescribedabove,however,isthattherepresentations(or attention weight matrices) learned on different scales are of different sizes. For example, in the above examples, the representation learned from x is a 9×d matrix, and the words representation learned from x is a 3×d matrix.",
    "modeling. An alternative approach is to take representations on differentscalesinthemulti-headself-attentionattentionmodules, whichmakesiteasierto modeltheinteractionsamongdifferentscales[Guoetal.,2020;Lietal.,2022b]. Aproblemwiththeapproachesdescribedabove,however,isthattherepresentations(or attention weight matrices) learned on different scales are of different sizes. For example, in the above examples, the representation learned from x is a 9×d matrix, and the words representation learned from x is a 3×d matrix. A simple solution to this problem phrases is to perform upsampling on the phrase-based representation to expand it to a 9×d matrix. Likewise,wecanperformdownsamplingontheword-basedrepresentationtoshrinkittoa 3×dmatrix. Then,thecombinationmodelCombine(·)canbethesameasthosedescribedin Section6.2.3. It is worth noting that multi-scale modeling is widely discussed in several fields. For example,incomputervision,multi-scalemodelingisoftenreferredtoasaprocessoflearning a series of feature maps on the input image [Fan et al., 2021; Li et al., 2022f]. Unlike the multi-branchmodelspresentedhere,themulti-scalevisionTransformermodelsmakeuseof thehierarchicalnatureoffeaturesinrepresentingimages. Systemsofthiskindareoftenbased on a stack of layers in which each layer learns the features on a larger scale (e.g., a higher channelcapacity)fromthefeaturesproducedbythepreviouslayer. 6.2.5 Transformers as Syntax Learners Sofarwehavediscussedsyntaxtreesasbeingconstraintsorpriorsontheencodingprocessso thatwecanmakeuseoflinguisticrepresentationsinlearningneuralnetworks. Itisnaturalto wonderwhethertheseneuralmodelscanlearnsomeknowledgeoflinguisticstructurefrom datawithouthumandesignlinguisticannotations. Thisreflectsoneofthegoalsofdeveloping NLPsystems: linguisticknowledgecanbelearnedfromdataandencodedinmodels. InordertoexplorethelinguisticpropertieslearnedbyNLPsystems,asimplemethodisto examinethesyntacticbehaviorsoftheoutputsofthesystems. Forexample,wecanexamine whethertheoutputsoflanguagegenerationsystemshavegrammaticalerrors. Anotherexample istoaskthesesystemstoaccomplishtasksthatmakesenseforlinguistics, thoughtheyare not trained to do so [Brown et al., 2020]. However, examining and explaining how model predictionsexhibitsyntacticabilitiesisnotsufficienttoanswerthequestion. Itisalsothecase thattheneuralnetworkshavelearnedsomeknowledgeaboutlanguage,butitisnotusedin prediction[Clarketal.,2019a]. Therefore,weneedtoseewhatismodeledandlearnedinside theseneuralnetworks. One approach to examining the latent linguistic structure in Transformer models is to developprobestoseewhetherandtowhatextentthesemodelscapturenotionsoflinguistics, such as dependency relations and parts-of-speech. A general approach to probing is to extracttheinternalrepresentationsofthemodelsandprobethemforlinguisticphenomena. ForTransformer, itisusuallyachievedbyexaminingtheattentionmapand/oroutputofan 292 Chapter 6. Transformers Loss(ProbingTraining) Output(Probing) Predictor Predictor Loss(Pre-training) ... ... ... ... ... ... Input(Pre-training) Input(ProbingTraining) Input(Probing) (a)TrainingtheTransformerModel (b)TrainingtheProbingPredictor (c)ProbingonNewData Figure 6.9: An overview of probing for Transformer-based models. Given a Transformer model(e.g.,aTransformer-basedlanguagemodel),wefirstoptimizethemodelparameterson someunlabeleddata. Then,wedevelopapredictorwhichtakesthestatesofahiddenlayer oftheTransformermodelandgeneratesoutputsforaprobingtask(seesub-figure(a)). The predictorcanbetrainedasusualinwhichonlytheparametersofthepredictorareoptimized andtheparametersoftheTransformermodelarefixed(seesub-figure(b)). TheTransformer modelandthepredictorareusedtogethertomakepredictionsonnewdataforprobing(see sub-figure(c)). attention layer. Then, we construct a probing predictor (or probing classifier) that takes these internal representations as input and produces linguistic notions as output [Belinkov, 2022]. Theprobingpredictorcanbebasedoneithersimpleheuristicsorparameterizedmodels optimizedontheprobingtask. Recentworkshowsthatlarge-scaleTransformer-basedlanguage modelsexhibitgoodbehaviors,calledemergentabilities,invariousprobingtasks. However, wewillnotdiscussdetailsoftheselanguagemodelingsystemsinthischapter,butleavethem inthefollowingchapters. Nevertheless,weassumeherethatwehaveaTransformerencoder thathasbeenwelltrainedonunlabeleddataandcanbeusedforprobing. Figure6.9illustrates theprocessofprobing. Many probing methods have been used in recent work on analyzing and understanding whatislearnedinneuralencoders. Herewedescribesomeofthepopularones. • Trees. GivenatrainedTransformerencoder,itiseasytoknowhow“likely”twowordsof asentencehavesomelinguisticrelationshipbycomputingtheattentionweightbetween them. We can use this quantity to define a metric measuring the syntactic distance betweenthetwowordsatpositionsiandj d (i,j) = 1−α(i,j) (6.61) s Byusingthismetricitisstraightforwardtoconstructtheminimum-spanningtreefor thesentence,thatis,weconnectallthewordstoformatreestructurewiththeminimum 6.2 Syntax-aware Models 293 totaldistance. Thetreestructurecanbeseenasalatenttreerepresentationofthesentence thatisinducedfromtheneuralnetwork. Whilethisdependency-tree-likestructurecan beusedasasourceoflearnedsyntacticinformationindownstreamtasks,itsaysnothing about our knowledge of syntax. An approach to aligning the representations in the encoderwithlinguisticstructureistolearntoproducesyntaxtreesthatareconsistent withhumanannotations. Todothis,weneedtodevelopaprobingpredictorthatcanbe trainedontree-annotateddata. Supposethatthereisahumanannotateddependencytree ofagivensentence. Foreachpairofwords,wecanobtainadistanceω(i,j)bycounting thenumberofedgesbetweenthem. Then,wecanlearnadistancemetricbasedonthe internalrepresentationsoftheencodertoapproximateω(i,j). Asimpleformofsucha metricisdefinedtobetheEuclideandistance[Manningetal.,2020]. LetA∈Rd×ks be aparametermatrix. TheformoftheEuclideandistanceisgivenby (cid:113) d (i,j) = ||(h −h )A||2 (6.62) s i j 2 whereh andh aretherepresentationsproducedbyanencodinglayeratpositionsi i j andj6. Givenasetoftree-annotatedsentencesS,wecanoptimizethemodelby (cid:88) 1 (cid:88) Aˆ = argmax |ω(i,j)−d2(i,j)| (6.63) |s|2 s A s∈S i∈s,j∈s where |s| is length of the sentence s, and (i,j) indicates a pair of words in s. The optimized model is then used to parse test sentences via the minimum-spanning tree algorithm,andwecancomparetheparsetreesagainstthehuman-annotatedtrees. To obtaindirectedtrees,whicharestandardformsofdependencysyntax,onecanupdate theabovemodelbyconsideringtherelativedistanceofawordtotheroot. Moredetails can be found in Manning et al. [2020]’s work. Here the probing predictor functions similarlytoaneuralparser,trainedtopredictasyntaxtreebasedonarepresentationof theinputsentence. Thisideacanbeextendedtootherformsofsyntacticstructure,such asphrasestructuretrees[Shietal.,2016]. • Syntactic and Semantic Labels. Many syntactic and semantic parsing tasks can be framedasproblemsofpredictinglinguisticlabelsgivenasentenceoritssegments. A simpleexampleispart-of-speechtagginginwhicheachwordofasentenceislabeled with a word class. A probe for part-of-speech tagging can be a classifier that takes a representation h each time and outputs the corresponding word class. One general j probingapproachtotheseproblemsisedgeprobing[Tenneyetal.,2019b;a]. Givena sentence,alabelededgeisdefinedasatuple (span ,span ,label) 1 2 wherespan isaspan[i ,j ],andspan isanotherspan[i ,j ](optionally),andlabel 1 1",
    "be framedasproblemsofpredictinglinguisticlabelsgivenasentenceoritssegments. A simpleexampleispart-of-speechtagginginwhicheachwordofasentenceislabeled with a word class. A probe for part-of-speech tagging can be a classifier that takes a representation h each time and outputs the corresponding word class. One general j probingapproachtotheseproblemsisedgeprobing[Tenneyetal.,2019b;a]. Givena sentence,alabelededgeisdefinedasatuple (span ,span ,label) 1 2 wherespan isaspan[i ,j ],andspan isanotherspan[i ,j ](optionally),andlabel 1 1 1 2 2 2 6Ingeneral,h iandh j aretheoutputsofthelastlayeroftheencoder.Alternatively,theycanbeweightedsums oftheoutputsofallthelayers. 294 Chapter 6. Transformers is the corresponding label. Our goal is to learn a probe to predict label given span 1 andspan . Forexample,forpart-of-speechtagging,span isaunitspan[j,j]foreach 2 1 positionj,span isanemptyspan,andlabelisthepart-of-speechtagcorresponding 2 to the j-th word of the sentence; for dependency parsing and coreference resolution, span andspan aretwowordsorentities,andlabelistherelationshipbetweenthem; 1 2 forconstituencyparsing,span isaspanofwords,span isanemptyspan,andlabel 1 2 isthesyntacticcategoryofthetreenodeyieldingspan . Insimplecases,theprobing 1 modelcanbeamulti-layerfeed-forwardneuralnetworkwithaSoftmaxoutputlayer. Asusual,thismodelistrainedonlabeleddata,andthentestedonnewdata. • SurfaceFormsofWordsandSentences. Probingtaskscanalsobedesignedtoexamine whethertherepresentationsembedthesurfaceinformationofsentencesorwords[Adi etal.,2016;Conneauet al.,2018]. Asimplesentence-levelprobingtaskissentence length prediction. To do this, we first represent the sentence as a single vector h7, andthenbuildaclassifiertocategorizehintothecorrespondinglengthbin. Similarly, probescanbebuilttopredictwhethertwowordsatpositionsiandj arereorderedinthe sentencegivenh andh . Also,wecandevelopprobestoaddressconventionalproblems i j inmorphology. Forexample,wereconstructthewordatpositionj orpredictitssense with the representation h . In addition, probing tasks can be focused on particular j linguisticproblems,forexample,numeracy[Wallaceetal.,2019]andfunctionwords [Kimetal.,2019]. • Cloze. Ofcourse,wecanprobeneuralmodelsforproblemsbeyondsyntaxandmorphol- ogy. Oneperspectiveonlarge-scalepre-trainedTransformermodelsistoviewthemas knowledgebasescontainingfactsabouttheworld. Itisthereforetemptingtoseeifwe canapplythemtotestfactualknowledge. Asimplemethodistoaskaprobetorecover themissingitemofasentence[Petronietal.,2019]. Forexample,ifwehaveaclozetest Shijiwaswrittenby . wewishtheprobetogiveananswerSimaQianbecausethereisasubject-object-relation fact(Shiji,SimaQian,written-by). Thisprobecansimplybeamaskedlanguagemodel thatiswidelyusedinself-supervisedlearningofTransformerencoders. InNLP,probingiscloselyrelatedtopre-trainingoflargelanguagemodels(seeChapters 7and8). Ingeneral,wecanseeprobingtasksasapplicationsofthesepre-trainedlanguage models,thoughprobingisordinarilyusedtogiveaquicktestofthemodels. Ideallywewould liketodevelopaprobethatmakesbestuseoftherepresentationstodealwiththeproblems. However,whenaprobeiscomplexandsufficientlywell-trained,itmightbedifficulttosay iftheproblemissolvedbyusingtherepresentationsortheprobeitself. Acommonwayto emphasizethecontributionofprobesinproblem-solvingistocomparethemwithreasonable baselines or conduct the comparison on control tasks [Hewitt and Liang, 2019; Belinkov, 2022]. 7hcanbecomputedbyperformingapoolingoperationon{h 1 ,...,hm} 6.3 Improved Architectures 295 6.3 Improved Architectures In this section we present several improvements to the vanilla Transformer model. Unlike theprevioussection,mostoftheimprovementsarefromtheperspectiveofmachinelearning, ratherthanlinguistics. 6.3.1 Locally Attentive Models Methods of self-attention, as discussed in Section 6.1.3, can also be viewed as learning representations of the entire input sequence. The use of this global attention mechanism can lead to a better ability to deal with long-distance dependencies, but this model has a shortcoming: localinformationisnotexplicitlycaptured. Hereweconsiderafewtechniques thatattempttomodelthelocalnessofrepresentations. 1. Priors of Local Modeling OneofthesimplestwaysofintroducinglocalmodelsintoTransformersistoaddapenalty termtotheattentionfunctioninordertodiscouragelargeattentionweightsbetweendistant positions. Ontheencoder-side,thisleadstoaformthatwehavealreadyencounteredseveral timesinthischapter. Hq[Hk]T AttLocal (H) = Softmax( √ −γ·G)Hv (6.64) self d whereγ istheweight(ortemperature)ofthepenaltyterm,andG∈Rm×m isthematrixof penalties. EachentryG(i,j)indicateshowmuchwepenalizethemodelgivenpositionsiand j. AsimpleformofG(i,j)isadistancemetricbetweeniandj,forexample G(i,j) = |i−j| (6.65) OrG(i,j)canbedefinedasaGaussianpenaltyfunction[Yangetal.,2018a] (i−j)2 G(i,j) = (6.66) 2σ2 i where σ is the standard deviation of the Gaussian distribution. For different j, both of the i above penalty terms increase, linearly or exponentially, away from the maximum at i with distance|i−j|. Thismethodcanbeextendedtothecross-attentionmodel,likethis Sq[Hk]T AttLocal (H,S) = Softmax( √ −γ·G)Hv (6.67) cross d whereGisann×mmatrix. EachentryofGcanbedefinedas (µ −j)2 i G(i,j) = (6.68) 2σ2 i 296 Chapter 6. Transformers where µ is the mean of the Gaussian distribution over the source-side positions. Both µ i i and σ can be determined using heuristics. Alternatively, we can develop additional neural i networkstomodelthemandlearncorrespondingparameterstogetherwithotherparametersof theTransformermodel. Forexample,wecanuseafeed-forwardneuralnetworktopredictµ i givens . i OnealternativetoEq. (6.64)(orEq. (6.67))treatsthepenaltytermasaseparatemodeland combinesitwiththeoriginalattentionmodel. Forexample,wecandefinetheself-attention modelas (cid:18) Hq[Hk]T (cid:19) AttLocal (H) = (1−β)·Softmax( √ )+β·Softmax(−γ·G) Hv (6.69) self d where β ∈[0,1] is the coefficient of the linear combination. Note that, to avoid empirical choicesofthevaluesofαandβ,wecanusegatingfunctionstopredictαandβ andtrainthese functionsasusual. Anotheralternativeistouseamultiplicativemasktoincorporatethepriorintomodeling, asinEq. (6.54). Thisisgivenby Hq[Hk]T AttLocal (H) = Softmax( √ ⊙G′)Hv (6.70) self d HereG′∈[0,1]m×m isamatrixofscalars. ThescalarG′(i,j)givesavalueof1wheni=j, andasmallervalueasj movesawayfromi. G′(i,j)canbeobtainedbynormalizing−G(i,j) overallj orusingalternativefunctions. 2. Local Attention The term local attention has been used broadly to cover a wide range of problems and to refer to many different models in the NLP literature. The methods discussed above",
    "AttLocal (H) = Softmax( √ ⊙G′)Hv (6.70) self d HereG′∈[0,1]m×m isamatrixofscalars. ThescalarG′(i,j)givesavalueof1wheni=j, andasmallervalueasj movesawayfromi. G′(i,j)canbeobtainedbynormalizing−G(i,j) overallj orusingalternativefunctions. 2. Local Attention The term local attention has been used broadly to cover a wide range of problems and to refer to many different models in the NLP literature. The methods discussed above are thosethatimposesoftconstraintsonattentionmodels. Infact,localattentionhasitsorigins in attempts to restrict the scope of attention models for considerations of modeling and computationalproblems[Luongetal.,2015]. Researchinthisareaoftenlooksintointroducing hardconstraints,sothattheresultingmodelscanfocusonpartsoftheinputandignorethe rest. Forexample,wecanpredictaspanofsource-sidepositionsforperformingtheattention function given a target-side position [Sperber et al., 2018; Yang et al., 2018a; Sukhbaatar etal.,2019]. Also,attentionspanscanbeinducedfromsyntaxtrees,forexample,knowing sub-treestructuresofasentencemayhelpwinnowthefieldthatthemodelconcentrateson inlearningtherepresentation. Thus,manyofthesyntax-constrainedmodelsareinstancesof localattention-basedmodels(seeSection6.2.4). Inaddition,theconceptoflocalattention canbeextendedtodeveloparichsetofmodels,suchassparseattentionmodels,although thesemodelsareoftendiscussedinthecontextofefficientmachinelearningmethods. Wewill seeafewexamplesoftheminSection6.4. Indeeplearning,oneofthemostwidelyusedmodelsforlearningfeaturesfromarestricted regionoftheinputisCNNs. ItisthusinterestingtoconsidermethodsofcombiningCNNs andTransformermodelstoobtainthebenefitsofbothapproaches,forexample,CNNsdeal 6.3 Improved Architectures 297 withshort-termdependencies, andself-attentionmodelsdealwithlong-termdependencies. Oneapproachistobuildatwo-branchsequencemodelwhereonebranchisbasedonCNNs and the other is based on self-attention models [Wu et al., 2020b]. Another approach is to incorporateCNNlayersintoTransformerblocksinsomewaythatwecanlearnbothlocaland globalrepresentationsthroughadeepmodel[Wuetal.,2019;Gulatietal.,2020]. 3. Relative Positional Embedding Relativepositionalembedding,alsoknownasrelativepositionalrepresentation(RPR),is animprovementtotheabsolutepositionalembeddingmethodusedinstandardTransformer systems[Shawetal.,2018;Huangetal.,2018]. TheideaofRPRisthatwemodelthedistance betweentwopositionsofasequenceratherthangivingeachpositionafixedrepresentation. As a result, we have a pair-wise representation PE(i,j) for any two positions i and j. One simplewaytodefinePE(i,j)istoconsideritasalookuptableforallpairsofiandj. More specifically, let u be a d-dimensional representation for a given distance π. The form of π PE(i,j)inthevanillaRPRmethodisgivenby PE(i,j) = u (6.71) clip(j−i,krpr) whereclip(x,k )isafunctionthatclipsxintheinterval[−k ,k ] rpr rpr rpr clip(x,k ) = max{−k ,min{x,k }} (6.72) rpr rpr rpr Thus,wehaveamodelwithparameters   u −krpr  . .   .    U =  u  (6.73) rpr  0   .   . .    u krpr Whilethismatrixnotationisusedinarelativelyinformalway,wecanviewU asamatrix rpr ∈R(2krpr+1)×d,andselectarowcorrespondingtoclip(j−i,k rpr )whenRPRisrequiredfor giveniandj. Using the above method, we can define three RPR models PEq(i,j), PEk(i,j) and PEv(i,j)forqueries,keys,andvalues,respectively. Then,followingtheformofEq. (6.17), theoutputoftheself-attentionmodelatpositionicanbewrittenas m c = (cid:88) α (cid:2) hv+PEv(i,j) (cid:3) i i,j j j=1 m m (cid:88) (cid:88) = α hv+ α PEv(i,j) (6.74) i,j j i,j j=1 j=1 298 Chapter 6. Transformers ··· ··· block2 block2 FFNSub-layer FFNSub-layer Self-attentionSub-layer Self-attentionSub-layer block1 block1 FFNSub-layer FFNSub-layer ... PE(i,j)(krpr=3) ··· 5 u−3u−3u−2u−1 u0 4 u−3u−2u−1 u0 u1 Self-attentionSub-layer Self-attentionSub-layer 3 u−2u−1 u0 u1 u2 2 u−1 u0 u1 u2 u3 1 u0 u1 u2 u3 u3 i x x ··· x x x ··· x j 1 2 3 4 5 ··· 1 2 m 1 2 m + + + + + + PE(1) PE(2) ··· PE(m) PE(1) PE(2) ··· PE(m) 1 2 ··· m (a)TransformerwithoutRPR (b)TransformerwithRPR Figure6.10: Transformerencoderswithoutandwithrelativepositionalrepresentation(RPR). InRPR,eachpairofpositionsisrepresentedasavectorPE(i,j)usingamodelparameterized by U . PE(i,j) is fed into each self-attention sub-layer so that we can make use of the rpr positionalinformationinintermediatestepsoflearningrepresentations. where hv is the j-th row vector of Hv. This representation comprises two components: j (cid:80)m α hv isthebasicrepresentation,and (cid:80)m α PEv(i,j)isthepositionalrepresenta- j=1 i,j j j=1 i,j tion. Theattentionweightα iscomputedinaregularway,butwithadditionaltermsPEq(i,j) i,j andPEk(i,j)addedtoeachqueryandkey. [hq+PEq(i,j)][hk+PEk(i,j)]T α = Softmax( i √ j ) (6.75) i,j d Figure6.10showstheTransformerencoderarchitectureswithandwithoutRPR.When RPRisadopted,PEq(i,j),PEk(i,j),PEv(i,j)aredirectlyfedtoeachself-attentionsub-layer, andsowecanmakebetteruseofpositionalinformationforsequencemodeling. Notethat, the use of the clipping function (see Eq. (6.72)) makes the modeling simple because we do not need to distinguish the relative distances for the cases |j−i|≥k . This clipped rpr distance-basedmodelcanlead,inturn,tobettermodelinginlocalcontextwindows. Eqs. (6.74)and(6.75)provideageneralapproachtoposition-sensitivesequencemodeling. There are many variants of this model. In Shaw et al. [2018]’s early work on RPR, the 6.3 Improved Architectures 299 positionalrepresentationsforqueriesareremoved,andthemodelworksonlywithPEk(i,j) andPEv(i,j),likethis hq[hk+PEk(i,j)]T α = Softmax( i j √ ) (6.76) i,j d By contrast, there are examples that attempt to",
    "|j−i|≥k . This clipped rpr distance-basedmodelcanlead,inturn,tobettermodelinginlocalcontextwindows. Eqs. (6.74)and(6.75)provideageneralapproachtoposition-sensitivesequencemodeling. There are many variants of this model. In Shaw et al. [2018]’s early work on RPR, the 6.3 Improved Architectures 299 positionalrepresentationsforqueriesareremoved,andthemodelworksonlywithPEk(i,j) andPEv(i,j),likethis hq[hk+PEk(i,j)]T α = Softmax( i j √ ) (6.76) i,j d By contrast, there are examples that attempt to improve the RPR model in computing attention weights but ignore PEv(i,j) in learning values [Dai et al., 2019; He et al., 2021]. InsteadoftreatingRPRasanadditivetermtoeachrepresentation,researchersalsoexplore otherwaysofintroducingRPRintoTransformer[Huangetal.,2020b;Raffeletal.,2020]. We refertheinterestedreaderstothesepapersformoredetails. 6.3.2 Deep Models Many state-of-the-art NLP systems are based on deep Transformer models. For example, recentlargelanguagemodelsgenerallycomprisetensofTransformerlayers(ormoreprecisely, hundreds of layers of neurons), demonstrating strong performance on many tasks [Ouyang etal.,2022;Touvronetal.,2023a]. BystackingTransformerlayers,itisstraightforwardto obtain a deep model. However, as is often the case, training very deep neural networks is challenging. Adifficultyarisesfromthefactthattheerrorsurfacesofdeepneuralnetworksare highlynon-convexandhavemanylocaloptimathatmakethetrainingprocesslikelytogetstuck inthem. Whilethereareoptimizationalgorithmsthatcanhelpalleviatethisproblem,most ofthepracticaleffortsexploretheuseofgradient-basedmethodsforoptimizingdeepneural networks. Asaresult,trainingamodelwithmanyTransformerlayersbecomeschallenging duetovanishingandexplodinggradientsduringback-propagation. Hereweconsiderseveral techniquesfortrainingdeepTransformermodels. 1. Re-thinking the Pre-Norm and Post-Norm Architectures Asintroducedpreviously,aTransformersub-layerisaresidualnetworkwhereashortcutis created to add the input of the network directly to the output of this sub-layer. This allows gradients to flow more directly from the output back to the input, mitigating the vanishing gradientproblem. Ingeneral,aresidualconnectioninTransformerisusedtogetherwithalayer normalizationunittoformasub-layer. Thisleadstotwotypesofarchitecture,calledpost-norm andpre-norm. Tobespecific,recallfromSection6.1.4thatthepost-normarchitecturecanbe expressedas zl = LNorm(Fl(zl−1)+zl−1) (6.77) wherezl andzl−1 aretheoutputandinputofthesub-layerl,andFl(·)isthecorefunction ofthissub-layer. Thepre-normarchitecturetakestheidentitymappingzl outsidethelayer normalizationfunction,givenintheform zl = LNorm(Fl(zl−1))+zl−1 (6.78) 300 Chapter 6. Transformers Considerthedifferencebetweentheinformationflowinthesetwoarchitectures: • Thepost-normarchitecturepreventstheidentitymappingoftheinputfromaddingtothe outputofthesub-layer. Thisisnotatrueresidualnetwork,becausealltheinformation ispassedonthroughanon-linearfunction(i.e.,thelayernormalizationunit). Thus,the post-normarchitectureisnotvery“efficient”forback-propagation. Wangetal.[2019a] showthatthegradientofthelossofanLsub-layerTransformernetworkwithrespectto zl isgivenby ∂E ∂E L (cid:89) −1 ∂LNorm(vk) L (cid:89) −1(cid:18) ∂Fk(zk) (cid:19) = · · 1+ (6.79) ∂zl ∂zL ∂vk ∂zk k=l k=l wherezL istheoutputofthelastlayer,vk isashortforFk(zk−1),andE istheerror ∂LNorm(vk) ∂Fk(zk) measuredbysomelossfunction. and arethegradientsofthelayer ∂vk ∂zk normalizationfunctionandthecorefunction,respectively. Althoughtheequationhere appearsabitcomplex,weseethat (cid:81)L−1∂LNorm(vk) issimplyaproductofL−lfactors. k=l ∂vk ThismeansthattheerrorgradientwillberescaledmoretimesifLbecomeslarger,and thereisahigherriskofvanishingandexplodinggradientsforadeepermodel. • Thepre-normarchitecturedescribesastandardresidualneuralnetworkwheretheinput ofawholenetworkisaddedtoitsoutput. Wecanwritethegradientoftheerroratzl as ∂E ∂E (cid:32) L (cid:89) −1 ∂Fk(LNorm(zk)) (cid:33) = · 1+ ∂zl ∂zL ∂zk k=l ∂E ∂E L (cid:89) −1 ∂Fk(LNorm(zk)) = + · (6.80) ∂zL ∂zL ∂zk k=l Itiseasytoseethat ∂E receivesdirectfeedbackregardingtheerrorsmadebythemodel, ∂zl becausethefirsttermofthesummationontheright-handside(i.e., ∂E )isthegradient ∂zL ofthemodeloutputwhichisindependentofthenetworkdepth. Theuseofthepre-normarchitecturealsohelpsoptimizationduringearlygradientdescent steps. For example, it has been found that pre-norm Transformer models can be trained by usingalargerlearningrateintheearlystageoftraininginsteadofgraduallyincreasingthe learningratefromasmallvalue[Xiongetal.,2020]. Whilethepre-normarchitectureleadstoeasieroptimizationofdeepTransformermodels, we would not simply say that it is a better choice compared to the post-norm architecture. In fact, both post-norm and pre-norm Transformer models have been successfully used in many applications. For example, the post-norm architecture is widely used in BERT-like models,whilethepre-normarchitectureisamorepopularchoiceinrecentgenerativelarge language models. Broadly, these two architectures provide different ways to design a deep Transformermodel,aswellasdifferentadvantagesanddisadvantagesindoingso. Thepost- normarchitectureforcestherepresentationtobelearnedthroughmorenon-linearfunctions, 6.3 Improved Architectures 301 but in turn results in a complicated model that is relatively hard to train. By contrast, the pre-normarchitecturecanmakethetrainingofTransformermodelseasier,butwouldbeless expressivethanthepost-normcounterpartifthelearnedmodelsareoverlydependentonthe shortcutpaths. Animprovementtothesearchitecturesistocontroltheextenttowhichwewantto“skip” a sub-layer. A simple way to do this is to weight different paths rather than treating them equally. Forexample,ascalarfactorofaresidualconnectioncanbeintroducedtodetermine howheavilyweweightthisresidualconnectionrelativetothepathofthecorefunction[He etal.,2016b;Liuetal.,2020a;b]. Amoregeneralformofthismodelisgivenby zl = LNorm(Fl(zl−1)+β·zl−1)+γ·zl−1 (6.81) whereβ istheweightoftheidentitymappinginsidethelayernormalizationfunction,andγ istheweightoftheidentitymappingoutsidethelayernormalizationfunction. Clearly,both thepost-normandpre-normarchitecturescanbeseenasspecialcasesofthisequation. That is, if β =1 and γ =0, then it will become Eq. (6.77); if β =0 and γ =1, it will become Eq. (6.78). Thismodelprovidesamulti-branchviewofbuildingresidualblocks. Theinput tothisblockcanbecomputedthroughmultiplepathswithdifferentmodelingcomplexities. Whenβ andγ aresmall,therepresentationisforcedtobelearnedthrougha“deep”model with multiple layers of cascaded non-linear units. In contrast, when β and γ are large, the representation is more likely to be learned using a “shallow” model with fewer layers. To determine the optimal choices of β and γ, one can give them fixed values by considering sometheoreticalpropertiesorsystemperformanceonvalidationsets,orcomputethesevalues byusingadditionalfunctionsthatcanbetrainedtodoso[Srivastavaetal.,2015]. Itshould be emphasized",
    "layers of cascaded non-linear units. In contrast, when β and γ are large, the representation is more likely to be learned using a “shallow” model with fewer layers. To determine the optimal choices of β and γ, one can give them fixed values by considering sometheoreticalpropertiesorsystemperformanceonvalidationsets,orcomputethesevalues byusingadditionalfunctionsthatcanbetrainedtodoso[Srivastavaetal.,2015]. Itshould be emphasized that many other types of architecture can be considered in the design of a Transformersub-layer. Itispossible,forinstance,tointroducemorelayernormalizationunits intoasub-layer[Dingetal.,2021;Wangetal.,2022b],or,onthecontrary,tosimplyremove themfromasub-layer[Bachlechneretal.,2021]. 2. Parameter Initialization As with other deep neural networks, there is interest in developing parameter initialization methodsfordeepTransformermodelsinordertoperformoptimizationonsomeregionaround a better local optimum. However, initialization is a wide-ranging topic for optimization of machine learning models, and the discussion of this general topic lies beyond the scope of this section. Here we will discuss some of the parameter initialization methods used in Transformer-basedsystemsratherthanthegeneraloptimizationproblems. While the parameters of a neural network can be set in various different ways, most practicalsystemsadoptsimpletechniquestogiveappropriateinitialvaluesofmodelparameters. Consider,forexample,theXavierinitializationforaparametermatrixW∈Rdin×dout [Glorot andBengio,2010]. Wedefineavariableη by (cid:114) 6 η = gain· (6.82) d +d in out 302 Chapter 6. Transformers where gain is a hyper-parameter which equals 1 by default. Then, each entry of W can be initializedbyusingauniformdistribution W ∼ U(−η,η) (6.83) or,alternatively,usingaGaussiandistribution W ∼ Gaussian (cid:0) 0,η2(cid:1) (6.84) ThismethodcanbeeasilyadaptedtoinitializeTransformermodelshavingalargenumber oflayers. Onecommonwayistofindamoresuitablevalueofgainbytakingintoaccountthe factthattheinitialstatesofoptimizationmightbedifferentforneuralnetworksofdifferent depths. Forexample,onecanincreasethevalueofgainasthedepthofthemodelgrows. Then, gaincanbedefinedasafunctionofthenetworkdepthintheform gain = a·Lb (6.85) whereaisthescalar,andLb isthenetworkdepthraisedtothepowerofb. Typically,aand b can be positive numbers, which means that it is preferred to have larger initial values for theparametersfordeepermodels. Forexample,Wangetal.[2022a]showthat,bychoosing appropriatevaluesforaandb,averydeepTransformermodelcanbesuccessfullytrained. Eq. (6.85)assignsgainthesamevalueforallofthesub-layers. However,itisfoundthat thenormofgradientsbecomessmallerwhenasub-layermovesawayfromtheoutputlayer. Thisconsistentapplicationofgainacrosstheentiremodelcouldresultinunder-trainingofthe lowerlayersduetothegradientvanishingproblem. Forthisreason,onecandevelopmethods thataresensitivetothepositionofasub-layerintheneuralnetwork. Thegeneralformofsuch methodsisgivenby a gain = (6.86) lb Hereldenotesthedepthofasub-layer. Iflislarger(i.e.,thesub-layerisclosertotheoutput), gain will be smaller and the corresponding parameters will be set to smaller values. An exampleofthismethodcanbefoundinZhangetal.[2019a]’swork. Itisalso,ofcourse,straightforwardtoapplygeneralmethodsofinitializingdeepmulti- layer neural networks to Transformer models. An example is to consider the Lipschitz constantinparameterinitialization,whichhasbeenshowntohelpimprovethestabilityof training deep models [Szegedy et al., 2014b; Xu et al., 2020]. Another approach is to use second-ordermethodstoestimatethepropervaluesoftheparameters. Forexample,onecan computetheHessianofeachparametermatrixtomodelitscurvature[Skorskietal.,2021]. Formodelswithalargenumberoflayers,itisalsopossibletopre-trainsomeofthelayers viasmallermodelsandusetheirtrainedparameterstoinitializebiggermodels[Chenetal., 2015]. Thatis,wefirstobtainaroughestimationoftheparametersinacheapway,andthen continuethetrainingprocessonthewholemodelasusual. Thesemethodsfallintoaclassof 6.3 Improved Architectures 303 trainingmethods,calledmodelgrowthordepthgrowth. Asasimpleexample,consideraTransformermodel(e.g.,aTransformerencoder)of2L sub-layers. Wecantrainthismodelbyusingtheshallow-to-deeptrainingmethod[Lietal., 2020b]. First,wetrainanL-sub-layermodel(callittheshallowmodel)inaregularway. Then, wecreatea2L-sub-layermodel(callitthedeepmodel)bystackingtheshallowmodeltwice, andfurthertrainthisdeepmodel. Toconstructdeepermodels,thisprocedurecanberepeated multiple times, say, we start with a model of L sub-layers, and obtain a model of L2I after I iterations. Note that many of the pre-training models are used in the same manner. For example,forBERT-likemethods,atransformerencoderistrainedonlarge-scaledata,andthe optimizedparametersarethenusedtoinitializedownstreamsystems. 3. Layer Fusion AnotherproblemwithtrainingadeepTransformermodelisthatthepredictionisonlycondi- tionedonthelastlayeroftheneuralnetwork. Whiletheuseofresidualconnectionsenables thedirectaccesstolower-levellayersfromahigher-levellayer,thereisstilla“long”pathof passinginformationfromthebottomtothetop. Onesimplewaytoaddressthisistocreate residualconnectionsthatskipmorelayers. Forexample,consideragroupofLTransformer sub-layers. For the sub-layer at depth l, we can build l−1 residual connections, each con- nectingthissub-layerwithaprevioussub-layer. Inthisway,wedevelopadenselyconnected networkwhereeachsub-layertakestheoutputsofallprevioussub-layers[Huangetal.,2017a]. Theoutputofthelastsub-layercanbeseenassomecombinationoftheoutputsatdifferent levelsofrepresentationoftheinput. Following the notation used in the previous subsections, we denote the output of the sub-layeratdepthlbyzl,anddenotethefunctionofthesub-layerbyLayerl(·). Then,zl can beexpressedas zl = Layerl(z1,...,zl−1) (6.87) WecansimplyviewLayerl(·)asafunctionthatfusestheinformationfrom{z1,...,zl−1}. There are many possible choices for Layerl(·). For example, a simple form of Layerl(·) is givenby Layerl(z1,...,zl−1) = LNorm(Fl(Zl)) (6.88) Zl = ϕ(z1,...,zl−1) (6.89) Hereϕ(·)takesthelayeroutputs{z1,...,zl−1}andfusesthemintoasinglerepresentationZl. Asimpleinstanceofϕ(·)isaveragepoolingwhichcomputesthesumof{z1,...,zl−1}divided byl−1. SeeTable6.2formoreexamplesofϕ(·). TakingasimilararchitectureofaTransformersub-layer,wecanalsoconsiderapost-norm 304 Chapter 6. Transformers Entry Function AveragePooling ϕ(z1,...,zl−1)= 1 (cid:80)l−1 zk l−1 k=1 WeightedSum ϕ(z1,...,zl−1)= (cid:80)l−1 weight ·zk k=1 k FeedforwardNetwork ϕ(z1,...,zl−1)=FFN([z1,...,zl−1]) SelfAttention ϕ(z1,...,zl−1)=FFN([Att (z1,...,zl−1)]) self Table6.2: Fusionfunctions. FFN(·)=feedforwardneuralnetwork, [·]=concatenatingthe input vectors, and Att (·) = self-attention function. All of the fusion functions can be self followedbyalayernormalizationfunction,forexample,wecanwritetheweightedsumof {z1,...,zl−1}asϕ(z1,...,zl−1)=LNorm( (cid:80)l−1 weight ·zk). k=1 k form Layerl(z1,...,zl−1) = LNorm(Zl) (6.90) Zl = ϕ(Fl(zl−1),z1,...,zl−1) (6.91) orapre-normform Layerl(z1,...,zl−1) = Zl (6.92) Zl = ϕ(LNorm(Fl(zl−1)),z1,...,zl−1) (6.93) Thesemodelsareverygeneral. Forexample,astandardpost-normencodersub-layercan berecoveredasaspecialcaseofEqs. (6.90-6.91),ifweremovethedependenciesofsub-layers from1tol−2,anddefineϕ(·)tobe ϕ(Fl(zl−1),z1,...,zl−1) = Fl(zl−1)+zl−1",
    "[·]=concatenatingthe input vectors, and Att (·) = self-attention function. All of the fusion functions can be self followedbyalayernormalizationfunction,forexample,wecanwritetheweightedsumof {z1,...,zl−1}asϕ(z1,...,zl−1)=LNorm( (cid:80)l−1 weight ·zk). k=1 k form Layerl(z1,...,zl−1) = LNorm(Zl) (6.90) Zl = ϕ(Fl(zl−1),z1,...,zl−1) (6.91) orapre-normform Layerl(z1,...,zl−1) = Zl (6.92) Zl = ϕ(LNorm(Fl(zl−1)),z1,...,zl−1) (6.93) Thesemodelsareverygeneral. Forexample,astandardpost-normencodersub-layercan berecoveredasaspecialcaseofEqs. (6.90-6.91),ifweremovethedependenciesofsub-layers from1tol−2,anddefineϕ(·)tobe ϕ(Fl(zl−1),z1,...,zl−1) = Fl(zl−1)+zl−1 (6.94) Denselyconnectednetworkmakestheinformationeasiertoflowthroughdirectconnections betweensub-layers,buttheresultingmodelsareabitmorecomplex,especiallywhenweuse parameterizedfusionfunctions. Inpractice,wetypicallyadddenseconnectionsonlytosome ofthesub-layers,andsotheoverallnetworksarenotverydense. Forexample,weonlyadd connections from bottom sub-layers to the last few sub-layers. Thus, the prediction can be madebyhavingdirectaccesstodifferentlevelsofrepresentation[Wangetal.,2018a]. 4. Regularization Inmachinelearning,regularizationisusedtoavoidoverfittingintrainingdeepneuralnetworks. ItisthereforestraightforwardtoapplyregularizationtechniquestoTransformermodels. Since theregularizationissuehasbeendiscussedinChapter2,hereweconsidersomeofthemethods thathavenotbeencoveredyetinthisbookbutcouldbeusedfortrainingdeepTransformer models. OneapproachtoregularizingadeepTransformermodelistorandomlyskipsub-layers or layers during training [Huang et al., 2016; Pham et al., 2019]. In each run of the model, such as running the backpropgation algorithm on a batch of samples, we select each of the 6.3 Improved Architectures 305 sub-layerswithaprobabilityρ,andstacktheselectedsub-layerstoforma“new”model. Thus, we essentially train different neural networks with shared architectures and parameters on thesamedataset. Inthisway,asub-layerlearnstooperatesomewhatindependently,andso overfittingisreducedbypreventingtheco-adaptionofsub-layers. Infact,droppingoutsub- layers(orlayers)anddroppingoutneuronsaretwodifferentmethodsonatheme. Sometimes, themethoddescribedhereiscalledsub-layerdropoutorlayerdropout. Attesttime,weneedtocombineallthepossiblenetworkstomakepredictionsofsome output. Asimplemethodtoachievethisistorescaletheoutputsofthestochasticcomponents of the model [Li et al., 2021a]. As an example, suppose each sub-layer has a pre-norm architecture. Then,theoutputofthesub-layeratdepthlisgivenby zl = ρ·LNorm(Fl(zl−1))+zl−1 (6.95) Anotherideaistoforcetheparameterstobesharedacrosssub-layers. Oneofthesimplest methodsistousethesameparametersforallthecorrespondingsub-layers[Dehghanietal., 2018],forexample,alltheFFNsub-layersarebasedonthesamefeedforwardnetwork. This methodhasasimilareffectasthemethodsthataddnormsofparametermatricestotheloss functionforpenalizingcomplexmodels. Forpracticalsystems,therecanbesignificantbenefit inadoptingasharedarchitecturebecausewecanreusethesamesub-modeltobuildamulti- layerneuralnetworkandreducethememoryfootprint. Wewillseemorediscussionsonthe efficiencyissueinSection6.4.4. 6.3.3 Numerical Method-Inspired Models A residual network computes its output through the sum of the identity mapping and some transformation of the input. Such a model can be interpreted as an Euler discretization of ordinarydifferentialequations(ODEs)[Ee,2017;HaberandRuthotto,2017]. Toillustrate thisidea,weconsiderageneralformofresidualnetworks zl = fl(cid:0) zl−1(cid:1) +zl−1 (6.96) wherefl(zl−1)denotesafunctiontakesaninputvariablezl−1 andproducesanoutputvariable in the same space. Clearly, a Transformer sub-layer is a special case of this equation. For example,forpre-normTransformer,wehavefl(·)=LNorm(Fl(·)). Fornotationalsimplicity,werewritetheaboveequationinanequivalentform (cid:0) (cid:1) z(l) = f z(l−1),l +z(l−1) (6.97) Weusethenotationsz(l)andf(z(·,l))toemphasizethatz(·)andf(·)arefunctionsofl. Here we assume that l is a discrete variable. If we relax l to a continuous variable and z(l) to a continuousfunctionofl,thenwecanexpressEq. (6.97)as (cid:0) (cid:1) z(l) = △l·f z(l−△l),l +z(l−△l) (6.98) 306 Chapter 6. Transformers Thiscanbefurtherwrittenas z(l)−z(l−△l) (cid:0) (cid:1) = f z(l−△l),l (6.99) △l Takingthelimit△l→0,wehaveanODE dz(l) (cid:0) (cid:1) = f z(l),l (6.100) dl We say that a pre-norm Transformer sub-layer (i.e., Eqs. (6.97) and (6.96)) is an Euler discretization of solutions to the above ODE. This is an interesting result! A sub-layer is actuallyasolveroftheODE. Eqs. (6.97) and (6.96) are standard forms of the Euler method. It computes a new estimation of the solution by moving from an old estimation one step forward along l. In general,twodimensionscanbeconsideredindesignofnumericalmethodsforODEs. • LinearMulti-stepMethods. Alinearmulti-stepmethodcomputesthecurrentestima- tionofthesolutionsbytakingtheestimationsandderivativeinformationfrommultiple previoussteps. Ageneralformulationofp-stepmethodscanbeexpressedas p p+1 (cid:88) (cid:88) (cid:0) (cid:1) z(l) = a ·z(l−i)+h b ·f z(l−i), l−i+1 (6.101) i i i=1 i=1 where h is the size of the step we move each time8, that is, △l in Eqs. (6.98) and (6.99). {a }and{b }arecoefficientsofthesolutionpointsandderivativesinthelinear i i combination. Giventhisdefinition,wecanthinkoftheEulermethodasasingle-step, low-ordermethodofsolvingODEs9. • (Higher-order)Runge-KuttaMethods. Runge-Kutta(RK)methodsandtheirvariants providewaystocomputethenextstepsolutionbytakingintermediateresultsinsolving anODE.Asaresult,weobtainhigher-ordermethodsbutstillfollowtheformofsingle- stepmethods,thatis,theestimatedsolutionisdependentonlyonz(l−1)ratherthanon theoutputsatmultipleprevioussteps. Infact,linearmulti-stepmethods,thoughnotexplicitlymentioned,havebeenusedinlayer fusion discussed in Section 6.3.2. For example, taking Eqs. (6.92) and (6.93) and a linear fusionfunction,apre-normsub-layerwithdenseconnectionstoallprevioussub-layerscanbe expressedas Layerl(z1,...,zl−1) = a ·zl−1+...+a ·z1+b ·LNorm(Fl(zl−1)) (6.102) 1 l−1 1 8Let{t 0 ,...,t i }denotethevaluesofthevariablelatsteps{0,...,i}.Inlinearmulti-stepmethods,itisassumed thatt i =t 0 +ih. 9Innumericalanalysis,thelocaltruncationerrorofamethodofsolvingODEsatastepisdefinedtobethe differencebetweentheapproximatedsolutioncomputedbythemethodandthetruesolution.Themethodiscalled orderpifithasalocaltruncationerrorO(hp+1). 6.3 Improved Architectures 307 ThisequationisaninstanceofEq. (6.101)whereweseth=1andremovesomeoftheterms ontheright-handside. ItisalsostraightforwardtoapplyRunge-KuttamethodstoTransformer[Lietal.,2022a]. GivenanODEasdescribedinEq. (6.100),anexplicitp-orderRunge-Kuttasolutionisgiven by p (cid:88) z(l) = z(l−1)+ γ ·g (6.103) i i i=1 i−1 (cid:0) (cid:88) (cid:1) g = h·f z(l−1)+ β ·g , l−1+λ ·h (6.104) i i,j j i j=1 Hereg representsanintermediatestepwhichispresentonlyduringtheaboveprocess. {γ }, i",
    "+ih. 9Innumericalanalysis,thelocaltruncationerrorofamethodofsolvingODEsatastepisdefinedtobethe differencebetweentheapproximatedsolutioncomputedbythemethodandthetruesolution.Themethodiscalled orderpifithasalocaltruncationerrorO(hp+1). 6.3 Improved Architectures 307 ThisequationisaninstanceofEq. (6.101)whereweseth=1andremovesomeoftheterms ontheright-handside. ItisalsostraightforwardtoapplyRunge-KuttamethodstoTransformer[Lietal.,2022a]. GivenanODEasdescribedinEq. (6.100),anexplicitp-orderRunge-Kuttasolutionisgiven by p (cid:88) z(l) = z(l−1)+ γ ·g (6.103) i i i=1 i−1 (cid:0) (cid:88) (cid:1) g = h·f z(l−1)+ β ·g , l−1+λ ·h (6.104) i i,j j i j=1 Hereg representsanintermediatestepwhichispresentonlyduringtheaboveprocess. {γ }, i i {β } and {λ } are coefficients that are determined by using the Taylor series of z(l). To i,j i simplifythemodel,weassumethatthesamefunctionf isusedforall{g }. Then,weremove i thedependencyoftheterml−1+λ ·hinf,andrewriteEq. (6.104)as i i−1 (cid:0) (cid:88) (cid:1) g = h·f z(l−1)+ β ·g (6.105) i i,j j j=1 wheref(·)isafunctionindependentofi. Asanexample,considerthe4th-orderRunge-Kutta(RK4)solution 1 z(l) = z(l−1)+ (g +2g +2g +g ) (6.106) 1 2 3 4 6 g = h·f(z(l−1)) (6.107) 1 1 g = h·f(z(l−1)+ g ) (6.108) 2 1 2 1 g = h·f(z(l−1)+ g ) (6.109) 3 2 2 g = h·f(z(l−1)+g ) (6.110) 4 3 These equations define a new architecture of sub-layer. For example, by setting h=1 and f(·)=LNorm(Fl(·)),weobtainanRK4Transformersub-layer,asshowninFigure6.11. This methodleadstoadeepmodelbecauseeachsub-layerinvolvesfourrunsoff(·)insequence. On the other hand, the resulting model is parameter efficient because we reuse the same functionf(·)withinthesub-layer,withoutintroducingnewparameters. So far in this subsection our discussion has focused on applying dynamic systems to TransformermodelsbydesigningarchitecturesofTransformersub-layers. Whilethebasic ODEmodeliscontinuouswithrespecttothedepthl,thesemethodsstillfollowthegeneral frameworkofneuralnetworksinwhichlisadiscretevariableandtherepresentationalpower ofthemodelsislargelydeterminedbythishyper-parameter. Analternativeapproachistouse neuralODEmodelstorelaxthe“depth”toatrulycontinuousvariable. Inthisway,wecan have a model with continuous depth for computing the solution of ODEs. However, as the 308 Chapter 6. Transformers z(l) z(l) z(l) 1 6 1 2 2 6 f 2 1 6 2 f f 1 1 2 6 f f f 1 2 f z(l−1) z(l−1) z(l−1) (a)Pre-norm (b)RK2 (c)RK4 Figure6.11: Pre-norm(a)andRunge-Kutta(bandc)sub-layerarchitectures. z(l−1)denotes the input of a sub-layer at depth l, z(l) denotes the output of the sub-layer, and f (in blue boxes)denotesthefunctionf(·)=LNorm(Fl(·)) discussionofneuralODEliesbeyondthescopeofthischapter,wereferthereadertorelated papersformoredetails[Chenetal.,2018c;Kidger,2022]. 6.3.4 Wide Models Mostofthemethodsthatwehavestudiedsofarinthissectionareexamplesoflearningand usingdeepmodels. Anotherdesignchoicewegenerallyfaceistodeterminethewidthfora neuralnetwork. Typically,thewidthofaTransformermodelcanbedefinedasthenumberof dimensionsofarepresentationatsomepositionoftheinputsequence,thatis,theparameter d. Increasingthiswidthisacommonmethodtoobtainamorecomplexandmorepowerful model. Forexample,inVaswanietal.[2017]’swork,awidemodel(calledTransformerbig) leadstosignificantimprovementsintranslationqualityformachinetranslationsystems. More recently,widermodelshavebeenproposedtoboostsystemsonlarge-scaletasks[Lepikhin etal.,2021;Fedusetal.,2022b]. However, developing very wide Transformer models is difficult. One difficulty is that trainingsuchsystemsiscomputationallyexpensive. Whilethenumberofthemodelparameters (or model size) grows linearly with d, the time complexity of the models grows quadratic withd(seeTable6.1). InsomeNLPtasks,itisfoundempiricallythatthetrainingeffortthat weneedtoobtainsatisfactoryperformanceisevenanexponentialfunctionofthemodelsize [Kaplanetal.,2020]. Theseresultssuggestwaystoimprovetheefficiencyoftrainingwhen weenlarged. Onesimplemethodistoincrementallygrowthemodelalongthedimensionofd,rather thantrainingthemodelfromscratch. Supposewehaveaninitialmodelinvolvingad ×d 1 1 parametermatrixW , forexample, thelineartransformationofeachqueryorkeyinsome 1 layer. We can train this model to obtain optimized W in a regular way. Then, we want to 1 extendthismodeltoawidermodelwhereW isreplacedbyad ×d parametermatrixW . 1 2 2 2 6.3 Improved Architectures 309 Letusassumeforsimplicitythatd =kd . Thereareseveralwaystoexpandad ×d matrix 2 1 1 1 toakd ×kd matrix. ThesimplestofthesemaybetouseW tofillW . WecanwriteW 1 1 1 2 2 intheform k times   W1 ··· W1 ρ ρ W 2 =   . . . . . .     W1 ··· W1 ρ ρ semit k (6.111) whereρisahyper-parameterthatisusedtocontrolthenormofW . Forexample,ifρ=k, 2 W will have thesame l norm asW . The above equation providesa goodstarting point 2 1 1 fortrainingthewidemodel,andwecantrainW asusualafterinitialization. Theprocedure 2 canberepeatedanumberoftimesforconstructingamodelwitharbitrarywidth. Boththis methodandthedepthgrowthmethoddescribedinSection6.3.2areinstancesofthegeneral methodofmodelgrowth. Inotherwords,wecanobtainalargermodelbyextendingasmall modeleitherverticallyorhorizontally,orboth. AlternativemethodsfortransformingW to 2 W involvethoseconsideringothermathematicalpropertiesofthetransformation[Chenetal., 1 2015]. Thesemodelscanfallunderthereusableneuralnetworkswhereweareconcernedwith modelsandalgorithmsfortransferringparametersfromsmallmodelsto(significantly)larger models[Wangetal.,2023b]. AseconddifficultyinbuildingawideTransformermodelisthelargememoryrequirement. Sincethefeedforwardnetworkgenerallyhasalargerhiddenlayerthanotherpartsofthemodel, itdemandsrelativelymorememoryasthemodelbecomeswider. Considerthefeedforward networkdescribedinSection6.1.5 H = FFN(H ) out in = ReLU(H ·W +b )·W +b (6.112) in h h f f whereW h ∈Rd×d ffn andW f ∈Rd ffn ×d aretheparametersofthelineartransformations. d ffn istypicallyseveraltimeslargerthand. Therefore,W andW willoccupythemodelifd h f andd haveverylargevalues. ffn Insomecases,thesizeofthefeedforwardnetworkmayexceedthememorycapacityofa singledevice.",
    "2015]. Thesemodelscanfallunderthereusableneuralnetworkswhereweareconcernedwith modelsandalgorithmsfortransferringparametersfromsmallmodelsto(significantly)larger models[Wangetal.,2023b]. AseconddifficultyinbuildingawideTransformermodelisthelargememoryrequirement. Sincethefeedforwardnetworkgenerallyhasalargerhiddenlayerthanotherpartsofthemodel, itdemandsrelativelymorememoryasthemodelbecomeswider. Considerthefeedforward networkdescribedinSection6.1.5 H = FFN(H ) out in = ReLU(H ·W +b )·W +b (6.112) in h h f f whereW h ∈Rd×d ffn andW f ∈Rd ffn ×d aretheparametersofthelineartransformations. d ffn istypicallyseveraltimeslargerthand. Therefore,W andW willoccupythemodelifd h f andd haveverylargevalues. ffn Insomecases,thesizeofthefeedforwardnetworkmayexceedthememorycapacityofa singledevice. Thisproblemcanbeaddressedbyusingthemixture-of-experts(MoE)models [Shazeeretal.,2017]. AnMoEmodelconsistsofM expertmodels{e (·),...,e (·)}. Given 1 M an input h ∈Rd, each expert model produces an output e (h ). The output of the MoE in k in modelisalinearcombinationof{e (h ),...,e (h )},givenby 1 in M in M (cid:88) h = g (h )·e (h ) (6.113) out i in i in i=1 where g(·) is a gating model (also called routing model). Its output is a vector g(h )= in (cid:104) (cid:105) g (h ) ... g (h ) inwhicheachentryg (h )indicatestheweightofthecorresponding 1 in M in i in 310 Chapter 6. Transformers expertmodel. Inmanyapplications,itisassumedthatg(h )isasparsevector. Thismeans in thatonlyasmallnumberofexpertmodelsareinvolvedincomputingtheoutput. Awidely-used formofg(h )isgivenbyusingtheSoftmaxlayer in g(h ) = Softmax(h ·W ) (6.114) in in g whereW ∈Rd×M istheparametermatrixofthelayer. Toenforcesparsityong(h ),wecan g in simplyselectthetop-k entriesofg(h ),thatis,wesetnon-top-k entriesto0. Analternative in method is to first perform top-k selection on h ·W and then normalize the top-k entries in g usingtheSoftmaxfunction. Let π be the set of the indices of the top-k expert models. The MoE model with top-k routinghasthefollowingform (cid:88) h = g (h )·e (h ) (6.115) out i in i in i∈π Anadvantageofthisapproachisthatwecandistributedifferentexpertmodelstodifferent processors,makingitpossibletoexecutethesemodelsonparallelcomputingmachines. In eachrunoftheMoEmodel,eitherduringtrainingorinference,weonlyneedtoactivateand use k expert models rather than all of the expert models. In this way, the MoE approach is automaticallylearningasparsemodelbylimitingthenumberofactiveexpertmodelseach time in training and inference. The sparsity is determined by the hyperparameter k, say, a smallvalueofk leadstoasparsemodel,andalargevalueofk leadstoadensemodel. Let us return to the discussion of Eq. (6.112). It is straightforward to apply the MoE approach to feedforward neural networks. To simplify the discussion, consider the linear transformationofthefirstlayerasshowninEq. (6.112),thatis,H ·W . Wecanapproximate in h H ·W inanMoEform in h (cid:88) H ·W ≈ g (H )·e (H ) in h i in i in i∈π (cid:88) = g (H )·[H ·Wi] (6.116) i in in h i∈π HereW isdividedintoM slides(orsub-matrices){W1,...,WM},writtenas h h h (cid:104) (cid:105) W = W1 ... WM (6.117) h h h Hence each expert model e (H )=H ·Wi solves a sub-problem of the original linear i in in h mapping, andEq. (6.116)canbethoughtofasadivide-and-conquersolutiontothematrix multiplicationproblem. Wecan,ofcourse,treatanyfeedforwardneuralnetworkasanexpertmodel,resultingin 6.3 Improved Architectures 311 . . . Hout FFNsub-layer (cid:80) i∈π gi(Hin)·FFNi(Hin) Add&LayerNorm Feed-ForwardNetwork Gating Model FFN1(·) FFN2(·) ··· FFNM(·) . . . Hin Hin Hin ··· Hin Figure6.12: AnillustrationoftheMoEmodelappliedtoanFFNsub-layer. ThereareM FFNs (call them expert models) and a gating model. Each FFN is weighted by the gating model. Theoutputofthemodelisthesumoftheweightedoutputsofthetop-k FFNs(denotedbyπ). BecausetheseFFNsworkindependentlyandcanbeplacedondifferentcomputingdevices, themodelcanbeeasilyscaledupasM islarger. thefollowingmodel (cid:88) H = g (H )·FFN (H ) (6.118) out i in i in i∈π whereFFN (·)isa“small”feedforwardneuralnetworkthathasthesameformasEq. (6.112). i This model is illustrated with an example in Figure 6.12. In practical implementations, all theseexpertmodelscanberuninparallelondifferentdevices,andsotheresultingsystemis efficient. Notethat,fromaperspectiveofmachinelearning,MoEisageneralapproachtocombining different neural networks, each of which is developed to address a different aspect of the problem [Yuksel et al., 2012; Masoudnia and Ebrahimpour, 2014]. The application here is justaspecialinstanceofthegeneralframeworkofMoE.Theapproachisalsooftenusedto improvetheoverallperformanceofpredictors,whichcanbediscussedinthefieldofensemble learning[Zhou,2012a]. AnotherdifficultyindevelopinglargeTransformermodelsisthetraininginstabilityprob- lem. Aswithmanyotherlargeneuralnetworks,straightforwardoptimizationofaTransformer modelwithalargenumberofparametersmayleadtogettingtrappedinlocalminimums,and, occasionally,largespikesinthelossduringtraining[Lepikhinetal.,2021;Fedusetal.,2022b; Chowdheryetal.,2022]. Evenwithcarefulchoicesabouthyperparameters,trainingstrategies, andinitialmodelparameters,westillencounterthesituationthatwehavetorestartthetraining atsomepointinordertojumpoutofthetoughregionsinoptimization. Oneofthereasonsfor",
    "Figure 6.12. In practical implementations, all theseexpertmodelscanberuninparallelondifferentdevices,andsotheresultingsystemis efficient. Notethat,fromaperspectiveofmachinelearning,MoEisageneralapproachtocombining different neural networks, each of which is developed to address a different aspect of the problem [Yuksel et al., 2012; Masoudnia and Ebrahimpour, 2014]. The application here is justaspecialinstanceofthegeneralframeworkofMoE.Theapproachisalsooftenusedto improvetheoverallperformanceofpredictors,whichcanbediscussedinthefieldofensemble learning[Zhou,2012a]. AnotherdifficultyindevelopinglargeTransformermodelsisthetraininginstabilityprob- lem. Aswithmanyotherlargeneuralnetworks,straightforwardoptimizationofaTransformer modelwithalargenumberofparametersmayleadtogettingtrappedinlocalminimums,and, occasionally,largespikesinthelossduringtraining[Lepikhinetal.,2021;Fedusetal.,2022b; Chowdheryetal.,2022]. Evenwithcarefulchoicesabouthyperparameters,trainingstrategies, andinitialmodelparameters,westillencounterthesituationthatwehavetorestartthetraining atsomepointinordertojumpoutofthetoughregionsinoptimization. Oneofthereasonsfor thistrainingdifficultyisthattheusualimplementationsofthelinearalgebraoperations,such asmatrixmultiplication,willbenumericallyunstableiftheyoperatesonverylargevectors andmatrices. Itisthereforepossibletoimprovethetrainingbyconsideringnumericallystable methodsinstead. 312 Chapter 6. Transformers 6.4 Efficient Models EfficiencyisanimportantconsiderationformanypracticalapplicationsofTransformermodels. Forexample,wemaywishtorunand/ortrainaTransformermodelgivenmemoryandtime constraints. Efficiencyisnotasingleproblem,butcoversawiderangeofproblems. While theseproblemscanbecategorizedinseveraldifferentways,therearetwofundamentalaspects onemayconsiderinanefficiencyproblem. • TimeandSpaceEfficiencies. Foragivenproblem,wewishthemodeltobesmalland fast,andmeanwhiletobeasaccurateaspossibleinsolvingtheproblem. Forexample, insomemachinetranslationapplications,wemaylearnamodelwithasmallnumberof parameterstofitthemodeltolimitedmemory,andmaydevelopafastsearchalgorithm toachievelow-latencytranslation. Apracticaldifficultyhereisthatimprovingefficiency oftenleadstoworsepredictions. Inmanycases,weneedtoseekatrade-offbetween efficiencyandaccuracy. • Scalability. Whentheproblemisscaledup,wewishthattheadditionaleffortwemade forsolvingthisproblemisassmallaspossible. Forexample,thetrainingofaneural network is called efficient if it takes a reasonably short time to optimize it as more trainingsamplesareinvolved. Anotherexampleofefficiencyisthatusedtomeasure theamountofresourcesconsumedinprocessingmoreinputs. Forexample,amachine translationsystemisinefficientintranslatinglongsentencesifthememoryfootprintand latencygrowexponentiallywiththenumberofinputwords. In this section, we will not discuss all the issues related to efficiency, which is a very broadtopic. Weinsteadconsiderthewidely-usedefficientapproachestoTransformer-based sequencemodelingandgeneration,someofwhicharerefinementsofmodelarchitectures,and someofwhicharemodel-freeapproachesandcouldbeusedinothersystemsaswell. Mostof thediscussionsherearefocusedondevelopinglightweightandfastTransformermodelsthat arerelativelyrobusttolonginputandoutputsequences. Ingeneral,thesameoptimizationmethodcanbeappliedtodifferentmodulesofaTrans- formersystem. Tosimplifythediscussion,wewillmostlyconsiderself-attentionsub-layers andFFNsub-layersinthissection. Ourdiscussion,however,isgeneralandthemethodspre- sentedherecanbeappliedtootherpartsofaTransformersystem,forexample,cross-attention sub-layers. 6.4.1 Sparse Attention Inpractice,theattentionapproachesusedinTransformeraretimeconsuming,especiallywhen the input sequences are long. To illustrate, consider a Transformer decoder that predicts a distribution of words at a time given the previous words. Suppose the sequence generated by the decoder is of size n and the input of a self-attention sub-layer is an n×d matrix S. First,SislinearlytransformedtoobtainthequeriesSq ∈Rn×d,keysSk ∈Rn×d,andvalues Sv ∈Rn×d. Tosimplifythenotationinthissubsection,weuseQ,KandVtorepresentSq, Sk,andSv,respectively. 6.4 Efficient Models 313 Theoutputoftheself-attentionsub-layercanthenbecomputedusing Att (S) = AV (6.119) self whereAisann×nattentionmatrixorattentionmap QKT A = Softmax( √ +M) (6.120) d Misamaskingmatrixthatisusedtopreventthemodelfromseeingtherightcontextwords ateachposition,thatis,forapositioni,M(i,j)=0forj ≤i,andM(i,j)=−∞otherwise. Boththetimeandspacecomplexitiesoftheself-attentionsub-layerarequadraticfunctionsof n10. Therefore,ifnislarge,themodelwouldbecomputationallyexpensive. Theusualimplementationoftheabovemodeldependsondensematrixcomputation,for example,thedensematrixmultiplicationsinEqs. (6.119-6.120). Oneapproachtoreducing theamountofmemoryandthenumberoffloating-pointcalculationsinadensecomputation systemistosparsifytheproblem. Todothis,weassumethatAisasparsematrix,forexample, only ϱ·n2 entries of M have non-zero values, where ϱ indicates how sparse the matrix is, also called sparsity ratio. Since we only need to store these non-zero entries, the memory requirementofAcanbereducedbyusingsparsematrixrepresentations. Anotheradvantage ofusingasparseattentionmatrixisthatthemodelsof Q √ KT andAVcanbesimplified,aswe d consideronlya“small”numberofrelatedpositionswhenlearningarepresentation. Given a position i, we define the attention field π to be the set of positions that are i consideredincomputingtherepresentationatthisposition. Wethereforeonlyneedtocompute thedot-productattentionbetweenthegivenpositioniandeachpositionj ∈π . Thisresultsin i asparseattentionmatrixA′ where (cid:40) someweight j ∈π andj ≤i A′(i,j) = i (6.121) 0 otherwise AsimpleimplementationofthismodelinvolvesaslightmodificationtoM,leadingtoanew maskingvariableM′ (cid:40) 0 j ∈π andj ≤i M′(i,j) = i (6.122) −∞ otherwise Inpracticalimplementation,amoreefficientapproachistoemploysparseoperationsforQKT andA′VbyconsideringM′ andA′,respectively. Thatis,wesaveoncomputationforpairs ofpositionswhoseattentionweightsarenon-zero,andskiptherest. Thereareseveralapproachesthatwecantaketothesparsemodelingofself-attention. We describebrieflysomeofthemasfollows 10Moreprecisely,theamountofmemoryusedbytheself-attentionfunctionisn2+n·d,andsoitwillbe dominatedbythequadratictermn2ifn>>d. 314 Chapter 6. Transformers • Span-based Attention/Local Attention. As discussed in Section 6.3.1, the use of contextinsequencemodelingislocalinmanycases. Thebasicideaoflocalattentionis tospantheattentionweightstoarestrictedregionoftheinputsequence. Wecanthen writeπ as i π = [al,ar] (6.123) i i i whereal andar andtheleftandrightendsofπ . ar−al+1determineshowsmallthe i i i i i regionis,andsowecanuseittocontrolthesparsityoftheattentionmodel,forexample, ifar−al+1<<n,themodelwouldbeverysparse. al andar canbeobtainedbyusing i i i i eitherheuristicsormachinelearningmethods. Thereadermayrefertorelatedpapers formoredetails[Luongetal.,2015;Sperberetal.,2018;Yangetal.,2018a;Sukhbaatar etal.,2019]. SeeFigure6.13(b)foranillustrationoflocalattention. • ChunkedAttention. Whenaproblemistoodifficulttosolve,onecantransformitinto easierproblemsandsolveeachofthemseparately,asisoftenthecaseinpractice. This motivatesthechunkedattentionapproachinwhichwesegmentasequenceintochunks and run the attention model on each of them [Parmar et al., 2018; Qiu et al., 2020a]. Givenasequence{1,...,n},wedefine{chunk ,...,chunk }tobeasegmentationofthe 1 q sequence. Achunkcanbeexpressedasaspan chunk = [cl,cr] (6.124) k k k Intheattentionstep,wetreateachchunkasasequenceandperformself-attentiononit asusual. Inotherwords,therepresentationatpositioniiscomputedbyusingonlythe contextinthechunkthatibelongsto. Inthissense,thismodelcanbethoughtofassome sortoflocalattentionmodel. Figure6.13(c)showsanillustrationofthismodel. There remainstheissueofhowtosegmentthesequence. Thereareseveralwaystodothis. Forexample,asdiscussedinSection6.2.4,wecandosegmentationfromalinguistic perspective,andsegmentthesequenceintolinguisticallymotivatedunits. Inpractical systems,itissometimesmoreconvenienttosegmentthesequenceintochunksthatare ofequallength. Thus,thesparsityofthemodeliscontrolledbythesizeofthesechunks, forexample,theuseofsmallerchunkswouldleadtoamoresparseattentionmodel. • StridedAttention. Sincethechunkedattentionapproachenforcesahardsegmentation on the input sequence, it may lose the ability to learn representations from inputs in differentchunks. Analternativewaytoachievechunk-wiseattentionistoallowoverlap between chunks [Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020]. This approachisanalogoustothefamilyofapproachesthatarecommonlyusedtoapplya localmodelto1Dor2Ddatatogenerateoutputsofthesameshape. LikeCNNs,we useacontextwindowtorepresentthefieldofinputoftheattentionmodel. Thecontext windowslidesalongthesequence,eachtimemovingforwardastepofsizestride. Asa specialcase,ifstrideequalsthesizeofthecontextwindow,thismodelisthesameas thechunkedattentionmodelmentionedabove. Ifstridechoosesavaluesmallerthan 6.4 Efficient Models 315 the size of the context window, the attention model will become denser. Figure 6.13 (d)showsthecaseofstrdie=1wherethechunkoverlappingismaximized. Awayto achieverelativelysparserattentionistouseadilatedcontextwindow. Figure6.13(e) showsanexampleofthedilatedstridedattentionmodel,wherethecontextwindowis discontinuous,withgapsofsize1. •",
    "between chunks [Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020]. This approachisanalogoustothefamilyofapproachesthatarecommonlyusedtoapplya localmodelto1Dor2Ddatatogenerateoutputsofthesameshape. LikeCNNs,we useacontextwindowtorepresentthefieldofinputoftheattentionmodel. Thecontext windowslidesalongthesequence,eachtimemovingforwardastepofsizestride. Asa specialcase,ifstrideequalsthesizeofthecontextwindow,thismodelisthesameas thechunkedattentionmodelmentionedabove. Ifstridechoosesavaluesmallerthan 6.4 Efficient Models 315 the size of the context window, the attention model will become denser. Figure 6.13 (d)showsthecaseofstrdie=1wherethechunkoverlappingismaximized. Awayto achieverelativelysparserattentionistouseadilatedcontextwindow. Figure6.13(e) showsanexampleofthedilatedstridedattentionmodel,wherethecontextwindowis discontinuous,withgapsofsize1. • Learning Attention Fields. Because the attention field π can be any sub-set of i {1,...,n},wecandevelopmoregeneralsparseattentionmodelsbyconsideringattention maps beyond chunk-based patterns. The only question is how to determine which positions the model attends to for a given position. One simple approach is to use a computationally cheaper model to estimate the “importance” of each position. Then, attention weights are computed only for some of the positions which are thought to bemostimportant[Zhouetal.,2021]. Asecondapproachisgrouping: positionsare grouped, and then the attention weights are computed only for positions in the same group. It is often relatively easy to achieve this by running clustering algorithms on keysandqueries. Forexample,wecanclusterkeysandqueriesviak-meansclustering. The centroids of the clusters can be treated as additional parameters of the attention model, and so can be learned during optimization [Roy et al., 2021]. One benefit of learning attention fields is that the model can spread its attention broader over the sequence. ThisisausefulpropertyformanyNLPproblemsbecauseworddependencies aresometimeslong-range,notrestrictedtoalocalcontextwindow. SeeFigure6.13(f) foranexampleoftheattentionmaplearnedthroughthismodel. Alternativeapproaches to learning to attend are to use sorting or hashing functions to group similar key and query vectors [Kitaev et al., 2020; Tay et al., 2020a]. These functions can be either heuristicallydesignedfunctionsorneuralnetworkswithlearnableparameters. Byusing thesefunctions,wecanreorderthesequencesothattheinputsinthesamegroupare adjacentinthereorderedsequence. Inthisway,theresultingattentionmapfollowsa chunk-wisepattern,andthemodeliscomputationallyefficientthroughtheuseofthe chunkedattentionapproach. • HybridMethods. Above,wehavediscussedarangeofdifferentsparseattentionmodels. It is natural to explore methods that combine multiple models together to make use of their benefits in some way. A simple way to do this is to combine the attention fieldsofdifferentmodels. Forexample,inZaheeretal.[2020]’ssystem,theattention mapisgeneratedbyconsideringthreedifferentsparsemodels,includinglocalattention (chunked attention), global attention, and random attention11. The resulting model is still a sparse model, but is somewhat more robust as it involves multiple patterns fromdifferentperspectivesofattentionmodeling. Anotherwayofcombiningmultiple attentionmodelsistousedifferentmodelsfordifferentheadsinmulti-headattention [Childetal.,2019;Beltagyetal.,2020]. Forexample,onecanuseoneheadasalocal attentionmodel,anduseanotherheadasaglobalattentionmodel(seeFigure6.13(g-h)). 11Here the global attention model attends each word only to a special word which accounts for the entire sequenceandisoftenplacedatthebeginningofthesequence.Therandomattentionmodelattendseachwordtoa randomsetofthewordsofthesequence. 316 Chapter 6. Transformers (a)StandardAttention (b)Span-basedAttention (c)ChunkedAttention (d)StridedAttention (e)DilatedStridedAttention (f)LearningAttentionFields (g)GlobalAttention (h)HybridMethods Figure6.13: Illustrationoftheattentionmapsofdifferentmodels(self-attentiononthedecoder side). DarkcellsmeanA′(i,j)̸=0(i.e.,iattendstoj),andlightcellsmeanA′(i,j)=0(i.e., idoesnotattendtoj). Inalltheseattentionmaps,weassumethateverypositionattendsto itselfbydefault(seediagonals). 6.4 Efficient Models 317 Onedisadvantageofsparsemodelscomparedtodensemodelsisthattheyarenotcom- putationally efficient on GPUs or CPUs. While sparse models can ideally reduce both the memoryandcomputationrequirements,theactualrateatwhichworkcanbedonebysparse models is much slower than by dense models. In practice, it is difficult for sparse models to approach the peak FLOPS of a GPU or CPU12. Therefore, they are often used for the purposeofhighmemoryefficiency, notreallyforthepurposeofefficientcomputation. On the other hand, sparse models are still of great use to NLP practitioners in the context of memory-efficient Transformer, especially when Transformer systems are used to deal with extremelylongsequences. 6.4.2 Recurrent and Memory Models For sequence generation problems, Transformer can also be thought of as a memory sys- tem. Consider again the general setting, in which we are given the states of previous i−1 positions, and we wish to predict the next state. In self-attention, this is done by using the query at position i (i.e., q ) to access the key-value pairs of the previous positions (i.e., i {(k ,v ),...,(k ,v )}). Then,wemovetopositioni+1,andadd(k ,v )tothecollec- 1 1 i−1 i−1",
    "of previous i−1 positions, and we wish to predict the next state. In self-attention, this is done by using the query at position i (i.e., q ) to access the key-value pairs of the previous positions (i.e., i {(k ,v ),...,(k ,v )}). Then,wemovetopositioni+1,andadd(k ,v )tothecollec- 1 1 i−1 i−1 i i tionofkey-valuepairs. Thisprocedurecanbeinterpretedintermsofthememorymechanism (seeChapter4). TheTransformermodelmaintainsamemorythatretainstheinformationof thepast. Whenmovingalongthesequence,werepeatthesameoperation,eachtimegenerating someoutputbyreadingthememory,andthenupdatingthememorysothatnewinformation couldbestoredinsomeway. ThisisillustratedinFigure6.14. 1. Cache-based Memory Thememoryherecanbeviewedasadatastoreofvectors. Fromamachinelearningperspective, this is a non-parametric model, and the cost of accessing the model grows as a longer sub- sequenceisobserved. Clearly,suchavariable-lengthmemorywillgenerallybeinfeasibleif themodeldealswithavery,verylongsequence. Forthemodelingproblemofarbitrarylength sequences,itiscommontouseafix-lengthmemoryinstead. AsinmanyNLPproblems,oneof thesimplestwaystodothisistoconsideracachesavingrecentinformation,thatis,werestrict themodelingtoacontextwindow. Letn bethesizeofthecontextwindow. Themodelkeeps c track of the n −1 latest states to the current position, so that its closest successors can be c consideredateachstep. Thismeansthat,foreachposition,aself-attentionsub-layerattendsto n −1positionsahead,likethis c Output Input i−5 i−4 i−3 i−2 i−1 i Ifwestackmultipleself-attentionsub-layers,alargercontextwindowwouldbeconsidered. 12FLOPS=floatingpointoperationspersecond. 318 Chapter 6. Transformers Read(self-attention) Memory({1,...,i−1}) State Update 1 2 3 ... i−2 i−1 i i+1 Positioni Read(self-attention) Memory({1,...,i−1,i}) State Update 1 2 3 ... i−2 i−1 i i+1 Positioni+1 Figure6.14: Transformerasamemorysystem. Atpositioni,thecollectionofthekey-value pairsofpositions{1,...,i−1}isusedasamemoryofthepastinformation. TheTransformer model accesses this memory to generate some output, and then adds the key-value pair of position i to the memory. Moving to the next position, we repeat the same procedure of memoryaccessandupdate. For example, a model involving two self-attention sub-layers has a context window of size 2n −1,asfollows c Output Layer2 Layer1 i−5 i−4 i−3 i−2 i−1 i Therefore, we can take a sufficiently large context by using a multi-layer Transformer model. Notethatthecontextwindowmodelhereisessentiallythesameasthestridedattention modelpresentedintheprecedingsection. Systemsofthistypeareofteneasytoimplement: we slide a window along the sequence, and, in each move, we make predictions at the last positionofthewindow(forinference),orback-propagateerrors(fortraining). Analternativewaytotrainthiscontextwindowmodelisbychunkedattention. Wedivide the sequence into chunks (or sub-sequences) which are of the same length n . Then, we c treat these chunks as individual training samples, and run the training program on each of 6.4 Efficient Models 319 themasusual. Thisapproach,however,completelyignorestherelationshipbetweeninputsin differentchunks. Onewaytoaddressthisissueistointroducedependencebetweenchunks. Forexample,theTransformer-XLmodelallowseverychunktoaccessoneormorepreceding chunks[Daietal.,2019]. Inthesimplestcase,consideranexampleinwhichchunk cansee k itssuccessorchunk . Eachpositioninchunk canattendtoallitsprecedingpositionsin k−1 k bothchunk andchunk . k k−1 InTransformer-XL,thisapproachisimplementedinasimplifiedform. First,eachposition isconstrainedtoattendton −1previouspositionssothatthesizeoftheattentionfieldofa c positionisthesameinthetrainingandinferencestages. Suchamethodturnstheproblemback tostridedattention,makingtheimplementationoftheattentionmodelstraightforward. Onthe otherhand,thedifferencebetweenthestandardstridedattentionmodelandtheTransformer-XL modelisthatinTransformer-XL,weperformtraininginachunk-wisemanner. Oncewefinish thetrainingonachunk,wedirectlymovetothenextchunk,ratherthanslidingthecontext windowasmallstepforward. Second,whilethisapproachallowsforconnectionsbetween chunks, the parameters of the sub-network on chunk are fixed, and we only update the k−1 parametersofthesub-networkonchunk inthek-thstep. SeeFigure6.15foranillustration. k Theabovemodelissimilarinspirittorecurrentmodelsbecauseallofthemrequirethe computationinonesteptodependonthestatesoftheprecedingsteps. However,itisnotin thestandardformofarecurrentmodel,inwhichtheoutputofarecurrentunitinonestepis the input in the next step. Instead, the “recurrence” is expressed by involving connections betweentwodifferentlayers,thatis,theoutputofonelayerinchunk isusedastheinput k−1 ofahigher-levellayerinchunk . k 2. Encoding Long-term Memory Another idea for representing the states of a sequence is to frame the task as an encoding problem. Instead of storing all the key-value vectors during left-to-right generation, we constructthememoryoftheentire“history”asafixednumberofencodedkey-valuevectors. Theseencodedkey-valuevectorscanbeeitherasmallsub-setof{(k ,v ),...,(k ,v )} 1 1 i−1 i−1 orasmallsetofnewly-generatedvectorsthatencodes{(k ,v ),...,(k ,v )}. 1 1 i−1 i−1 Onewaytodotheencodingistoapplyapoolingoperationto{(k ,v ),...,(k ,v )} 1 1 i−1 i−1 [Rae et al., 2019a]. For example, by using average pooling, the memory contains only one key-valuepair(k¯,v¯) i−1 1 (cid:88) k¯ = k (6.125) j i−1 j=1 i−1 1 (cid:88) v¯ = v (6.126) j i−1 j=1 Thisleadstoaveryefficientmodel,andweonlyneedtoupdatethevectors(k¯,v¯)atatime [Zhangetal.,2018a]. Let(k¯[i],v¯[i])bethestateofthememoryatpositioni. Amoregeneral 320 Chapter 6. Transformers chunk chunk chunk k−1 k k+1 Output Layer2 Layer1 i−5 i−4 i−3 i−2 i−1 i i+1 i+2 i+3 (a)Stepkofchunk-wisetraining chunk chunk chunk k−1 k k+1 Output Layer2 Layer1 i−5 i−4 i−3 i−2 i−1",
    "1 (cid:88) v¯ = v (6.126) j i−1 j=1 Thisleadstoaveryefficientmodel,andweonlyneedtoupdatethevectors(k¯,v¯)atatime [Zhangetal.,2018a]. Let(k¯[i],v¯[i])bethestateofthememoryatpositioni. Amoregeneral 320 Chapter 6. Transformers chunk chunk chunk k−1 k k+1 Output Layer2 Layer1 i−5 i−4 i−3 i−2 i−1 i i+1 i+2 i+3 (a)Stepkofchunk-wisetraining chunk chunk chunk k−1 k k+1 Output Layer2 Layer1 i−5 i−4 i−3 i−2 i−1 i i+1 i+2 i+3 (b)Stepk+1ofchunk-wisetraining Figure 6.15: Illustration of chunk-wise training [Dai et al., 2019]. The input sequence is dividedintochunksofthesamelengthn . Trainingisperformedonthesechunks,eachtime c dealingwithachunk. Inchunk ,theattentionfieldforeverypositioninthischunkisaleft k contextwindowofsizen . Hencethismodelallowsforattentionacrosschunks,forexample, c positioni−2inchunk canattendtopositionsi−3andi−4inchunk (seesub-figure k k−1 (a)). For training, errors are back-propagated only in the sub-network for chunk , leaving k otherpartsofthemodelunchanged. Hereweusedashedlinestodenoteinformationflowthat weconsiderintheforwardpassbutnotinthebackwardpass. Oncewefinishthetrainingon chunk ,wemovetothenextchunk,andrepeatthesametrainingprocedure. k definitionof(k¯[i],v¯[i])isgiveninarecursiveform k¯[i] = KMem(k¯[i−1],k ) (6.127) i−1 v¯[i] = VMem(v¯[i−1],v ) (6.128) i−1 whereKMem(·)andVMem(·)arefunctionsthatupdatethememorybytakingboththestates ofthememoryatthepreviousposition(i.e.,k¯[i−1]andv¯[i−1])andthenewstates(i.e.,k i−1 andv ). TherearemanyformsofthefunctionslikeKMem(·)andVMem(·)incommon i−1 use. Forexample,ifKMem(·)andVMem(·)areweightedsumfunctions,wecanderivethe same forms as Eqs. (6.125) and (6.126). If KMem(·) and VMem(·) are recurrent cells in 6.4 Efficient Models 321 RNNsorLSTM,weobtainarecurrentmodelofmemory. Extension of the above model to memories having more than one key-value pair is straightforward. One approach is to use the memory to represent sub-sequences. Let {(k¯ ,v¯ ),...,(k¯ ,v¯ )} be a memory of size κ. Each (k¯ ,v¯ ) is a snapshot of a chunk of 1 1 κ κ j j lengthn . Thus,thismemorycanencodeasequencewithmaximumlengthκ·n . Then,we c c cancompute(k¯ ,v¯ )onthecorrespondingchunkusingEqs. (6.127)and(6.128). Asecond j j approachistoorganize{(k¯ ,v¯ ),...,(k¯ ,v¯ )}intoapriorityqueue. Wedesignsomefunction 1 1 κ κ to assign a score to any given key-value pair. The key-value pair can be inserted into the priorityqueuethroughthepushoperation. Ideally,wewishtodevelopascoringfunctionto estimatethevalueofakey-valuepair,forexample,weuseanotherneuralnetworktoevaluate thekey-valuepair. Inthisway,thememoryisacollectionofthemostvaluablekey-valuepairs overtheinputsequence. Althoughrepresentingthememoryasasetofvectorsisanobviouschoiceforthemodel designinTransformer,thememoryisdiscreteanditscapacityisdeterminedbythenumber ofthevectors. Analternativeformofmemoryiscontinuousmemory. Thistypeofmodel typicallybuildsontheideaoffunctionapproximation,inwhich{k ,...,k }or{v ,...,v } 1 i−1 1 i−1 isviewedasaseriesofdatapoints,andacontinuousfunctionisdevelopedtofitthesedata points. Then, we no longer need to store {k ,...,k } and {v ,...,v }. Instead, the 1 i−1 1 i−1 memoryisrepresentedbythefunctionsfittingthesevectors. Asimplemethodistocombine simplefunctionstofitcomplexcurvesofdatapoints. Forexample,wecandevelopasetof basisfunctionsandusealinearcombinationofthemtoapproximatethekeyorvaluevectors [Martinsetal.,2022]. Theresultingmodelisparameterizedbythesebasisfunctionsandthe correspondingweightsinthecombination. Itisalsostraightforwardtouseashort-termmemoryandalong-termmemorysimultane- ouslysothatwecancombinethemeritsofboth. Forexample,weuseacache-basedmemory tocapturelocalcontext,anduseanefficientlong-termmemorythatencodestheentirehistory tomodellong-rangedependency. Thisideaisalsosimilartothatusedincombiningdifferent sparseattentionmodelsasdiscussedintheprevioussubsection. 3. Retrieval-based Methods Sofarinthissubsection,wehavediscussedapproachesbasedonfixed-lengthmodels. Itis alsopossibletodevelopefficientmemorymodelsbyimprovingtheefficiencyofaccessingthe memories,insteadofjustreducingthememorycapacities. Onewaytoachievethisistostore thepastkey-valuepairsinadatabase(callitavectordatabase),andtofindthemostsimilar oneswhenqueryingthedatabase. Tobemoreprecise,givenaqueryq,weusethedatabaseto findasetoftop-prelevantkey-valuepairs(denotedbyΩ )byperformingsimilaritysearch p basedonthedot-productsimilaritymeasurebetweenqueryandkeyvectors. Then,weattend qtoΩ asinstandardself-attentionmodels. Theideabehindthismethodistoconsideronlya p smallnumberofelementsthatcontributemosttotheattentionresult. Therefore,themodelis essentiallyasparseattentionmodelwhichiscomputationallyefficient. Anotheradvantageof thismethodisthatitallowsforfastsimilaritysearchoveraverylargesetofvectorsbecauseof thehighlyoptimizedimplementationofvectordatabases. Buildingamemoryasaretrieval 322 Chapter 6. Transformers systemcanfallunderthegeneralframeworkcalledtheretrieval-augmentedapproach. It providesasimplewaytoincorporateexternalmemoriesintoneuralmodelslikeTransformer [Guuetal.,2020;Lewisetal.,2020b;Wuetal.,2021]. 6.4.3 Low-dimensional Models Inmanypracticalapplications,Transformermodelsare“high-dimensional”models. Thisis notonlybecausetheinputand/oroutputdataisinhigh-dimensionalspaces,butalsobecause someoftheintermediaterepresentationsofthedatainthemodelarehigh-dimensional. As discussedinSection6.4.1,thishighdimensionalityarisesinpartfromthestepsofcomputing theattentionmatrixasinEq. (6.119)(foreaseofpresentation,werepeattheequationhere) Att (S) = AV (6.129) self andtheweightedsumofvaluevectorsasinEq. (6.120) QKT A = Softmax( √ +M) (6.130) d whichinvolveslargematrixmultiplicationsQKT andAVwhenthelengthnandthehidden dimensionalitydhavelargevalues. TheAVandQKT operationshaveatimecomplexityofO(n2·d)andaspacecomplexity ofO(n2+n·d). Severalpreviouslydescribedapproacheshavereducedthiscomplexityby usingsparsemodels. Inthissubsection,wefocusonmethodsthatapproximatetheseoperations viadensecomputation. OnesimpleideaistotransformQ,K,andVintosmallermatrices, andthustoreducethecomputationalburdenofmatrixmultiplication. SinceQ,K,andVare all in Rn×d, we can achieve this by reducing either the n dimension or the d dimension, or both. 1. Reducing n NotethattheoutputAtt (S)isrequiredtobeann×dmatrix,andsowecannotreducethe self numberofqueries. Weinsteadconsiderreducingthenumberofkeysandvalues. Supposen′ isanumberlessthann,andKandVcanbetransformedinton′×dmatricesK′ andV′ in someway. Wecanobtaina“smaller”modelsimplybyreplacingKandVwithK′ andV′ , giving ′ Att (S) = AV (6.131) self Q[K′]T A = Softmax( √ +M) (6.132) d Thismodelisinthestandardformofself-attention,buthaslowertimeandspacecomplexities, thatis,O(n′·n·d)<O(n2·d)andO(n′·n+n′·d)<O(n2+n·d). Ifn′<<n,theresulting modelwillbelinearwithn. The key problem here is how to obtain K′ and V′ in a way that retains much of the 6.4 Efficient Models 323 informationinKandV. Thereareseveralwaystodoso. Onesimplemethodistoselectthe keysandvaluesthatarethoughttobeimportant. Theimportanceofakey(orvalue)canbe computedintermsofsomecomputationallycheapmeasure. Forexample,wecansamplea smallnumberofquery-keydot-productsandestimatetheimportanceofakeybycollecting thesedot-productresults. Theabovemethodisstraightforwardbutstillrequiressparseoperations,suchassampling and collection. As an alternative, we can use dense computation to transform K and V to K′ and V′ . A typical choice is to use CNNs [Liu et al., 2018]. Let Conv(·) be a function describingasetoffiltersthatslidealongthendimension. K′ isthengivenby K′ = Conv(K,W ,size ,stride) (6.133) c r",
    "smallnumberofquery-keydot-productsandestimatetheimportanceofakeybycollecting thesedot-productresults. Theabovemethodisstraightforwardbutstillrequiressparseoperations,suchassampling and collection. As an alternative, we can use dense computation to transform K and V to K′ and V′ . A typical choice is to use CNNs [Liu et al., 2018]. Let Conv(·) be a function describingasetoffiltersthatslidealongthendimension. K′ isthengivenby K′ = Conv(K,W ,size ,stride) (6.133) c r where W is the parameter matrix of the filters, size is the size of the receptive field, and c r stride is the number of units the filters are translated at a time. In general, we can achieve a high compression rate by choosing large values for size and stride. Likewise, we can r computeV′ usinganotherconvolutionalfunction. Itisworthnotingthat,iftheparametern′ is fixed for all samples, compression of K and V along the length dimension is essentially the same as the fixed-length memory model as described in the preceding subsection. The methodspresentedherearemoregeneralandcouldbeappliedtovariable-lengthmemories. We might also be tempted to model the attention function by considering the attention matrixAasahigh-dimensionalrepresentationofdataandthenapplyingconventionaldimen- sionalityreductionmethods. Formanyproblems,itisfoundthatA(ormorepreciselyQKT) isalow-rankmatrix. Inthiscase,wecancompressAwhileretainingasmuchinformation aspossible. Therearemanywaystodoso. Forexample,wemightuseaproductofsmaller matrices as an approximation to A via the SVD technique (see Chapter 3). However, this introducescomputationaloverheadinusingSVDcomparedwiththestandardattentionmodel. AsimplerideatodirectlytransformKandVintosmaller-sizedmatricesvialinearmappings, givenby K′ = UkK (6.134) V ′ = UvV (6.135) where Uk ∈Rn′×n and Uv ∈Rn′×n are parameter matrices. Clearly, this leads to a model which is equivalent to that described in Eqs. (6.131) and (6.132). While such a method is intuitiveandsimple,itisproventoobtainasufficientlysmallapproximationerrorϵifn′ isa linearfunctionofd/ϵ2 [Wangetal.,2020b]. 2. Reducing d Anotherapproachtoworkinginalow-dimensionalspaceistoreducetheddimension. Oneof thesimplestmethodsistoprojectallqueriesandkeysontoad′-dimensionalspace(d′<d), andtocomputethedot-productofanykey-valuepairinthenewspace. Formodeling,weonly needtoreplaceQ∈Rn×dandK∈Rn×dbynewrepresentationsQ′∈Rn×d′ andK′∈Rn×d′ . 324 Chapter 6. Transformers WecaneasilymodifyEq. (6.130)touseQ′ andK′ incomputingtheattentionmatrix Q′[K′]T A = Softmax( √ +M) (6.136) d Q′ andK′ aregivenby Q′ = QUq (6.137) K′ = KUk (6.138) whereUq ∈Rd×d′ andUk ∈Rd×d′ areparametermatricesoflineartransformations. Itisalsopossibletoexploitkernelmethodstoobtainanefficientdot-productattention model. The basic idea is to map all data points (represented as vectors) from one space to anotherspace,sothattheproblem,whichmightbedifficulttosolveintheoriginalspace,is easiertosolveinthenewspace. The“trick”ofkernelmethodsisthatweactuallydonotneed toknowthemappingfunction,butonlyneedtoknowhowtocomputetheinnerproductof vectors in the new space in one operation13. This operation of the inner product is usually calledthekernelanddenotedbyK(·,·). ItisinterestingtoapproximateAinafashionanalogoustoK(·,·)inkernelmethods. To illustrate,noteinEq. (6.130)Aisafractiondenotingthenormalizedattentionweights. The numeratorcanbewrittenintheform QKT A(cid:101) = Mask(exp( √ )) (6.140) d HereMask(·)isafunctionwhichhasthesameeffectasusingtheadditivemaskingvariable M. Then,Acanbeexpressedas A = D−1A(cid:101) (6.141) whereDisann×ndiagonalmatrix. Eachentryofthemaindiagonalisthesumoftheentries ofthecorrespondingrowinA(cid:101),denotingthenormalizationfactorofSoftmax. Substitutingthis equationintoEq. (6.130),wehave Att self (S) = D−1A(cid:101)V (6.142) Inthismodel,A(cid:101)(i,j)canbeviewedasasimilarityfunctionoverallquery-keypairsina 13Inmathematicalanalysis,theinnerproductisageneralizednotionofthedot-product.Itistypicallydenoted by⟨·,·⟩.Aformaldefinitionoftheinnerproductrequiresthat⟨·,·⟩satisfiesseveralpropertiesinavectorspace. Althoughtheinnerproducthasdifferentformsindifferentcontexts,intheEuclideanspaceRd,itisthesamething asthedot-product,thatis,giventwovectorsa∈Rdandb∈Rd,wehave ⟨a,b⟩ = a·b d (cid:88) = a i ·b i (6.139) i=1 6.4 Efficient Models 325 d-dimensionalspace. Hereweassumethatthisfunction,whichisintheformofthedot-product ofvectors,canbeapproximatedbyakernelfunction A(cid:101)(i,j) = K(q i ,k j ) = ⟨ϕ(q ),ϕ(k )⟩ i j ϕ(·)isamappingfromRd toRd′ . Wecanrepresentthequeriesandkeysinthefollowingform Q′ = ϕ(Q)   ϕ(q ) 1 . =   . .   (6.143) ϕ(q ) n K′ = ϕ(K)   ϕ(k ) 1 . =   . .   (6.144) ϕ(k ) n Then,wedevelopakernelizedattentionmodelbyapproximatingtheattentionweightα i,j intheform ϕ(q )ϕ(k )T i j α ≈ (6.145) i,j (cid:80)n ϕ(q )ϕ(k )T j′=1 i j′ ThekeyideabehindthiskernelizedattentionmodelisthatwecanremovetheSoftmax functionifthequeriesandkeysaremappedtoanewspace. Usingthisapproximation,thei-th outputvectoroftheattentionmodel(i.e.,thei-throwvectorofAtt (S))isgivenby self n (cid:88) c = α ·v i i,j j j=1 (cid:88) n (cid:16) ϕ(q i )ϕ(k j )T (cid:17) ≈ ·v (cid:80)n ϕ(q )ϕ(k )T j j=1 j′=1 i j′ (cid:80)n ϕ(q )ϕ(k )Tv j=1 i j j = (cid:80)n ϕ(q )ϕ(k )T j′=1 i j′ ϕ(q )( (cid:80)n ϕ(k )Tv ) i j=1 j j = (6.146) ϕ(q )( (cid:80)n ϕ(k )T) i j′=1 j′ Althoughtheequationappearsabitcomplicated,theideaissimple: insteadofattendingthe querytoallkeystoobtaintheattentionweightα",
    "≈ ·v (cid:80)n ϕ(q )ϕ(k )T j j=1 j′=1 i j′ (cid:80)n ϕ(q )ϕ(k )Tv j=1 i j j = (cid:80)n ϕ(q )ϕ(k )T j′=1 i j′ ϕ(q )( (cid:80)n ϕ(k )Tv ) i j=1 j j = (6.146) ϕ(q )( (cid:80)n ϕ(k )T) i j′=1 j′ Althoughtheequationappearsabitcomplicated,theideaissimple: insteadofattendingthe querytoallkeystoobtaintheattentionweightα ,wecancomputethesumofthemultiplica- i,j tions (cid:80)n ϕ(k )Tv ∈Rd′×dandthenmultiplyitwiththekernelizedqueryϕ(q ). Returning j=1 j j i tothenotationusedinEq. (6.142),wedefinethei-thentryofDtobeϕ(q ) (cid:80)n ϕ(k )T. i j′=1 j′ 326 Chapter 6. Transformers Then,theattentionmodelcanbere-expressedintheform Att (S) = D−1ϕ(Q)ϕ(K)TV self = D−1Q′K′T V = D−1(cid:0) Q′(K′T V) (cid:1) (6.147) Herewechangetheorderofcomputationfromleft-to-righttoright-to-leftusingparentheses. Given that Q′ ∈ Rn×d′ and K′ ∈ Rn×d′ , this model has time and space complexities of O(n·d·d′) and O(n·d+n·d′+d·d′), respectively. Therefore, the model is linear with respecttothesequencelengthn,andissometimescalledthelinearattentionmodel. One computationaladvantageofthismodelisthatweneedonlycomputethemultiplicationK′TV (i.e., (cid:80)n ϕ(k )Tv )andthecorrespondingnormalizationfactor(i.e., (cid:80)n ϕ(k )T)once. j=1 j j j′=1 j′ Theresultscanthenbeusedforanyquery[Katharopoulosetal.,2020]. Thememoryneeds to maintain (cid:80)n ϕ(k )Tv and (cid:80)n ϕ(k )T and update them when new key and value j=1 j j j′=1 j′ vectorscome. Still, there are several problems regarding this kernelized model, for example, how to developthefeaturemapϕ(·)toobtainagoodapproximationtothestandardattentionmodel. InterestedreadersmayrefertoChoromanskietal.[2020]’sworkformoredetails. A second idea for reducing d is to take sub-space models, in which a problem in a d- dimensional space is transformed into sub-problems in lower-dimensional spaces, and the solutiontotheoriginalproblemisapproximatedbysomecombinationofthesolutionstothese sub-problems. In a general sub-space model, a d-dimensional key vector k can be mapped intoasetofd′-dimensionalvectors{K′ ,...,K′ }. Tosimplifymodeling,wecandothisby 1 η vectorsegmentation,thatis,wesegmentkintoη sub-vectors,eachhavingd′= d dimensions. η Wecantransformallqueryandvaluevectorsinthesameway. Then,theattentionmodelis appliedineachofthesesub-spaces. Thismethod,however,doesnotreducethetotalamountofcomputation. Aspresentedin Lampleetal.[2019]’swork,wecaninsteadapproximatethedot-productattentionoveraset ofkey-valuepairsbyconsideringtop-pcandidatesineachsub-space. Moreprecisely,wefind p-best key-value pairs in each sub-space, which is computationally cheaper. The Cartesian productofthesep-bestkeysetsconsistsofpη productkeys. Likewise,weobtainpη product values. Theremainingworkissimple: thed-dimensionalqueriesattendtothesed-dimensional product keys and values. An interesting difference between this sub-space model and the d-dimensionalspacemodelisthatthegeneratedproductkeysandvaluesmaybedifferentfrom anyoftheoriginalkey-values{(k ,v ),...,(k ,v )}. Thisprovidesawayforlearning 1 1 i−1 i−1 newrepresentationsofthepastinformation. Sofarwehavediscussedapproachestodimensionalityreductionalongeitherthenord dimension. Itisstraightforwardtocombinethemtodevelopa“lower-dimensional”model. As anexample,supposethatwehavethen→n′ reductionforkeysandvalues,andthed→d′ 6.4 Efficient Models 327 reductionforqueriesandkeys. Themodeltakestheform ′ Att (S) = AV self Q′K′T A = Softmax( √ +M) (6.148) d′ where Q′ ∈Rn×d′ , K′ ∈Rn′×d′ , and V′ ∈Rn′×d′ are low-dimensional representations for queries,keysandvalues. Asusual,wecaneasilyobtaintheserepresentationsthroughthelinear mappingsofQ,KandV. ThetimeandspacecomplexitiesofthismodelareO(n′·n·d′)and O(n′·n+n′·d′). 6.4.4 Parameter and Activation Sharing Redundancy is common to most large-scale neural networks. As a result, many of these modelsareover-parameterized,makingthetrainingandinferencelessefficient. Onecommon approachtoredundancyreductionistosimplifythemodelingbyremovinguselesscomponents ofthemodels,forexample,wecaneitherpruneacomplexmodelorsharesub-modelsamong differentcomponentsofittoobtainareasonablysmallmodel. Inthissubsection,wediscuss methods of parameter and intermediate state sharing in Transformer models. We leave the discussionofmodeltransferandpruningtoSection6.4.7. Shared-parameterarchitecturesarewidelyusedinneuralnetwork-basedsystems. Well- knownexamplesincludeCNNsandRNNs, wherethesamesetofparameters(orlayers)is appliedacrossdifferentregionsoftheinput. Thisproducesa“big”neuralnetwork,partsof whichhavethesamearchitectureandthesamesharedparameters. ForTransformersaswellas othersequencemodels,thesharingmechanismcanbeappliedtodifferentlevelsofmodeling. Asimpleexample,whichmightbenotrelatedtoarchitecturedesign,issharedembedding. In machinetranslation,atypicalstrategyfordealingwithwordsintwolanguagesistodevelop two separate embedding models. Alternatively, one can use a single embedding model for bothlanguages. Theparametersofthemodelarethenlearnedduringthetrainingofboththe source-sideandtarget-sidenetworks. Suchastrategyisalsooftenadoptedinmulti-lingual sequencemodels,suchaslanguagemodelsthatareabletodealwithtextsinmanydifferent languages. Formulti-layerneuralnetworks,apopularmethodislayer-wisesharing. Supposethereis astackoflayers,allofwhichhavethesameform Sl = Layer(Sl−1;θl) (6.149) Wecantietheparametersforsomeoralloftheselayers. Forexample,givenasetoflayers {l 1 ,l 2 ,...,l n },weenforcetheconstraintθl1 =θl2 =...=θln,sothatwecanobtainasmaller modelandtheoptimizationofthemodelcanbeeasier. Inpractice,thisshared-layermodelis highlyadvantageousifmanylayersareinvolved,becausewecanrepeatthesameprocessmany timestoconstructaverydeepneuralnetwork[Dehghanietal.,2018]. Forexample,sharinga singleFFNsub-layeracrosstheTransformerencoderisfoundtobeeffectiveinreducingthe redundancyinmachinetranslationsystems[Piresetal.,2023]. 328 Chapter 6. Transformers ForTransformers,sharingcanalsobeperformedinmulti-headattention. Anexampleof thisismulti-queryattention[Shazeer,2019]. RecallfromSection6.1.3thattheoutputofa headhinstandardmulti-headself-attentioncanbewrittenas Chead = Att (Sq,Sk,Sv) h qkv h h h = Att (SWq,SWk,SWv) (6.150) qkv h h h Here Sq = SWq , Sk = SWk, and Sv = SWv are the query, key, and value, which are h h h h h v obtainedbylinearlytransformingtheinputSwithdistinctparametermatricesWq ,Wk,and h h Wv. Inmulti-queryattention,wesharethesamekeyandvalueacrossalltheheads,butuse h differentqueriesfordifferentheads. Theformofthismodelisgivenby Chead = Att (SWq,SWk,SWv) (6.151) h qkv h 0 0 HerethekeySWk andvalueSWv areirrelevanttoh. Henceweneedonlycomputethem 0 0 onceratherthancomputingthemseveraltimes. Asaresult,wecanmakeasignificantsaving incomputationalcost,especiallyifthenumberofheadsislarge. Multi-queryattentionhas beensuccessfullyincorporatedintorecentlargelanguagemodels,suchasLlama2[Touvron etal.,2023b]andFalcon14. Byextendingtheideaofsharingtomoregeneralsituations,anyintermediatestatescanbe sharedacrossaneuralnetwork. Forexample,reusingneuronactivationsallowsasub-modelto beappliedmultipletimes. ForTransformers,sharingcanbeconsideredinsidetheprocessof self-attention.",
    "the query, key, and value, which are h h h h h v obtainedbylinearlytransformingtheinputSwithdistinctparametermatricesWq ,Wk,and h h Wv. Inmulti-queryattention,wesharethesamekeyandvalueacrossalltheheads,butuse h differentqueriesfordifferentheads. Theformofthismodelisgivenby Chead = Att (SWq,SWk,SWv) (6.151) h qkv h 0 0 HerethekeySWk andvalueSWv areirrelevanttoh. Henceweneedonlycomputethem 0 0 onceratherthancomputingthemseveraltimes. Asaresult,wecanmakeasignificantsaving incomputationalcost,especiallyifthenumberofheadsislarge. Multi-queryattentionhas beensuccessfullyincorporatedintorecentlargelanguagemodels,suchasLlama2[Touvron etal.,2023b]andFalcon14. Byextendingtheideaofsharingtomoregeneralsituations,anyintermediatestatescanbe sharedacrossaneuralnetwork. Forexample,reusingneuronactivationsallowsasub-modelto beappliedmultipletimes. ForTransformers,sharingcanbeconsideredinsidetheprocessof self-attention. ItisfoundthattheattentionmapsofdifferentlayersaresimilarinsomeNLP tasks[Xiaoetal.,2019]. Therefore,itisreasonabletocomputetheattentionmaponlyonce andthenuseitinthefollowinglayers. Ifwemakeafurthergeneralizationofthesharingmechanism,wecanviewitasaprocess by which we use the result produced previously rather than computing it on the fly. It is thus possible to reuse the information across different runs of a neural network. A related exampleisreversibleresidualnetworks,inwhichactivationsofonelayercanberecovered from the activations of the following layer [Gomez et al., 2017]. Hence we only keep the output of the latest layer in the forward pass. Then, in the backward pass of training, we reconstruct the output of each layer from its successor. One advantage of this reversible treatment is that the information produced in the forward pass is shared implicitly, and the modelismemory-efficient[Kitaevetal.,2020]. 6.4.5 Alternatives to Self-Attention Wehaveseenthattheuseofself-attentionisaprimarysourceofthelargecomputationand memoryrequirementsforTransformersystems. Itisnaturaltowonderifthereareefficient alternativestoself-attentionmodels. HerewepresentbrieflysomeoftheTransformervariants in which self-attention sub-layers are not required and we instead replace them with other typesofneuralnetworks. 14https://falconllm.tii.ae/index.html 6.4 Efficient Models 329 1. CNN as A Replacement of Self-Attention CNNsaresimpleandwidelyusedneuralnetworks,andareconsideredaspotentialalternatives to self-attention models. To apply CNNs to Transformers, all we need is to construct a convolutionalsub-layertoreplacetheself-attentionsub-layerinaTransformerblock. While afilterofCNNshasarestrictedreceptivefieldandthustakesinputsfroma“local”context window,largecontextscanbeeasilymodeledbystackingmultipleconvolutionalsub-layers. One key advantage of CNNs is that the number of elementary operations required to run CNNs is a linear function of the sequence length n, compared with the quadratic function for self-attention networks. In practical systems, there have been many highly-optimized implementationsforCNNs,makingiteasiertoapplythemtosequencemodeling. Forfurther improvements to memory efficiency, we can use lightweight CNN variants, for example, depth-wiseCNNs[Wuetal.,2018a]15. 2. Linear Attention Aswithmanypracticalapproachestosequencemodeling,thereisalsoconsiderableinterestin developinglinearmodelsinordertospeeduptheprocessingoflongsequences. Whilethere aremanywaystodefinealinearmodel,onegeneralformthatiscommonlyusedinsequence modelsis z = f(a·z +b·s ) (6.153) i i−1 i Here s represents some intermediate states of the model at step i, and z represents the i i summaryofthehistorystatesuptostepi. Itiseasytoseethatthisisarecurrentmodel: the output at step i depends only on the input at the current step and the output at the previous step. Aswiththepopulardesignchoicesinneuralnetwork-basedsystems,thelinearpartis followedbyatransformationf(·)whichcanbeeitheranactivationfunctionorafeedforward neuralnetwork. Notethat,Eq. (6.153)definesastandardlinearmodelonlyiff(·)isalinear function. Theuseoff(·)givesgreaterflexibilityinmodelingtheproblem,althoughtheterm linearmodelmaynotbeappliediff(·)choosesanon-linearform. Theaboveformuladescribesalinearlystructuredmodelwhichcanbeseenasaninstance ofageneralfamilyofmathematicalmodels. Typically,itcanberepresentedasachainstructure, 15RecallfromChapter2thatinCNNsafilter(orasetoffilters)combinestheinputvariablesinthereceptive fieldintoanoutputvariable(orasetofoutputvariables)vialinearmapping.Supposethattheinputandoutput ofaproblemarerepresentedassequencesoffeaturevectors. Givenafilterhavingad×kreceptivefield,we slideitalongthesequence.Ateachstep,thefiltertakesd×kinputfeaturesandproducesanoutputfeature.This procedureistypicallyexpressedby y = ReduceSum(x⊙W) (6.152) wherex∈Rk×disthevectorrepresentationoftheinput,y∈Ristheoutputfeature,andW∈Rk×distheweight matrix. ThefunctionReduceSum(·)computesthesumofallelement-wiseproductsbetweenxandW. Ifwe wanttheinputandoutputtohavethesamenumberoffeatures,wecandesigndfiltersandthenumberofparameters willbed2·k. Indepth-wiseCNNs,wetietheweightsacrossdifferentfeaturedimensions. Moreprecisely,allthecolumn vectorsofWarethesame.Thus,thenumberoftheuniqueparametersofthemodelisreducedtod·k(eachW correspondingtoafilterhavingkuniqueparameters). 330 Chapter 6. Transformers or an ordered set of nodes. The model repeats the same computation process from the first node to the last, each time taking the information from the current and previous steps and producinganoutputvectorthatisusedinthefollowingtimesteps. Asaresult,thespaceand timecostofthemodelscaleslinearlywiththelengthofthechain. WecanextendEq. (6.153)toastandardRNNmodelbysimplymakingalineartransfor- mationofthecurrentinputandthepreviousstate,thatis,z =f(z ·W +s ·W ). Itis i i−1 z i s thusstraightforwardtoapplyRNNanditsvariantstoTransformertoobtainahybridmodel. For example, we can use LSTM and GRUs in building some of the Transformer layers to combinethemeritsofbothrecurrentmodelsandself-attentivemodels[Chenetal.,2018b]. AstheconventionalrecurrentmodelshavebeendiscussedatlengthinChapter2,weskipthe discussionofthemhere. In fact, we may be more interested in developing linear attention models, so that we canobtainanefficientsystem,whilestillretainingthebenefitofgloballyattentivesequence modeling. Partofthedifficultyindoingthisisthattheformofself-attentionisnotlinear. Let ustakeamomenttoseehowthisdifficultyarises. Recallthattheresultofself-attentioncanbe writteninthefollowingform Att = A·V self = ψ(Q·KT)·V (6.154) Here ψ(·) is a function that is composed by taking the scaling, exponentiating, masking andnormalizationoperations(i.e.,ψ(a)=Normalize(Mask(exp(√a )))). Becauseψ(·)isa d complexnon-linearfunction,thereisnoobviousequivalentthatsimplifiesthecomputation, andwehavetocalculatethetwomatrixmultiplicationsseparately(oneinsideψ(·)andone outsideψ(·)). Asaconsequence,weneedtostoreallthekey-valuepairsexplicitly,andvisit eachofthemgivenaquery. Notsurprisingly,thisleadstoamodelwhosecomputationalcost growsquadraticallywiththesequencelengthn. Although in self-attention keys and values are coupled, they are used in separate steps. Anelegantformofthismodelmightbethatallowsforadirectinteractionbetweenthekeys andqueries,sothatwecanencodethecontextinformationinawaythatisirrelevanttothe queries. A trick here is that we can remove the non-linearity from ψ(·) by using a feature",
    "by taking the scaling, exponentiating, masking andnormalizationoperations(i.e.,ψ(a)=Normalize(Mask(exp(√a )))). Becauseψ(·)isa d complexnon-linearfunction,thereisnoobviousequivalentthatsimplifiesthecomputation, andwehavetocalculatethetwomatrixmultiplicationsseparately(oneinsideψ(·)andone outsideψ(·)). Asaconsequence,weneedtostoreallthekey-valuepairsexplicitly,andvisit eachofthemgivenaquery. Notsurprisingly,thisleadstoamodelwhosecomputationalcost growsquadraticallywiththesequencelengthn. Although in self-attention keys and values are coupled, they are used in separate steps. Anelegantformofthismodelmightbethatallowsforadirectinteractionbetweenthekeys andqueries,sothatwecanencodethecontextinformationinawaythatisirrelevanttothe queries. A trick here is that we can remove the non-linearity from ψ(·) by using a feature spacemappingϕ(·)onthequeriesandkeys,andreformulateψ(Q·KT)(i.e.,A)inaformof matrixproducts. Forexample,recallfromSection6.4.3thatwecantransformQandKto Q′=ϕ(Q)∈Rn×d′ andK′=ϕ(K)∈Rn×d′ throughthemappingϕ(·). Then,wedefinethe formoftheattentionmodeltobe Att ≡ ψ′(Q′·K′T )·V self Q′·K′T = ·V D Q′· (cid:0) K′T·V (cid:1) = (6.155) D whereψ′(a)= a. Fromthisdefinition,weseethat,inthecaseoftransformedqueriesandkeys, D 6.4 Efficient Models 331 thequery-keyproductneedsnotbenormalizedviaSoftmax,butneedsonlybenormalizedviaa simplefactorD. Hencethemodelhasaverysimpleforminvolvingonlymatrixmultiplication anddivision,allowingustochangetheorderoftheoperationsusingtheassociativityofmatrix multiplication. Thisleadstoaninterestingprocedure: keysandvaluesarefirstencodedviaK′T·V,and theneachqueryattendstothisencodingresult. GiventhatK′T·V= (cid:80)n k′T·v ,wecan j=1 j j writeK′T·VintheformofEq. (6.153),asfollows µ = µ +k′T ·v (6.156) j j−1 j j Here µ ∈Rd′×d is a variable that adds k′T·v at a time. Likewise, we can define another j j j variableν ∈Rd′ j ν = ν +k′T (6.157) j j−1 j Then,theoutputofself-attentionforthej-thquerycanbewrittenas(seealsoEq. (6.146)) q′ ·µ j n Att = (6.158) self,j q′ ·ν j n Clearly,thisisalinearmodel,becauseµ andν arelinearwithrespectton. Insimple n n implementationsofthismodel,onlyµ andν arekept. Eachtimeanewqueryisencountered, j j weupdateµ andν usingEqs. (6.156)and(6.157),andthencomputeAtt = q′ j ·µj16. j j self,j q′ j ·νj OnestraightforwardextensiontothelinearattentionmodelistoallowEqs. (6.156)and (6.157)tocombinedifferenttermswithdifferentweights. Forexample,wecanredefineµ j andν as j µ = a·µ +(1−a)·k′T ·v (6.159) j j−1 j j ν = a·ν +(1−a)·k′T (6.160) j j−1 j and train the parameter a as usual. Also, we can treat a as a gate and use another neural networktocomputea[Pengetal.,2021]. AnothermodeldesignistoaddmoretermstoEqs. (6.156)and(6.157)inordertogiveamorepowerfultreatmentofthelinearattentionapproach [Bello,2020;Schlagetal.,2021]. Wehaveseenageneralideaofdesigninglinearmodelsfortheattentionmechanism. The key design choice of such models is to remove the Softmax-based normalization, thereby takinglinearformsofrepresentationsbasedonvariousintermediatestatesofthemodels. This motivatesseveralrecentlydevelopedalternativestoself-attentioninwhichefficientinference systemsaredevelopedonthebasisofrecurrentmodelsofsequencemodeling[Pengetal.,2023; Sunetal.,2023]. Whilethesesystemshavedifferentarchitectures,theunderlyingmodelshave a similar form, as described in Eq. (6.153). Note that, by using the general formulation of 16Inautoregressivegeneration,wegenerateasequencefromlefttoright.Inthiscase,weneednotconsiderthe keysandvaluesforpositions>j. 332 Chapter 6. Transformers recurrentmodels,weneednotrestrictthemodelingtothestandardQKVattention. Insteadwe maygivenewmeaningsandformstothequeries,keys,andvalues. The discussion here is also related to the memory models discussed in Section 6.4.2. Fromthememoryviewpoint,thekeysandvaluescanbetreatedasencodingsofthecontext. Therefore,inthelinearattentionmodelabovewehaveamemorysysteminwhichtwosimple variables µ and ν are used to represent all the context information up to position j. This j j resultsinafixed-lengthmemorywhichisveryusefulinpractice. Therearealsootherlinear approachestoencodinglongsequences. Forexample,wecanviewthemovingaveragemodel asaninstanceofEq. (6.153),andaverageaseriesofstatevectorsofaTransformersystem, eitherweightedorunweighted. 3. State-Space Models Incontrolsystems,state-spacemodels(SSMs)arerepresentationsofasystemwhoseinput and output are related by some state variables (or states for short), and whose dynamics is described by first-order differential equations of these states. As a simple example, we consideracontinuoustime-invariantlinearsystemwhichisgivenintheformofthestate-space representation dz(t) = z(t)·A+s(t)·B (6.161) dt o(t) = z(t)·C+s(t)·D (6.162) Heres(t),o(t),andz(t)arethevaluesoftheinputvariable,outputvariableandstatevariable attimet17. Inageneralsetting,s(t),o(t),andz(t)mayhavedifferentnumbersofdimensions. Tosimplifythediscussionhere,weassumethats(t),o(t)∈Rd andz(t)∈Rdz18. Eq. (6.161) iscalledthestateequation,whereA∈Rdz×dz isthestatematrixandB∈Rd×dz istheinput matrix. Eq. (6.162)iscalledtheoutputequation,whereC∈Rdz×d istheoutputmatrixand D∈Rd×d isthefeedforwardmatrix. These equations describe a continuous mapping from the variable s(t) to the variable o(t)overtime. Theyare,therefore,oftenusedtodealwithcontinuoustimeseriesdata. To apply this model to the sequence modeling problem discussed in this chapter, we need to modifytheaboveequationstogiveadiscreteformofthestate-spacerepresentation. Suppose that {s ,s ,...,s } is a sequence of input data points sampled from s(t) with time step ∆t. 0 1 n Similarly,wedefine{z ,z ,...,z }and{o ,o ,...,o }assequencesofthestateandoutput 0 1 n 0 1 n vectors. Giventhisnotation,wenowhaveadiscretizedversionoftheSSM,writtenas z = z ·A+s ·B (6.163) t t−1 t o = z ·C+s ·D (6.164) t t t 17Weuseboldfaceletterstoemphasizethatthevariablesarevectors. 18Inageneralstate-spacemodel,allthesevariablesarerepresentedasvectorsofcomplexnumbers.Becausethe modelsdefinedonthefieldofcomplexnumbersisapplicabletocaseofrealnumber-basedstate-spaces,werestrict ourdiscussiontovariablesinthemulti-dimensionalrealnumberfield. 6.4 Efficient Models 333 ThisformulationoftheSSMdefinesanRNNwitharesidualconnection. Tobemoreprecise, Eq. (6.163)describesarecurrentunitthatreadstheinputatsteptandthestateatstept−1, withoutusinganyactivationfunction. Eq. (6.164)describesanoutputlayerthatsumsboththe lineartransformationsofthestatez andtheidentitymappings . t t TheparametersA,B,C,andDcanbeinducedfromA,B,CandDinseveraldifferent ways,dependingonhowEq. (6.161)isapproximatedbyEq. (6.163)19. Oneapproachtotime discretization, called bilinear transform or Tustin’s method, gives a model in which the parameterstaketheform ∆t ∆t A = (I− ·A)·(I− ·A)−1 (6.171)",
    "t t t 17Weuseboldfaceletterstoemphasizethatthevariablesarevectors. 18Inageneralstate-spacemodel,allthesevariablesarerepresentedasvectorsofcomplexnumbers.Becausethe modelsdefinedonthefieldofcomplexnumbersisapplicabletocaseofrealnumber-basedstate-spaces,werestrict ourdiscussiontovariablesinthemulti-dimensionalrealnumberfield. 6.4 Efficient Models 333 ThisformulationoftheSSMdefinesanRNNwitharesidualconnection. Tobemoreprecise, Eq. (6.163)describesarecurrentunitthatreadstheinputatsteptandthestateatstept−1, withoutusinganyactivationfunction. Eq. (6.164)describesanoutputlayerthatsumsboththe lineartransformationsofthestatez andtheidentitymappings . t t TheparametersA,B,C,andDcanbeinducedfromA,B,CandDinseveraldifferent ways,dependingonhowEq. (6.161)isapproximatedbyEq. (6.163)19. Oneapproachtotime discretization, called bilinear transform or Tustin’s method, gives a model in which the parameterstaketheform ∆t ∆t A = (I− ·A)·(I− ·A)−1 (6.171) 2 2 ∆t B = ∆t·B·(I− ·A)−1 (6.172) 2 C = C (6.173) D = D (6.174) An alternative approach is to use the Zero-Order-Hold (ZOH) discretization which has the form A = exp(∆t·A) (6.175) B = ∆t·B·(exp(∆t·A)−I)·(∆t·A)−1 (6.176) C = C (6.177) D = D (6.178) A detailed discussion of these approaches lies beyond the scope of this book, and we refer theinterestedreadertostandardtextbooksoncontroltheoryforfurtherdetails[Åströmand 19Thediscretizationprocesscanbeinterpretedasanumericalmethodofsolvingthedifferentialequation.Note thatEq.(6.161)isanODE dz(t) = g(z(t),t) (6.165) dt where g(z(t),t) = z(t)·A+s(t)·B (6.166) TherearemanynumericalapproximationstothesolutionstotheODE.Forexample,theEulermethodofsolving theODEcanbeexpressedintheform(seeinSection6.3.3) zt = z t−1 +∆t·g(z t−1 ,t) (6.167) SubstitutingEq.(6.166)intoEq.(6.167)yields zt = z t−1 +∆t(z t−1 ·A+st·B) = z t−1 ·(I+∆t·A)+st·(∆t·B) (6.168) Thisgivesoneofthesimplestformsofthediscretizedstateequations[Guetal.,2022b],thatis, A = I+∆t·A (6.169) B = ∆t·B (6.170) 334 Chapter 6. Transformers Wittenmark,2013]. TherecurrentformofEq. (6.163)makesiteasytocomputethestatesandoutputsovera sequenceofdiscretetimesteps. Wecanunrollz ando inafeedforwardfashion t t z =s ·B o =s ·B·C+s ·D 0 0 0 0 0 z =s ·B·A+s ·B o =s ·B·A·C+s ·B·C+s ·D 1 0 1 1 0 1 1 2 2 z =s ·B·A +s ·B·A+s ·B o =s ·B·A ·C+s ·B·A·C+ 2 0 1 2 2 0 1 s ·B·C+s ·D 2 2 ...... ...... Itiseasytowrite t (cid:88) t−i z = s ·B·A (6.179) t i i=0 t (cid:88) t−i o = s ·B·A ·C+s ·D (6.180) t i t i=0 Clearly,theright-handsideofEq. (6.180)canbeinterpretedasamergedoutputofaconvolu- tionallayerandalinearlayer. Giventhat t (cid:88) t−i (cid:104) (cid:105) s ·B·A ·C = s s ... s · i 0 1 t i=0 (cid:104) (cid:105) t t−1 B·A ·C B·A ·C ... B·C (6.181) wedefineafilterhavingtheparameters (cid:104) (cid:105) W = B·A nmax·C B·A nmax−1 ·C ... B·C (6.182) ssm where n is the maximum length of the sequence20. Then, the output of the state-space max   s 0 . modelforasequenceS=  . .   canbeexpressedas s n O = Conv(S,W )+Linear(S,D) (6.183) ssm whereConv(·)istheconvolutionoperation,andLinear(·)isthelineartransformationopera- tion. Suchatreatmentofthestate-spacemodelenablesthesystemtobeefficientlyimplemented usingfastparallelconvolutionalgorithms. Unfortunately,theabovemodelperformspoorlyinmanycases. Aswithmanydeepneural networks,carefulinitializationofthemodelparametersplaysanimportantroleinsuchmodels. 20HereWssmcanberepresentedasannmax×d×dtensor. 6.4 Efficient Models 335 Forexample,restrictingthestatematrixtoparticulartypesofmatricesisfoundtobeusefulfor learningandgeneralizingonlongsequences[Guetal.,2022a]. Another problem with the basic state-space model is that it involves multiplication of n multiplematrices. Ifthesequenceislong(i.e.,nisalargenumber),computingA willbe computationallyexpensiveandnumericallyunstable. Oneofthemostpopularapproachesto developingpracticalstate-spacemodelsforsequencemodelingisdiagonalization. Thebasic ideaisthatwecantransformastate-spacemodelintoanewstate-spacewhereA(orA)is diagonalized. Givenastate-spacemodelparameterizedby(A,B,C,D),wecandefineanew state-spacemodel(UAU−1,BU−1,UC,D)byintroducinganinvertiblematrixU. Itiseasy toprovethatthetwomodelsareequivalentunderthestate-spacetransformationU21. Byusing this state-space transformation, and by noting that A (or A) can be written as a canonical formP−1ΛP22,wecanenforcetheconstraintthatA(orA)isadiagonalmatrix,givingrise to diagonal state-space models. To illustrate, consider the filter used in the convolutional representationofthestate-spacemodel(seeEq. (6.181)). AssumingthatA=P−1ΛP,we t canwriteB·A ·Cas B·A t ·C = B·(P−1ΛP)t·C = B·(P−1ΛP)·(P−1ΛP)···(P−1ΛP)·C = (B·P−1)·Λt·(P·C) (6.185) SinceΛisadiagonalmatrix,wecanefficientlycomputeΛtbysimplyraisingalltheentries ofΛtothet-thpower. Wethenhaveacomputationallycheapermodel,inwhich ′ A = Λ (6.186) B ′ = B·P−1 (6.187) ′ C = P·C (6.188) ′ D = D (6.189) Moredetaileddiscussionsofdiagonalstate-spacemodelsinsequencemodelingcanbefound inGuetal.[2021]’swork. Theapplicationofstate-spacemodelstoTransformerissimple. Eachself-attentionsub- layerisreplacedinthiscasebyanSSMsub-layerasdescribedinEqs. (6.163)and(6.164). Aswehaveseenthereisacloserelationshipbetweenstate-spacemodelsandbothCNNsand RNNs. For sequence modeling, we can deal with a sequence of tokens either sequentially as in RNNs, or in parallel as in CNNs. This leads to a new paradigm that takes both the sequentialviewandtheparallelviewofthesequencemodelingproblem—fortraining,the 21Astatespacetransformationcanbeseenasaprocessofmappingallstatesfromtheoldspacetothenewspace, by s′(t) = s(t)·U (6.184) 22Λdenotesadiagonalmatrix. 336 Chapter 6. Transformers system operates like CNNs to make use of fast parallel training algorithms; for",
    "a sequence of tokens either sequentially as in RNNs, or in parallel as in CNNs. This leads to a new paradigm that takes both the sequentialviewandtheparallelviewofthesequencemodelingproblem—fortraining,the 21Astatespacetransformationcanbeseenasaprocessofmappingallstatesfromtheoldspacetothenewspace, by s′(t) = s(t)·U (6.184) 22Λdenotesadiagonalmatrix. 336 Chapter 6. Transformers system operates like CNNs to make use of fast parallel training algorithms; for prediction, the problem is re-cast as a sequential update problem which can be efficiently solved by using RNN-like models. It should be noted, however, that state-space models are found to underperformTransformermodelsforNLPproblems,suchaslanguagemodeling,although they have achieved promising results in several other fields. Further refinements are often neededtomakethemcompetitivewithotherwidelyusedsequencemodels[Fuetal.,2022]. While the formalism of state-space models is different from those we discussed in this chapter,itprovidesageneralframeworkofsequencemodelinginwhichtheproblemcanbe viewed fromeither oftwo differentperspectivesand wechoosedifferent onesfordifferent purposes. Several recent sequence models were motivated by this idea, leading to systems exhibitingpropertiesofbothparalleltrainingandRNN-styleinference[Orvietoetal.,2023; Sunetal.,2023]. 6.4.6 Conditional Computation So far in our discussion of efficient Transformer models, we have assumed that the model architectureisgivenbeforebeginningthetrainingofamodelandisthenfixedthroughout. We nowturntothecaseoflearningefficientmodelarchitectures. Withoutlossofgenerality,we canwriteamodelintheform y = Model(x,g(x)) (6.190) wherexandy aretheinputandoutputofthemodel. g(x)isamodelfunctionthatreturns the model architecture and corresponding parameters for the given input x. In general, we adopttheconventionprevalentinlearningproblemsofusingafixedmodelarchitectureand learning only the parameters, say, g(x)=θ. In this case, the goal of learning is to find the optimalvaluesoftheparametersgiventhemodelarchitectureandtrainingdata. Ontestdata, wemakepredictionsusingthesamemodelarchitecturealongwiththeoptimizedparameters. Anaturalextensionofthisapproachistoconsiderthelearningofboththemodelarchi- tectureandparameters. Inarchitecturelearning,wewouldliketofindamodelfunctiongˆ(x) thatproducestheoptimalmodelarchitectureandparametervaluesgiventheinputx. However, searching a hypothesis space of all possible combinations of architectures and parameter choices is extremely difficult, and so we need practical methods to achieve the goal. Two classesofmethodscanbeapplied. • Neural Architecture Search (NAS). In automated machine learning (AutoML), neuralarchitecturesearchistheprocessofexploringaspaceofneuralnetworkstofind onethatbestfitssomecriterions[ZophandLe,2016;Elskenetal.,2019b]. Oncethe optimalneuralnetworkisdetermined,itsparameterswillbetrainedasusual,andthenbe appliedtonewdata. Inordertomakesearchtractable,severaladditionaltechniques,such assearchspacepruningandfastsearchalgorithms,aretypicallyused. Applyingneural architecturesearchtothedevelopmentofefficientneuralnetworksisstraightforward [Howardetal.,2019;TanandLe,2019]. Weneedonlyincorporateefficiencymeasures into the performance estimation of neural networks, for example, the search can be 6.4 Efficient Models 337 guided by a criterion that penalizes neural networks with high latency or excessive memoryrequirements. • Dynamic Neural Networks. The key idea of dynamic neural networks is to adapt a neural network dynamically to various inputs [Gupta et al., 2004; Han et al., 2021b]. Ideally,wewouldliketolearngˆ(·),andthen,foranyinputx ,weapplythemodel new Model(x ,gˆ(x )). Asaresult,attesttimewemayhavedifferentmodelstructures new new and/or different parameters for different inputs. However, it is infeasible to develop a function gˆ(·) that can model arbitrary neural networks. In practice, gˆ(·) is often consideredtorepresentafamilyofsub-networksofasuper-network. Theproblemis thereforereframedasasimplerproblemtolearntochoosewhichsub-networkisused foragiveninput. From a machine learning perspective, the approaches to neural architecture search are general and can be applied to any neural network. On the other hand, from a practical perspective,itisstilldifficulttofindanefficientneuralnetworkthatissufficientlypowerful andgeneralizeswell. Whileneuralarchitecturesearchprovidesinterestingideasfordeveloping efficientTransformermodels,wemakenoattempttodiscussithere. Instead,thereadercan refertotheabovepaperstohaveageneralideaofit,andrefertoSoetal.[2019],Wangetal. [2020a],andHuetal.[2021]’sworkforitsapplicationtoTransformers. In this subsection, we focus on a particular family of approaches to dynamic neural networks, called conditional computation. This concept was originally motivated by the dynamicselectionofneuronsofaneuralnetwork[Bengioetal.,2013;2015]. Morerecently,it hasoftenbeenusedtorefertoasaprocessofdynamicallyselectingpartsofaneuralnetwork. Anarrowviewofconditionalcomputationistoseeg(·)asanadaptiveneuralnetworkwhich dynamicallyreducesorgrowsthenumberofcomputationunits(suchasneuronsandlayers). Asaresult,computationcanadapttochangingconditions,andwecanseekagoodaccuracy- latencytrade-offbythisadaptationmechanism. A common way to achieve this is to learn how to skip some computation steps so that we can work with a necessary sub-set of the network [Xu and Mcauley, 2023]. One of the simplestmethods,sometimescalledearlystopping,istostopthecomputationatsomepoint duringreadingorgeneratingasequence. Thistechniqueisoftenusedinpracticalsequence generation applications where a low latency is required. Suppose y ...y is the longest 1 nmax sequence that the system can generate, and s ...s is the corresponding sequence of the 1 nmax states of the top-most Transformer layer. Then we develop a model f (·) that takes one stop hiddenstates atatimeandproducesadistributionofabinaryvariablec∈{stop,nonstop} i Pr(c|s ) = f (s )",
    "y ...y is the longest 1 nmax sequence that the system can generate, and s ...s is the corresponding sequence of the 1 nmax states of the top-most Transformer layer. Then we develop a model f (·) that takes one stop hiddenstates atatimeandproducesadistributionofabinaryvariablec∈{stop,nonstop} i Pr(c|s ) = f (s ) (6.191) i stop i ThegenerationprocessterminatesifPr(stop|s )issufficientlylarge,forexample i Pr(stop|s ) ≥ Pr(nonstop|s )+θ (6.192) i i stop 338 Chapter 6. Transformers whereθ denotestheminimalmarginfordistinguishingthetwoactions23. Thisformulation stop isalsorelatedtothestoppingcriterionproblemthatisfrequentlydiscussedinsearchalgorithms forsequencegeneration(seeChapter5). f (·)canbedesignedinseveraldifferentways. For stop example,inmanypracticalapplications,thestoppingcriterionisbasedonsimpleheuristics. Alternatively,wecandefinethefunctionf (·)asaneuralnetworkandtrainitusinglabeled stop data. The above approach can be easily extended to handle situations in which some of the tokensareskipped. Thislearning-to-skipapproachistypicallyusedintheencodingstagein which all input tokens are given in advance. Let h ...h be low-level representations of a 1 m sequencex ...x . LikeEq. (6.191),wecandevelopamodelPr(c|s )(c∈{skip,nonskip}) 1 m i todeterminewhetherthetokenx canbeskipped. Figure6.16(a)and(b)showillustrations i ofearlystoppingandskipping. Notethatthelearning-to-skipmethodhasoverlapwithother linesofresearchontrainingneuralnetworks. Forexample,erasingsomeoftheinputtokensin trainingisfoundtobeusefulforachievinghighergeneralizationofTransformermodels[Shen etal.,2020a;KimandCho,2021]. Thismethodisalsorelatedtothedownsamplingmethods whichwillbediscussedinSection6.4.8. Asecondapproachtoconditionalcomputationistoresorttosparseexpertmodels, or itspopularinstance—MoE[Yukseletal.,2012]. Indeeplearning, amodelofthiskindis typicallybuiltfromanumberofexpertswhichareneuralnetworkshavingthesamestructure butwithdifferentparameters. Inthisway,wecanconstructabigmodelbysimplyincreasing the number of experts. When running this model, during either training or prediction, we activateonlyasmallnumberoftheexpertsbysomeroutingalgorithms(seeFigure6.16(c)). AnMoEmodelisanadaptivenetworksinceeachtimewehaveanewinput,themodelroutes ittodifferentexperts. InSection6.3.4,wepresentedthebasicformofMoE,andshowedhow Transformermodelscanbescaledupbythissparsemethod. Foracomprehensivereviewof therecentadvancesinMoE,werefertheinterestedreadertoFedusetal.[2022a]’swork. AthirdapproachthatcanbeusedtoadaptaTransformermodeltochanginginputisto dynamicallyshrinkthenumberoflayers. Severalmethodshavebeenproposedtodothisin an attempt to improve inference efficiency. The simplest of these is to exit at some hidden layersbywhichwecanstillmakeaccuratepredictionsforthesample(seeFigure6.16(d)and (e)). Todothis, wecaneitherdeterminetheappropriatedepthfortheentiresequence(call itasentence-leveldepth-adaptivemodel),oruseanadaptivedepthforeachtoken(callita token-leveldepth-adaptivemodel). Hereweconsidertoken-leveldepth-adaptivemodelsbut themethodscanbeeasilyextendedtosequence-leveldepth-adaptivemodels. SupposethereareLstackedlayersatpositioni24. Wewouldideallyliketofindalayer in the stack, which can be used as the last hidden layer for making predictions, and whose depthisaslowaspossible. However,wecannotsimplyusetheL-thlayerofthestackasthe oracleforthisproblem,becauseweneverknowinadvancewhatthelastlayergeneratesduring inference. Instead,weneedtodeterminewhetherthenetworkshouldstopgrowingatdepthi, consideringthelayersgeneratedsofar. 23AnequivalentformofEq.(6.192)isPr(stop|s i )≥ 1+θ 2 stop. 24AlayerisastandardTransformerblockconsistingofafewsub-layers. 6.4 Efficient Models 339 y 1 y 2 y 3 stop ... ... ... ... ... h1 h2 h3 h4 h5 y 0 y 1 y 2 − − x 1 x 2 x 3 x 4 x 5 (a)EarlyStopping (b)TokenSkipping (Decoder) (Encoder) ... ... ... ... ... y y y y y 1 2 3 4 5 expert1 expert2 expert3 expert4 x x x x x y y y y y 1 2 3 4 5 0 1 2 3 4 (c)MoE (d)Sentence-levelDepthAdaptation (Encoder) (Decoder) y y y y y y y y y y 1 2 3 4 5 1 2 3 4 5 y y y y y y y y y y 0 1 2 3 4 0 1 2 3 4 (e)Token-levelDepthAdaptation (f)LayerSkipping (Decoder) (Decoder) Figure6.16: Methodsofconditionalcomputation,includingearlystopping,tokenskipping, MoE,sentence-leveldepthadaptation,token-leveldepthadaptation,andlayerskipping. While thesemethodsareillustratedusingeithertheencodingordecodingprocess,mostofthemcan beappliedtobothTransformerencodersanddecoders. 340 Chapter 6. Transformers NowsupposewehaveaTransformerdecoderwhichproducesadistributionoveravocabu- laryV ateachstep. Asusual,wedenotetheoutputofthel-thlayeratstepibysl. Foreachsl, i i wecreateanoutputlayerthatproducesadistributionpl overthevocabulary(callitanearly i exitclassifier),givenby pl = Softmax(sl·Wl) (6.193) i i o whereWl ∈Rd×|V| istheparametermatrix. HencewehaveL−1additionaloutputlayers, o eachcorrespondingtoahiddenlayerfromdepth1toL−1. Attrainingtime,weconsiderthe cross-entropylossesof{p1,...,pL−1},andtraintheselayerstogetherwiththeTransformer i i model. Attesttime,thedepthofthenetworkgrowsasusual,andweuse{p1,...,pl}and/or i i {s1,...,sl}todeterminewhetherweshouldexitatthel-thlayer. Thereareseveralexitcriteria, i i forexample, • Common criteria are based on measures of the confidence of predictions. A simple methodistocomputetheentropyofpl,andexitifthisentropyisaboveapre-defined i value. • Alternatively,onecanviewthemaximumprobabilityoftheentriesofpl astheconfi- i denceoftheprediction. • Insteadofconsideringtheoutputofasinglelayer,wecanalsoexaminethechangein theoutputsorhiddenstatesoveranumberoflayers. Forexample,wecanmeasurethe similaritybetweenpl−1 andpl orbetweensl−1 andsl. Ifthesimilarityisaboveagiven i i i i threshold,thenwesaythattheoutputoftheneuraltendstoconvergeandthenumberof layerscanstopgrowing. • Theabovemethodscanbeextendedtoexaminethechangeinthepredictionsmadeby theclassifiersassociatedwiththelayers. Forexample,themodelcanchoosetoexitif thepredictionsmadebytheclassifiersremainunchangedforanumberoflayers. Discussions of these criteria can be found in the related papers [Xin et al., 2020; Zhou et al., 2020; Schuster et al., 2022]. There are a variety of ways to improve these early exit methods. Oneistoexploreotherformsofthepredictionforeachlayer. Forexample,wecan developamodelthatdirectlypredictshowmanylayersweneedtomodeltheinput[Elbayad etal.,2020]. Anotherlineofresearchonearlyexitfocusesonbettertrainingforthesemodels, forexample,wecanconsidervariouslossfunctionsfortrainingtheclassifiers[Schwartzetal., 2020; Schuster et al., 2022]. In addition, there is also interest in learning the combination oftheoutputsofmultiplelayerssothatwecanmakepredictionsbyusingmultiplelevelsof representation[Zhouetal.,2020;Liaoetal.,2021]. Aproblemwithtoken-leveladaptive-depthmodelsisthattherepresentationsatcertain depthsmaybeabsentintheprevioussteps. Inthiscase,standardself-attentionisnotdirectly applicable,becausewemaynotattendtotheprevioustokensinthesamelevelofrepresentation. Fortraining,thiscanbeaddressedbyusingalltheLlayersofthefullmodel. Forinference,",
    "2020; Zhou et al., 2020; Schuster et al., 2022]. There are a variety of ways to improve these early exit methods. Oneistoexploreotherformsofthepredictionforeachlayer. Forexample,wecan developamodelthatdirectlypredictshowmanylayersweneedtomodeltheinput[Elbayad etal.,2020]. Anotherlineofresearchonearlyexitfocusesonbettertrainingforthesemodels, forexample,wecanconsidervariouslossfunctionsfortrainingtheclassifiers[Schwartzetal., 2020; Schuster et al., 2022]. In addition, there is also interest in learning the combination oftheoutputsofmultiplelayerssothatwecanmakepredictionsbyusingmultiplelevelsof representation[Zhouetal.,2020;Liaoetal.,2021]. Aproblemwithtoken-leveladaptive-depthmodelsisthattherepresentationsatcertain depthsmaybeabsentintheprevioussteps. Inthiscase,standardself-attentionisnotdirectly applicable,becausewemaynotattendtotheprevioustokensinthesamelevelofrepresentation. Fortraining,thiscanbeaddressedbyusingalltheLlayersofthefullmodel. Forinference, we can either duplicate the layer from which we exit to fill up the layer stack, or modify theself-attentionmodeltoenableittoattendtotherepresentationsoftheprevioustokensat 6.4 Efficient Models 341 differentdepths. Itisalsopossibletoselectanysub-setofthelayersforconstructingashallownetwork. The adaptive models therefore can be generalized to skipping models (see Figure 6.16 (f)). As with the early exit problem, the skipping problem can be framed as a learning task, in whichaclassifieristrainedtodecidewhetheralayershouldbedropped. Thelearning-to-skip problemhasbeenstudiedinthefieldofcomputervision[Wangetal.,2018b;Wuetal.,2018b]. However, learning a skipping model for large-scale, deep neural networks is difficult. For practicalsystems,itstillseemsreasonabletouseheuristicsorcheapmodelstoobtainaneural networkhavingskippedlayers,whichhasbeendiscussedinrecentpre-trainedNLPmodels [Wangetal.,2022c;DelCorroetal.,2023]. 6.4.7 Model Transfer and Pruning ManylargeTransformermodelshavebeensuccessfullydevelopedtoaddressNLPproblems. Acommonquestionis: canwetransformalarge,well-trainedmodelintoasmalleronethat allows for more efficient inference? At a high level, this can be thought of as a transfer learningprobleminwhichtheknowledgeistransferredfromonemodeltoanother. Butwe will not discuss this general topic, which spans a broad range of issues and models, many outsidethescopeofthischapter. Instead,wenarrowourdiscussiontotwokindsofapproaches thatarewidelyusedinlearningsmallneuralnetworksfromlargeneuralnetworks. 1. Knowledge Distillation Knowledgedistillationisaprocessofcompressingtheknowledgeinalargeneuralnetwork(or anensembleofneuralnetworks)intoasmallneuralnetwork[Hintonetal.,2015]. Insupervised learningofneuralnetworks,theobjectivefunctionsaregenerallydesignedtorepresentsome lossofreplacingthetrueanswerwiththepredictedanswer. Hencewecanminimizethisloss sothatthemodelsaretrainedtooutputthetrueanswer. Whilemodelsaretypicallyoptimized on the training data in this manner, what we really want is to generalize them to new data. This is, however, difficult because we have no information about generalization in training withtheground-truth. Inknowledgedistillation,insteadofforcingamodeltostaycloseto theground-truthoutput,wetrainthismodeltogeneralize. Todothis,wedirectlytransferthe knowledge(i.e.,thegeneralizationability)ofapre-trainedmodeltothemodelthatwewantto train. A frequently used approach to knowledge distillation is teacher-student training. A teachermodelistypicallyarelativelylargeneuralnetworkthathasalreadybeentrainedandcan generalizewell. Astudentmodelisarelativelysmallneuralnetwork,suchasaneuralnetwork withfewerlayers,towhichwetransfertheknowledge. Asimplewaytodistilltheknowledge fromtheteachermodelintothestudentmodelistousetheoutputoftheteachermodelasthe “correct”answerfortrainingthestudentmodel. SupposewehaveateacherTransformermodel thatcangenerateasequenceofdistributions{Pr(·|y ,x),...,Pr(·|y ...y ,x)}fortheinput 0 0 n−1 x. Tokeepthenotationsimple,wedenotethedistributionPr(·|y ...y ,x)asp . Similarly, 0 i−1 (cid:101)i wedenotetheoutputofthestudentTransformermodelforthesameinputasp . Asusual,we i consideralossfunctionLoss(p ,p )(suchasthecross-entropyfunction)forcomputingsome (cid:101)i i 342 Chapter 6. Transformers distancebetweenp andp . Then,wecandefinethelossovertheentiresequenceas (cid:101)i i n 1(cid:88) L(x,θ) = Loss(p ,p ) (6.194) (cid:101)i i n i=1 whereθ denotestheparametersofthestudentmodel25. Usingthisloss, wecanoptimizeθ, foranygivensetofsourcesequences{x ,...,x },insuchawayastominimizethequality 1 K (cid:80)N L(x ,θ). k=1 k Severaldifferentextensionstothisbasicmethodhavebeendevelopedtomodeltheproblem ofknowledgetransferbetweentwomodels. Asimplewayistousethehiddenstatesinstead of the output probabilities as the training targets [Romero et al., 2014]. In this case, the objectiveistominimizethedifferencebetweensomehiddenstatesoftheteachermodeland thecorrespondingstatesofthestudentmodel. Ratherthanusingtheoutputsofvariouslayersas thetargetsfortrainingthestudentmodel,anothertechniqueistomodeltherelationsbetween samples and train the student model by minimizing some differences between the relation encodingsoftheteacherandstudentmodels[Parketal.,2019;Pengetal.,2019]. Forexample, wecandeveloparelationencodingmodelbasedontheTransformerarchitecture. Thegoalis thentooptimizethestudentmodelsothatitscorrespondingrelationencodingofagroupof samplesisascloseaspossibletothatoftheteachermodel. Forsequencegenerationproblems,aspecialcaseofknowledgedistillation,whichcanbe viewedasameansofdataaugmentation,isoftenusedfordevelopinglightweightmodels [KimandRush,2016]. Forexample,considertheproblemoftransferringthetranslationability ofawell-developedmachinetranslationmodel(i.e.,theteachermodel)toanewmodel(i.e., thestudentmodel). Givenasetofsource-sidesentences{x ,...,x },wecanusetheteacher 1 K modeltotranslateeachx toatarget-sidesentencey . Then,bytreatingx andy aspaired k (cid:101)k k (cid:101)k sentences, we obtain a bilingual dataset consisting of {(x ,y ),...,(x ,y )}. We can use 1 (cid:101)1 K (cid:101)K thisbilingualdatasetasthelabeleddatasettotrainthestudentmodelasusual. Oneadvantage ofthisdataargumentationmethodisthatitisarchitecturefree,andwedonotevenneedto understandtheinternalarchitecturesoftheteacherandstudentmodels. Hencewecanapply thismethodifwehaveablack-boxteachermodel. Moredetaileddiscussionsofknowledge distillationcanbefoundinGouetal.[2021]andWangandYoon[2021]’ssurveys. 2. Structured Pruning Pruningisamongthemostpopularofthemodelcompressionmethodsandhasbeenapplied to a broad range of systems. One common approach to pruning is unstructured pruning, by which we activate only some of the connections between neurons. However, as with mostsparsemodels,modelsprunedinthiswaytypicallyrequirespecialimplementationsand hardwaresupport,whichinturnreducestheirefficiencyinsomeapplications. Asimplebut moreaggressivewaytodopruningistousestructuredpruning. Indeeplearning,structured pruningisatechniquethatremovesagroupofneuronsorconnectionstogether. Forexample, wecanremoveanentirelayerofneuronfromaneuralnetworktoobtainashallowermodel. 25Weomittheparametersoftheteachermodelbecausetheyarefixedthroughoutthetrainingprocess. 6.4 Efficient Models 343 Asmulti-layer, multi-headneuralnetworks, Transformersarenaturallysuitedtostructured pruning,andwecanpruneaTransformernetworkinseveraldifferentways. Forexample,we canprunesomeoftheheadsinmulti-headattention[Voitaetal.,2019;Micheletal.,2019],or someofthelayersinthelayerstack[Houetal.,2020;KimandAwadalla,2020]. Formally, we can represent a neural network as a set of parameter groups {θ ,...,θ }, 1 R eachcorrespondingtoacomponentorsub-modelofthemodel. Ourgoalistofindasub-set of{θ ,...,θ }bywhichwecanbuildamodelthatyieldsgoodperformance,whilehavinga 1 R lowermodelcomplexity. However,asimplesearchofsuchamodelisinfeasiblebecausethere areacombinatoriallylargenumberofpossiblemodelcandidatesandevaluatingallofthese modelsiscomputationallyexpensive. Oneapproachto structuredpruningistorandomly prunecomponentsofa model. One canruntherandompruningprocessanumberoftimestogenerateapoolofmodelcandidates andselectthebestonefromthepool. Anotherapproachistouseheuristicstodecidewhich componentsarenotimportantandcanberemoved. Commonmeasuresoftheimportanceofa parametergroupθ includevariousqualitiesbasedonnormsoftheweightsorgradientsofθ r r [Santacroceetal.,2023]. Wecanpruneθ ifthevaluesofthesemeasuresarebelow(orabove) r giventhresholds. Athirdapproachistoframethepruningproblemasanoptimizationtaskby introducingtrainablegatesindicatingthepresenceofdifferentcomponents[McCarleyetal., 2019;Wangetal.,2020d;Lagunasetal.,2021]. Theprunedmodelcanbeinducedbyusing thetrainedgates. Notethat,inmanycases,pruningisnotapost-processingstepforagiven trainedmodel,butpartofthetraining. 6.4.8 Sequence Compression Insequencemodelingandgenerationproblems,thetimeandspacecomplexitiesarestrongly influencedbythelengthoftheinputoroutputsequence,andwepreferthesequencetobeshort. ThisisparticularlyimportantforTransformermodels,astheirtimeandspacecomplexitiesare quadraticwiththesequencelength,andthememoryfootprintandlatencycanbeheavyburdens if the sequence is very long. In the",
    "of{θ ,...,θ }bywhichwecanbuildamodelthatyieldsgoodperformance,whilehavinga 1 R lowermodelcomplexity. However,asimplesearchofsuchamodelisinfeasiblebecausethere areacombinatoriallylargenumberofpossiblemodelcandidatesandevaluatingallofthese modelsiscomputationallyexpensive. Oneapproachto structuredpruningistorandomly prunecomponentsofa model. One canruntherandompruningprocessanumberoftimestogenerateapoolofmodelcandidates andselectthebestonefromthepool. Anotherapproachistouseheuristicstodecidewhich componentsarenotimportantandcanberemoved. Commonmeasuresoftheimportanceofa parametergroupθ includevariousqualitiesbasedonnormsoftheweightsorgradientsofθ r r [Santacroceetal.,2023]. Wecanpruneθ ifthevaluesofthesemeasuresarebelow(orabove) r giventhresholds. Athirdapproachistoframethepruningproblemasanoptimizationtaskby introducingtrainablegatesindicatingthepresenceofdifferentcomponents[McCarleyetal., 2019;Wangetal.,2020d;Lagunasetal.,2021]. Theprunedmodelcanbeinducedbyusing thetrainedgates. Notethat,inmanycases,pruningisnotapost-processingstepforagiven trainedmodel,butpartofthetraining. 6.4.8 Sequence Compression Insequencemodelingandgenerationproblems,thetimeandspacecomplexitiesarestrongly influencedbythelengthoftheinputoroutputsequence,andwepreferthesequencetobeshort. ThisisparticularlyimportantforTransformermodels,astheirtimeandspacecomplexitiesare quadraticwiththesequencelength,andthememoryfootprintandlatencycanbeheavyburdens if the sequence is very long. In the previous subsections, we have discussed modifications to the Transformer architecture for dealing with long sequences. Here we instead consider methodsforcompressingthesequencesintooneswithacceptablelengths. One simple approach is to map the input sequence to a fixed-size representation. For example, using the recurrent models discussed in Section 6.4.2, we can encode a sequence of vectors into a single vector. This method can be easily extended to generate a “larger” representation so that this representation can retain more information of the original input. Forexample,wecanselectafixednumberofthehiddenstatesoverthesequencetoforma newsequenceoffixed-length. Anotherwaytorepresentavariable-lengthsequenceasafixed- lengthsequenceistoattendtheinputvectorstosomehiddenstates,usuallyafixednumberof learnablehiddenrepresentations. InJaegleetal.[2021]’swork,thisisdonebyintroducingr hiddenrepresentations{u ,...,u },andthenattendingtheinputvectors{x ,...,x }tothese 1 r 1 m hiddenrepresentations. TheattentionmodelcanbeastandardQKVattentionmodelinwhich weview{u ,...,u }asqueriesand{x ,...,x }askeysandvalues. Theoutputofthismodel 1 r 1 m isasequenceofr vectors,whichcanbeusedasfixed-lengthinputtodownstreamsystems. 344 Chapter 6. Transformers Asecondapproachistousedownsamplingtocompressthesequenceintoashorterone. A typical method of downsampling is strided convolution, which has been widely used in computervisionandspeechprocessing. Forexample,supposethereisasequenceofmvectors ∈Rd. Wecandevelopafilterwithawidthof2andastrideof2. Bytakingthesequenceas input,thefilterproducesasequenceof m newvectors∈Rd,andsowehaveareductionrate 2 of2. Also,wecanstackmultipleconvolutionallayersorpoolinglayerstoachieveadesired leveloflengthreduction,calledprogressivedownsampling. However,itseemsinevitable thatdownsamplingwillleadtoinformationloss[Hanetal.,2020;BurchiandVielzeuf,2021]. Weneedtoconsideratrade-offbetweenthecompressedsequencelengthandtheperformance ofdownstreamsystems[Xuetal.,2023b]. In NLP, the problem of sequence compression is also closely related to the problem of tokenizinginputstrings. Therefore,tokenizationisapracticalapproachthatcanbetakento addressthelengthissue. Segmentingastringintosmalltokens(suchascharacters)generally reduces the sparsity of the data, which makes it easier to learn the embeddings of these tokens,butsuchapproachesoftenleadtoalongsequence. Bycontrast,wewillhaveashorter sequenceifwesegmenttheinputstringintolargerunits,butthiswillsufferfromsparsedata. In deterministic tokenization methods, which produce tokenization results using statistics collectedfromtheentiredataset,thesequencelengthcanbesomehowcontrolledbyadjusting somehyper-parameter,forexample,inbytepairencoding[Sennrichetal.,2016b],increasing thesizeofthevocabularygenerallyreducesthenumberoftheresultingtokens. Anotherway toobtainanappropriatesequenceoftokensistouseamodelforchoosingamongtokenization candidates[Kudo,2018;Provilkovetal.,2020]. Aswithmanyprobabilisticmodelsfortext generation,inthiscase,wecanaddpriorstothecriterionfortokenizationselectionsothatwe canexpressapreferenceforshortersequencesoverlongersequences. Afourthapproachtosequencecompressionistodropsomeofthetokensinthesequence. Forexample,inmanypracticalapplications,wechopthesequencewhenitslengthexceedsa threshold. Wecanrelatethistotheearlystoppingandskippingapproachesinconditionalcom- putation. ThusthemethodsdiscussedinSection6.4.6aredirectlyapplied. Thetokendropping methodscanalsobeviewedaspruningmethods,calledtokenpruning. Bydiscardingtokens thatarelessimportantforrepresentingtheentiresequence,tokenpruningcansignificantly reducethesequencelengthwhilemaintainingtheperformanceofNLPsystemsondownstream tasks[Kimetal.,2023]. 6.4.9 High Performance Computing Methods Sofarinthissection,wehavediscussedefficientTransformermodelsfromtheperspectives of deep learning and NLP. However, we have not considered their efficiency on hardware. Asmodernhardwareprovidesavarietyofmodesforrunningaprogram, thepracticaltime and memory footprint savings generally depend on the specifications of hardware systems. One line of research on efficient use of computing resources explores methods of parallel computing. There have been many attempts to develop large-scale Transformer models by usingaclusterofmachines. Typically,scalingTransformerstomodelswithbillionsoreven tensofbillionsofparametersrequiresacarefuldesignofparallelismstrategiesforsharding 6.4 Efficient Models 345 thebignetworks. Moreefficientimplementationsofsuchsystemsalsoneedconsiderationsof networkingandcommunicationinthecluster,aswellastheutilizationofsparsemodelsthat activateonlyasmallsub-setoftheparametersforeachsample,enablingtheuseofverylarge models. Mostofthesemethodshavebeenstudiedinanextensiveliteratureonhowtoscale upthetrainingofdeepneuralnetworkslikeTransformersefficiently[Lepikhinetal.,2021; Barhametal.,2022;Fedusetal.,2022b]. Theresultsofthesestudieswerefoundationalto manyfollow-onworksoninvestigatingthescalinglawsforlargelanguagemodels[Brown etal.,2020;Chowdheryetal.,2022]. Sincelarge-scaledistributedmodelsaregenericandnot specializedtothecaseofTransformers,weskipthediscussionofthemhere. Theinterested readerscanrefertotheabovepapersformoredetaileddiscussions. In this subsection, we consider hardware-aware methods to seek greater computational efficiencyforTransformermodels. Wefirstconsiderasimplebutwidelyusedmethodthataims tostoreandexecuteneuralnetworksusinglowerormixed-precisionnumberrepresentations [Gholamietal.,2022]. Conventionalneuralnetworksaretypicallybasedonsingle-precision and/ordouble-precisionfloating-pointrepresentationsofdata. Whilesingle-precisionfloating- pointdatatypesprovideasufficientlyprecisewaytorepresentparametersandintermediate statesinmostcases,insomeapplications,theyarenotessential. Asanalternative,onecanuse half-precision(orevenlower-precision)formatsinstoringfloating-pointnumbersforneural networks. The size of the resulting model is thus half the size of the original model. One advantageofusinghalf-precisionfloating-pointrepresentationsisthat,althoughprocessing suchdatatypesrequiresnewAPIsoflinearalgebraoperationsandhardwaresupport,itdoes notchangethemodelarchitecture,andsoweneedonlyaslightmodificationtothesystems. Forexample,half-precisionfloating-pointrepresentationscanbeappliedtoeithertrainingor inferenceofTransformers,orboth. Recently, the deployment of large Transformer models has been further improved by quantizingthesemodels. Insignalprocessing,quantizationisaprocessofmappingcontinuous values(i.e.,floating-pointrepresentations)toasetofdiscretevalues(i.e.,fix-pointrepresen- tations) . This process is in general implemented using a system called quantizer. In the contextofneuralnetworks,aquantizerinvolvestwofunctions—thequantizationfunction andthede-quantizationfunction. Thequantizationfunctionmapsafloatingpointnumbertoa (lower-bit)integer. Asimplequantizationfunctionisgivenby x Q(x) = ⌊ ⌉ (6.195) s where⌊·⌉isaroundingfunction26,xisthereal-valuedinput,andsisthequantizationstep size that controls the level of quantization. The quantization function is coupled with a de-quantizationfunction D(r) = s·r (6.196) With this notation, the quantizer can be expressed as D(Q(x)) = s·⌊x⌉. The difference s 26⌊a⌉returnstheintegerclosesttoa. 346 Chapter 6. Transformers betweenD(Q(x))andxiscalledquantizationerror. Asmallervalueofstypicallyreducesthe quantizationerror. Inpractice,however,wewishtochooseanappropriatevalueofsinorderto max{D(r)} spreadpossiblevaluesofQ(r)evenlyacrossvaluesofaninteger,forexample,s= 2p−1 wherepisthenumberofbitsusedtorepresentanintegerandmax{D(r)}isthemaximum valueforD(r). Theaboveequationsshowoneofthesimplestcasesofquantization. More general discussions of quantization can be found in books on digital signal processing and relatedsurveys[OppenheimandSchafer,1975;RabinerandGold,1975;Gray,1998]. Applying quantization to Transformers is relatively straightforward. The idea is that we quantize the inputs and model parameters",
    "s 26⌊a⌉returnstheintegerclosesttoa. 346 Chapter 6. Transformers betweenD(Q(x))andxiscalledquantizationerror. Asmallervalueofstypicallyreducesthe quantizationerror. Inpractice,however,wewishtochooseanappropriatevalueofsinorderto max{D(r)} spreadpossiblevaluesofQ(r)evenlyacrossvaluesofaninteger,forexample,s= 2p−1 wherepisthenumberofbitsusedtorepresentanintegerandmax{D(r)}isthemaximum valueforD(r). Theaboveequationsshowoneofthesimplestcasesofquantization. More general discussions of quantization can be found in books on digital signal processing and relatedsurveys[OppenheimandSchafer,1975;RabinerandGold,1975;Gray,1998]. Applying quantization to Transformers is relatively straightforward. The idea is that we quantize the inputs and model parameters using Q(x), and feed them to a quantized Transformermodelinwhichallthelayersoperateoninteger-valuedtensors. Inotherwords, we implement the model using integer-only arithmetic. However, the price to be paid for thiscompressedmodel,aswithmanyapproximationapproachestodeeplearning,isthatits predictionisnotasaccurateasthatofthestandardTransformermodel. Usingintegeroperations toapproximatecontinuous-valuedoperationsgenerallyleadstoapproximationerrors. These errorswillbeaccumulatedifthequantizedneuralnetworkisdeep. Furthermore,Transformer modelsinvolvecomponents(suchasself-attentionsub-layers)thatrequirerelativelycomplex linear algebra operations. Simply applying quantization to these sub-models will lead to highaccuracyloss. Onesolutionistosimplifythemodelarchitectureanddevelopnewsub- models that is more feasible for quantization. Alternatively, a more common paradigm in quantizedneuralnetworksistoaddde-quantizationfunctionstotheneuralnetworkssothat theoutputofalayerisfloating-pointtensorsandcanbeusedasusualinthefollowingsteps. Considerasimpleexamplewherewemultiplyareal-valuedinputmatrixawithareal-valued parametermatrixA. WefirstquantizeaandA,andmultiplythemusinginteger-basedmatrix multiplication. Theresultisthende-quantizedtoareal-valuedmatrix. Inthisway,weobtain anapproximationD(Q(a)·Q(A))toa·Ainaverycheapway. However,sandwichingeachlayerbetweenQ(·)andD(·)willleadtoadditionalcostof runningQ(·)andD(·). Insomepracticalapplications,thecomputationaloverheadintroduced byQ(·)andD(·)isevenbiggerthanthetimesavingofperforminginteger-basedoperations. Ingeneral,thebenefitofquantizingneuralnetworkswouldbelargerthanitscostiftheneural networks are large. Therefore, in practice it is common to perform quantized computation onlyforoperationswhosecomputationalcostsarehigh. Forexample,inrecentlargelanguage models, quantization is primarily applied to the multiplication of large matrices, yielding significanttimeandmemorysavings. Whilethequantizationapproachescanbeusedinbothtrainingandinference,awidely- used approach is to get Transformer models quantized after training (call it post-training quantization). Inthisapproach,quantizationisperformedonwell-trainedfloating-point-based neural networks and there will be fewer quantization-related errors. However, these errors cannot be compensated for because they exist after training. A more promising idea is to involvequantizationintrainingsothatthemodelcanlearntocompensateforquantization- relatederrors[Jacobetal.,2018;Nageletal.,2021]. Therehavebeenseveralattemptstoapply quantization-awaretrainingtoTransformers[Bondarenkoetal.,2021;Stocketal.,2021;Yang et al., 2023b]. In addition to computational efficiency, another important consideration for 6.5 Applications 347 high-performancesystemsistherestrictionsofthememoryhierarchy. Ingeneral,bettersystem designrequiresconsideringthespeedsandsizesofdifferentlevelsofmemory. Theproblemis evenmorecomplicatedwhenwetrainlargeTransformermodelsonmodernhardwarewhere bothGPUsandCPUsareused. Ageneralprincipleofsystemdesignisthatmemorytransfer betweendifferentmemorylevelsshouldbeminimized. Whilewewouldideallyliketohavea largehigh-levelmemoryonwhichwecanstoreallthedatathatweneedtoprocess,inmany practicalsituationsthesizeofthefast,on-chipmemoryisordersofmagnitudesmallerthan thesizeofdata. Inthiscase,wecanre-orderthememoryaccessinthealgorithmssothatthe datausedinnearbycomputationstepscanbeloadedintothehigh-speedmemoryatonetime. Thisideamotivatesthedevelopmentofmanyfastlinearalgebralibraries. Forexample,there are matrix multiplication algorithms that are highly optimized for different shapes of input matrices. It is relatively straightforward to use these optimized linear algebra algorithms to build a Transformer system. But the modules of this system are not optimized as a whole for efficiencyimprovement. Forexample,aself-attentionsub-layerinvolvesaseriesofoperations ofscaling,normalization,andmatrixmultiplication. Althougheachoftheseoperationshas beenimplementedinseveralsupportedandefficientlibrariesoflinearalgebra,successivecalls tothemstillrequiremultipletimesofmemorytransferwhenweswitchfromoneoperation to another. In practice, a better approach would be that we keep some of the intermediate statesintheon-chipmemory,andreusetheminthefollowingcomputationstepsinsteadof fetchingthemagainfromtheslowmemory. Forexample,onmodernGPUs,asimplewayto achievethisistomergemultipleoperationsintoasingleoperation,knownaskernelfusion. ForTransformermodels,ageneralideaistodesigndatapartitioningandlayoutstrategiesby which we maximize the computation on each data block loaded into the high-performance memory, while at the same time minimizing the memory transfer. There have been several attemptstousethesestrategiestoimprovetheattentionmodelsinTransformers[Ivanovetal., 2021;Popeetal.,2023]. Someofthesemethods,suchasflashattentionandpagedattention, havebeensuccessfullyincorporatedintorecentlargelanguagemodels[Daoetal.,2022;Kwon etal.,2023]. 6.5 Applications Transformershaveawiderangeofapplications,coveringnumerousNLPproblems. Whilethe TransformermodelintroducedbyVaswanietal.[2017]isbasedonastandardencoder-decoder architecture,itismainlyusedinthreedifferentways. • Decoder-onlyModels. Byremovingthecross-attentionsub-layersfromaTransformer decoder, the decoder becomes a standard language model. Hence this decoder-only model can be applied totext generation problems. For example, given a sequence of left-contexttokens,weusethemodeltopredictthenextandfollowingtokens. • Encoder-onlyModels. Transformerencoderscanbetreatedassequencemodelsthat takeasequenceoftokensatonceandproduceasequenceofrepresentations,eachof 348 Chapter 6. Transformers whichcorrespondstoaninputtoken. Theserepresentationscanbeseenassomesort ofencodingoftheinputsequence,andareoftentakenasinputtoapredictionmodel. Thisencoder+predictorarchitectureformsthebasisofmanyNLPsystems,forexample, systemsofsentenceclassification,sequencelabeling,andsoon. Pre-trainedTransformer encoderscanalsobeusedtomaptextsintothesamevectorspacesothatwecancompute thedistanceorsimilaritybetweenanytwotexts. • Encoder-Decoder Models. Encoder-decoder models are typically used to model sequence-to-sequenceproblems. ApplicationsincludemanytasksinNLPandrelated fields,suchasmachinetranslationandimagecaptioning. NotethatwhilemostTransformer-basedsystemscanfallintotheabovethreecategories, the same NLP problem can generally be addressed using different types of models. For example,recentdecoder-onlymodelshavedemonstratedgoodperformanceonabroadrange ofproblemsbyframingthemastextgenerationtasks,thoughsomeoftheseproblemswere oftenaddressedbyusingencoder-decoderorencoder-onlymodels. Toillustratehowtheabove modelsareapplied,thissectionconsidersafewapplicationswhereTransformersaschosenas thebackbonemodels. 6.5.1 Language Modeling Language modeling is an NLP task in which we predict the next token given its preceding tokens. Thisisgenerallyformulatedasaproblemofestimatingthedistributionoftokensat position i+1 given tokens at positions 0∼i (denoted by Pr(·|x ,...,x ) where {x ,...,x } 0 i 0 i denotethetokensuptopositioni). Thebestpredictedtokenistheonewhichmaximizesthe probability,givenby xˆ = argmaxPr(x |x ,...,x ) (6.197) i+1 i+1 0 i xi+1∈V whereV isthevocabulary. Thepredictioncanbeextendedtothetokensfollowingxˆ i+1 xˆ = argmaxPr(x |x ,...,x ,xˆ ,...,xˆ ) (6.198) k+1 k+ 0 i i+1 k x ∈V k+1 This model forms the basis of many systems for text generation: given the context tokens x ...x , we",
    "|x ,...,x ) (6.197) i+1 i+1 0 i xi+1∈V whereV isthevocabulary. Thepredictioncanbeextendedtothetokensfollowingxˆ i+1 xˆ = argmaxPr(x |x ,...,x ,xˆ ,...,xˆ ) (6.198) k+1 k+ 0 i i+1 k x ∈V k+1 This model forms the basis of many systems for text generation: given the context tokens x ...x , we generate the remaining tokens xˆ ...xˆ to make the sequence complete and 1 i i+1 k+1 coherent. AsdiscussedinSection6.1.1,Transformerdecodersareessentiallylanguagemodels. The onlydifferencebetweentheproblemofdecodinginanencoder-decoderTransformerandthe problemoflanguagemodelingisthattheTransformerdecodermakespredictionsconditioned onthe“context”tokensonboththeencoderanddecodersides,ratherthanbeingconditioned onprecedingtokenssolelyononeside. TomodifytheTransformerdecodertoimplementa standardlanguagemodel,thecross-attentionsub-layersaresimplyremovedandaTransformer 6.5 Applications 349 decodingblockcanbeexpressedas Sl = Layer (Sl ) (6.199) ffn self Sl = Layer (Sl−1) (6.200) self self Here Sl denotes the output of the block at depth l. Layer (·) denotes the self-attention self sub-layer,andLayer (·)denotestheFFNsub-layer. Weseethatthisdecodingblockhasthe ffn sameformasanencodingblock. Thedifferencebetweenthedecodingandencodingblocks arisesfromthemaskingstrategiesadoptedintraining,becausetheformermaskstheattention fromapositionitoanyright-contextpositionk>iwhereasthelatterhasnosuchrestriction. ASoftmaxlayerisstackedonthetopofthelastblock,andisusedtoproducethedistribution over the vocabulary at each position. For inference, the Transformer decoder works in an auto-regressivemanner,asdescribedinEq. (6.198). The training of this model is standard. We learn the model by repeatedly updating the parameters, based on the gradients of the loss on the training samples. This paradigm can be extended to the training of large Transformer-based language models, which have been widely applied in generative AI. However, training Transformer models at scale, including decoder-only,encoder-only,andencoder-decodermodels,mayleadtonewdifficulties,such astraininginstabilities. Wewilldiscusstheseissuesfurtherinthefollowingchapters,where large-scalepre-trainingistheprimaryfocus. 6.5.2 Text Encoding For many NLP problems, a widely used paradigm is to first represent an input sequence in someform,andthenmakepredictionsfordownstreamtasksbasedonthisrepresentation. Asa result,weseparatesequencemodelingorsequencerepresentationfromNLPtasks. Oneofthe advantagesofthisparadigmisthatwecantrainasequencemodelthatisnotspecializedto particulartasks,therebygeneralizingwell. Clearly, Transformer encoders are a type of sequence model, and can be used as text encoders. Consider a Transformer encoder with L encoding blocks. The output of the last encodingblockcanbeseenastheencodingresult. Hereaddaspecialtokenx toanysequence, 0 indicatingthebeginningofasequence(writtenas⟨SOS⟩or[CLS]). Ifthereisasequenceof m+1inputtokensx x ...x ,theoutputoftheencoderwillbeasequenceofm+1vectors 0 1 m hLhL...hL. Sincex isnotarealtokenandhasafixedpositionalembedding,itservesasatag 0 1 m 0 forcollectinginformationfromotherpositionsusingtheself-attentionmechanism. HencehL 0 isarepresentationoftheentiresequence,withnobiasesforanyspecifictokensorpositions. In manycases,weneedasinglerepresentationofasequenceandtakeitasinputtodownstream components of the system, for example, we can construct a sentence classification system based on a single vector generated from {hL,...,hL}. In this case, we can simply use hL 0 m 0 as the representation of the sequence. A more general approach is to add a pooling layer totheencoder. Thisallowsustoexplorevariouspoolingmethodstogeneratethesequence embeddingfrom{hL,...,hL}. 0 m Intextencoding,tokensequencesarerepresentedbyreal-valuedvectors,oftenreferredto 350 Chapter 6. Transformers assentencerepresentationsorsentenceembeddings,whichcanbeseenaspointsinamulti- dimensionalspace[Hilletal.,2016]. Anotherwaytomakeuseoftextencoding,therefore,is toobtainsemanticorsyntacticsimilaritiesoftokensequencesbasedontheirrelativepositions or proximity in this space. A straightforward method for this is to compute the Euclidean distancesbetweensequenceembeddings. Theshorterthedistancebetweentwosequences,the moresimilartheyareconsideredtobe. Therearemanydistancemetricswecanchoose,andit ispossibletocombinethemtoobtainabettermeasureofsequencesimilarity. Suchsimilarity computationsareappliedinareassuchastextentailment, informationretrieval, translation evaluation,amongothers[Ceretal.,2018;ReimersandGurevych,2019]. Additionally,they areoftenusedtoassessthequalityoftextencodingmodels. Textencodingisalsoacrucialcomponentofsequence-to-sequencemodels. Giventhis,we candevelopaseparateTransformerencoderforsource-sidesequencemodelinginanencoder- decodersystem(seeFigure6.17). Forexample,wecanpre-trainaTransformerencoderon large-scalesource-sidetexts,anduseitastheencoderinadownstreamencoder-decodermodel. ItisworthnotingthatwhiletheencoderisdesignedbasedontheTransformerarchitecture, the decoder is not confined to just Transformers. Such flexibility enables us to incorporate pre-trained Transformer encoders into hybrid sequence-to-sequence architectures, such as systemsthatcombineaTransformerencoderwithanLSTMdecoder. Insupervisedlearningscenarios,trainingaTransformerencoderisstraightforward. We cantreatitasaregularcomponentofthetargetmodelandtrainthismodelontask-specific labeleddata. However,suchamethodrequirestheencodertobeoptimizedoneachtask,and the resulting encoder might not always generalize well to other tasks, especially given that labeled data is scarce in most cases. A more prevalent approach is to frame the training of textencodersasanindependenttaskinwhichsupervisionsignalsarederivedsolelyfromraw text. This led researchers to develop self-supervised Transformer encoders, such as BERT, which make use of large-scale unlabeled text, and these encoders were found to generalize wellacrossmanydownstreamtasks. Furtherdiscussionsofpre-trainedTransformerencoders canbefoundinChapter7. 6.5.3 Speech Translation AsillustratedinSection6.1,thestandardencoder-decoderTransformermodelwasproposed tomodelsequence-to-sequenceproblems. Hereweconsidertheproblemoftranslatingspeech in one language to text in another language — a problem that is conventionally addressed usingbothautomaticspeechrecognition(ASR)andmachinetranslationtechniques. Instead ofcascadinganautomaticspeechrecognitionsystemandamachinetranslationsystem,we canuseTransformermodelstobuildanend-to-endspeech-to-text(S2T)translationsystemto directlytranslatetheinputspeechtotheoutputtext. To simplify the discussion, we assume that the input of an S2T translation system is a sequenceofsource-sideacousticfeaturevectors,denotedbya ...a , andtheoutputofthe 1 m systemisasequenceoftarget-sidetokens,denotedbyy ...y .27",
    "Speech Translation AsillustratedinSection6.1,thestandardencoder-decoderTransformermodelwasproposed tomodelsequence-to-sequenceproblems. Hereweconsidertheproblemoftranslatingspeech in one language to text in another language — a problem that is conventionally addressed usingbothautomaticspeechrecognition(ASR)andmachinetranslationtechniques. Instead ofcascadinganautomaticspeechrecognitionsystemandamachinetranslationsystem,we canuseTransformermodelstobuildanend-to-endspeech-to-text(S2T)translationsystemto directlytranslatetheinputspeechtotheoutputtext. To simplify the discussion, we assume that the input of an S2T translation system is a sequenceofsource-sideacousticfeaturevectors,denotedbya ...a , andtheoutputofthe 1 m systemisasequenceoftarget-sidetokens,denotedbyy ...y .27 Mappinga ...a toy ...y 1 n 1 m 1 n isasequence-to-sequenceproblem. Thusitisstraightforwardtomodeltheproblemusingan 27Inordertoobtaintheinputsequencetothesystem,weneedtodiscretizecontinuousspeechintosignals 6.5 Applications 351 Similarity=0.7 Classifier(e.g.,Softmax) SimilarityComputation(e.g.,FFN) Pooling Pooling Pooling Encoder Encoder Encoder ⟨CLS⟩Never give up . ⟨CLS⟩Never give up . ⟨CLS⟩Never say die ! (a)Classification (b)SimilarityComputation 永 不 放弃 。 Encoder Decoder ⟨CLS⟩Never give up . ⟨SOS⟩ 永 不 放弃 (c)Sequence-to-SequenceModeling Figure6.17: IntegratingTransformerencodersascomponentsofdifferentsystems. Acommon approachistofeedtheoutputoftheencoder(withpooling)intoaclassifiertoobtainasequence classificationsystem. AnotherwaytoutilizeTransformerencodersistocomputethesimilarity betweentwosequences. Weusethesameencodertorepresentthetwosequences,andthen constructaneuralnetworkontopofthetworepresentationsforproducingasimilarityscore betweenthem. Asusual,Transformerencoderscanalsobeusedinencoder-decodersystems tomodelsequence-to-sequenceproblems. encoder-decoderTransformermodel,andthetrainingandinferenceofthismodelarestandard, likeinneuralmachinetranslation. InS2Ttranslation,however,wehavetodealwithsequencemappingsbetweenmodalities andbetweenlanguagessimultaneously. Thisposesnewchallengescomparedwithconventional machinetranslationproblemsandinfluencesthedesignofS2Ttranslationmodels. Therehave beenseveralimprovementstoTransformermodelsforadaptingthembettertoS2Ttranslation tasks. SomeoftheimprovementsconcernthedesignofTransformerblocks[DiGangietal., 2019]. Forexample,inGulatietal.[2020]’ssystem,aCNNsub-layerandrelativepositional embeddings are integrated into each Transformer block, enabling the model to efficiently representedbyfeaturevectors.Thisprocessistypicallynontrivial,requiringeitherafeatureextractorbasedona varietyofsignalprocessingoperationsoraneuralnetworkthatlearnsfeaturemappingsinanend-to-endmanner. Butwewillnotdiveintothedetailsofthesemethodsandsimplytreattheinputfeatureextractorasanupstream system. 352 Chapter 6. Transformers capturebothlocalandglobalfeatures. Another line of research on S2T translation focuses on improving the encoder-decoder architecture. Thisinvolvesmodificationstoeitherencodersordecoders,orboth. Toillustrate, Figure6.18showsthearchitecturesofthreeS2Ttranslationmodels. Allofthemarebasedon Transformers,buthavedifferentencoderarchitectures. Asshowninthefigure,thestandard encoder-decoderarchitecturehasoneTransformerencoderforreadingthesource-sideinput a ...a andoneTransformerdecoderforproducingthetarget-sideoutputy ...y . Bycontrast, 1 m 1 n the decoupled encoder model separates the encoder into two stacked encoders — one for acousticmodeling(callitthespeechencoder),andonefortextualmodeling(callitthetext encoder)[Liuetal.,2020d;Xuetal.,2021a]. Thisdesignreflectsamodelinghierarchyin whichrepresentationsindifferentlevelsofthenetworkareconcernedwithdifferentaspectsof theproblem,forexample,thespeechencodermodelslow-levelfeaturesinmappingacoustic embeddingsintolargerlanguageunits,andthetextencodermodelsthesemanticorsyntactic features in representing the entire input sequence. An advantage of separating out the text encoderisthattheencodingprocessfollowsourpriorknowledgethatweneedtofirsttranscribe thespeechinputandthentranslatethetranscriptintothetargetlanguage. Therefore,wecan trainthespeechencoderinsomewaywetrainanASRsystem. Thisenablesustopre-train the speech encoder and the text encoder on unlabeled data, and incorporate the pre-trained encodersintoS2Ttranslationsystems. Analternativeencodingarchitectureisthetwo-streamarchitecture, asshowninFigure 6.18(c). Likethedecoupledencoderarchitecture,thisarchitecturehasaspeechencoderanda textencoder,butthetwoencodersworkinparallelratherthaninsequence[Yeetal.,2021]. Thespeechencodertakesacousticfeaturesasinputandthetextencodertakestokens(ortheir embeddings)asinput. Athirdencoder,calledsharedencoder,integratestheoutputsfrom boththespeechandtextencoders,mergingtherepresentationsfromthetwomodalities. This two-streamarchitectureisflexiblebecauseitprovidesmultiplewaystotrainS2Ttranslation models. Acommonapproachistotraineachbranchindividually. Forexample,ifwemaskthe speechencoder,thenthemodelwilltransformintoamachinetranslationmodelwhichcanbe trainedusingbilingualtexts. Conversely,ifwemaskthetextencoder,thenwecantrainthe modelasastandardS2Ttranslationmodel. Forinference,thetextencodercanbedropped, andthespeechinputismodeledusingthespeechencoderandthesharedencoder. In deep learning, training is often related to architecture design. Here, we have data in two modalities and two languages, and so we can develop multiple supervision signals for multi-task learning of S2T translation models. A widely used method is to introduce ASR- relatedlossintothetrainingofspeechencoders. Forexample,inthedecoupledencodermodel, aclassifiercanbeconstructedbasedontheoutputfromthespeechencoder. Byminimizing the connectionist temporal classification (CTC) loss for this classifier, the speech encoder canbeoptimizedinamannersimilartoASR.Ingeneral,trainingS2Ttranslationmodelsis challenging because speech-to-text aligned data is scarce. Among typical responses to this challengearedataaugmentation,pre-training,knowledgedistillationwithmachinetranslation, andsoon. However,anin-depthdiscussionofthesemethodsgoesbeyondthescopeofthis discussion on Transformers. The interested reader can refer to a recent survey on speech 6.5 Applications 353 Text(Target) Encoder Decoder (Speech) Speech(Source) (a)SingleEncoder+SingleDecoder Text(Target) Encoder Encoder Decoder (Speech) (Text) Speech(Source) Transcript(Source) (b)DecoupledEncoder+SingleDecoder Text(Target) Encoder Encoder Shared Decoder (Speech) (Text) Encoder Speech(Source) Transcript(Source) (c)Two-streamEncoder+SingleDecoder Figure 6.18: Architectures of speech-to-text translation models based on Transformers. In additiontothestandardencoder-decoderarchitecture,wecanexplicitlymodeltheacousticand textual(semantic)informationusingtwoseparateencoders,calledthespeechencoderandthe textencoder. Inthedecoupledencoderarchitecture,thetwoencodersarestacked,thatis,text encodingisasubsequentprocessafterspeechencoding. Inthetwo-streamencoderarchitecture, thetwoencodersworkinparallel,andtheiroutputsaremergedusinganadditionalencoder, calledthesharedencoder. Thedottedlineindicatesthepotentialforinteractionbetweenthe twoencoders. Forexample,wecoulddefinealossfunctiontominimizethedifferencebetween theiroutputs,therebyguidingthemodeltowardsmorealignedrepresentations. translationformoreinformation[Xuetal.,2023a]. 6.5.4 Vision Models While Transformers were first used in NLP, their application to other domains has been a prominent research topic. In computer vision, for instance, there is a notable trend of shiftingfromCNNstoTransformersasthebackbonemodels. Inthissubsection,weconsider VisionTransformer(ViT)-aninterestingapplicationofTransformerstoimageclassification 354 Chapter 6. Transformers Isitabuilding? Classifier TransformerEncoder C=ExtraLearnable C [CLS]embedding PatchEmbedding+PositionalEmbedding FlattenedImagePatches Image Figure 6.19: Illustration of Vision Transformer for image classification[Dosovitskiy et al., 2021]. Therearethreesteps. Inthefirststep,theinputimageissegmentedintopatches,which arethenflattenedandmappedintoembeddings. Inthesecondstep, aTransformerencoder isemployedtoprocessthesequenceofembeddings,representingtheimageasareal-valued vector(e.g.,theoutputoftheencoderatthefirstposition). Inthelaststep,aclassifierisbuilt ontopofthisimagerepresentation. [Dosovitskiy et al., 2021]. Vision Transformer is a milestone model which opens the door topurelyTransformer-basedvisionmodels. HereweconsiderthebasicstructureofVision transformer to make this section concentrated and coherent, although there has been an extensiveliteratureonVisiontransformeranditsvariants. Moredetaileddiscussionsofvision transformercanbefoundinrecentsurveys[Hanetal.,2022;Liuetal.,2023e]. ThecoreideabehindVisionTransformeristotransformanimageintoasequenceofvisual tokens, and input this sequence into a Transformer encoder to generate a representation of theimage.",
    "Inthelaststep,aclassifierisbuilt ontopofthisimagerepresentation. [Dosovitskiy et al., 2021]. Vision Transformer is a milestone model which opens the door topurelyTransformer-basedvisionmodels. HereweconsiderthebasicstructureofVision transformer to make this section concentrated and coherent, although there has been an extensiveliteratureonVisiontransformeranditsvariants. Moredetaileddiscussionsofvision transformercanbefoundinrecentsurveys[Hanetal.,2022;Liuetal.,2023e]. ThecoreideabehindVisionTransformeristotransformanimageintoasequenceofvisual tokens, and input this sequence into a Transformer encoder to generate a representation of theimage. TheTransformerencoderisstandard,andsowewillnotdiscussithere,giventhe introductiontoTransformerswehavepresentedsofarinthischapter. Mappinga2Dimage intoasequenceoftokensneedssomeadditionalwork. Supposewehaveanimagerepresented as an H×W ×C feature map, where H is the height of the image, W is the width of the image,andC isthenumberofchannels. Thefirststepistosegmentthisimageintoanumber ofpatches. SupposeallpatchesaresquaresofsidelengthP. Thentheresultingpatchescan berepresentedbyfeaturemapsofshapeP ×P ×C. Byorderingthesepatchesinsomeway, weobtainasequenceof HW patches,witheachpatchbeingtreatedasa“token”. P2 Giventhispatchsequence,thesubsequentstepsarestraightforward. Forthepatchateach position,weobtainad-dimensionalembeddingbyalineartransformationoftheinputfeature map. TheinputoftheTransformerencoderisasequenceofd-dimensionalvectors,eachof whichisthesumofthecorrespondingpatchandpositionalembeddings. Figure6.19illustrates thepatchingandembeddingstepsinVisionTransformer. Once we have a sequence of vectors for representing the image, we can employ the Transformerencodertoencodethesequence. Theencodingprocessisexactlythesameasthat 6.5 Applications 355 in text encoding as discussed in Section 6.5.2. For classification problems, we need only a singlerepresentationoftheinput. Itisconvenienttotaketheoutputoftheencoderatposition0 (denotedbyhL)andfeeditintoaclassifier. Giventhatthefirsttoken[CLS]servesasaspecial 0 tokenthatwouldbeattendedtobyallothertokens,hL providesanunbiasedrepresentationof 0 theentiresequence. Typically,astandardwaytotrainVisionTransformeristominimizesomelossonlabeled data, such as ImageNet. More recently, inspired by self-supervised learning in BERT-like models,therehavebeensuccessfulattemptstotrainTransformer-basedimageencoderson large-scaleunlabeleddata[Caronetal.,2021;Baoetal.,2021;Heetal.,2022]. Notethatone ofthemostsignificantcontributionsofVisionTransformeristhatitunifiestherepresentation modelsfordifferentmodalities. Thissuggeststhatifanobject,whetheranimageortext,is represented as a sequence of embeddings, it can be easily modeled using the Transformer architecture. 6.5.5 Multimodal Models TheabovediscussionofVisionTransformeroffersthepossibilityofunifyingtherepresenta- tionsfrommultiplemodalitiesusingthesameTransformerarchitecture. Infact,manyrecent multimodal systems draw inspiration largely from Transformers [Xu et al., 2023c]. Such systemsconvertobjectsfromdifferentmodalitiesintovectorsequencesandfeedthesevectors intoasingleTransformermodel. Theoutputisafusedrepresentationofallinputs,whichcan thenbeusedindownstreamsystems. Asasimpleexample,considerthetaskofencodingapairconsistingoftextanditscorre- spondingimage. First,werepresentboththetextandtheimageassequencesofembeddings thathavethesamedimensionality. Thisisacommonstepinsequencemodeling,whichwe haveconfrontedmanytimessofar. Wecandothisbyusingeitherasimpleembeddingmodel (e.g., a word or patch embedding model) or a well-trained sequence model (e.g., a vision model). Then, these two sequences are concatenated into a long sequence involving both textualandvisualembeddings. Thefollow-onstepisstandard: aTransformerencodertakes theconcatenatedsequenceofembeddingsasinputandproducesrepresentationsofthetextand imageasoutput. Notethatconcatenatingtextualandvisualsequencesisoneofthesimplest methodsforvision-textmodeling. Thereareseveralalternativewaystomergeinformation fromdifferentmodalities,forexample,wecanfeedvisualrepresentationsintotheattention layersofatextencoderordecoder[Lietal.,2022e;Alayracetal.,2022]. The above multimodal encoder can be used in both encoder-only and encoder-decoder systems. For encoder-only systems, consider an example where, given an image and a description of it, we predict the class of the image using a classifier built on top of the encoder[Kimetal.,2021]. Forencoder-decodersystems,wepairtheencoderwithadecoder, as in sequence-to-sequence modeling [Cho et al., 2021]. For example, we might employ a Transformerdecodertogeneratetextbasedontheoutputoftheencoder. Acommonapplication ofthisarchitectureisvisualquestionanswering(VQA),whereanimageandaquestionabout theimageareprovided,andthesystemistaskedwithgeneratingananswer[Antoletal.,2015]. ThearchitecturesofthesemodelsareillustratedinFigure6.20(a-b). 356 Chapter 6. Transformers Classifier Isitananimal? TransformerEncoder C WordandPositionalEmbeddings PatchandPositionalEmbeddings ⟨CLS⟩Acubewithgreencolor. (a)Multi-modalEncoder+Classifier 一个 绿色 的 立方体 。 TransformerEncoder Decoder C WordandPositionalEmbeddings PatchandPositionalEmbeddings ⟨SOS⟩一个 绿色 的 立方体 ⟨CLS⟩Acubewithgreencolor. (b)Multi-modalEncoder+TextDecoder(Translation) The color is green . TransformerDecoder C WordandPositionalEmbeddings PatchandPositionalEmbeddings ⟨SOS⟩ The color is green ⟨CLS⟩What’sthecolorofthecube? (c)Multi-modalDecoder(LanguageModeling) Figure6.20: Vision-textmodels. Blueboxesrepresentword+positionembeddings,andred boxesrepresentimagepatch+positionembeddings. 6.6 Summary 357 Morerecently,NLPhasseennewadvancesbyusinglargelanguagemodelstodealwith both textual and other forms of data, such as images, videos, and audio, leading to new breakthroughsinmultimodalprocessing[Liuetal.,2023a;Yinetal.,2023]. Byrepresenting allinputsasasequenceoftokenembeddings,theproblemwillbesimple: wepredictthenext tokengivenitscontext. Thiscanbedonebyusingdecoder-onlysystems,asshowninFigure 6.20(c). 6.6 Summary Transformermodelshaveachievedwidespreaduseoverthepastfewyearssincetheconceptof TransformerwasproposedbyVaswanietal.[2017]. Thishasacceleratedthedevelopmentof thesemodels,leadingtoagreatvarietyofnewalgorithms,systemsandconcepts. Athorough discussion of Transformers requires a broad scope, and so it is impossible to cover every problemandtoprovideacompletelistofthecorrespondingreferences. Whilethischapterhas presentedadetailedintroductiontoTransformers,therearestilltopicsthatwedidnotmention, suchasthetheoreticalaspectsofthesemodels. Figure6.21showsanoverviewofTransformer models,whereweattempttogiveabigpicture. Notethatthesemodelsandrelatedtechniques canbeclassifiedinmanydifferentways,andwejustshowoneofthem. Tosummarize,we wouldliketohighlightthefollowingpoints. • FoundationsofTransformers. AlthoughtheimpactofTransformershasbeenrevo- lutionary, they are not completely \"new\" models. From a deep learning perspective, Transformersarecomposedofcommonbuildingblocks,includingwordandpositional embeddings[Bengioetal.,2003a;Mikolovetal.,2013c;Gehringetal.,2017b],attention mechanisms[Bahdanauetal.,2014;Luongetal.,2015],residualconnections[Heetal., 2016b], layer-normalization [Ba et al., 2016], and so on. Many of these components werepresentedinearliersystems,forexample,similarideaswithQKVattentioncanbe foundinmemorynetworks[Sukhbaataretal.,2015]andhierarchicalattentionnetworks [Yangetal.,2016]. Transformersofferanovelapproachtointegratingthesecomponents, resulting in a unique architecture. For example, in Transformers, the combination of multi-headattentionanddot-productQKVattention, alongwiththeincorporationof layer-normalizationandresidualconnections,givesrisetoadistinctiveneuralnetwork block,specificallyaself-attentionsub-layer. Thisdesignhassincebecomeadefacto standardinmanyfollow-onsequencemodelingsystems. • AttentionModels. ThesuccessofTransformersonNLPtaskshaslargelybeenattributed totheuseofmulti-headself-attentionforsequencemodeling. Thishasledtoasurge of interest in enhancing the attention mechanisms within Transformers. While it is impossibletodetaileveryattentionmodel,thereareseveralnotableresearchdirections. One prominent direction involves modifying the forms of QKV attention and multi- headattentionforimprovedperformance. Thescopeofthisdirectionisvast,asthere are numerous aspects to consider when enhancing Transformers [Lin et al., 2022a]. Forexample,onemayaddnewcomponentstoself-attentionsub-layerstoadaptthem 358",
    "layer-normalizationandresidualconnections,givesrisetoadistinctiveneuralnetwork block,specificallyaself-attentionsub-layer. Thisdesignhassincebecomeadefacto standardinmanyfollow-onsequencemodelingsystems. • AttentionModels. ThesuccessofTransformersonNLPtaskshaslargelybeenattributed totheuseofmulti-headself-attentionforsequencemodeling. Thishasledtoasurge of interest in enhancing the attention mechanisms within Transformers. While it is impossibletodetaileveryattentionmodel,thereareseveralnotableresearchdirections. One prominent direction involves modifying the forms of QKV attention and multi- headattentionforimprovedperformance. Thescopeofthisdirectionisvast,asthere are numerous aspects to consider when enhancing Transformers [Lin et al., 2022a]. Forexample,onemayaddnewcomponentstoself-attentionsub-layerstoadaptthem 358 Chapter 6. Transformers Applications Supervised Transformers in NLP Vision Neural Machine Translation, ViT, Swin, MAE, BEiT, DETR, Summarization, Sentiment Analysis, iGPT, etc. Self-supervised Transformers in NLP Question Answering, Named Entity Speech Natural Language Natural Language Recognition, Syntactic Analysis, etc. Wav2vec 2.0, Whisper, Understanding: Generation: HuBERT, Conformer, etc. BERT, RoBERTa, GPT Series, T5, BART, Multi-modal ALBERT, SpanBERT, MASS, PaLM, LaMDA, Bioinformatics Robotics CLIP, ViLBERT, VisualBERT, VL- ERNIE, XLM, XLNet, Megatron-Turing NLG, BERT, UNITER, LXMERT, ViLT, Recommendation Systems etc. BLOOM, LLaMA, etc. VLT5, Data2vec, etc. Time Series Analysis Training Inference Positional Encoding Data Efficiency Search Absolute Positional Encoding Algorithms Rotational Positional Encoding Quantization M P A a c o r t a i d v m e a l t e C i t o o e n m r S a p h n r a d e r s in si g on Ea C r a ly ch S i t n o g p L E e x n tr g a t p h o lation R E e n la c t o iv d e in P g ositional • Knowledge Distillation • Structured Pruning Attention Sequence Compression • Token Pruning IO-aware Attention Syntax- • Progressive Downsampling aware Sparse Attention Attention • Span--based / Local Regularization Attention • Layer Skipping / Wide Models • Chunked Attention Stochastic Layers • Mixture of • Strided Attention Experts • Learning Attention Fields Scalability Alternatives Recurrent and Memory • Convolutional Attention Models Deep Models • Linear Attention • Cache-based Memory • Encoding Long-term Memory • Layer Fusion • Retrieval-based Methods • Multi-scale Models Architecture • Multi-branch Models Efficiency Improvement • Numerical Method-inspired Models Foundations of Transformers Transformer Sub-models Architectures Theoretical Analysis • Word Encoding and Positional Encoding • Encoder Only • Linguistics • Multi-head Self-attention • Decoder Only • Machine Learning • Feed-forward Networks • Encoder-Decoder • Formal Systems • Layer Normalization and Residual Connections Foundations of Transformers Efficiency Attention Inference Training Architecture Improvement Positional Embedding Scalability Applications Figure6.21: AnoverviewofTransformers. 6.6 Summary 359 to specific tasks, resulting in various Transformer variants. A second direction is to incorporate prior knowledge into the design of attention models. This makes sense, becausemuchoftheemphasisintraditionalNLPhasbeenonusinglinguisticinsightsto guidesystemdesign,andwegenerallywantNLPsystemstobelinguisticallyexplainable. Forexample,manyTransformer-basedsystemstakesyntacticparsesasinputinvarious forms and make use of syntax in sequence modeling. A third direction is to develop efficientattentionmodels[Tayetal.,2020b]. Self-attentionhaslongbeencriticizedfor itsquadratictimecomplexityanddependencyonallprevioustokensforeachnewtoken. Inresponse,manyresearchershavefocusedonsimplifyingthestructureofself-attention, oronapproximatingitusingsparseorrecurrentmodels. Thisconcernforefficiencyalso motivatesthedevelopmentofalternativestoself-attention,suchasattentionmodelswith lineartimecomplexity. Inadditiontoexploringstrongerandmoreefficientattention models,itisnaturaltoexaminewhatknowledgeislearnedbysuchmodels. Interestingly, researchers have found that the underlying structure of languages can be learned by multi-headself-attentionmodels,althoughthesemodelsarenottrainedtorepresentsuch knowledge[Manningetal.,2020]. • WordandPositionalEmbeddings. Transformersrepresenteachinputwordasaword embedding, along with its positional embedding. Learning these word embeddings isnotaspecificproblemforTransformers. Wecaneitherresorttowell-trainedword embeddings, suchastheWord2VecorGloVeembeddings, ortreatthemaslearnable parameters of Transformers. A related issue is tokenization of the input sequences. In general, tokenization impacts the number of resulting tokens and the difficulty of learning the corresponding embeddings. In many applications, therefore, one needs tocarefullychooseatokenizationmethod. Furthermore, positionalembeddingplays an important role",
    "Learning these word embeddings isnotaspecificproblemforTransformers. Wecaneitherresorttowell-trainedword embeddings, suchastheWord2VecorGloVeembeddings, ortreatthemaslearnable parameters of Transformers. A related issue is tokenization of the input sequences. In general, tokenization impacts the number of resulting tokens and the difficulty of learning the corresponding embeddings. In many applications, therefore, one needs tocarefullychooseatokenizationmethod. Furthermore, positionalembeddingplays an important role in Transformers, as the attention mechanisms are order-insensitive bydesign[Dufteretal.,2022]. Althoughpositionalembeddingisageneralproblem, muchoftheresearchisfocusedonimprovingTransformers,leadingtomodifications to Transformer models [Shaw et al., 2018; Huang et al., 2018]. Additionally, studies show that, when we deal with sequences that are much longer than those in training data,extrapolationcanbeachievedbyreplacingsinusoidalpositionalembeddingswith rotarypositionalembeddingsorsimplyscalingattentionweightswithapositionalscalar [Raffeletal.,2020;Suetal.,2021;Pressetal.,2021]. • Training and Model Scaling. In the era of deep learning, powerful systems are typically obtained by using large neural networks. A simple approach to increasing the model capacity of Transformers is to stack more layers and/or enlarge the size of each representation. We can see many cases where deep and wide Transformer models consistently outperform small models. However, challenges arise when we attempttotrainextremelylargeTransformermodels,especiallywhengradientdescentis appliedovervastamountsofdata,demandingsubstantialcomputationalresources. An engineeringsolutionistodistributethetrainingacrossaclusterofcomputers[Lepikhin et al., 2021; Chowdhery et al., 2022]. While distributed training is a very general 360 Chapter 6. Transformers methodandisnotrestrictedtoTransformers,itindeedinfluencesthedesignofmodel architectures,forexample,sparseexpertmodelscaneasethetrainingwithdistributed parameters,servingasthefoundationformanyexpansiveTransformer-basedsystems. ScalingupthetrainingofTransformersallowsustostudythescalinglawoflargeneural networks: howmodelperformancerelatestomodelsize,trainingdatasize,andtraining cost[Hestnessetal.,2017;Kaplanetal.,2020]. Thisissometimesaccompaniedbyan interestingbehavior,knownasemergence[Weietal.,2022b]. InrecentNLPresearch, the acquisition of emergent abilities has been considered one of the prerequisites for developingstronglanguagemodels. • EfficientModels. Therearedifferentgoalsforefficiency. Forexample,onemaywisha systemtobememoryefficientwhentheproblemismemorybound,oronemaywishit tobespeedefficientwhenlatencyisanimportantconsideration. Ingeneral,weneedto seekabalancebetweenthesegoals,resultingindifferentefficiencyoptimizations. In thecontextofTransformers, manyoftheseoptimizationsareachievedbymodifying the attention models, as mentioned above. For example, several variants of the self- attention models are proposed to reduce the memory footprint when processing long sequences[Tayetal.,2020b]. Similarly,othervariantsaimtoreducecomputationand thusgivelowerlatency. Furthermore,beingatypeofneuralnetwork,Transformerscan beoptimizedinwaysindependentofmodelarchitectures. Typicalmethodsincludebut arenotlimitedtoconditionalcomputation,knowledgedistillation,structuredpruning, andsequencecompression. Efficiencyoptimizationscanalsobeconsideredfromthe perspectiveofcomputerarchitecture[Kimetal.,2023]. Forexample,whenapplying Transformerstosequence-to-sequenceproblems,theencodinganddecodingprocesses are generally compute-intensive and IO-intensive, respectively. Therefore, we can employdifferentoptimizationmethodsfordifferentcomponentsofTransformers. • Inference. Theinferenceproblemiscommonlydiscussedinsequencegeneration. In NLP,weoftenneedtofindthe“best”hypothesisinaspaceinvolvingsequencesoftens orevenhundredsoftokensoveravocabulary. Consideringthisaninstanceofthesearch probleminartificialintelligence,manyalgorithmscanbeapplied,suchasbreadth-first search, depth-first search and A* search. In many practical applications of NLP, the efficiencyofthesearchsystemsisanimportantconsideration. Asaresult,optimized searchalgorithmsarerequired. Mostofthesealgorithmshavebeenexploredinmachine translationandASR,andaredirectlyapplicabletoneuraltextgenerationmodelslike Transformer. Therearealsooptimizationsofconventionaldecodingmethodstailored to Transformers [Leviathan et al., 2023]. Moreover, the above-mentioned efficient approaches,suchastheefficientattentionmodels,arealsoinwidespreaduse,withmany successfulexamplesindeployingneuralmachinetranslationsystemsandlargelanguage models[Heafieldetal.,2021;Daoetal.,2023]. • Applications. Applications of Transformers cover a wide variety of NLP problems. During the development of Transformers, they were at first used to build supervised models that perform particular tasks. Later, a greater success was achieved by using 6.6 Summary 361 themasbackbonenetworksforlargescaleself-supervisedlearningoffoundationmodels [Bommasanietal.,2021]. ThismarkedlychangedtheparadigminNLP.Weneedonly pre-trainamodeltoobtaingeneralknowledgeoflanguagesonhugeamountsoftext. Then,weadaptthismodeltodownstreamtasksusingmethodswithlittleeffort,such asfine-tuningorprompting. Overthepastfewyears,wehavealsoseenanexplosion of applications for Transformers in fields other than NLP, such as computer vision, speechprocessing, andbioinformatics. Theideabehindtheseapplicationsisthatwe canrepresentanyinputdataasasequenceoftokensanddirectlyemployTransformers tomodelthissequence. ThisapproachextendsTransformerstogeneralrepresentation modelsacrossdifferentmodalities,makingiteasiertouseTransformersforhandling multi-modaldata. • LargeLanguageModelsasFoundationModels. Transformersformthebasisofrecent largelanguagemodels,suchastheGPTseries,whichshowsurprisingbreakthroughs inNLP,andeveninartificialgeneralintelligence(AGI)[Bubecketal.,2023;Yang et al., 2023a]. Much of the research in large language models is more or less related to Transformers. For example, as discussed in Section 6.5.1, the problem of training theselanguagemodelsisthesameasthatoftrainingTransformerdecoders. Andthe modificationstoTransformerdecoderscanbedirectlyappliedtolargelanguagemodels. On the other hand, the rapid development of large language models has also driven further improvements in various techniques for Transformers, such as efficient and low-costadaptationoflargeTransformerstodifferenttasks. • TheoreticalAnalysis. AlthoughTransformershaveshownstrongempiricalresultsin variousfields,theirtheoreticalaspectshavereceivedrelativelylessattentioncompared totheextensiveresearchonmodelimprovementandengineering. Thisisnotaspecific problemforTransformers,butacommonproblemfortheNLPandmachinelearning communities. In response, researchers have made attempts to analyze Transformers moredeeply. OnewayistoviewTransformersasdeepneuralnetworksandinterpret themviamathematicaltools. Forexample,theresidualnetworksinTransformersare mathematically equivalent to the Euler solvers for ODEs. This equivalence suggests that we can leverage insights from numerical ODE methods to inform model design. Another promising avenue of research aims to develop a theoretical understanding of the self-attention mechanism, which distinguishes Transformers from other deep learning models. For example, there have been studies on interpreting self-attention andTransformersfrommachinelearningperspectives,suchasdatacompression[Yu etal.,2023a],optimization[Lietal.,2022c],andfunctionapproximation[Yunetal., 2019].",
    "equivalence suggests that we can leverage insights from numerical ODE methods to inform model design. Another promising avenue of research aims to develop a theoretical understanding of the self-attention mechanism, which distinguishes Transformers from other deep learning models. For example, there have been studies on interpreting self-attention andTransformersfrommachinelearningperspectives,suchasdatacompression[Yu etal.,2023a],optimization[Lietal.,2022c],andfunctionapproximation[Yunetal., 2019]. Moreover,Transformerscanalsoberelatedtoformalsystems,includingTuring machines [Pérez et al., 2018], counter machines [Bhattamishra et al., 2020], regular and context-free languages [Hahn, 2020], Boolean circuits [Hao et al., 2022; Merrill etal.,2022],programminglanguages[Weissetal.,2021],first-orderlogic[Chiangetal., 2023a],andsoon. TheseprovidetoolstostudytheexpressivityofTransformers. Itis, however,worthnotingthat,whilewecanunderstandTransformersinseveraldifferent 362 Chapter 6. Transformers ways,therearenogeneraltheoriestoexplainthenatureofthesemodels. Perhapsthis isachallengeforthefieldofmachinelearning,andmanyresearchersareworkingon thisissue. Butitisindeedanimportantissue,asthedevelopmentofthetheoriesbehind complexneuralnetworkslikeTransformerscanhelpdevelopsystemswithexplainable andpredictablebehaviors. III Large Language Models 7 Pre-training ............................. 365 7.1 Pre-trainingNLPModels 7.2 Self-supervisedPre-trainingTasks 7.3 Example: BERT 7.4 ApplyingBERTModels 7.5 Summary 8 Generative Models ....................... 403 8.1 ABriefIntroductiontoLLMs 8.2 TrainingatScale 8.3 LongSequenceModeling 8.4 Summary 9 Prompting ............................... 467 9.1 GeneralPromptDesign 9.2 AdvancedPromptingMethods 9.3 LearningtoPrompt 9.4 Summary 10 Alignment ............................... 533 10.1 AnOverviewofLLMAlignment 10.2 InstructionAlignment 10.3 HumanPreferenceAlignment: RLHF 10.4 ImprovedHumanPreferenceAlignment 10.5 Summary 11 Inference ................................ 587 11.1 PrefillingandDecoding 11.2 EfficientInferenceTechniques 11.3 Inference-timeScaling 11.4 Summary https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 7 Pre-training Thedevelopmentofneuralsequencemodels,suchasTransformers,alongwiththeimprove- ments in large-scale self-supervised learning, has opened the door to universal language understanding and generation. This achievement is largely motivated by pre-training: we separatecommoncomponentsfrommanyneuralnetwork-basedsystems,andthentrainthem onhugeamountsofunlabeleddatausingself-supervision. Thesepre-trainedmodelsserveas foundationmodelsthatcanbeeasilyadaptedtodifferenttasksviafine-tuningorprompting. Asaresult,theparadigmofNLPhasbeenenormouslychanged. Inmanycases,large-scale supervisedlearningforspecifictasksisnolongerrequired,andinstead,weonlyneedtoadapt pre-trainedfoundationmodels. While pre-training has gained popularity in recent NLP research, this concept dates back decades to the early days of deep learning. For example, early attempts to pre-train deeplearningsystemsincludeunsupervisedlearningforRNNs,deepfeedforwardnetworks, autoencoders, and others [Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurgence of pre-training, caused in part by the large-scale unsupervised learningofvariouswordembeddingmodels[Mikolovetal.,2013c;Penningtonetal.,2014]. During the same period, pre-training also attracted significant interest in computer vision, wherethebackbonemodelsweretrainedonrelativelylargelabeleddatasetssuchasImageNet, andthenappliedtodifferentdownstreamtasks[Heetal.,2019;Zophetal.,2020]. Large-scale researchonpre-traininginNLPbeganwiththedevelopmentoflanguagemodelsusingself- supervisedlearning. Thisfamilyofmodelscoversseveralwell-knownexampleslikeBERT [Devlinetal.,2019]andGPT[Brownetal.,2020],allwithasimilarideathatgenerallanguage understandingandgenerationcanbeachievedbytrainingthemodelstopredictmaskedwords in a huge amount of text. Despite the simple nature of this approach, the resulting models show remarkable capability in modeling linguistic structure, though they are not explicitly trainedtoachievethis. Thegeneralityofthepre-trainingtasksleadstosystemsthatexhibit strongperformanceinalargevarietyofNLPproblems,evenoutperformingpreviouslywell- developedsupervisedsystems. Morerecently,pre-trainedlargelanguagemodelshaveachieved greatersuccess,showingtheexcitingprospectsformoregeneralartificialintelligence[Bubeck etal.,2023]. 366 Chapter 7. Pre-training Thischapterdiscussestheconceptofpre-traininginthecontextofNLP.Itbeginswitha generalintroductiontopre-trainingmethodsandtheirapplications. BERTisthenusedasan exampletoillustratehowasequencemodelistrainedviaaself-supervisedtask,calledmasked language modeling. This is followed by a discussion of methods for adapting pre-trained sequencemodelsforvariousNLPtasks. Notethatinthischapter,wewillfocusprimarilyonthe pre-trainingparadigminNLP,andtherefore,wedonotintendtocoverdetailsaboutgenerative large language models. A detailed discussion of these models will be left to subsequent chapters. 7.1 Pre-training NLP Models Thediscussionofpre-trainingissuesinNLPtypicallyinvolvestwotypesofproblems: sequence modeling(orsequenceencoding)andsequencegeneration. Whiletheseproblemshavedifferent forms,forsimplicity,wedescribethemusingasinglemodeldefinedasfollows: o = g(x ,x ,...,x ;θ) 0 1 m = g (x ,x ,...,x ) (7.1) θ 0 1 m where{x ,x ,...,x }denotesasequenceofinputtokens1,x denotesaspecialsymbol(⟨s⟩or 0 1 m 0 [CLS])attachedtothebeginningofasequence,g(·;θ)(alsowrittenasg (·))denotesaneural θ networkwithparametersθ,andodenotestheoutputoftheneuralnetwork. Differentproblems canvarybasedontheformoftheoutputo. Forexample,intokenpredictionproblems(asin languagemodeling),oisadistributionoveravocabulary;insequenceencodingproblems,ois arepresentationoftheinputsequence,oftenexpressedasareal-valuedvectorsequence. Therearetwofundamentalissueshere. • Optimizing θ on a pre-training task. Unlike standard learning problems in NLP, pre- trainingdoesnotassumespecificdownstreamtaskstowhichthemodelwillbeapplied. Instead,thegoalistotrainamodelthatcangeneralizeacrossvarioustasks. • Applyingthepre-trainedmodelg (·)todownstreamtasks. Toadaptthemodeltothese θˆ tasks,weneedtoadjusttheparametersθˆ slightlyusinglabeleddataorpromptthemodel withtaskdescriptions. Inthissection,wediscussthebasicideasinaddressingtheseissues. 7.1.1 Unsupervised, Supervised and Self-supervised Pre-training Indeeplearning,pre-trainingreferstotheprocessofoptimizinganeuralnetworkbeforeit is further trained/tuned and applied to the tasks of interest. This approach is based on an assumptionthatamodelpre-trainedononetaskcanbeadaptedtoperformanothertask. As aresult,wedonotneedtotrainadeep,complexneuralnetworkfromscratchontaskswith 1Hereweassumethattokensarebasicunitsoftextthatareseparatedthroughtokenization.Sometimes,wewill usethetermstokenandwordinterchangeably,thoughtheyhavecloselyrelatedbutslightlydifferentmeaningsin NLP. 7.1 Pre-training NLP Models 367 limitedlabeleddata. Instead,wecanmakeuseoftaskswheresupervisionsignalsareeasierto obtain. Thisreducestherelianceontask-specificlabeleddata,enablingthedevelopmentof moregeneralmodelsthatarenotconfinedtoparticularproblems. Duringtheresurgenceofneuralnetworksthroughdeeplearning,manyearlyattemptsto achievepre-trainingwerefocusedonunsupervisedlearning. Inthesemethods,theparameters ofaneuralnetworkareoptimizedusingacriterionthatisnotdirectlyrelatedtospecifictasks. Forexample,wecanminimizethereconstructioncross-entropyoftheinputvectorforeach layer[Bengioetal.,2006]. Unsupervisedpre-trainingiscommonlyemployedasapreliminary stepbeforesupervisedlearning,offeringseveraladvantages,suchasaidinginthediscovery ofbetterlocalminimaandaddingaregularizationeffecttothetrainingprocess[Erhanetal., 2010]. Thesebenefitsmakethesubsequentsupervisedlearningphaseeasierandmorestable. Asecondapproachtopre-trainingistopre-trainaneuralnetworkonsupervisedlearning tasks. Forexample,considerasequencemodeldesignedtoencodeinputsequencesintosome representations. Inpre-training,thismodeliscombinedwithaclassificationlayertoforma classificationsystem. Thissystemisthentrainedonapre-trainingtask,suchasclassifying sentencesbasedonsentiment(e.g.,determiningifasentenceconveysapositiveornegative sentiment). Then, we adapt the sequence model to a downstream task. We build a new classificationsystembasedonthispre-trainedsequencemodelandanewclassificationlayer (e.g.,determiningifasequenceissubjectiveorobjective). Typically,weneedtofine-tunethe parametersofthenewmodelusingtask-specificlabeleddata,ensuringthemodelisoptimally adjusted to perform well on this new type of data. The fine-tuned model is then",
    "ofaneuralnetworkareoptimizedusingacriterionthatisnotdirectlyrelatedtospecifictasks. Forexample,wecanminimizethereconstructioncross-entropyoftheinputvectorforeach layer[Bengioetal.,2006]. Unsupervisedpre-trainingiscommonlyemployedasapreliminary stepbeforesupervisedlearning,offeringseveraladvantages,suchasaidinginthediscovery ofbetterlocalminimaandaddingaregularizationeffecttothetrainingprocess[Erhanetal., 2010]. Thesebenefitsmakethesubsequentsupervisedlearningphaseeasierandmorestable. Asecondapproachtopre-trainingistopre-trainaneuralnetworkonsupervisedlearning tasks. Forexample,considerasequencemodeldesignedtoencodeinputsequencesintosome representations. Inpre-training,thismodeliscombinedwithaclassificationlayertoforma classificationsystem. Thissystemisthentrainedonapre-trainingtask,suchasclassifying sentencesbasedonsentiment(e.g.,determiningifasentenceconveysapositiveornegative sentiment). Then, we adapt the sequence model to a downstream task. We build a new classificationsystembasedonthispre-trainedsequencemodelandanewclassificationlayer (e.g.,determiningifasequenceissubjectiveorobjective). Typically,weneedtofine-tunethe parametersofthenewmodelusingtask-specificlabeleddata,ensuringthemodelisoptimally adjusted to perform well on this new type of data. The fine-tuned model is then employed to classify new sequences for this task. An advantage of supervised pre-training is that the trainingprocess,eitherinthepre-trainingorfine-tuningphase,isstraightforward,asitfollows thewell-studiedgeneralparadigmofsupervisedlearninginmachinelearning. However,as thecomplexityoftheneuralnetworkincreases,thedemandformorelabeleddataalsogrows. This,inturn,makesthepre-trainingtaskmoredifficult,especiallywhenlarge-scalelabeled dataisnotavailable. Athirdapproachtopre-trainingisself-supervisedlearning. Inthisapproach,aneural networkistrainedusingthesupervisionsignalsgeneratedbyitself,ratherthanthoseprovided by humans. This is generally done by constructing its own training tasks directly from unlabeleddata,suchashavingthesystemcreatepseudolabels. Whileself-supervisedlearning hasrecentlyemergedasaverypopularmethodinNLP,itisnotanewconcept. Inmachine learning,arelatedconceptisself-trainingwhereamodelisiterativelyimprovedbylearning from the pseudo labels assigned to a dataset. To do this, we need some seed data to build an initial model. This model then generates pseudo labels for unlabeled data, and these pseudolabelsaresubsequentlyusedtoiterativelyrefineandbootstrapthemodelitself. Sucha methodhasbeensuccessfullyusedinseveralNLPareas,suchaswordsensedisambiguation [Yarowsky,1995]anddocumentclassification[BlumandMitchell,1998]. Unlikethestandard self-trainingmethod,self-supervisedpre-traininginNLPdoesnotrelyonaninitialmodelfor annotatingthedata. Instead,allthesupervisionsignalsarecreatedfromthetext,andtheentire modelistrainedfromscratch. Awell-knownexampleofthisistrainingsequencemodelsby successivelypredictingamaskedwordgivenitsprecedingorsurroundingwordsinatext. This 368 Chapter 7. Pre-training Prompting Zero/Few ShotLearning Pre-training Training Pre-training Tuning Pre-training Tuning Self- Unsupervised Supervised Supervised Supervised Supervised Supervised Unlabeled Labeled Labeled Labeled Unlabeled Labeled Data Data Data Data Data Data Task2 Task1 (a)UnsupervisedPre-training (b)SupervisedPre-training (c)Self-supervisedPre-training Figure 7.1: Illustration of unsupervised, supervised, and self-supervised pre-training. In unsupervisedpre-training,thepre-trainingisperformedonlarge-scaleunlabeleddata. Itcan beviewedasapreliminarysteptohaveagoodstartingpointforthesubsequentoptimization process, though considerable effort is still required to further train the model with labeled dataafterpre-training. Insupervisedpre-training,theunderlyingassumptionisthatdifferent (supervised)learningtasksarerelated. Sowecanfirsttrainthemodelononetask,andtransfer the resulting model to another task with some training or tuning effort. In self-supervised pre-training, a model is pre-trained on large-scale unlabeled data via self-supervision. The model can be well trained in this way, and we can efficiently adapt it to new tasks through fine-tuningorprompting. enableslarge-scaleself-supervisedlearningfordeepneuralnetworks,leadingtothesuccessof pre-traininginmanyunderstanding,writing,andreasoningtasks. Figure7.1showsacomparisonoftheabovethreepre-trainingapproaches. Self-supervised pre-trainingissosuccessfulthatmostcurrentstate-of-the-artNLPmodelsarebasedonthis paradigm. Therefore,inthischapterandthroughoutthisbook,wewillfocusonself-supervised pre-training. Wewillshowhowsequencemodelsarepre-trainedviaself-supervisionandhow thepre-trainedmodelsareapplied. 7.1.2 Adapting Pre-trained Models Asmentionedabove,twomajortypesofmodelsarewidelyusedinNLPpre-training. • SequenceEncodingModels. Givenasequenceofwordsortokens,asequenceencoding modelrepresentsthissequenceaseitherareal-valuedvectororasequenceofvectors, andobtainsarepresentationofthesequence. Thisrepresentationistypicallyusedas inputtoanothermodel,suchasasentenceclassificationsystem. • Sequence Generation Models. In NLP, sequence generation generally refers to the problemofgeneratingasequenceoftokensbasedonagivencontext. Thetermcontext has different meanings across applications. For example, it refers to the preceding 7.1 Pre-training NLP Models 369 tokensinlanguagemodeling,andreferstothesource-languagesequenceinmachine translation2. We need different techniques for applying these models to downstream tasks after pre- training. Hereweareinterestedinthefollowingtwomethods. 1. Fine-tuning of Pre-trained Models For sequence encoding pre-training, a common method of adapting pre-trained models is fine-tuning. LetEncode (·)denoteanencoderwithparametersθ,forexample,Encode (·) θ θ canbeastandardTransformerencoder. Providedwehavepre-trainedthismodelinsomeway andobtainedtheoptimalparametersθˆ ,wecanemployittomodelanysequenceandgenerate thecorrespondingrepresentation,likethis H = Encode (x) (7.2) θˆ where x is the input sequence {x ,x ,...,x }, and H is the output representation which is 0 1 m asequenceofreal-valuedvectors{h ,h ,...,h }. Becausetheencoderdoesnotworkasa 0 1 m standaloneNLPsystem,itisoftenintegratedasacomponentintoabiggersystem. Consider, for example, a text classification problem in which we identify the polarity (i.e., positive, negative,andneutral)ofagiventext. Wecanbuildatextclassificationsystembystackinga classifierontopoftheencoder. LetClassify (·)beaneuralnetworkwithparametersω. Then, ω thetextclassificationmodelcanbeexpressedintheform Pr (·|x) = Classify (H) ω,θˆ ω = Classify (Encode (x)) (7.3) ω θˆ HerePr (·|x)isaprobabilitydistributionoverthelabelset{positive,negative,neutral}, ω,θˆ andthelabelwiththehighestprobabilityinthisdistributionisselectedasoutput. Tokeepthe notationuncluttered,wewilluseF (·)todenoteClassify (Encode (·)). ω,θˆ ω θˆ Becausethemodelparametersω andθˆ arenotoptimizedfortheclassificationtask, we cannot directly use this model. Instead, we must use a modified version of the model that is adapted to the task. A typical way is to fine-tune the model by giving explicit labeling in downstream tasks. We can train F (·) on a labeled dataset, treating it as a common ω,θˆ supervisedlearningtask. Theoutcomeofthefine-tuningistheparametersω˜ andθ˜ thatare further optimized. Alternatively, we can freeze the encoder parameters θˆ to maintain their pre-trainedstate,andfocussolelyonoptimizingω. Thisallowstheclassifiertobeefficiently adaptedtoworkintandemwiththepre-trainedencoder. Once we have",
    "is to fine-tune the model by giving explicit labeling in downstream tasks. We can train F (·) on a labeled dataset, treating it as a common ω,θˆ supervisedlearningtask. Theoutcomeofthefine-tuningistheparametersω˜ andθ˜ thatare further optimized. Alternatively, we can freeze the encoder parameters θˆ to maintain their pre-trainedstate,andfocussolelyonoptimizingω. Thisallowstheclassifiertobeefficiently adaptedtoworkintandemwiththepre-trainedencoder. Once we have obtained a fine-tuned model, we can use it to classify a new text. For example,supposewehaveacommentpostedonatravelwebsite: Ilovethefoodhere. It’samazing! 2Moreprecisely,inauto-regressivedecodingofmachinetranslation,eachtarget-languagetokenisgenerated basedonbothitsprecedingtokensandsource-languagesequence. 370 Chapter 7. Pre-training Wefirsttokenizethistextintotokens3,andthenfeedthetokensequencex intothefine- new tunedmodelF (·). Themodelgeneratesadistributionoverclassesby ω˜,θ˜ (cid:104) (cid:105) F (x ) = Pr(positive|x ) Pr(negative|x ) Pr(neutral|x ) (7.4) ω˜,θ˜ new new new new Andweselectthelabeloftheentrywiththemaximumvalueasoutput. Inthisexampleitis positive. In general, the amount of labeled data used in fine-tuning is small compared to that of the pre-training data, and so fine-tuning is less computationally expensive. This makes the adaptationofpre-trainedmodelsveryefficientinpractice: givenapre-trainedmodelanda downstream task, we just need to collect some labeled data, and slightly adjust the model parametersonthisdata. Amoredetaileddiscussionoffine-tuningcanbefoundinSection7.4. 2. Prompting of Pre-trained Models Unlikesequenceencodingmodels,sequencegenerationmodelsareoftenemployedindepen- dently to address language generation problems, such as question answering and machine translation,withouttheneedforadditionalmodules. Itisthereforestraightforwardtofine-tune thesemodelsascompletesystemsondownstreamtasks. Forexample,wecanfine-tuneapre- trainedencoder-decodermultilingualmodelonsomebilingualdatatoimproveitsperformance onaspecifictranslationtask. Among various sequence generation models, a notable example is the large language modelstrainedonverylargeamountsofdata. Theselanguagemodelsaretrainedtosimply predictthenexttokengivenitsprecedingtokens. Althoughtokenpredictionissuchasimple taskthatithaslongbeenrestrictedto“languagemodeling”only,ithasbeenfoundtoenable thelearningofthegeneralknowledgeoflanguagesbyrepeatingthetaskalargenumberof times. Theresultisthatthepre-trainedlargelanguagemodelsexhibitremarkablygoodabilities intokenprediction,makingitpossibletotransformnumerousNLPproblemsintosimpletext generationproblemsthroughpromptingthelargelanguagemodels. Forexample,wecanframe theabovetextclassificationproblemasatextgenerationtask Ilovethefoodhere. It’samazing! I’m Here indicates the word or phrase we want to predict (call it the completion). If the predictedwordishappy,orglad,orsatisfied orarelatedpositiveword,wecanclassifythe textaspositive. Thisexampleshowsasimplepromptingmethodinwhichweconcatenate theinputtextwithI’mtoformaprompt. Then, thecompletionhelpsdecidewhichlabelis assignedtotheoriginaltext. Giventhestrongperformanceoflanguageunderstandingandgenerationoflargelanguage models,apromptcaninstructthemodelstoperformmorecomplextasks. Hereisaprompt whereweprompttheLLMtoperformpolarityclassificationwithaninstruction. 3Thetextcanbetokenizedinmanydifferentways. OneofthesimplestistosegmentthetextintoEnglish wordsandpunctuations{I,love,the,food,here,.,It,’s,amazing,!} 7.1 Pre-training NLP Models 371 Assumethatthepolarityofatextisalabelchosenfrom{positive,negative, neutral}. Identifythepolarityoftheinput. Input: Ilovethefoodhere. It’samazing! Polarity: Thefirsttwosentencesareadescriptionofthetask. InputandPolarityareindicatorsofthe input and output, respectively. We expect the model to complete the text and at the same timegivethecorrectpolaritylabel. Byusinginstruction-basedprompts,wecanadaptlarge languagemodelstosolveNLPproblemswithouttheneedforadditionaltraining. Thisexamplealsodemonstratesthezero-shotlearningcapabilityoflargelanguagemodels, whichcanperformtasksthatwerenotobservedduringthetrainingphase. Anothermethodfor enablingnewcapabilitiesinaneuralnetworkisfew-shotlearning. Thisistypicallyachieved throughin-contextlearning(ICT).Morespecifically,weaddsomesamplesthatdemonstrate howaninputcorrespondstoanoutput. Thesesamples,knownasdemonstrations,areused to teach large language models how to perform the task. Below is an example involving demonstrations Assumethatthepolarityofatextisalabelchosenfrom{positive,negative, neutral}. Identifythepolarityoftheinput. Input: Thetrafficisterribleduringrushhours,makingitdifficulttoreachthe airportontime. Polarity: Negative Input: Theweatherhereiswonderful. Polarity: Positive Input: Ilovethefoodhere. It’samazing! Polarity: Promptingandin-contextlearningplayimportantrolesintherecentriseoflargelanguage models. WewilldiscusstheseissuesmoredeeplyinChapter9. However,itisworthnoting thatwhilepromptingisapowerfulwaytoadaptlargelanguagemodels,sometuningefforts are still needed to ensure the models can follow instructions accurately. Additionally, the fine-tuningprocessiscrucialforaligningthevaluesofthesemodelswithhumanvalues. More detaileddiscussionsoffine-tuningcanbefoundinChapter10. 372 Chapter 7. Pre-training 7.2 Self-supervised Pre-training Tasks Inthissection,weconsiderself-supervisedpre-trainingapproachesfordifferentneuralarchi- tectures,includingdecoder-only,encoder-only,andencoder-decoderarchitectures. Werestrict ourdiscussiontoTransformerssincetheyformthebasisofmostpre-trainedmodelsinNLP. However, pre-training is a broad concept, and so we just give a brief introduction to basic approachesinordertomakethissectionconcise. 7.2.1 Decoder-only Pre-training Thedecoder-onlyarchitecturehasbeenwidelyusedindevelopinglanguagemodels[Radford etal.,2018]. Forexample,wecanuseaTransformerdecoderasalanguagemodelbysimply removingcross-attentionsub-layersfromit. Suchamodelpredictsthedistributionoftokensat apositiongivenitsprecedingtokens,andtheoutputisthetokenwiththemaximumprobability. Thestandardwaytotrainthismodel, asinthelanguagemodelingproblem, istominimize alossfunctionoveracollectionoftokensequences. LetDecoder (·)denoteadecoderwith θ parametersθ. Ateachpositioni,thedecodergeneratesadistributionofthenexttokensbased onitsprecedingtokens{x ,...,x },denotedbyPr (·|x ,...,x )(orpθ forshort). Suppose 0 i θ 0 i i+1 wehavethegold-standarddistributionatthesameposition,denotedbypgold . Forlanguage i+1 modeling, we can think of pgold as a one-hot representation of the correct predicted word. i+1 WethendefinealossfunctionL(pθ ,pgold)tomeasurethedifferencebetweenthemodel i+1 i+1 predictionandthetrueprediction. InNLP,thelog-scalecross-entropylossistypicallyused. Givenasequenceofmtokens{x ,...,x },thelossonthissequenceisthesumoftheloss 0 m overthepositions{0,...,m−1},givenby m−1 Loss (x ,...,x ) = (cid:88) L(pθ ,pgold) θ 0 m i+1 i+1 i=0 m−1 = (cid:88) LogCrossEntropy(pθ ,pgold) (7.5) i+1 i+1 i=0 whereLogCrossEntropy(·)isthelog-scalecross-entropy,andpgold istheone-hotrepresenta- i+1 tionofx . i+1 ThislossfunctioncanbeextendedtoasetofsequencesD. Inthiscase,theobjectiveof pre-trainingistofindthebestparametersthatminimizethelossonD (cid:88) θˆ = argmin Loss (x) (7.6) θ θ x∈D Notethatthisobjectiveismathematicallyequivalenttomaximumlikelihoodestimation,and 7.2 Self-supervised Pre-training Tasks 373 canbere-expressedas (cid:88) θˆ = argmax logPr (x) θ θ x∈D i−1 (cid:88)(cid:88) = argmax logPr (x |x ,...,x ) (7.7) θ i+1 0 i θ x∈Di=0 Withtheseoptimizedparametersθˆ ,wecanusethepre-trainedlanguagemodelDecoder (·) θˆ tocomputetheprobabilityPr (x |x ,...,x )ateachpositionofagivensequence. θˆ i+1 0 i 7.2.2 Encoder-only Pre-training As defined in",
    "7.2 Self-supervised Pre-training Tasks 373 canbere-expressedas (cid:88) θˆ = argmax logPr (x) θ θ x∈D i−1 (cid:88)(cid:88) = argmax logPr (x |x ,...,x ) (7.7) θ i+1 0 i θ x∈Di=0 Withtheseoptimizedparametersθˆ ,wecanusethepre-trainedlanguagemodelDecoder (·) θˆ tocomputetheprobabilityPr (x |x ,...,x )ateachpositionofagivensequence. θˆ i+1 0 i 7.2.2 Encoder-only Pre-training As defined in Section 7.1.2, an encoder Encoder (·) is a function that reads a sequence of θ tokensx=x ...x andproducesasequenceofvectorsH=h ...h 4. Trainingthismodel 0 m 0 m is not straightforward, as we do not have gold-standard data for measuring how good the outputofthereal-valuedfunctionis. Atypicalapproachtoencoderpre-trainingistocombine the encoder with some output layers to receive supervision signals that are easier to obtain. Figure7.2showsacommonarchitectureforpre-trainingTransformerencoders,whereweadd aSoftmaxlayerontopoftheTransformerencoder. Clearly,thisarchitectureisthesameasthat ofthedecoder-basedlanguagemodel,andtheoutputisasequenceofprobabilitydistributions  pW,θ 1 .   . .   = Softmax W (Encoder θ (x)) (7.9) pW,θ m Here pW,θ is the output distribution Pr(·|x) at position i. We use Softmax (·) to denote i W that the Softmax layer is parameterized by W, that is, Softmax (H)=Softmax(H·W). W For notation simplicity, we will sometimes drop the superscripts W and θ affixed to each probabilitydistribution. Thedifferencebetweenthismodelandstandardlanguagemodelsisthattheoutputp has i different meanings in encoder pre-training and language modeling. In language modeling, p istheprobabilitydistributionofpredictingthenextword. Thisfollowsanauto-regressive i decodingprocess: alanguagemodelonlyobservesthewordsuptopositioniandpredictsthe next. Bycontrast,inencoderpre-training,theentiresequencecanbeobservedatonce,andso itmakesnosensetopredictanyofthetokensinthissequence. 4Ifweviewh iasarowvector,Hcanbewrittenas h  0 H =  . .  (7.8)  .  hm 374 Chapter 7. Pre-training Self-supervision E.g.,evaluatehowwellthe OutputforDownstreamTasks modelreconstructsthemaskedtoken Softmax PredictionNetwork Encoder Pre-trainedEncoder e e e e e e e e e e 0 1 2 3 4 0 1 2 3 4 x x x x x x x x x x 0 1 2 3 4 0 1 2 3 4 (masked) (a)Pre-training (b)ApplyingthePre-trainedEncoder Figure7.2: Pre-trainingaTransformerencoder(left)andthenapplyingthepre-trainedencoder (right). In the pre-training phase, the encoder, together with a Softmax layer, is trained via self-supervision. Intheapplicationphase,theSoftmaxlayerisremoved,andthepre-trained encoderiscombinedwithapredictionnetworktoaddressspecificproblems. Ingeneral,for betteradaptationtothesetasks,thesystemisfine-tunedusinglabeleddata. 1. Masked Language Modeling One of the most popular methods of encoder pre-training is masked language modeling, which forms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of masked language modeling is to create prediction challenges by masking out some of the tokensintheinputsequenceandtrainingamodeltopredictthemaskedtokens. Inthissense, the conventional language modeling problem, which is sometimes called causal language modeling, is a special case of masked language modeling: at each position, we mask the tokensintheright-context,andpredictthetokenatthispositionusingitsleft-context. However, incausallanguagemodelingweonlymakeuseoftheleft-contextinwordprediction,while thepredictionmaydependontokensintheright-context. Bycontrast,inmaskedlanguage modeling, all the unmasked tokens are used for word prediction, leading to a bidirectional modelthatmakespredictionsbasedonbothleftandright-contexts. More formally, for an input sequence x=x ...x , suppose that we mask the tokens at 0 m positionsA(x)={i ,...,i }. Henceweobtainamaskedtokensequencex¯ wherethetoken 1 u at each position in A(x) is replaced with a special symbol [MASK]. For example, for the followingsequence Theearlybirdcatchestheworm wemayhaveamaskedtokensequencelikethis 7.2 Self-supervised Pre-training Tasks 375 The[MASK]birdcatchesthe[MASK] wherewemaskthetokensearlyandworm(i.e.,i =2andi =6). 1 2 Nowwehavetwosequencesxandx¯. Themodelisthenoptimizedsothatwecancorrectly predictxbasedonx¯. Thiscanbethoughtofasanautoencoding-likeprocess,andthetraining objectiveistomaximizethereconstructionprobabilityPr(x|x¯). Notethatthereisasimple position-wisealignmentbetweenxandx¯. Becauseanunmaskedtokeninx¯ isthesameasthe tokeninxatthesameposition,thereisnoneedtoconsiderthepredictionforthisunmasked token. Thisleadstoasimplifiedtrainingobjectivewhichonlymaximizestheprobabilitiesfor maskedtokens. Wecanexpressthisobjectiveinamaximumlikelihoodestimationfashion (W(cid:99),θˆ) = argmax (cid:88) (cid:88) logPrW i ,θ(x i |x¯) (7.10) W,θ x∈Di∈A(x) oralternativelyexpressitusingthecross-entropyloss (W(cid:99),θˆ) = argmin (cid:88) (cid:88) LogCrossEntropy(pW,θ,pgold) (7.11) i i W,θ x∈Di∈A(x) wherePrW,θ(x |x¯)istheprobabilityofthetruetokenx atpositionk giventhecorrupted k k k inputx¯,andpW,θ istheprobabilitydistributionatpositionk giventhecorruptedinputx¯. To k illustrate,considertheaboveexamplewheretwotokensofthesequence“theearlybirdcatches theworm”aremasked. Forthisexample,theobjectiveistomaximizethesumoflog-scale probabilities Loss = logPr(x =early|¯x=[CLS]The[MASK]birdcatchesthe[MASK])+ 2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x¯2 x¯6 logPr(x",
    "logPrW i ,θ(x i |x¯) (7.10) W,θ x∈Di∈A(x) oralternativelyexpressitusingthecross-entropyloss (W(cid:99),θˆ) = argmin (cid:88) (cid:88) LogCrossEntropy(pW,θ,pgold) (7.11) i i W,θ x∈Di∈A(x) wherePrW,θ(x |x¯)istheprobabilityofthetruetokenx atpositionk giventhecorrupted k k k inputx¯,andpW,θ istheprobabilitydistributionatpositionk giventhecorruptedinputx¯. To k illustrate,considertheaboveexamplewheretwotokensofthesequence“theearlybirdcatches theworm”aremasked. Forthisexample,theobjectiveistomaximizethesumoflog-scale probabilities Loss = logPr(x =early|¯x=[CLS]The[MASK]birdcatchesthe[MASK])+ 2 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x¯2 x¯6 logPr(x =worm|¯x=[CLS]The[MASK]birdcatchesthe[MASK]) (7.12) 6 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) x¯2 x¯6 OnceweobtaintheoptimizedparametersW(cid:99) andθˆ ,wecandropW(cid:99). Then,wecanfurther fine-tunethepre-trainedencoderEncoder (·)ordirectlyapplyittodownstreamtasks. θˆ 2. Permuted Language Modeling Whilemaskedlanguagemodelingissimpleandwidelyapplied,itintroducesnewissues. One drawbackistheuseofaspecialtoken,[MASK],whichisemployedonlyduringtrainingbut notat testtime. Thisleads toa discrepancybetween trainingandinference. Moreover, the auto-encodingprocessoverlooksthedependenciesbetweenmaskedtokens. Forexample,in theaboveexample,thepredictionofx (i.e.,thefirstmaskedtoken)ismadeindependentlyof 2 x (i.e.,thesecondmaskedtoken),thoughx shouldbeconsideredinthecontextofx . 6 6 2 Theseissuescanbeaddressedusingthepermutedlanguagemodelingapproachtopre- training[Yangetal.,2019]. Similartocausallanguagemodeling,permutedlanguagemodeling involves making sequential predictions of tokens. However, unlike causal modeling where 376 Chapter 7. Pre-training predictionsfollowthenaturalsequenceofthetext(likeleft-to-rightorright-to-left),permuted languagemodelingallowsforpredictionsinanyorder. Theapproachisstraightforward: we determine an order for token predictions and then train the model in a standard language modelingmanner,asdescribedinSection7.2.1. Notethatinthisapproach,theactualorder oftokensinthetextremainsunchanged,andonlytheorderinwhichwepredictthesetokens differs from standard language modeling. For example, consider a sequence of 5 tokens x x x x x . Lete representtheembeddingofx (i.e.,combinationofthetokenembedding 0 1 2 3 4 i i andpositionalembedding). Instandardlanguagemodeling,wewouldgeneratethissequence intheorderofx →x →x →x →x . Theprobabilityofthesequencecanbemodeled 0 1 2 3 4 viaagenerationprocess. Pr(x) = Pr(x )·Pr(x |x )·Pr(x |x ,x )·Pr(x |x ,x ,x )· 0 1 0 2 0 1 3 0 1 2 Pr(x |x ,x ,x ,x ) 4 0 1 2 3 = Pr(x )·Pr(x |e )·Pr(x |e ,e )·Pr(x |e ,e ,e )· 0 1 0 2 0 1 3 0 1 2 Pr(x |e ,e ,e ,e ) (7.13) 4 0 1 2 3 Now, let us consider a different order for token prediction: x →x →x →x →x . 0 4 2 1 3 Thesequencegenerationprocesscanthenbeexpressedasfollows: Pr(x) = Pr(x )·Pr(x |e )·Pr(x |e ,e )·Pr(x |e ,e ,e )· 0 4 0 2 0 4 1 0 4 2 Pr(x |e ,e ,e ,e ) (7.14) 3 0 4 2 1 This new prediction order allows for the generation of some tokens to be conditioned on a broader context, rather than being limited to just the preceding tokens as in standard language models. For example, in generating x , the model considers both its left-context 3 (i.e., e ,e ,e ) and right-context (i.e., e ). The embeddings e ,e ,e ,e incorporate the 0 1 2 4 0 1 2 4 positionalinformationofx ,x ,x ,x ,preservingtheoriginalorderofthetokens. Asaresult, 0 1 2 4 this approach is somewhat akin to masked language modeling: we mask out x and use its 3 surroundingtokensx ,x ,x ,x topredictthistoken. 0 1 2 4 The implementation of permuted language models is relatively easy for Transformers. Because the self-attention model is insensitive to the order of inputs, we do not need to explicitlyreorderthesequencetohaveafactorizationlikeEq. (7.14). Instead,permutation canbedonebysettingappropriatemasksforself-attention. Forexample,considerthecaseof computingPr(x |e ,e ,e ). Wecanplacex ,x ,x ,x ,x inorderandblocktheattention 1 0 4 2 0 1 2 3 4 fromx tox inself-attention,asillustratedbelow 3 1 x x x x x 0 1 2 3 4 MasksforSelf-attention: Bluebox=validattention Graybox=blockedattention 7.2 Self-supervised Pre-training Tasks 377 For a more illustrative example, we compare the self-attention masking results of causal languagemodeling,maskedlanguagemodelingandpermutedlanguagemodelinginFigure 7.3. 3. Pre-training Encoders as Classifiers Anothercommonly-usedideatotrainan encoderistoconsiderclassificationtasks. Inself- supervisedlearning,thisistypicallydonebycreatingnewclassificationchallengesfromthe unlabeled text.",
    "4 fromx tox inself-attention,asillustratedbelow 3 1 x x x x x 0 1 2 3 4 MasksforSelf-attention: Bluebox=validattention Graybox=blockedattention 7.2 Self-supervised Pre-training Tasks 377 For a more illustrative example, we compare the self-attention masking results of causal languagemodeling,maskedlanguagemodelingandpermutedlanguagemodelinginFigure 7.3. 3. Pre-training Encoders as Classifiers Anothercommonly-usedideatotrainan encoderistoconsiderclassificationtasks. Inself- supervisedlearning,thisistypicallydonebycreatingnewclassificationchallengesfromthe unlabeled text. There are many different ways to design the classification tasks. Here we presenttwopopulartasks. Asimplemethod,callednextsentenceprediction(NSP),ispresentedinBERT’soriginal paper[Devlinetal.,2019]. TheassumptionofNSPisthatagoodtextencodershouldcapture the relationship between two sentences. To model such a relationship, in NSP we can use the output of encoding two consecutive sentences Sent and Sent to determine whether A B Sent is the next sentence following Sent . For example, suppose Sent =’Itisraining.’ B A A andSent =’Ineedanumbrella.’. Theinputsequenceoftheencodercouldbe B [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] where [CLS] is the start symbol (i.e., x ) which is commonly used in encoder pre-training, 0 and [SEP] is a separator that separates the two sentences. The processing of this sequence follows a standard procedure of Transformer encoding: we first represent each token x as i itscorrespondingembeddinge ,andthenfeedtheembeddingsequence{e ,...,e }intothe i 0 m encodertoobtaintheoutputsequence{h ,...,h }. Sinceh isgenerallyconsideredasthe 0 m 0 representationoftheentiresequence,weaddaSoftmaxlayerontopofittoconstructabinary classificationsystem. Thisprocessisillustratedasfollows token: [CLS] It is raining . [SEP] I need an umbrella . [SEP] embedding: e e e e e e e e e e e e 0 1 2 3 4 5 6 7 8 9 10 11 ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ Encoder ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ encoding: h h h h h h h h h h h h 0 1 2 3 4 5 6 7 8 9 10 11 ↓ Softmax ↓ IsNextorNot? Inordertogeneratetrainingsamples,weneedtwosentenceseachtime,oneforSent and A theotherforSent . Asimplewaytodothisistoutilizethenaturalsequenceoftwoconsecutive B sentencesinthetext. Forexample,weobtainapositivesamplebyusingactualconsecutive 378 Chapter 7. Pre-training x x x x x 0 1 2 3 4 x Pr(x )=1 0 0 x Pr(x |e ) 1 1 0 x Pr(x |e ,e ) 2 2 0 1 x Pr(x |e ,e ,e ) 3 3 0 1 2 x Pr(x |e ,e ,e ,e ) 4 4 0 1 2 3 (a)CausalLanguageModeling(order: x →x →x →x →x ) 0 1 2 3 4 masked masked x x x x x 0 1 2 3 4 x 0 1 x Pr(x |e ,e ,e ,e ,e ) 1 1 0 mask 2 mask 4 masked x 2 1 x Pr(x |e ,e ,e ,e ,e ) 3 3 0 mask 2 mask 4 masked x 4 1 (b)MaskedLanguageModeling(order: x ,[MASK],x ,[MASK],x →x ,x ) 0 2 4 1 3 x x x x x 0 1 2 3 4 x Pr(x )=1 0 0 x Pr(x |e ,e ,e ) 1 1 0 4 2 x Pr(x |e ,e ) 2 2 0 4 x Pr(x |e ,e ,e ,e ) 3 3 0 4 2 1 x Pr(x |e ) 4 4 0 (c)PermutedLanguageModeling(order: x →x →x →x →x ) 0 4 2 1 3 Figure7.3: Comparisonofself-attentionmaskingresultsofcausallanguagemodeling,masked language modeling and permuted language modeling. The gray cell denotes the token at positionj doesnotattendtothetokenatpositioni. Thebluecell(i,j)denotesthatthetoken atpositionj attendstothetokenatpositioni. e representstheembeddingofthesymbol mask [MASK],whichisacombinationofthetokenembeddingandthepositionalembedding.",
    ") 3 3 0 4 2 1 x Pr(x |e ) 4 4 0 (c)PermutedLanguageModeling(order: x →x →x →x →x ) 0 4 2 1 3 Figure7.3: Comparisonofself-attentionmaskingresultsofcausallanguagemodeling,masked language modeling and permuted language modeling. The gray cell denotes the token at positionj doesnotattendtothetokenatpositioni. Thebluecell(i,j)denotesthatthetoken atpositionj attendstothetokenatpositioni. e representstheembeddingofthesymbol mask [MASK],whichisacombinationofthetokenembeddingandthepositionalembedding. 7.2 Self-supervised Pre-training Tasks 379 sentences,andanegativesamplebyusingrandomlysampledsentences. Consequently,training thismodelisthesameastrainingaclassifier. Typically,NSPisusedasanadditionaltraining lossfunctionforpre-trainingbasedonmaskedlanguagemodeling. AsecondexampleoftrainingTransformerencodersasclassifiersistoapplyclassification- basedsupervisionsignalstoeachoutputofanencoder. Forexample,Clarketal.[2019b]in theirELECTRAmodel,proposetrainingaTransformerencodertoidentifywhethereachinput tokenisidenticaltotheoriginalinputorhasbeenalteredinsomemanner. Thefirststepof thismethodistogenerateanewsequencefromagivensequenceoftokens,wheresomeof the tokens are altered. To do this, a small masked language model (call it the generator) is applied: werandomlymasksomeofthetokens,andtrainthismodeltopredictthemasked tokens. Foreachtrainingsample,thismaskedlanguagemodeloutputsatokenateachmasked position,whichmightbedifferentfromtheoriginaltoken. Atthesametime,wetrainanother Transformerencoder(callitthediscriminator)todeterminewhethereachpredictedtokenis thesameastheoriginaltokenoraltered. Morespecifically,weusethegeneratortogeneratea sequencewheresomeofthetokensarereplaced. Belowisanillustration. original: [CLS] The boy spent hours working on toys . ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ masked: [CLS] The boy spent [MASK] working on [MASK] . ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ Generator(smallmaskedlanguagemodel) ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ replaced: [CLS] The boy spent decades working on toys . Then, weusethediscriminatortolabeleachofthesetokensasoriginalorreplaced, as follows replaced: [CLS] The boy spent decades working on toys . ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ Discriminator(themodelwewant) ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ label: original original original original replaced original original original original For training, the generator is optimized as a masked language model with maximum likelihoodestimation,andthediscriminatorisoptimizedasaclassifierusingaclassification- basedloss. InELECTRA,themaximumlikelihood-basedlossandtheclassification-basedloss arecombinedforjointlytrainingboththegeneratoranddiscriminator. Analternativeapproach istousegenerativeadversarialnetworks(GANs),thatis,thegeneratoristrainedtofoolthe discriminator,andthediscriminatoristrainedtodistinguishtheoutputofthegeneratorfrom thetruedistribution. However,GAN-styletrainingcomplicatesthetrainingtaskandismore 380 Chapter 7. Pre-training difficulttoscaleup. Nevertheless,oncetrainingiscomplete,thegeneratorisdiscarded,andthe encodingpartofthediscriminatorisappliedasthepre-trainedmodelfordownstreamtasks. 7.2.3 Encoder-Decoder Pre-training InNLP,encoder-decoderarchitecturesareoftenusedtomodelsequence-to-sequenceproblems, suchasmachinetranslationandquestionanswering. Inadditiontothesetypicalsequence-to- sequenceproblemsinNLP,encoder-decodermodelscanbeextendedtodealwithmanyother problems. A simple idea is to consider text as both the input and output of a problem, and so we can directly apply encoder-decoder models. For example, given a text, we can ask a modeltooutputatextdescribingthesentimentoftheinputtext,suchaspositive,negative,and neutral. Suchanideaallowsustodevelopasingletext-to-textsystemtoaddressanyNLPproblem. We can formulate different problems into the same text-to-text format. We first train an encoder-decodermodeltogaingeneral-purposeknowledgeoflanguageviaself-supervision. Thismodelisthenfine-tunedforspecificdownstreamtasksusingtargetedtext-to-textdata. 1. Masked Encoder-Decoder Pre-training InRaffeletal.[2020]’sT5model,manydifferenttasksareframedasthesametext-to-text task. EachsampleinT5followstheformat SourceText → TargetText Here→separatesthesourcetext,whichconsistsofataskdescriptionorinstructionandthe inputgiventothesystem,fromthetargettext,whichistheresponsetotheinputtask. Asan example, consider a task of translating from Chinese to English. A training sample can be expressedas [CLS]TranslatefromChinesetoEnglish: 你好！ → ⟨s⟩Hello! where[CLS]and⟨s⟩arethestartsymbolsonthesourceandtargetsides,respectively5. 5Wecouldusethesamestartsymbolfordifferentsequences.Hereweusedifferentsymbolstodistinguishthe sequencesontheencoderanddecoder-sides. 7.2 Self-supervised Pre-training Tasks 381 Likewise,wecanexpressothertasksinthesameway. Forexample [CLS] Answer: whenwasAlbertEinsteinborn? → ⟨s⟩HewasbornonMarch14,1879. [CLS] Simplify: theprofessor,whohaspublishednumerouspapersinhisfield, willbegivingalectureonthetopicnextweek. → ⟨s⟩Theexperiencedprofessorwillgivealecturenextweek. [CLS] Text: Johnboughtanewcar. Hypothesis: Johnhasacar. → ⟨s⟩Entailment [CLS] ScorethetranslationfromEnglishtoChinese. English: wheninRome,doas theRomansdo. Chinese: 人在罗马就像罗马人一样做事。 → ⟨s⟩0.81 whereinstructionsarehighlightedingray. Aninterestingcaseisthatinthelastexamplewe reframe the scoring problem as the text generation problem. Our goal is to generate a text representingthenumber0.81,ratherthanoutputtingitasanumericalvalue. The approach described above provides a new framework of universal language under- standing and generation. Both the task instructions and the problem inputs are provided to thesystemintextform. Thesystemthenfollowstheinstructionstocompletethetask. This methodputsdifferentproblemstogether,withthebenefitoftrainingasinglemodelthatcan performmanytaskssimultaneously. Ingeneral,fine-tuningisnecessaryforadaptingthepre-trainedmodeltoaspecificdown- stream task. In this process, one can use different ways to instruct the model for the task, suchasusingashortnameofthetaskastheprefixtotheactualinputsequenceorproviding adetaileddescriptionofthetask. Sincethetaskinstructionsareexpressedintextformand involved as part of the input, the general knowledge of instruction can be gained through learningthelanguageunderstandingmodelsinthepre-trainingphase. Thismayhelpenable zero-shotlearning. Forexample,pre-trainedmodelscangeneralizetoaddressnewproblems wherethetaskinstructionshaveneverbeenencountered. There have been several powerful methods of self-supervised learning for either Trans- formerencodersordecoders. Applyingthesemethodstopre-trainencoder-decodermodelsis relativelystraightforward. Onecommonchoiceistotrainencoder-decodermodelsaslanguage models. Forexample,theencoderreceivesasequenceprefix,whilethedecodergeneratesthe remainingsequence. However,thisdiffersfromstandardcausallanguagemodeling,where theentiresequenceisautoregressivelygeneratedfromthefirsttoken. Inourcase,theencoder processestheprefixatonce,andthenthedecoderpredictssubsequenttokensinthemannerof causallanguagemodeling. Putmoreprecisely,thisisaprefixlanguagemodelingproblem: a 382 Chapter 7. Pre-training languagemodelpredictsthesubsequentsequencegivenaprefix,whichservesasthecontext forprediction. Considerthefollowingexample [CLS]Thepuppiesarefrolicking → ⟨s⟩outsidethehouse. (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Prefix SubsequentSequence Wecandirectlytrainanencoder-decodermodelusingexampleslikethis. Then,theencoder learns to understand the prefix, and the decoder learns to continue writing based on this understanding. For large-scale pre-training, it",
    "models. Forexample,theencoderreceivesasequenceprefix,whilethedecodergeneratesthe remainingsequence. However,thisdiffersfromstandardcausallanguagemodeling,where theentiresequenceisautoregressivelygeneratedfromthefirsttoken. Inourcase,theencoder processestheprefixatonce,andthenthedecoderpredictssubsequenttokensinthemannerof causallanguagemodeling. Putmoreprecisely,thisisaprefixlanguagemodelingproblem: a 382 Chapter 7. Pre-training languagemodelpredictsthesubsequentsequencegivenaprefix,whichservesasthecontext forprediction. Considerthefollowingexample [CLS]Thepuppiesarefrolicking → ⟨s⟩outsidethehouse. (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Prefix SubsequentSequence Wecandirectlytrainanencoder-decodermodelusingexampleslikethis. Then,theencoder learns to understand the prefix, and the decoder learns to continue writing based on this understanding. For large-scale pre-training, it is easy to create a large number of training examplesfromunlabeledtext. It is worth noting that for pre-trained encoder-decoder models to be effective in multi- lingual and cross-lingual tasks, such as machine translation, they should be trained with multi-lingual data. This typically requires that the vocabulary includes tokens from all the languages. Bydoingso,themodelscanlearnsharedrepresentationsacrossdifferentlanguages, therebyenablingcapabilitiesinbothlanguageunderstandingandgenerationinamulti-lingual andcross-lingualcontext. Asecondapproachtopre-trainingencoder-decodermodelsismaskedlanguagemodeling. Inthisapproach,asdiscussedinSection7.2.2,tokensinasequencearerandomlyreplaced withamasksymbol,andthemodelisthentrainedtopredictthesemaskedtokensbasedonthe entiremaskedsequence. Asanillustration,considerthetaskofmaskingandreconstructingthesentence Thepuppiesarefrolickingoutsidethehouse. Bymaskingtwotokens(say,frolickingandthe),wehavetheBERT-styleinputandoutputof themodel,asfollows [CLS]Thepuppiesare[MASK]outside[MASK]house. → ⟨s⟩ frolicking the Here denotesthemaskedpositionatwhichwedonotmaketokenpredictions. Byvarying thepercentageofthetokensinthetext,thisapproachcanbegeneralizedtowardseitherBERT- styletrainingorlanguagemodeling-styletraining[Songetal.,2019]. Forexample,ifwemask outallthetokens,thenthemodelistrainedtogeneratetheentiresequence [CLS][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK] → ⟨s⟩Thepuppiesarefrolickingoutsidethehouse. Inthiscase,wetrainthedecoderasalanguagemodel. Notethat,inthecontextoftheencoder-decoderarchitecture,wecanusetheencoderto read the masked sequence, and use the decoder to predict the original sequence. With this objective,weessentiallyhaveadenoisingautoencoder: theencodertransformsacorrupted 7.2 Self-supervised Pre-training Tasks 383 Loss [M] [M] [M]frolicking[M] the [M] [M] Encoder Decoder [CLS] Thepuppiesare [M] in [M] house . ⟨s⟩ [M] [M] [M]frolicking[M] the [M] (a)Traininganencoder-decodermodelwithBERT-stylemaskedlanguagemodeling Lossoverthesequence The puppies arefrolicking in the house . Encoder Decoder [CLS] Thepuppiesare [M] in [M] house . ⟨s⟩ The puppies arefrolicking in the house (b)Traininganencoder-decodermodelwithdenoisingautoencoding Figure7.4: Traininganencoder-decodermodelusingBERT-styleanddenoisingautoencoding methods. In both methods, the input to the encoder is a corrupted token sequence where sometokensaremaskedandreplacedwith[MASK](or[M]forshort). Thedecoderpredicts thesemaskedtokens,butindifferentways. InBERT-styletraining,thedecoderonlyneeds tocomputethelossforthemaskedtokens, whiletheremainingtokensinthesequencecan be simply treated as [MASK] tokens. In denoising autoencoding, the decoder predicts the sequence of all tokens in an autoregressive manner. As a result, the loss is obtained by accumulatingthelossesofallthesetokens,asinstandardlanguagemodeling. inputintosomehiddenrepresentation,andthedecoderreconstructstheuncorruptedinputfrom thishiddenrepresentation. Hereisanexampleofinputandoutputfordenoisingtraining. [CLS]Thepuppiesare[MASK]outside[MASK]house. → ⟨s⟩Thepuppiesarefrolickingoutsidethehouse. By learning to map from this corrupted sequence to its uncorrupted counterpart, the model gainstheabilitytounderstandontheencodersideandtogenerateonthedecoderside. See Figure7.4foranillustrationofhowanencoder-decodermodelistrainedwithBERT-styleand denoisingautoencodingobjectives. As we randomly select tokens for masking, we can certainly mask consecutive tokens 384 Chapter 7. Pre-training [Joshietal.,2020]. Hereisanexample. [CLS]Thepuppiesare[MASK]outside[MASK][MASK]. → ⟨s⟩Thepuppiesarefrolickingoutsidethehouse. Anotherwaytoconsiderconsecutivemaskedtokensistorepresentthemasspans. Here wefollowRaffeletal.[2020]’swork,anduse[X],[Y]and[Z]todenotesentineltokensthat cover one or more consecutive masked tokens. Using this notation, we can re-express the abovetrainingexampleas [CLS]Thepuppiesare[X]outside[Y]. → ⟨s⟩[X]frolicking[Y]thehouse[Z] Theideaisthatwerepresentthecorruptedsequenceasasequencecontainingplaceholder slots. The training task is to fill these slots with the correct tokens using the surrounding context. Anadvantageofthisapproachisthatthesequencesusedintrainingwouldbeshorter, making the training more efficient. Note that masked language modeling provides a very generalframeworkfortrainingencoder-decodermodels. Varioussettingscanbeadjustedto have different training versions, such as altering the percentage of tokens masked and the maximumlengthofthemaskedspans. 2. Denoising Training Ifweviewtheproblemoftrainingencoder-decodermodelsasaproblemoftrainingdenoising autoencoders,therewilltypicallybemanydifferentmethodsforintroducinginputcorruption andreconstructingtheinput. Forinstance,beyondrandomlymaskingtokens,wecanalsoalter someofthemorrearrangetheirorder. Supposewehaveanencoder-decodermodelthatcanmapaninputsequencextoanoutput sequencey y = Decode (Encode (x)) ω θ = Model (x) (7.15) θ,ω whereθ andω aretheparametersoftheencoderandthedecoder,respectively. Indenoising autoencodingproblems,weaddsomenoisetoxtoobtainanoisy,corruptedinputx . By noise feedingx intotheencoder,wewishthedecodertooutputtheoriginalinput. Thetraining noise objectivecanbedefinedas (θˆ,ωˆ) = argminLoss(Model (x ),x) (7.16) θ,ω noise θ,ω HerethelossfunctionLoss(Model (x ),x)evaluateshowwellthemodelModel (x ) θ,ω noise θ,ω noise reconstructstheoriginalinputx. Wecanchoosethecross-entropylossasusual. Asthemodelarchitectureandthetrainingapproachhavebeendeveloped,theremaining 7.2 Self-supervised Pre-training Tasks 385 issue is the corruption of the input. Lewis et al. [2020a], in their BART model, propose corruptingtheinputsequenceinseveraldifferentways. • TokenMasking. Thisisthesamemaskingmethodthatweusedinmaskedlanguage modeling. Thetokensintheinputsequencearerandomlyselectedandmasked. • Token Deletion. This method is similar to token masking. However, rather than replacingtheselectedtokenswithaspecialsymbol[MASK],thesetokensareremoved fromthesequence. Seethefollowingexampleforacomparisonofthetokenmasking andtokendeletionmethods. Original(x): Thepuppiesarefrolickingoutsidethehouse. TokenMasking(x ): Thepuppiesare[MASK]outside[MASK]house. noise TokenDeletion(x ): Thepuppiesarefrolickingoutsidethehouse. noise wheretheunderlinedtokensintheoriginalsequencearemaskedordeleted. • SpanMasking. Non-overlappingspansarerandomlysampledoverthesequence. Each span is masked by [MASK]. We also consider spans of length 0, and, in such cases, [MASK]issimplyinsertedatapositioninthesequence. Forexample,wecanusespan maskingtocorrupttheabovesequenceas Original(x): The0puppiesarefrolickingoutsidethehouse. SpanMasking(x ): The[MASK]puppiesare[MASK]house. noise Here the span frolicking",
    "rather than replacingtheselectedtokenswithaspecialsymbol[MASK],thesetokensareremoved fromthesequence. Seethefollowingexampleforacomparisonofthetokenmasking andtokendeletionmethods. Original(x): Thepuppiesarefrolickingoutsidethehouse. TokenMasking(x ): Thepuppiesare[MASK]outside[MASK]house. noise TokenDeletion(x ): Thepuppiesarefrolickingoutsidethehouse. noise wheretheunderlinedtokensintheoriginalsequencearemaskedordeleted. • SpanMasking. Non-overlappingspansarerandomlysampledoverthesequence. Each span is masked by [MASK]. We also consider spans of length 0, and, in such cases, [MASK]issimplyinsertedatapositioninthesequence. Forexample,wecanusespan maskingtocorrupttheabovesequenceas Original(x): The0puppiesarefrolickingoutsidethehouse. SpanMasking(x ): The[MASK]puppiesare[MASK]house. noise Here the span frolicking outside the is replaced with a single [MASK]. 0 indicates a length-0span,andsoweinsertan[MASK]betweenTheandpuppies. Spanmasking introduces new prediction challenges in which the model needs to know how many tokensaregeneratedfromaspan. Thisproblemisverysimilartofertilitymodelingin machinetranslation[Brownetal.,1993]. Ifweconsiderasequenceconsistingofmultiplesentences,additionalmethodsofcorrup- tioncanbeapplied. IntheBARTmodel,therearetwosuchmethods. • SentenceReordering. Thismethodrandomlypermutesthesentencessothatthemodel canlearntoreordersentencesinadocument. Consider,forexample,twoconsecutive sentences Hardworkleadstosuccess. Successbringshappiness. Wecanreorderthetwosentencestohaveacorruptedinputsequence Successbringshappiness. Hardworkleadstosuccess. • DocumentRotation. Thegoalofthistaskistoidentifythestarttokenofthesequence. First,atokenisrandomlyselectedfromthesequence. Then,thesequenceisrotatedso thattheselectedtokenisthefirsttoken. Forexample,supposeweselectthetokenleads fromtheabovesequence. Therotatedsequenceis 386 Chapter 7. Pre-training selected Hardwork leadstosuccess. Successbringshappiness. Hardwork wherethesubsequenceHardworkbeforeleadsisappendedtotheendofthesequence. For pre-training, we can apply multiple corruption methods to learn robust models, for example,werandomlychooseoneofthemforeachtrainingsample. Inpractice,theoutcome ofencoder-decoderpre-trainingdependsheavilyontheinputcorruptionmethodsused,andso wetypicallyneedtochooseappropriatetrainingobjectivesthroughcarefulexperimentation. 7.2.4 Comparison of Pre-training Tasks Sofar,wehavediscussedanumberofpre-trainingtasks. Sincethesametrainingobjective canapplytodifferentarchitectures(e.g.,usingmaskedlanguagemodelingforbothencoder- onlyandencoder-decoderpre-training),categorizingpre-trainingtasksbasedsolelyonmodel architecture does not seem ideal. Instead, we summarize these tasks based on the training objectives. • LanguageModeling. Typically,thisapproachreferstoanauto-regressivegeneration procedureofsequences. Atonetime, itpredictsthenexttokenbasedonitsprevious context. • MaskedLanguageModeling. MaskedLanguageModelingbelongstoageneralmask- predictframework. Itrandomlymaskstokensinasequenceandpredictsthesetokens usingtheentiremaskedsequence. • PermutedLanguageModeling. Permutedlanguagemodelingfollowsasimilaridea to masked language modeling, but considers the order of (masked) token prediction. Itreorderstheinputsequenceandpredictsthetokenssequentially. Eachpredictionis basedonsomecontexttokensthatarerandomlyselected. • Discriminative Training. In discriminative training, supervision signals are created from classification tasks. Models for pre-training are integrated into classifiers and trainedtogetherwiththeremainingpartsoftheclassifierstoenhancetheirclassification performance. • Denoising Autoencoding. This approach is applied to the pre-training of encoder- decodermodels. Theinputisacorruptedsequenceandtheencoder-decodermodelsare trainedtoreconstructtheoriginalsequence. Table 7.1 illustrates these methods and their variants using examples. The use of these examplesdoesnotdistinguishbetweenmodels,butwemarkthemodelarchitectureswhere thepre-trainingtaskscanbeapplied. Ineachexample,theinputconsistsofatokensequence, and the output is either a token sequence or some probabilities. For generation tasks, such as language modeling, superscripts are used to indicate the generation order on the target side. If the superscripts are omitted, it indicates that the output sequence can be generated 7.2 Self-supervised Pre-training Tasks 387 Method Enc Dec E-D Input Output CausalLM • • The1kitten2is3chasing4the5ball6.7 PrefixLM • • [C]Thekittenis chasing1the2ball3.4 MaskedLM • • [C]Thekitten[M]chasingthe[M]. is ball MASS-style • • [C]Thekitten[M][M][M]ball. is chasingthe BERT-style • • [C]Thekitten[M]playingthe[M]. kitten is chasing ball PermutedLM • [C]Thekittenischasingtheball. The5kitten7is6chasing1the4ball2.3 NextSentence • [C]Thekittenischasingtheball. Pr(IsNext|representation-of-[C]) Prediction Birdseatworms. Sentence • Encodeasentenceash and Score(h ,h ) a a b Comparison anothersentenceash b TokenClassification • [C]Thekittenischasingtheball. Pr(·|The)Pr(·|kitten)... Pr(·|.) TokenReordering • [C]. kittenthechasingTheisball The1kitten2is3chasing4the5ball6.7 TokenDeletion • [C]Thekittenischasingtheball. The1kitten2is3chasing4the5ball6.7 SpanMasking • [C]Thekitten[M]is[M]. The1kitten2is3chasing4the5ball6.7 SentinelMasking • [C]Thekitten[X]the[Y] [X]1is2chasing3[Y]4ball5.6 Sentence • [C]Theballrollsawayswiftly. The The1kitten2is3chasing4the5ball6.7 Reordering kittenischasingtheball. The8ball9rolls10away11swiftly12.13 Document • [C]chasingtheball. Theballrolls The1kitten2is3chasing4the5ball6.7 Rotation awayswiftly. Thekittenis The8ball9rolls10away11swiftly12.13 Table7.1: Comparisonofpre-trainingtasks,includinglanguagemodeling,maskedlanguage modeling,permutedlanguagemodeling,discriminativetraining,anddenoisingautoencoding. [C]=[CLS],[M]=[MASK],[X],[Y]=sentineltokens. Enc,DecandE-Dindicatewhether theapproachcanbeappliedtoencoder-only,decoder-only,encoder-decodermodels,respec- tively. Forgenerationtasks,superscriptsareusedtorepresenttheorderofthetokens. eitherautoregressivelyorsimultaneously. Onthesourceside,weassumethatthesequence undergoesastandardTransformerencodingprocess,meaningthateachtokencanseetheentire sequenceinself-attention. Theonlyexceptionisinpermutedlanguagemodeling,wherean autoregressivegenerationprocessisimplementedbysettingattentionmasksontheencoder side. Tosimplifythediscussion,weremovethetoken⟨s⟩fromthetarget-sideofeachexample. While these pre-training tasks are different, it is possible to compare them in the same frameworkandexperimentalsetup[Dongetal.,2019;Raffeletal.,2020;Lewisetal.,2020a]. Notethatwecannotlistallthepre-trainingtaskshereastherearemanyofthem. Formore discussionsonpre-trainingtasks,theinterestedreadermayrefertosomesurveysonthistopic [Qiuetal.,2020b;Hanetal.,2021a]. 388 Chapter 7. Pre-training 7.3 Example: BERT Inthissection, weintroduceBERTmodels, whichareamongthemostpopularandwidely usedpre-trainedsequenceencodingmodelsinNLP. 7.3.1 The Standard Model ThestandardBERTmodel,whichisproposedinDevlinetal.[2019]’swork,isaTransformer encodertrainedusingbothmaskedlanguagemodelingandnextsentencepredictiontasks. The lossusedintrainingthismodelisasumofthelossofthetwotasks. Loss = Loss +Loss (7.17) BERT MLM NSP Asisregularintrainingdeepneuralnetworks,weoptimizethemodelparametersbyminimizing this loss. To do this, a number of training samples are collected. During training, a batch of training samples is randomly selected from this collection at a time, and Loss is BERT accumulatedoverthesetrainingsamples. Then,themodelparametersareupdatedviagradient descentoritsvariants. Thisprocessisrepeatedmanytimesuntilsomestoppingcriterionis satisfied,suchaswhenthetraininglossconverges. 1. Loss Functions Ingeneral,BERTmodelsareusedtorepresentasinglesentenceorapairofsentences,and thus can handle various downstream language understanding problems. In this section we assumethattheinputrepresentationisasequencecontainingtwosentencesSent andSent , A B expressedas [CLS] Sent [SEP] Sent",
    "training, a batch of training samples is randomly selected from this collection at a time, and Loss is BERT accumulatedoverthesetrainingsamples. Then,themodelparametersareupdatedviagradient descentoritsvariants. Thisprocessisrepeatedmanytimesuntilsomestoppingcriterionis satisfied,suchaswhenthetraininglossconverges. 1. Loss Functions Ingeneral,BERTmodelsareusedtorepresentasinglesentenceorapairofsentences,and thus can handle various downstream language understanding problems. In this section we assumethattheinputrepresentationisasequencecontainingtwosentencesSent andSent , A B expressedas [CLS] Sent [SEP] Sent [SEP] A B HerewefollowthenotationinBERT’spaperanduse[SEP]todenotetheseparator. Given this sequence, we can obtain Loss and Loss separately. For masked lan- MLM NSP guage modeling, we predict a subset of the tokens in the sequence. Typically, a certain percentage of the tokens are randomly selected, for example, in the standard BERT model, 15%ofthetokensineachsequenceareselected. Thenthesequenceismodifiedinthreeways • TokenMasking. 80%oftheselectedtokensaremaskedandreplacedwiththesymbol [MASK]. Forexample Original: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] Masked: [CLS]Itis[MASK]. [SEP]Ineed[MASK]umbrella. [SEP] wheretheselectedtokensareunderlined. Predictingmaskedtokensmakesthemodel learntorepresenttokensfromtheirsurroundingcontext. • RandomReplacement. 10%oftheselectedtokensarechangedtoarandomtoken. For 7.3 Example: BERT 389 example Original: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] RandomToken: [CLS]Itisraining. [SEP]Ineedanhat. [SEP] Thishelpsthemodellearntorecoveratokenfromanoisyinput. • Unchanged. 10%oftheselectedtokensarekeptunchanged. Forexample, Original: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] UnchangedToken: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] Thisisnotadifficultpredictiontask,butcanguidethemodeltouseeasierevidencefor prediction. LetA(x)bethesetofselectedpositionsofagiventokensequencex,andx¯bethemodified sequenceofx. Thelossfunctionofmaskedlanguagemodelingcanbedefinedas (cid:88) Loss = − logPr (x |x¯) (7.18) MLM i i i∈A(x) wherePr (x |x¯)istheprobabilityofpredictingx atthepositionigivenx¯. Figure7.5shows i i i arunningexampleofcomputingLoss . MLM For next sentence prediction, we follow the method described in Section 7.2.2. Each trainingsampleisclassifiedintoalabelset{IsNext,NotNext},forexample, Sequence: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP] Label: IsNext Sequence: [CLS]Thecatsleepsonthewindowsill. [SEP]Applesgrowontrees. [SEP] Label: NotNext The output vector of the encoder for the first token [CLS] is viewed as the sequence repre- sentation,denotedbyh (orh ). Aclassifierisbuiltontopofh . Then,wecancompute cls 0 cls theprobabilityofalabelcgivenh ,i.e.,Pr(c|h ). Therearemanylossfunctionsonecan cls cls choose for classification problems. For example, in maximum likelihood training, we can defineLoss as NSP Loss = −logPr(c |h ) (7.19) NSP gold cls wherec isthecorrectlabelforthissample. gold 390 Chapter 7. Pre-training Input: [CLS] It is raining . [SEP] I need an umbrella . [SEP] Selecttokenswithaprobabilityof15% TokenSelection: [CLS] It is raining . [SEP] I need an umbrella . [SEP] Maskselectedtokenswithaprobabilityof80% TokenMasking: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP] Alterselectedtokenswithaprobabilityof10% Token: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP] Replacement Keepselectedtokensunchangedwithaprobabilityof10% Unchanged: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP] TraintheTransformerencoderwiththemodifiedsequence training I an umbrella h h h h h h h h h h h h 0 1 2 3 4 5 6 7 8 9 10 11 Transformer Encoder e e e e e e e e e e e e 0 1 2 3 4 5 6 7 8 9 10 11 [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP] Figure 7.5: A running example of BERT-style masked language modeling. First, 15% of the tokens are randomly selected. These selected tokens are then processed in one of three ways: replacedwitha[MASK]token(80%ofthetime),replacedwitharandomtoken(10% of the time), or kept unchanged (10% of the time). The model is trained to predict these selectedtokensbasedonthemodifiedsequence. e representstheembeddingofthetokenat i thepositioni. GrayboxesrepresenttheSoftmaxlayers. 2. Model Setup AsshowninFigure7.6,BERTmodelsarebasedonthestandardTransformerencoderarchi- tecture. Theinputisasequenceofembeddings,eachbeingthesumofthetokenembedding, 7.3 Example: BERT 391 thepositionalembedding,andthesegmentembedding. e = x+e +e (7.20) pos seg Boththetokenembedding(x)andpositionalembedding(e )areregular,asinTransformer pos models. Thesegmentembedding(e )isanewtypeofembeddingthatindicateswhethera seg tokenbelongstoSent orSent . Thiscanbeillustratedbythefollowingexample. A B Token [CLS] It is raining . [SEP] I need an umbrella . [SEP] x x x x x x x x x x x x x 0 1",
    "391 thepositionalembedding,andthesegmentembedding. e = x+e +e (7.20) pos seg Boththetokenembedding(x)andpositionalembedding(e )areregular,asinTransformer pos models. Thesegmentembedding(e )isanewtypeofembeddingthatindicateswhethera seg tokenbelongstoSent orSent . Thiscanbeillustratedbythefollowingexample. A B Token [CLS] It is raining . [SEP] I need an umbrella . [SEP] x x x x x x x x x x x x x 0 1 2 3 4 5 6 7 8 9 10 11 e PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) PE(10) PE(11) pos e e e e e e e e e e e e e seg A A A A A A B B B B B B ThemainpartofBERTmodelsisamulti-layerTransformernetwork. ATransformerlayer consistsofaself-attentionsub-layerandanFFNsub-layer. Bothofthemfollowthepost-norm architecture: output=LNorm(F(input)+input), where F(·) is the core function of the sub-layer(eitheraself-attentionmodeloranFFN),andLNorm(·)isthelayernormalization unit. Typically,anumberofTransformerlayersarestackedtoformadeepnetwork. Ateach positionofthesequence,theoutputrepresentationisareal-valuedvectorwhichisproduced bythelastlayerofthenetwork. ThereareseveralaspectsonemayconsiderindevelopingBERTmodels. • VocabularySize(|V|). InTransformers,eachinputtokenisrepresentedasanentryina vocabularyV. Largevocabulariescancovermoresurfaceformvariantsofwords,but mayleadtoincreasedstoragerequirements. • EmbeddingSize(d ). Everytokenisrepresentedasad -dimensionalreal-valuedvector. e e Aspresentedabove,thisvectoristhesumofthetokenembedding,positionalembedding, andsegmentembedding,allofwhicharealsod -dimensionalreal-valuedvectors. e • Hidden Size (d). The input and output of a sub-layer are of d dimensions. Besides, mostofthehiddenstatesofasub-layerared-dimensionalvectors. Ingeneral,dcanbe roughlyviewedasthewidthofthenetwork. • NumberofHeads(n ). Inself-attentionsub-layers,oneneedstospecifythenumber head ofheadsusedinmulti-headself-attention. Thelargerthisnumberis,themoresub-spaces inwhichattentionisperformed. Inpracticalsystems,weoftensetn ≥4. head • FFNHiddenSize(d ). ThesizeofthehiddenlayeroftheFFNsusedinTransform- ffn ers is typically larger than d. For example, a typical setting is d =4d. For larger ffn Transformers,suchasrecentlargemodels,d maybesettoaverylargevalue. ffn • ModelDepth(L). Usingdeepnetworksisaneffectivewaytoimprovetheexpressive power of Transformers. For BERT models, L is typically set to 12 or 24. However, networks with even greater depth are also feasible and can be applied for further en- hancements. 392 Chapter 7. Pre-training ... OutputLayer EncoderOutput h ∈Rd isthecontextual i h h ...h 0 1 m representationofx i LayerNormalization FFNSub-layer hiddensize: d FFNhiddensize: d ffn FFN layers LayerNormalization Self-attentionSub-layer hiddensize: d numberofheads: n head Self-attention Embedding e=x+e pos +e seg ∈Rde Token Position Segment Input x correspondstoanentryofV i x x ...x 0 1 m Figure7.6: ThemodelarchitectureofBERT(Transformerencoder). Theinputtokensarefirst representedasembeddings,eachofwhichisthesumofthecorrespondingtokenembedding, positionalembeddingandsegmentembedding. Then,theembeddingsequenceisprocessedby astackofTransformerlayers. Eachlayerinthisstackincludesaself-attentionsub-layeranda FFNsub-layer. TheoutputoftheBERTmodelisasequenceofvectorsproducedbythefinal Transformerlayer. Differentsettingsofthesehyper-parametersleadtodifferentmodelsizes. Therearetwo widely-usedBERTmodels. • BERT : d=768,L=12,n =12,totalnumberofparameters=110M. base head • BERT : d=1,024,L=24,n =16,totalnumberofparameters=340M. large head TrainingBERTmodelsfollowsthestandardtrainingprocessofTransformers. Training larger models such as BERT requires more training effort and time. This is a common large problemforpre-training,especiallywhenamodelistrainedonaverylargeamountofdata. Inpractice,thereareoftenconsiderationsoftrainingefficiency. Forexample,apracticeisto firsttrainaBERTmodelonrelativelyshortsequencesforalargenumberoftrainingsteps,and 7.3 Example: BERT 393 thencontinuetrainingitonfull-lengthsequencesfortheremainingtrainingsteps. 7.3.2 More Training and Larger Models BERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One direction is to scale up the model itself, including increasing training data and developing largermodels. RoBERTa,anextensionofthestandardBERTmodel,isanexampleofsuchefforts[Liu etal.,2019]. Itintroducestwomajorimprovements. First,simplyusingmoretrainingdataand morecomputecanimproveBERTmodelswithoutneedofchangingthemodelarchitectures. Second,removingtheNSPlossdoesnotdecreasetheperformanceondownstreamtasksifthe trainingisscaledup. Thesefindingssuggestexploringageneraldirectionofpre-training: we cancontinuetoimprovepre-trainingbyscalingituponsimplepre-trainingtasks. A second approach to improving BERT models is to increase the number of model parameters. Forexample,inHeetal.[2021]’swork,a1.5billion-parameterBERT-likemodel isbuiltbyincreasingboththemodeldepthandhiddensize. However,scalingupBERTand variousotherpre-trainedmodelsintroducesnewchallengesintraining,forexample,training verylargemodelsoftenbecomesunstableanddifficulttoconverge. Thismakestheproblem more complicated, and requires careful consideration of various aspects, including model architecture, parallel computation, parameter initialization, and so on. In another example, Shoeybi et al. [2019] successfully trained a 3.9 billion-parameter BERT-like model, where hundredsofGPUswereusedtomanagetheincreasedcomputationaldemands. 7.3.3 More Efficient Models Comparedtoitspredecessors,BERTisarelativelylargemodelforthetimeitwasproposed. Thisincreaseinmodelsizeresultsinlargermemoryrequirementsandaconsequentslowdown in system performance. Developing smaller and faster BERT models is part of the broader challengeofbuildingefficientTransformers,whichhasbeenextensivelydiscussedinChapter 6. However, a deeper discussion of this general topic is beyond the scope of our current discussion. HereweinsteadconsiderafewefficientvariantsofBERT. Several threads of research are of interest to NLP researchers in developing efficient BERT models. First, work on knowledge distillation, such as training student models with theoutputofwell-trainedteachermodels,showsthatsmallerBERTmodelscanbeobtained by transferring knowledge from larger BERT models.",
    "a deeper discussion of this general topic is beyond the scope of our current discussion. HereweinsteadconsiderafewefficientvariantsofBERT. Several threads of research are of interest to NLP researchers in developing efficient BERT models. First, work on knowledge distillation, such as training student models with theoutputofwell-trainedteachermodels,showsthatsmallerBERTmodelscanbeobtained by transferring knowledge from larger BERT models. Given that BERT models are multi- layernetworkswithseveraldifferenttypesoflayers,knowledgedistillationcanbeappliedat differentlevelsofrepresentation. Forexample,beyonddistillingknowledgefromtheoutput layers,itisalsopossibletoincorporatetraininglossthatmeasuresthedifferenceinoutputof hiddenlayersbetweenteachermodelsandstudentmodels[Sunetal.,2020b;Jiaoetal.,2020]. Indeed,knowledgedistillationhasbeenoneofthemostwidely-usedtechniquesforlearning smallpre-trainedmodels. Second, conventional model compression methods can be directly applied to compress BERTmodels. Onecommonapproachistousegeneral-purposepruningmethodstoprunethe Transformerencodingnetworks[Galeetal.,2019]. Thisgenerallyinvolvesremovingentire 394 Chapter 7. Pre-training layers[Fanetal.,2019]oracertainpercentageofparametersinthenetworks[Sanhetal.,2020; Chenetal.,2020b]. Pruningisalsoapplicabletomulti-headattentionmodels. Forexample, Michel et al. [2019] show that removing some of the heads does not significantly decrease the performance of BERT models, but speeds up the inference of these models. Another approachtocompressingBERTmodelsisquantization[Shenetal.,2020b]. Byrepresenting modelparametersaslow-precisionnumbers,themodelscanbegreatlycompressed. While thismethodisnotspecifictoBERTmodels,itproveseffectiveforlargeTransformer-based architectures. Third, considering that BERT models are relatively deep and large networks, another threadofresearchusesdynamicnetworkstoadaptthesemodelsforefficientinference. An ideainthisparadigmistodynamicallychoosethelayersforprocessingatoken,forexample, indepth-adaptivemodelsweexitatsomeoptimaldepthandthusskiptherestofthelayersin thelayerstack[Xinetal.,2020;Zhouetal.,2020]. Similarly,wecandeveloplength-adaptive modelsinwhichthelengthoftheinputsequenceisdynamicallyadjusted. Forexample,we canskipsomeofthetokensintheinputsequencesothatthemodelcanreducecomputational loadonlessimportanttokens,enhancingoverallefficiency. Fourth, it is also possible to share parameters across layers to reduce the size of BERT models. AsimplewaytodothisistosharetheparametersofawholeTransformerlayeracross thelayerstack[Dehghanietal.,2018;Lanetal.,2020]. Inadditiontothereducednumberof parameters,thisenablesreuseofthesamelayerinamulti-layerTransformernetwork,leading tosavingsofmemoryfootprintattesttime. 7.3.4 Multi-lingual Models TheinitialBERTmodelwasprimarilyfocusedonEnglish. Soonafterthismodelwasproposed, itwasextendedtomanylanguages. Onesimplewaytodothisistodevelopaseparatemodel foreachlanguage. Anotherapproach,whichhasbecomemorepopularinrecentworkonlarge languagemodels,istotrainmulti-lingualmodelsdirectlyondatafromallthelanguages. In response,multi-lingualBERT(mBERT)modelsweredevelopedbytrainingthemontext from104languages6. TheprimarydifferencefrommonolingualBERTmodelsisthatmBERT models use larger vocabularies to cover tokens from multiple languages. As a result, the representationsoftokensfromdifferentlanguagesaremappedintothesamespace,allowing forthesharingofknowledgeacrosslanguagesviathisuniversalrepresentationmodel. Oneimportantapplicationofmulti-lingualpre-trainedmodelsiscross-linguallearning. In the cross-lingual setting, we learn a model on tasks in one language, and apply it to the sametasksinanotherlanguage. Incross-lingualtextclassification,forexample,wefine-tunea multi-lingualpre-trainedmodelonEnglishannotateddocuments. Then,weusethefine-tuned modeltoclassifyChinesedocuments. Animprovementtomulti-lingualpre-trainedmodelslikemBERTistointroducebilingual dataintopre-training. Ratherthantrainingsolelyonmonolingualdatafrommultiplelanguages, bilingual training explicitly models the relationship between tokens in two languages. The 6https://github.com/google-research/bert/ 7.3 Example: BERT 395 resultingmodelwillhaveinnatecross-lingualtransferabilities,andthuscanbeeasilyadapted to different languages. Lample and Conneau [2019] propose an approach to pre-training cross-linguallanguagemodels(XLMs). Intheirwork,across-linguallanguagemodelcan betrainedineitherthecausallanguagemodelingormaskedlanguagemodelingmanner. For masked language modeling pre-training, the model is treated as an encoder. The training objective is the same as BERT: we maximize the probabilities of some randomly selected tokenswhichareeithermasked,replacedwithrandomtokens,orkeptunchangedintheinput. Ifweconsiderbilingualdatainpre-training,wesampleapairofalignedsentenceseachtime. Then,thetwosentencesarepackedtogethertoformasinglesequenceusedfortraining. For example,consideranEnglish-Chinesesentencepair 鲸鱼 是 哺乳 动物 。 ↔ Whalesaremammals . Wecanpackthemtoobtainasequence,likethis [CLS]鲸鱼 是 哺乳 动物 。 [SEP] Whalesaremammals . [SEP] Wethenselectacertainpercentageofthetokensandreplacethemwith[MASK]. [CLS][MASK] 是 [MASK] 动物 。 [SEP] Whales[MASK][MASK] . [SEP] Thegoalofpre-trainingistomaximizetheproductoftheprobabilitiesofthemaskedtokens giventheabovesequence. Byperformingtraininginthisway,themodelcanlearntorepresent boththeEnglishandChinesesequences,aswellastocapturethecorrespondencesbetween tokensinthetwolanguages. Forexample,predictingtheChinesetoken 鲸鱼mayrequirethe informationfromtheEnglishtokenWhales. Aligningtherepresentationsofthetwolanguages essentiallytransformsthemodelintoa“translation”model. Sothistrainingobjectiveisalso calledtranslationlanguagemodeling. Figure7.7showsanillustrationofthisapproach. Abenefitofmulti-lingualpre-trainedmodelsistheirinherentcapabilityofhandlingcode- switching. InNLPandlinguistics,code-switchingreferstoswitchingamonglanguagesina text. Forexample,thefollowingisamixedlanguagetextcontainingbothChineseandEnglish: 周末 我们 打算 去 做 hiking ， 你 想 一起 来 吗 ？ (Weplantogohikingthisweekend,wouldyouliketojoinus?) Formulti-lingualpre-trainedmodels,wedonotneedtoidentifywhetheratokenisChineseor English. Instead,everytokenisjustanentryofthesharedvocabulary. Thiscanbeimagined ascreatinga“new”languagethatencompassesallthelanguageswewanttoprocess. The result of multi-lingual pre-training is influenced by several factors. Given that the modelarchitectureisfixed,oneneedstospecifythesizeofthesharedvocabulary,thenumber (orpercentage)ofsamplesineachlanguage,thesizeofthemodel,andsoon. Conneauetal. 396 Chapter 7. Pre-training 鲸鱼 哺乳 are mammals h h h h h h h h h h h h 0 1 2 3 4 5 6 7 8 9 10 11 Transformer Encoder e e e e e e e e e e e e 0 1 2 3 4 5 6 7 8 9 10 11 [CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales[MASK][MASK] . [SEP] (zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en) Figure7.7: Anillustrationoftranslationlanguagemodeling. Foreaseofunderstanding,we presentasimpleexamplewherealltheselectedtokensaremasked. Themodelistrainedto predictthesemaskedtokens. Asthesequencecontainstokensintwolanguages,predicting a token in one language allows access to tokens in the other language, thereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., e ) is the sum of the token embedding, positional embedding, and language embedding. i This requires that each token is assigned with a language label. Thus we can",
    "access to tokens in the other language, thereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., e ) is the sum of the token embedding, positional embedding, and language embedding. i This requires that each token is assigned with a language label. Thus we can distinguish tokensindifferentlanguages. Inmulti-lingualpre-training,particularlyinworkusingshared vocabularies, specifying the language to which a token belongs is not necessary. The use of language embeddings in turn makes it difficult to handle code-switching. Therefore, we assumeherethatalltokenrepresentationsarelanguage-independent. [2020]pointoutseveralinterestingissuesregardinglarge-scalemulti-lingualpre-trainingfor XLM-likemodels. First,asthenumberofsupportedlanguagesincreases,alargermodelis neededtohandletheselanguages. Second,alargersharedvocabularyishelpfulformodeling theincreaseddiversityinlanguages. Third,low-resourcelanguagesmoreeasilybenefitfrom cross-lingualtransferfromhigh-resourcelanguages,particularlywhensimilarhigh-resource languages are involved in pre-training. However, interference may occur if the model is trainedforanextendedperiod,meaningtheoverallperformanceofthepre-trainedmodelstarts decreasingatacertainpointduringpre-training. Thus,inpracticalsystems,onemayneedto stopthepre-trainingearlytopreventinterference. 7.4 Applying BERT Models Once a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models are not immediately ready for performing specific downstream tasks. In general, additionalfine-tuningworkisrequiredtomakethemadapt. Asafirststep,weneedapredictor toaligntheoutputofthemodelwiththeproblemofinterest. LetBERT (·)beaBERTmodel θˆ withpre-trainedparametersθˆ ,andPredict (·)beapredictionnetworkwithparametersω. By ω 7.4 Applying BERT Models 397 integratingthepredictionnetworkwiththeoutputoftheBERTmodel,wedevelopamodelto tacklethedownstreamtasks. Thismodelcanbeexpressedas y = Predict (BERT (x)) (7.21) ω θˆ wherexistheinputandy istheoutputthatfitstheproblem. Forexample,inclassification problems,themodeloutputsaprobabilitydistributionoverlabels. Then,wecollectasetoflabeledsamplesD,andfine-tunethemodelby (cid:88) (ω˜,θ˜) = argmin Loss(y ,y ) (7.22) ω,θˆ+ gold ω,θˆ+ (x,y )∈D gold where(x,y )representsatupleofaninputanditscorrespondingoutput. Thenotationof gold thisequationseemsabitcomplicated,butthetraining/tuningprocessisstandard. Weoptimize the model by minimizing the loss over the tuning samples. The outcome is the optimized parametersω˜ andθ˜ . Theoptimizationstartswiththepre-trainedparametersθˆ . Hereweuse θˆ+ toindicatethattheparametersareinitializedwithθˆ , andusey todenotethemodel ω,θˆ+ outputcomputedusingtheparametersω andθˆ+. Withthefine-tunedparametersω˜ andθ˜ ,wecanapplythemodelPredict (BERT (·))to ω˜ θ˜ newdataofthesametasksforwhichthemodelwasfine-tuned. Theformofthedownstream tasksdeterminestheinputandoutputformatsofthemodel,aswellasthearchitectureofthe predictionnetwork. InthefollowingwelistsometaskstowhichBERTmodelsaregenerally suited. • Classification(SingleText). Oneofthemostwidely-usedapplicationsofBERTmodels is text classification. In this task, a BERT model receives a sequence of tokens and encodesitasasequenceofvectors. Thefirstoutputvectorh (orh )istypicallyused cls 0 as the representation of the entire text. The prediction network takes h as input to cls produceadistributionoflabels. Let[CLS]x x ...x beaninputtext. Seebelowforan 1 2 m illustrationofBERT-basedtextclassification. Class h cls h 1 h 2 ... hm h m+1 BERT e cls e 1 e 2 ... em e m+1 [CLS] x 1 x 2 ... xm [SEP] Here the gray box denotes the prediction network. Many NLP problems can be cat- egorized as text classification tasks, and there have been several text classification benchmarksforevaluatingpre-trainedmodels. Forexample,wecanclassifytextsby theirgrammaticalcorrectness(grammaticality)oremotionaltone(sentiment)[Socher 398 Chapter 7. Pre-training et al., 2013; Warstadt et al., 2019]. Note that the prediction network could be any classificationmodel,suchasadeepneuralnetworkoramoretraditionalclassification model. Theentiremodelcanthenbetrainedorfine-tunedinthemannerofastandard classification model. For example, the prediction network can be simply a Softmax layerandthemodelparameterscanbeoptimizedbymaximizingtheprobabilitiesofthe correctlabels. • Classification(PairofTexts). Classificationcanalsobeperformedonapairoftexts. Supposewehavetwotexts,x ...x andy ...y . Wecanconcatenatethesetextstoform 1 m 1 n a single sequence with a length len. Then, we predict a label for this combined text sequencebasedontheh vector,asfollows cls Class h cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len BERT e cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len [CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP] Text1 Text2 wherelen=n+m+2. Textpairclassificationcoversseveralproblems,includingse- manticequivalencejudgement(determinewhethertwotextsaresemanticallyequivalent) [DolanandBrockett,2005],textentailmentjudgement(determinewhetherahypothesis canbelogicallyinferredorentailedfromapremise)[BentivogliandGiampiccolo,2011; Williamsetal.,2018],groundedcommonsenseinference(determinewhetheranevent is likely to happen given its context) [Zellers et al., 2018], and question-answering inference(determinewhetherananswercorrespondstoagivenquestion). • Regression. Instead of generating a label distribution, we can have the prediction network output a real-valued score. For example, by adding a Sigmoid layer",
    "... yn [SEP] Text1 Text2 wherelen=n+m+2. Textpairclassificationcoversseveralproblems,includingse- manticequivalencejudgement(determinewhethertwotextsaresemanticallyequivalent) [DolanandBrockett,2005],textentailmentjudgement(determinewhetherahypothesis canbelogicallyinferredorentailedfromapremise)[BentivogliandGiampiccolo,2011; Williamsetal.,2018],groundedcommonsenseinference(determinewhetheranevent is likely to happen given its context) [Zellers et al., 2018], and question-answering inference(determinewhetherananswercorrespondstoagivenquestion). • Regression. Instead of generating a label distribution, we can have the prediction network output a real-valued score. For example, by adding a Sigmoid layer to the prediction network, the system can be employed to compute the similarity between twogivensentences. ThearchitectureisthesameasthatofBERT-basedclassification systems,withonlythechangeoftheoutputlayer. 7.4 Applying BERT Models 399 Number(similarity,evaluationscore,etc.) h cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len BERT e cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len [CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP] Text1 Text2 Fortrainingorfine-tuning,wecanminimizetheregressionlossofthemodeloutputas usual. • SequenceLabeling. Sequencelabelingisamachinelearningapproachapplicabletoa widerangeofNLPproblems. Thisapproachassignsalabeltoeachtokeninaninput sequence, and some linguistic annotations can then be derived from this sequence of labels. AnexampleofsequencelabelinginNLPispart-of-speech(POS)tagging. We label each word in a sentence with its corresponding POS tag. Another example is named entity recognition (NER) in which we label each word with an NER tag, and namedentitiesareidentifiedusingthesetags. Seebelowforanillustrationofthemodel architectureforNER. Tag Tag Tag {B,I,O}{B,I,O} {B,I,O} h cls h 1 h 2 ... hm h m+1 BERT e cls e 1 e 2 ... em e m+1 [CLS] x 1 x 2 ... xm [SEP] Here{B,I,O}isthetagsetofNER.Forexample,B-ORGmeansthebeginningofan organization,I-ORGmeansthewordisinsideanorganization,andOmeanstheword doesnotbelongtoanynamedentity. ThisNERmodelcanoutputadistributionover thetagsetateachposition,denotedasp . Thetrainingorfine-tuningofthemodelcan i beperformedoverthesedistributions{p ,...,p }. Forexample, supposep (tag )is 1 m i i theprobabilityofthecorrecttagatpositioni. Thetraininglosscanbedefinedtobethe negativelikelihood m 1 (cid:88) Loss = − logp (tag ) (7.23) m i i i=1 FindingthebestlabelsequencegivenatrainedNERmodelisawell-studiedissuein 400 Chapter 7. Pre-training NLP.Thisisoftenachievedviadynamicprogramming,which,inthecontextofpath findingoveralattice,haslinearcomplexity[Huang,2009]. • Span Prediction. Some NLP tasks require predicting a span in a text. A common example is reading comprehension. In this task, we are given a query x ...x and a 1 m contexttexty ...y . Thegoalistoidentifyacontinuousspaniny ...y thatbestanswers 1 n 1 n the query. This problem can be framed as a sequence labeling-like task in which we predictalabelforeachy toindicatethebeginningorendingofthespan. FollowingSeo j etal.[2017],weaddtwonetworksontopoftheBERToutputfory : oneforgenerating j the probability of y being the beginning of the span (denoted by pbeg ), and one for j j generating the probability of y being the ending of the span (denoted by pend). The j j resultingmodelarchitectureisshownasfollows End End End (pe 1 nd) (pe 2 nd) (pe n nd) Beg Beg Beg (pb 1 eg) (pb 2 eg) (pb n eg) h cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len BERT e cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len [CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP] Query ContextText Wepackthequeryandcontexttexttogethertoobtaintheinputsequence. Theprediction networksareonlyappliedtooutputsforthecontexttext,generatingtheprobabilities pbeg andpend ateachposition. Thelosscanbecomputedbysummingthenegativelog j j likelihoodsofthetwomodelsacrosstheentirecontexttext. n Loss = − 1(cid:88)(cid:0) logpbeg+logpend(cid:1) (7.24) n j j j=1 Attesttime,wesearchforthebestspanby (ˆj ,ˆj ) = argmax (cid:0) logpbeg+logpend(cid:1) (7.25) 1 2 j1 j2 1≤j1≤j2≤n • Encoding for Encoder-Decoder Models. While our focus in this section has been primarily on language understanding problems, it is worth noting that BERT models can be applied to a broader range of",
    "j j j=1 Attesttime,wesearchforthebestspanby (ˆj ,ˆj ) = argmax (cid:0) logpbeg+logpend(cid:1) (7.25) 1 2 j1 j2 1≤j1≤j2≤n • Encoding for Encoder-Decoder Models. While our focus in this section has been primarily on language understanding problems, it is worth noting that BERT models can be applied to a broader range of NLP tasks. In fact, BERT models can be used inallthescenarioswhereweneedtoencodeapieceoftext. Oneapplicationthatwe havenotmentionedistextgenerationwhichincludesarangeoftaskssuchasmachine translation,summarization,questionanswering,anddialoguegeneration. Thesetasks 7.5 Summary 401 canbeformulatedassequence-to-sequenceproblems: weuseanencodertorepresentthe sourcetext,andadecodertogeneratethecorrespondingtargettext. Astraightforward method to apply BERT models is to consider them as encoders. Before fine-tuning, we can initialize the parameters of the encoder with those from a pre-trained BERT model. Then,theencoder-decodermodelcanbefine-tunedonpairsoftextsasusual. Thefollowingshowsthearchitectureofaneuralmachinetranslationsystemwherea BERTmodelisappliedonthesourceside. TargetText Adapter y 1 y 2 y 3 ... yn BERT(Encoder) Decoder ex ex ... ex ex ey ey ey ... ey cls 1 m m+1 0 1 2 n−1 [CLS] x 1 ... xm [SEP] ⟨s⟩ y 1 y 2 ... y n−1 SourceText Herex ...x denotesthesourcesequence,y ...y denotesthetargetsequence,ex...ex 1 m 1 n 1 m denotes the embedding sequence of x ...x , and ey...ey denotes the embedding se- 1 m 1 n quenceofy ...y . Theadapter,whichisoptional,mapstheoutputoftheBERTmodel 1 n totheformthatisbettersuitedtothedecoder. Fine-tuning BERT models is a complicated engineering problem, influenced by many factors, such as the amount of fine-tuning data, the model size, and the optimizer used in fine-tuning. Ingeneral,wewishtofine-tunethesemodelssufficientlysothattheycanperform wellinthedownstreamtasks. However,fine-tuningBERTmodelsforspecifictasksmaylead to overfitting, which in turn reduces their ability to generalize to other tasks. For example, supposewehaveaBERTmodelthatperformswellonaparticulartask. Ifwethenfine-tuneit fornewtasks,thismaydecreaseitsperformanceontheoriginaltask. Thisproblemisrelated tothecatastrophicforgettingproblemincontinualtraining,whereaneuralnetworkforgets previously learned information when updated on new samples. In practical applications, a commonwaytoalleviatecatastrophicforgettingistoaddsomeolddataintofine-tuningand trainthemodelwithmorediversedata. Also,onemayusemethodsspecializedtocatastrophic forgetting,suchasexperiencereplay[Rolnicketal.,2019]andelasticweightconsolidation [Kirkpatricketal.,2017]. Theinterestedreadercanrefertosomesurveysformoredetailed discussionsofthisissueincontinuallearning[Parisietal.,2019;Wangetal.,2023a;f]. 7.5 Summary Inthischapterwehavediscussedthegeneralideaofpre-traininginNLP.Inparticular,wehave discussedself-supervisedpre-traininganditsapplicationtoencoder-only,decoder-only,and 402 Chapter 7. Pre-training encoder-decoderarchitectures. Moreover,wehavepresentedandcomparedavarietyofpre- trainingtasksforthesearchitectures. Asanexample,BERTisusedtoillustratehowsequence modelsarepre-trainedviamaskedlanguagemodelingandappliedtodifferentdownstream tasks. RecentyearshaveshownremarkableprogressinNLP,ledbythelarge-scaleuseofself- supervisedpre-training. Andsweepingadvancesarebeingmadeacrossmanytasks,notonly inNLPbutalsoincomputervisionandotherareasofAI.Oneideabehindtheseadvancesis thatasignificantamountofknowledgeabouttheworldcanbelearnedbysimplytrainingthese AIsystemsonhugeamountsofunlabeleddata. Forexample,alanguagemodelcanlearnsome generalknowledgeofalanguagebyrepeatedlypredictingmaskedwordsinlarge-scaletext. As aresult,thispre-trainedlanguagemodelcanserveasafoundationmodel,whichcanbeeasily adaptedtoaddressspecificdownstreamNLPtasks. ThisparadigmshiftinNLPhasenabled thedevelopmentofincrediblypowerfulsystemsforlanguageunderstanding,generation,and reasoning [Manning, 2022]. However, it is important to recognize that we are still in the earlystagesofcreatingtrulyintelligentsystems,andthereisalongwaytogo. Nevertheless, large-scale pre-training has opened a door to intelligent systems that researchers have long aspired to develop, though several key research areas remain open for exploration, such as learning intelligence efficiently using reasonably small-sized data and acquiring complex reasoningandplanningabilities. Notethatthischapterismostlyintroductoryandcannotcoverallaspectsofpre-training. Forexample,therearemanymethodstofine-tuneapre-trainedmodel,offeringdifferentways tobetteradaptthemodeltodiversesituations. Moreover,largelanguagemodels,whichare consideredoneofthemostsignificantachievementsinAIinrecentyears,areskippedinthis section. Weleavethediscussionofthesetopicstothefollowingchapters. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 8 Generative Models OneofthemostsignificantadvancesinNLPinrecentyearsmightbethedevelopmentoflarge languagemodels(LLMs). Thishashelpedcreatesystemsthatcanunderstandandgenerate natural languages like humans. These systems have even been found to be able to reason, whichisconsideredaverychallengingAIproblem. Withtheseachievements,NLPmadebig stridesandenteredaneweraofresearchinwhichdifficultproblemsarebeingsolved,suchas buildingconversationalsystemsthatcancommunicatewithhumanssmoothly. Theconceptoflanguagemodelingorprobabilisticlanguagemodelingdatesbacktoearly experimentsconductedbyShannon[1951]. Inhiswork,alanguagemodelwasdesignedto estimate the predictability of English — how well can the next letter of a text be predicted whentheprecedingN lettersareknown. AlthoughShannon’sexperimentswerepreliminary, thefundamentalgoalsandmethodsoflanguagemodelinghaveremainedlargelyunchanged overthedecadessincethen. Forquitealongperiod,particularlybefore2010,thedominant approach to language modeling was the n-gram approach [Jurafsky and Martin, 2008]. In n-gramlanguagemodeling,weestimatetheprobabilityofawordgivenitsprecedingn−1 words,andthustheprobabilityofasequencecanbeapproximatedbytheproductofaseries ofn-gramprobabilities. Theseprobabilitiesaretypicallyestimatedbycollectingsmoothed relative counts of n-grams in text. While such an approach is straightforward and simple, it has been extensively used in NLP. For example, the success of modern statistical speech recognitionandmachinetranslationsystemshaslargelydependedontheutilizationofn-gram languagemodels[Jelinek,1998;Koehn,2010]. Applying neural networks to language modeling has long been attractive, but a real breakthroughappearedasdeeplearningtechniquesadvanced. AwidelycitedstudyisBengio etal.[2003a]’sworkwheren-gramprobabilitiesaremodeledviaafeed-forwardnetworkand learnedbytrainingthenetworkinanend-to-endfashion. Aby-productofthisneurallanguage modelisthedistributedrepresentationsofwords,knownaswordembeddings. Ratherthan representingwordsasdiscretevariables,wordembeddingsmapwordsintolow-dimensional real-valuedvectors,makingitpossibletocomputethemeaningsofwordsandwordn-grams in a continuous representation space. As a result, language models are no longer burdened withthecurseofdimensionality,butcanrepresentexponentiallymanyn-gramsviaacompact 404 Chapter 8. Generative Models anddenseneuralmodel. Theideaoflearningwordrepresentationsthroughneurallanguagemodelsinspiredsub- sequent research in representation learning in NLP. However, this approach did not attract significantinterestindevelopingNLPsystemsinthefirstfewyearsafteritsproposal. Starting inabout2012, though, advancesweremadeinlearningwordembeddingsfromlarge-scale text via simple word prediction tasks.",
    "modelisthedistributedrepresentationsofwords,knownaswordembeddings. Ratherthan representingwordsasdiscretevariables,wordembeddingsmapwordsintolow-dimensional real-valuedvectors,makingitpossibletocomputethemeaningsofwordsandwordn-grams in a continuous representation space. As a result, language models are no longer burdened withthecurseofdimensionality,butcanrepresentexponentiallymanyn-gramsviaacompact 404 Chapter 8. Generative Models anddenseneuralmodel. Theideaoflearningwordrepresentationsthroughneurallanguagemodelsinspiredsub- sequent research in representation learning in NLP. However, this approach did not attract significantinterestindevelopingNLPsystemsinthefirstfewyearsafteritsproposal. Starting inabout2012, though, advancesweremadeinlearningwordembeddingsfromlarge-scale text via simple word prediction tasks. Several methods, such as Word2Vec, were proposed to effectively learn such embeddings, which were then successfully applied in a variety of NLPsystems[Mikolovetal.,2013a;c]. Asaresultoftheseadvances,researchersbeganto thinkoflearningrepresentationsofsequencesusingmorepowerfullanguagemodels, such as LSTM-based models [Sutskever et al., 2014; Peters et al., 2018]. And further progress andinterestinsequencerepresentationexplodedafterTransformerwasproposed. Alongside the rise of Transformer, the concept of language modeling was generalized to encompass modelsthatlearntopredictwordsinvariousways. ManypowerfulTransformer-basedmodels werepre-trainedusingthesewordpredictiontasks, andsuccessfullyappliedtoavarietyof downstreamtasks[Devlinetal.,2019]. Indeed, training language models on large-scale data has led NLP research to exciting times. Whilelanguagemodelinghaslongbeenseenasafoundationaltechniquewithnodirect link to the goals of artificial intelligence that researchers had hoped for, it helps us see the emergenceofintelligentsystemsthatcanlearnacertaindegreeofgeneralknowledgefrom repeatedlypredictingwordsintext. Recentresearchdemonstratesthatasingle,well-trained LLM can handle a large number of tasks and generalize to perform new tasks with a small adaptationeffort[Bubecketal.,2023]. Thissuggestsasteptowardsmoreadvancedformsof artificialintelligence,andinspiresfurtherexplorationintodevelopingmorepowerfullanguage modelsasfoundationmodels. In this chapter, we consider the basic concepts of generative LLMs. For simplicity, we usethetermslargelanguagemodelsorLLMstorefertogenerativemodelslikeGPT,though thistermcanbroadlycoverothertypesofmodelslikeBERT.Webeginbygivingageneral introductiontoLLMs,includingthekeystepsofbuildingsuchmodels. Wethendiscusstwo scalingissuesofLLMs: howLLMsaretrainedatscale,andhowLLMscanbeimprovedto handleverylongtexts. Finally,wegiveasummaryofthesediscussions. 8.1 A Brief Introduction to LLMs InthissectionwegiveanintroductiontothebasicideasofLLMsasrequiredfortherestofthis chapterandthefollowingchapters. Wewillusetermsword andtokeninterchangeably. Both ofthemrefertothebasicunitsusedinlanguagemodeling,thoughtheiroriginalmeaningsare different. Before presenting details, let us first consider how language models work. The goal of language modeling is to predict the probability of a sequence of tokens occurring. Let {x ,x ,...,x }beasequenceoftokens,wherex isthestartsymbol⟨s⟩(or⟨SOS⟩)1. The 0 1 m 0 1Thestartsymbolcanalsobe[CLS]followingBERTmodels. 8.1 A Brief Introduction to LLMs 405 probabilityofthissequencecanbedefinedusingthechainrule Pr(x ,...,x ) = Pr(x )·Pr(x |x )·Pr(x |x ,x )···Pr(x |x ,...,x ) 0 m 0 1 0 2 0 1 m 0 m−1 m (cid:89) = Pr(x |x ,...,x ) (8.1) i 0 i−1 i=0 oralternativelyinalogarithmicform m (cid:88) logPr(x ,...,x ) = logPr(x |x ,...,x ) (8.2) 0 m i 0 i−1 i=0 Here Pr(x |x ,...,x ) is the probability of the token x given all its previous tokens i 0 i−1 i {x ,...,x } 2. In the era of deep learning, a typical approach to language modeling is 0 i−1 toestimatethisprobabilityusingadeepneuralnetwork. Neuralnetworkstrainedtoaccom- plish this task receive a sequence of tokens x ,...,x and produce a distribution over the 0 i−1 vocabularyV (denotedbyPr(·|x ,...,x )). TheprobabilityPr(x |x ,...,x )isthevalue 0 i−1 i 0 i−1 ofthei-thentryofPr(·|x ,...,x ). 0 i−1 Whenapplyingatrainedlanguagemodel,acommontaskistofindthemostlikelytoken givenitspreviouscontexttokens. Thistokenpredictiontaskcanbedescribedas xˆ = argmaxPr(x |x ,...,x ) (8.3) i i 0 i−1 xi∈V Wecanperformwordpredictionmultipletimestogenerateacontinuoustext: eachtime wepredictthebesttokenxˆ ,andthenaddthispredictedtokentothecontextforpredicting i thenexttokenxˆ . Thisresultsinaleft-to-rightgenerationprocessimplementingEqs. (8.1) i+1 and(8.2). Toillustrate,considerthegenerationofthefollowingthreewordsgiventheprefix ‘⟨s⟩ a’, as shown in Table 8.1. Now we discuss how LLMs are constructed, trained, and applied. 8.1.1 Decoder-only Transformers As is standard practice, the input of a language model is a sequence of tokens (denoted by {x ,...,x }). For each step, an output token is generated, shifting the sequence one 0 m−1 positionforwardforthenextprediction. Todothis,thelanguagemodeloutputsadistribution Pr(·|x ,...,x )ateachpositioni,andthetokenx isselectedaccordingtothisdistribution. 0 i−1 i Thismodelistrainedbymaximizingtheloglikelihood (cid:80)m logPr(x |x ,...,x )3. i=1 i 0 i−1 Here, we focus on the decoder-only Transformer architecture, as it is one of the most popularmodelarchitecturesusedinLLMs. Theinputsequenceoftokensisrepresentedbya sequenceofd -dimensionalvectors{e ,...,e }. e isthesumofthetokenembeddingof e 0 m−1 i x andthepositionalembeddingofi. ThemajorbodyofthemodelisastackofTransformer i 2We",
    "Todothis,thelanguagemodeloutputsadistribution Pr(·|x ,...,x )ateachpositioni,andthetokenx isselectedaccordingtothisdistribution. 0 i−1 i Thismodelistrainedbymaximizingtheloglikelihood (cid:80)m logPr(x |x ,...,x )3. i=1 i 0 i−1 Here, we focus on the decoder-only Transformer architecture, as it is one of the most popularmodelarchitecturesusedinLLMs. Theinputsequenceoftokensisrepresentedbya sequenceofd -dimensionalvectors{e ,...,e }. e isthesumofthetokenembeddingof e 0 m−1 i x andthepositionalembeddingofi. ThemajorbodyofthemodelisastackofTransformer i 2We assume that when i = 0, Pr(x i |x 0 ,...,x i−1 ) = Pr(x 0 ) = 1. Hence Pr(x 0 ,...,xm) = Pr(x 0 )Pr(x 1 ,...,xm|x 0 )=Pr(x 1 ,...,xm|x 0 ). 3Notethat (cid:80)m i=1 logPr(x i |x 0 ,...,x i−1 )= (cid:80)m i=0 logPr(x i |x 0 ,...,x i−1 )sincelogPr(x 0 )=0. 406 Chapter 8. Generative Models Context Predict DecisionRule SequenceProbability ⟨s⟩ a b argmax Pr(x |⟨s⟩a) Pr(⟨s⟩)·Pr(a|⟨s⟩)· Pr(b|⟨s⟩a) x2∈V 2 ⟨s⟩ a b c argmax Pr(x |⟨s⟩ab) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)· x3∈V 3 Pr(c|⟨s⟩ab) ⟨s⟩ a b c d argmax Pr(x |⟨s⟩abc) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)· x4∈V 4 Pr(c|⟨s⟩ab)· Pr(d|⟨s⟩abc) Table 8.1: Illustration of generating the three tokens b c d given the prefix ⟨s⟩ a via a languagemodel. Ineachstep,themodelpicksatokenx fromV sothatPr(x |x ,...,x )is i i 0 i−1 maximized. Thistokenisthenappendedtotheendofthecontextsequence. Inthenextstep, werepeatthesameprocess,butbasedonthenewcontext. blocks(orlayers). EachTransformerblockhastwostackedsub-layers,oneforself-attention modelingandoneforFFNmodeling. Thesesub-layerscanbedefinedusingthepost-norm architecture output = LNorm(F(input)+input) (8.4) orthepre-normarchitecture output = LNorm(F(input))+input (8.5) whereinputandoutputdenotetheinputandoutput,bothbeinganm×dmatrix. Thei-th rowsofinputandoutputcanbeseenascontextualrepresentationsofthei-thtokeninthe sequence. F(·)isthecorefunctionofasub-layer. ForFFNsub-layers,F(·)isamulti-layerFFN.For self-attentionsub-layers,F(·)isamulti-headself-attentionfunction. Ingeneral,self-attention isexpressedinaformofQKVattention QKT Att (Q,K,V) = Softmax( √ +Mask)V (8.6) qkv d whereQ,KandV∈Rm×d arethequeries,keys,andvalues,respectively. Itisimportantto notethatonlyprevioustokensareconsideredwhenpredictingatoken. Soamaskingvariable Mask∈Rm×m isincorporatedintoself-attentiontoachievethis. Theentry(i,k)ofMask hasavalueof0ifi≤k,andavalueof−inf otherwise. GivenarepresentationH∈Rm×d,themulti-headself-attentionfunctioncanbedefinedas F(H) = Merge(head ,...,head )Whead (8.7) 1 τ whereMerge(·)representeesaconcatenationofitsinputs, andWhead ∈Rd×d representsa 8.1 A Brief Introduction to LLMs 407 Post-normorPre-norm FFN x 1 x 2 ... xm Pr(xm|x0x1...xm−1) Pr(x2|x0x1) Pr(x1|x0) Post-normorPre-norm ... Self-attention hL hL ... hL 0 1 m−1 LanguageModel e 0 e 1 ... e m−1 x 0 x 1 ... x m−1 z 0 z 1 ... z m−1 skcolBL Figure8.1: TheTransformer-decoderarchitectureforlanguagemodeling. Thecentralcom- ponentsareLstackedTransformerblocks,eachcomprisingaself-attentionsub-layerandan FFN sub-layer. To prevent the model from accessing the right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to generate a probability distribution for the next token, given the sequence of previous tokens. During inference,themodeltakesthepreviouslypredictedtokentopredictthenextone,repeating this process until the end of the sequence is reached. {z ,...,z } denote the inputs of a 0 m−1 Transformerblock,and{hL,...,hL }denotetheoutputsofthelastTransformerblock. 0 m−1 parametermatrix. head istheoutputofQKVattentiononasub-spaceofrepresentation j head = Att (Q[j],K[j],V[j]) (8.8) j qkv Q[j],K[j],andV[j] arethequeries,keys,andvaluesprojectedontothej-thsub-spacevialinear transformations Q[j] = HWq (8.9) j K[j] = HWk (8.10) j V[j] = HWv (8.11) j whereW j q ,W j k,andW j v ∈Rd× τ d aretheparametermatricesofthetransformations. SupposewehaveLTransformerblocks. ASoftmaxlayerisbuiltontopoftheoutputof thelastblock. TheSoftmaxlayeroutputsasequenceofmdistributionsoverthevocabulary, 408 Chapter 8. Generative Models likethis   Pr(·|x ,...,x ) 0 m−1  . .    .   = Softmax(HLWo) (8.12)  Pr(·|x ,x )   0 1  Pr(·|x ) 0 where HL is the output of the last Transformer block, and Wo ∈Rd×|V| is the parameter matrix. Figure 8.1 shows the Transformer architecture for language modeling. Applying this languagemodelfollowsanautoregressiveprocess. Eachtimethelanguagemodeltakesatoken x asinputandpredictsatokenx thatmaximizestheprobabilityPr(x |x ,...,x ). Itis i−1 i i 0 i−1 importanttonotethat,despitedifferentimplementationdetails,manyLLMssharethesame architecturedescribedabove. Thesemodelsarecalledlargebecauseboththeirdepthandwidth aresignificant. Table8.2showsthemodelsizesforafewLLMs,aswellastheirmodelsetups. 8.1.2 Training LLMs NowsupposethatwearegivenatrainingsetD comprisingK sequences. Thelog-likelihood ofeachsequencex=x ...x inD canbecalculatedusingalanguagemodel 0 m m (cid:88) L (x) = logPr (x |x ,...,x ) (8.13) θ",
    "architecture for language modeling. Applying this languagemodelfollowsanautoregressiveprocess. Eachtimethelanguagemodeltakesatoken x asinputandpredictsatokenx thatmaximizestheprobabilityPr(x |x ,...,x ). Itis i−1 i i 0 i−1 importanttonotethat,despitedifferentimplementationdetails,manyLLMssharethesame architecturedescribedabove. Thesemodelsarecalledlargebecauseboththeirdepthandwidth aresignificant. Table8.2showsthemodelsizesforafewLLMs,aswellastheirmodelsetups. 8.1.2 Training LLMs NowsupposethatwearegivenatrainingsetD comprisingK sequences. Thelog-likelihood ofeachsequencex=x ...x inD canbecalculatedusingalanguagemodel 0 m m (cid:88) L (x) = logPr (x |x ,...,x ) (8.13) θ θ i 0 i−1 i=1 Herethesubscriptθ affixedtoL(·)andPr(·)denotestheparametersofthelanguagemodel. Then,theobjectiveofmaximumlikelihoodtrainingisdefinedas (cid:88) θˆ = argmax L (x) (8.14) θ θ x∈D TrainingTransformer-basedlanguagemodelswiththeaboveobjectiveiscommonlyviewed asastandardoptimizationprocessforneuralnetworks. Thiscanbeachievedusinggradient descentalgorithms,whicharewidelysupportedbyoff-the-shelfdeeplearningtoolkits. Some- whatsurprisingly,betterresultswerecontinuouslyyieldedaslanguagemodelswereevolved intomorecomputationallyintensivemodelsandtrainedonlargerdatasets[Kaplanetal.,2020]. ThesesuccesseshaveledNLPresearcherstocontinueincreasingboththetrainingdataand modelsizeinordertobuildmorepowerfullanguagemodels. However,aslanguagemodelsbecomelarger,weconfrontnewtrainingchallenges,which significantlychangetheproblemcomparedtotrainingrelativelysmallmodels. Oneofthese challengesarisesfromtheneedforlarge-scaledistributedsystemstomanagethedata,model parameters,trainingroutines,andsoon. Developingandmaintainingsuchsystemsrequiresa significantamountofworkinbothsoftwareandhardwareengineering,aswellasexpertisein deeplearning. Arelatedissueisthatwhenthetrainingisscaledup,weneedmorecomputing resourcestoensurethetrainingprocesscanbecompletedinanacceptabletime. Forexample, 8.1 A Brief Introduction to LLMs 409 LLM #ofParameters DepthL Widthd #ofHeads (Q/KV) GPT-1[Radfordetal.,2018] 0.117B 12 768 12/12 GPT-2[Radfordetal.,2019] 1.5B 48 1,600 25/25 GPT-3[Brownetal.,2020] 175B 96 12,288 96/96 7B 32 4,096 32/32 LLaMA2[Touvronetal.,2023b] 13B 40 5,120 40/40 70B 80 8,192 64/64 8B 32 4,096 32/8 LLaMA3/3.1[Dubeyetal.,2024] 70B 80 8,192 64/8 405B 126 16,384 128/8 2B 26 2,304 8/4 Gemma2[Teametal.,2024] 9B 42 3,584 16/8 37B 46 4,608 32/16 0.5B 24 896 14/2 Qwen2.5[Yangetal.,2024] 7B 28 3,584 28/4 72B 80 8,192 64/8 DeepSeek-V3[Liuetal.,2024a] 671B 61 7,168 128/128 7B 32 4,544 71/71 Falcon[Penedoetal.,2023] 40B 60 8,192 128/128 180B 80 14,848 232/232 Mistral[Jiangetal.,2023a] 7B 32 4,096 32/32 Table8.2: ComparisonofsomeLLMsintermsofmodelsize,modeldepth,modelwidth,and numberofheads(a/bmeansaheadsforqueriesandbheadsforbothkeysandvalues). itgenerallyrequireshundredsorthousandsofGPUstotrainanLLMwithtensofbillionsof parametersfromscratch. Thisrequirementdrasticallyincreasesthecostoftrainingsuchmodels, especiallyconsideringthatmanytrainingrunsareneededasthesemodelsaredeveloped. Also, fromtheperspectiveofdeeplearning,thetrainingprocesscanbecomeunstableiftheneural networksareverydeepand/orthemodelsizeisverylarge. Inresponse,wetypicallyneedto modifythemodelarchitecturetoadaptLLMstolarge-scaletraining. InSection8.2wewill presentmorediscussionsontheseissues. 8.1.3 Fine-tuning LLMs Once we have pre-trained an LLM, we can then apply it to perform various NLP tasks. Traditionally language models are used as components of other systems, for example, they arewidelyappliedtoscoretranslationsinstatisticalmachinetranslationsystems. Bycontrast, ingenerativeAI,LLMsareconsideredcompletesystemsandareemployedtoaddressNLP problemsbymakinguseoftheirgenerationnature. Acommonapproachistodescribethetask 410 Chapter 8. Generative Models wewanttoaddressintextandthenpromptLLMstogeneratetextbasedonthisdescription. Thisisastandardtextgenerationtaskwherewecontinueorcompletethetextstartingfroma givencontext. More formally, let x=x ...x denote a token sequence of context given by users, and 0 m y=y ...y denoteatokensequencefollowingthecontext. Then,theinferenceofLLMscan 1 n bedefinedasaproblemoffindingthemostlikelysequenceybasedonx: yˆ = argmaxlogPr(y|x) y n (cid:88) = argmax logPr(y |x ,...,x ,y ,...,y ) (8.15) i 0 m 1 i−1 y i=1 Here (cid:80)n logPr(y |x ,...,x ,y ,...,y )essentiallyexpressesthesamethingastheright- i=1 i 0 m 1 i−1 handsideofEq. (8.2). Itmodelsthelogprobabilityofpredictingtokensfrompositionm+1, ratherthanposition0. Throughoutthischapterandsubsequentones,wewillemployseparate variablesxandytodistinguishtheinputandoutputofanLLM,thoughtheycanbeseenas sub-sequencesfromthesamesequence. Byadoptingsuchnotation,weseethattheformofthe aboveequationcloselyresemblesthoseusedinothertextgenerationmodelsinNLP,suchas neuralmachinetranslationmodels. ToillustratehowLLMsareapplied,considertheproblemofdeterminingthegrammatical- ityforagivensentence. Wecandefineatemplatelikethis {*sentence*} Question: Isthissentencegrammaticallycorrect? Answer: Here representsthetextweintendtogenerate. {*sentence*}isaplaceholdervariablethat willbereplacedbytheactualsentenceprovidedbytheusers. Forexample,supposewehavea sentence“Johnseemshappytoday.”. Wecanreplacethe{*sentence*}inthetemplatewith thissentencetohaveaninputtothelanguagemodel Johnseemshappytoday. Question: Isthissentencegrammaticallycorrect? Answer: Toperformthetask,thelanguagemodelisgiventhecontextx=“Johnseemshappytoday .\\n Question : Is this sentence grammatically correct?\\n Answer :”4. It then generates the following text as the answer, based on the context. For example, the language model may output“Yes”(i.e.,y=“Yes”)ifthistextistheonewiththemaximumprobabilityofprediction giventhiscontext. 4\\nisaspecialcharacterusedforlinebreaks. 8.1 A Brief Introduction to LLMs 411 Likewise, we can define more templates to address other tasks. For example, we can translateanEnglishsentenceintoChineseusingthefollowingtemplate {*sentence*} Question: WhatistheChinesetranslationofthisEnglishsentence? Answer: orusinganinstruction-liketemplate {*sentence*} TranslatethissentencefromEnglishintoChinese. orusingacode-liketemplate. [src-lang]=English[tgt-lang]=Chinese[input]={*sentence*} [output]= Theabovetemplatesprovideasimplebuteffectivemethodto“prompt”asingleLLMto performvarioustaskswithoutadaptingthestructureofthemodel. However, thisapproach requiresthattheLLMcanrecognizeandfollowtheinstructionsorquestions. Onewaytodo thisistoincorporatetrainingsampleswithinstructionsandtheircorrespondingresponsesinto thepre-trainingdataset. Whilethismethodisstraightforward,buildingandtrainingLLMsfrom scratchiscomputationallyexpensive. Moreover,makinginstruction-followingdataeffective forpre-trainingrequiresasignificantamountofsuchdata,butcollectinglarge-scalelabeled dataforalltasksofinterestisverydifficult. Asecondmethod,whichhasbeenadefactostandardinrecentresearch,istoadaptLLMs via fine-tuning. As such, the token prediction ability learned in the pre-training phase can be generalized to accomplish new tasks. The idea behind fine-tuning is that some general knowledgeoflanguagehasbeenacquiredinpre-training,butweneedamechanismtoactivate thisknowledgeforapplyingittonewtasks. Toachievethis,wecanslightlyfine-tunethemodel parametersusinginstruction-followingdata. Thisapproachiscalledinstructionfine-tuning. An instruction fine-tuning sample, which is represented by a sequence of tokens, can be seen as a tuple consisting of an input and the desired output. Here, the input includes instructions,systeminformation(orsystemprefix),andanyotheruser-providedinformation5. Toillustrate,considerthefollowingexamples(bluetext=inputandunderlinedtext=output). 5Systeminformationreferstoasequenceoftokensaddedatthebeginningofaninputinordertoguidethe behaviorofanLLM,suchas,youareahelpfulassistantandshouldnotoutputtoxiccontent. 412 Chapter 8. Generative Models Readthefollowingsentence,andanswerwhetheritisgrammaticallycorrect. LLMsarepowerfulmodelsbutareexpensivetobuild. Yes LLMsarepowerfulmodelsbutareexpensivetobuild. Doesthissentencemakesensegrammatically? AnswerOption -Yes -No Yes Yes or No? Yes = the following sentence is grammatically correct. No =",
    "tokens, can be seen as a tuple consisting of an input and the desired output. Here, the input includes instructions,systeminformation(orsystemprefix),andanyotheruser-providedinformation5. Toillustrate,considerthefollowingexamples(bluetext=inputandunderlinedtext=output). 5Systeminformationreferstoasequenceoftokensaddedatthebeginningofaninputinordertoguidethe behaviorofanLLM,suchas,youareahelpfulassistantandshouldnotoutputtoxiccontent. 412 Chapter 8. Generative Models Readthefollowingsentence,andanswerwhetheritisgrammaticallycorrect. LLMsarepowerfulmodelsbutareexpensivetobuild. Yes LLMsarepowerfulmodelsbutareexpensivetobuild. Doesthissentencemakesensegrammatically? AnswerOption -Yes -No Yes Yes or No? Yes = the following sentence is grammatically correct. No = it containsgrammaticalerrors. LLMsarepowerfulmodelsbutareexpensivetobuild. Yes Allthesesamplesdescribethesamebinaryclassificationtask,butwithdifferentinstruc- tions. To increase the diversity in the fine-tuning data and make LLMs generalize, we can definemoretasksusinginstructions. Writeashortstoryaboutthreecharacters. Character1: AboywhosenameisAndy. Character2: Andy’sdog. Character3: Andy’sfriend,Jane. In the heart of a bustling city, Andy, his loyal dog Rocket, and his friend Janediscoveredahiddengardenbeneaththeglowoftwilight. Surroundedby ancient trees and whispering winds, they shared dreams and secrets, forging an unbreakable bond. As the night deepened, they made a pact under the stars to always find magic in the mundane. Together, they realized that true adventurelayinthemomentsspentwithfriends,andeverydaywasatreasure waitingtobediscovered. 8.1 A Brief Introduction to LLMs 413 Showmearecipeformakingicecream. Ingredients: 2cupsheavycream;1cupmilk;3/4cupsugar;2tspvanillaextract; Pinchofsalt. Steps: 1. Whiskallingredientsuntilsugardissolves. 2. Chillmixturefor2hoursinthefridge. 3. Freezeinashallowdish,stirringevery30minutes,3-4times. 4. Freezesolidfor2hours. 5. Serveyourhomemadeicecream. If you buy 5 apples and each apple costs $1.20, how much do you spend in total? $6.00 Write a Python program to calculate the sum of squares of the following numbers. 1,2,10,-9,78 numbers=[1,2,10,-9,78] sum_of_squares=sum(x**2forxinnumbers) print(sum_of_squares) Toacquireinstruction-followingabilities,acertainamountoffine-tuningdataisrequired. This data may include diverse instructions and possible responses. It has been found that scalingthenumberoffine-tuningtasksisbeneficialforimprovingtheperformanceofLLMs [Chungetal.,2022]. Notethatalthoughmorefine-tuningdataisfavorable,theamountofthis dataisgenerallyordersofmagnitudesmallerthanthatofthepre-trainingdata. Forexample, LLMscanbefine-tunedwithtensorhundredsofthousandsofsamples,orevenfewerifthese samplesareofhighquality[Zhouetal.,2023a;Chenetal.,2023b],whereaspre-trainingsuch modelsmayrequirebillionsortrillionsoftokens,resultinginsignificantlylargercomputational demandsandlongertrainingtimes[Touvronetal.,2023a]. It is also worth noting that we should not expect the fine-tuning data to cover all the downstreamtaskstowhichweintendtoapplyLLMs. Acommonunderstandingofhowthepre- training+fine-tuningapproachworksisthatLLMshavegainedknowledgeforunderstanding instructionsandgeneratingresponsesinthepre-trainingphase. However,theseabilitiesarenot fullyactivateduntilweintroducesomeformofsupervision. Thegeneralinstruction-following behavioremergesaswefine-tunethemodelswitharelativelysmallamountoflabeleddata. 414 Chapter 8. Generative Models As a result, we can achieve some level of zero-shot learning: the fine-tuned models can handlenewtasksthattheyhavenotbeenexplicitlytrainedorfine-tunedfor[Sanhetal.,2022; Weietal.,2022a]. Thiszero-shotlearningabilitydistinguishesgenerativeLLMsfromearlier pre-trainedmodelslikeBERT,whichareprimarilyfine-tunedforspecifictasks. Oncewehavepreparedacollectionofinstruction-describeddata,thefine-tuningprocess isrelativelysimple. Thisprocesscanbeviewedasastandardtrainingprocessaspre-training, butonamuchsmallertrainingdataset. LetD bethefine-tuningdatasetandθˆ bethemodel tune parametersoptimizedviapre-training. WecanmodifyEq. (8.14)toobtaintheobjectiveof fine-tuning (cid:88) θ˜ = argmax L (sample) (8.16) θˆ+ θˆ+ sample∈Dtune Here θ˜ denotes the optimal parameters. The use of notation θˆ+ means that the fine-tuning startswiththepre-trainedparametersθˆ . Foreachsample∈D ,wedivideitintoaninputsegmentx andanoutputsegment tune sample y ,thatis, sample sample = [y ,x ] (8.17) sample sample Wethendefinethelossfunctiontobe L (sample) = −logPr (y |x ) (8.18) θˆ+ θˆ+ sample sample In other words, we compute the loss over the sub-sequence y , rather than the entire sample sequence. Inapracticalimplementationofback-propagationforthisequation,thesequence [y ,x ]isconstructedintheforwardpassasusual. However,inthebackwardpass, sample sample errorgradientsarepropagatedbackonlythroughthepartsofthenetworkthatcorrespondto y ,leavingtherestofthenetworkunchanged. Asanexample,considerasequence sample ⟨s⟩Squarethisnumber.2. Theresultis4. (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) Context(Input) Prediction(Output) ThelossiscalculatedandbackpropagatedonlyforTheresultis4.. Instruction fine-tuning also requires substantial engineering work. In order to achieve satisfactoryresults,onemayexperimentwithdifferentsettingsofthelearningrate,batchsize, number of fine-tuning steps, and so on. This typically requires many fine-tuning runs and evaluations. Thecostandexperimentaleffortoffine-tuningremaincriticalandshouldnotbe overlooked,thoughtheyaremuchlowerthanthoseofthepre-trainingphase. While we focus on instruction fine-tuning for an illustrative example here, fine-tuning techniques play an important role in developing various LLMs and are more widely used. Examplesincludefine-tuningLLMsaschatbotsusingdialogdata,andadaptingthesemodels to handle very long sequences. The wide application of fine-tuning has led researchers to improvethesetechniques,suchasdesigningmoreefficientfine-tuningalgorithms. Whilethe 8.1 A Brief Introduction to LLMs 415 researchonfine-tuningisfruitful,inthissectionwejustgiveaflavourofthekeystepsinvolved. Wewillseemoredetaileddiscussionsonthistopicinthefollowingchapters. 8.1.4 Aligning LLMs with the World Instructionfine-tuningprovidesasimplewaytoadaptLLMstotasksthatcanbewelldefined. Thisproblemcanbroadlybecategorizedasanalignmentproblem. Here,alignmentisreferred to as a process of guiding LLMs to behave in ways that align with human intentions. The guidancecancomefromlabeleddata,humanfeedback,oranyotherformofhumanpreferences. Forexample,wewantLLMsnotonlytobeaccurateinfollowinginstructions,butalsotobe unbiased,truthful,andharmless. Soweneedtosupervisethemodelstowardshumanvalues andexpectations. AcommonexampleisthatwhenweaskanLLMhowtobuildaweapon,it mayprovidealistofkeystepstodosoifitisnotcarefullyaligned. However,aresponsible modelshouldrecognizeandavoidrespondingtorequestsforharmfulorillegalinformation. AlignmentinthiscaseiscrucialforensuringthatLLMsactresponsiblyandinaccordance withethicalguidelines. ArelatedconcepttoalignmentisAIsafety. OneultimategoalofAIistobuildintelligent systemsthataresafeandsociallybeneficial. Toachievethisgoalweshouldkeepthesesystems robust,secure,andsubjective,inanyconditionsofreal-worlduse,eveninconditionsofmisuse or adverse use. For LLMs, the safety can be increased by aligning them with appropriate",
    "Thisproblemcanbroadlybecategorizedasanalignmentproblem. Here,alignmentisreferred to as a process of guiding LLMs to behave in ways that align with human intentions. The guidancecancomefromlabeleddata,humanfeedback,oranyotherformofhumanpreferences. Forexample,wewantLLMsnotonlytobeaccurateinfollowinginstructions,butalsotobe unbiased,truthful,andharmless. Soweneedtosupervisethemodelstowardshumanvalues andexpectations. AcommonexampleisthatwhenweaskanLLMhowtobuildaweapon,it mayprovidealistofkeystepstodosoifitisnotcarefullyaligned. However,aresponsible modelshouldrecognizeandavoidrespondingtorequestsforharmfulorillegalinformation. AlignmentinthiscaseiscrucialforensuringthatLLMsactresponsiblyandinaccordance withethicalguidelines. ArelatedconcepttoalignmentisAIsafety. OneultimategoalofAIistobuildintelligent systemsthataresafeandsociallybeneficial. Toachievethisgoalweshouldkeepthesesystems robust,secure,andsubjective,inanyconditionsofreal-worlduse,eveninconditionsofmisuse or adverse use. For LLMs, the safety can be increased by aligning them with appropriate humanguidance,suchashumanlabeleddataandinteractionswithusersduringapplication. Alignmentisdifficultashumanvaluesandexpectationsarediverseandshifting. Some- times,itishardtodescribepreciselywhathumanswant,unlessweseetheresponseofLLMs touserrequests. ThismakesalignmentnolongeraproblemoftuningLLMsonpredefined tasks,butabiggerproblemoftrainingthemwiththeinteractionswiththerealworld. AsaresultoftheconcernswithcontrollingAIsystems,therehasbeenasurgeinresearch onthealignmentissueforLLMs. Typically,twoalignmentstepsareadoptedafterLLMsare pre-trainedonlarge-scaleunlabeleddata. • Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLMsonnew,task-oriented,labelleddata. AcommonlyusedSFTtechniqueisinstruc- tionfine-tuning. Asdescribedintheprevioussubsection,bylearningfrominstruction- response annotated data, LLMs can align with the intended behaviors for following instructions, thereby becoming capable of performing various instruction-described tasks. Supervised fine-tuning can be seen as following the pre-training + fine-tuning paradigm,andoffersarelativelystraightforwardmethodtoadaptLLMs. • LearningfromHumanFeedback. AfteranLLMfinishespre-trainingandsupervised fine-tuning,itcanbeusedtorespondtouserrequestsifappropriatelyprompted. But this model may generate content that is unfactual, biased, or harmful. To make the LLMmorealignedwiththeusers,onesimpleapproachistodirectlylearnfromhuman feedback. Forexample,givensomeinstructionsandinputsprovidedbytheusers,experts areaskedtoevaluatehowwellthemodelrespondsinaccordancewiththeirpreferences andinterests. ThisfeedbackisthenusedtofurthertraintheLLMforbetteralignment. 416 Chapter 8. Generative Models Comparisons y1≻y4≻y2≻y3 SFTData Writeapoemaboutthe AnnotatingDatawithHumanPreferences weatherinLondon.... Pre-trainingData ModelOutput HowcanIgetthere?... 1............. 2............. Ilovethefoodhere!... 3............. 4............. Pre-training& Predicting Supervisedfine-tuning UserInput LLM LLM HowcanIlivemore environmentallyfriendly? (a)LearninganInitialLLM (b)AnnotatingDatawithHumanPreferences RewardScores {r(x,y)} EvaluatetheInput-outputPairs RewardModel ComparisonData Input-outputPairs {(x,yk1 ≻yk2 )} {x,y} Training SamplingyviathePolicyPr(y|x) DatasetD RewardModel LLM x∼D (Policy) (c)TrainingtheRewardModel (d)Training/Fine-tuningthePolicy gninut-eniFLR Figure8.2: AnoverviewofRLHF.Thereare4keystepsinvolved: a)traininganinitialLLM (i.e.,policy)usingpre-trainingandsupervisedfine-tuning;b)collectinghumanpreferencedata byrankingtheoutputsoftheLLM;c)trainingarewardmodelusingtherankingresults;d) RLfine-tuningofthepolicybasedontherewardmodel. Doublelinearrowsmeantrainingor fine-tuning. Atypicalmethodforlearningfromhumanfeedbackistoconsideritasareinforcement learning(RL)problem,knownasreinforcementlearningfromhumanfeedback(RLHF) [Ouyangetal.,2022]. TheRLHFmethodwasinitiallyproposedtoaddressgeneralsequential decision-making problems [Christiano et al., 2017], and was later successfully employed 8.1 A Brief Introduction to LLMs 417 in the development of the GPT series models [Stiennon et al., 2020]. As a reinforcement learningapproach,thegoalofRLHFistolearnapolicybymaximizingsomerewardfromthe environment. Specifically,twocomponentsarebuiltinRLHF: • Agent. Anagent,alsocalledanLMagent,istheLLMthatwewanttotrain. Thisagent operatesbyinteractingwithitsenvironment: itreceivesatextfromtheenvironmentand outputsanothertextthatissentbacktotheenvironment. Thepolicyoftheagentisthe functiondefinedbytheLLM,thatis,Pr(y|x). • RewardModel. Arewardmodelisaproxyoftheenvironment. Eachtimetheagent producesanoutputsequence,therewardmodelassignsthisoutputsequenceanumerical score(i.e.,thereward). Thisscoretellstheagenthowgoodtheoutputsequenceis. InRLHF,weneedtoperformtwolearningtasks: 1)rewardmodellearning,whichinvolves training a reward model using human feedback on the output of the agent, and 2) policy learning,whichinvolvesoptimizingapolicyguidedbytherewardmodelusingreinforcement learningalgorithms. HereisabriefoutlineofthekeystepsinvolvedinRLHF. • Buildaninitialpolicyusingpre-trainingandinstructionfine-tuning. • Use the policy to generate multiple outputs for each input, and then collect human feedbackontheseoutputs(e.g.,comparisonsoftheoutputs). • Learnarewardmodelfromthehumanfeedback. • Fine-tunethepolicywiththesupervisionfromtherewardmodel. Figure 8.2 shows an overview of RLHF. Given that this section serves only as a brief introduction to concepts of LLMs, a detailed discussion of RLHF techniques will not be included. WeinsteadillustratethebasicideasbehindRLHFusingasimpleexample. SupposewehavetrainedanLLMviapre-trainingandinstructionfine-tuning. ThisLLM isdeployedtorespondtorequestsfromusers. Forexample,ausermayinput HowcanIliveamoreenvironmentallyfriendlylife? WeusetheLLMtogenerate4differentoutputs(denotedby{y ,...,y })bysamplingthe 1 4 outputspace 418 Chapter 8. Generative Models Output1(y ): Considerswitchingtoanelectricvehicleorbicycleinsteadof 1 traditionalcarstoreducecarbonemissionsandprotectour planet. Output2(y ): Adoptaminimalistlifestyle. Ownfewerpossessionstoreduce 2 consumptionandtheenvironmentalimpactofmanufacturing anddisposal. Output3(y ): Gooff-grid. Generateyourownrenewableenergyandcollect 3 rainwatertobecomecompletelyself-sufficientandreduce relianceonnon-renewableresources. Output4(y ): Supportlocalfarmproductstoreducethecarbonfootprintof 4 transportingfood,whileenjoyingfresh,healthyfood. Wethenaskannotatorstoevaluatetheseoutputs. Onestraightforwardwayistoassigna ratingscoretoeachoutput. Inthiscase, therewardmodellearningproblemcanbeframed asataskoftrainingaregressionmodel. ButgivingnumericalscorestoLLMoutputsisnot an easy task for annotators. It is usually difficult to design an annotation standard that all annotatorscanagreeonandeasilyfollow. Analternativemethod,whichismorepopularinthe developmentofLLMs,istoranktheseoutputs. Forexample,apossiblerankingoftheabove outputsis y ≻y ≻y ≻y 1 4 2 3 Arewardmodelisthentrainedusingthisrankingresult. Ingeneral, arewardmodelin RLHF is a language model that shares the same architecture as the target LLM, but with a smallermodelsize. Giventheinputxandoutputy ,weconcatenatethemtoformasequence k seq =[x,y ]. This sequence is processed from left to right using forced decoding. Since k k eachpositioncanonlyaccessitsleftcontextinlanguagemodeling,theoutputofthetop-most Transformer layer at the first position cannot be used as the representation of the sequence. Instead,aspecialsymbol(e.g.,⟨\\s⟩)isaddedtotheendofthesequence,andthecorresponding outputoftheTransformerlayerstackisconsideredastherepresentationoftheentiresequence. Anoutputlayer,suchasalineartransformationlayer,isbuiltontopofthisrepresentationto generatethereward,denotedbyR(seq )orR(x,y ). k k We train this reward model using ranking loss. For example, a pair-wise ranking loss functioncanbewrittenintheform Loss (D ) = −E log(Sigmoid(R (x,y )−R (x,y ))) (8.19) ω r (x,y k1 ,y k2 )∼Dr ω k1 ω k2 whereω representstheparametersoftherewardmodel,andD representsasetoftuplesofan r inputandapairofoutputs. (x,y ,y )∼D isasamplingoperationwhichdrawsasample k1 k2",
    "k k We train this reward model using ranking loss. For example, a pair-wise ranking loss functioncanbewrittenintheform Loss (D ) = −E log(Sigmoid(R (x,y )−R (x,y ))) (8.19) ω r (x,y k1 ,y k2 )∼Dr ω k1 ω k2 whereω representstheparametersoftherewardmodel,andD representsasetoftuplesofan r inputandapairofoutputs. (x,y ,y )∼D isasamplingoperationwhichdrawsasample k1 k2 r (x,y ,y )fromD withsomeprobability. Asanexample,supposewefirstdrawamodel k1 k2 r inputxwithauniformdistributionandthendrawapairofmodeloutputswithaprobabilityof 8.1 A Brief Introduction to LLMs 419 y ≻y givenx(denotedbyPr(y ≻y |x)). Thecorrespondinglossfunctionisgivenby k1 k2 k1 k2 Loss (D ) ω r (cid:88) = − Pr(x)·Pr(y ≻y |x)·log(Sigmoid(R (x,y )−R (x,y ))) k1 k2 ω k1 ω k2 1 (cid:88) = − Pr(y ≻y |x)·log(Sigmoid(R (x,y )−R (x,y ))) (8.20) K k1 k2 ω k1 ω k2 whereK representsthenumberofmodelinputsinvolvedinsampling. Whiletheformofthese functions may seem complex, their idea is simple: we penalize the model if the predicted rankingoftwooutputsdiffersfromthehuman-labeledranking. Bycontrast,themodelreceives abonus,ifthepredictedrankingmatchesthehuman-labeledranking. Wecantraintherewardmodelbyminimizingtheaboverankingloss ωˆ = argminLoss (D ) (8.21) ω r ω The resulting model R (·) can be employed to evaluate any given pair of input and output. ωˆ Notethatalthoughtherewardmodelistrainedusingaranking-basedobjective,itisusedfor scoring. Thisallowsittoprovidecontinuoussupervisionsignals,whichisverybeneficialfor trainingothermodels. Wenowturntothepolicylearningproblem. Acommonlyadoptedobjectiveistomaximize therewardonasetofinput-outputpairs. FollowingananalogousformofEq. (8.16),weobtain asimpletrainingobjectiveforRLfine-tuning θ˜ = argmaxE R (x,y ) (8.22) θˆ+ (x,y θˆ+)∼D rlft ωˆ θˆ+ wheretheoptimalparametersθ˜ areobtainedbyfine-tuningthepre-trainedparametersθˆ . D rlft istheRLfine-tuningdataset. Foreachsample(x,y ),xissampledfromaprepareddataset θˆ+ ofinputsequences,andy issampledfromthedistributionPr (y|x)givenbythepolicy. θˆ+ θˆ+ Inpractice,moreadvancedreinforcementlearningalgorithms,suchasproximalpolicy optimization (PPO), are often used for achieving more stable training, as well as better performance. We leave the detailed discussion of reinforcement learning algorithms to the followingpartsofthisbookwhereRLHFisextensivelyusedforalignment. Aninterestingquestionariseshere: whynotconsiderlearningfromhumanpreferencesas astandardsupervisedlearningproblem? Thisquestioniscloselyrelatedtoouraforementioned discussiononthedifficultyofdataannotation. Often,describinghumanvaluesandgoalsis challenging,anditisevenmoredifficultforhumanstoprovideoutputsthatarewellaligned. Asanalternative,annotatingthepreferencesofagivenlistofmodeloutputsoffersasimpler task. Bydoingso,wecancreateamodelthatunderstandshumanpreferences,whichcanthen be used as a reward model for training policies. From the perspective of machine learning, RLHFisparticularlyusefulforscenarioswherethedesiredbehaviorofanagentisdifficult todemonstratebutcanbeeasilyrecognizedbyhumans. AnotheradvantageofRLHFisits abilitytoexplorethesamplespace. Byemployingsamplingtechniques,modelstrainedwith 420 Chapter 8. Generative Models reinforcementlearningcanventurebeyondtheannotateddatasettoexploreadditionalsamples. ThisexploratoryabilityallowsRLHFtodiscoverpotentiallybeneficialpoliciesthatarenot immediatelyapparentfromthelabeleddataalone. 8.1.5 Prompting LLMs We have so far shown that LLMs can be used to perform various tasks by giving them appropriate prompts. There are no restrictions on these prompts, which can include any informationwewishtoaskorcommunicatewithLLMs,suchasnaturallanguageinstructions andthecontextofconversations. Sincethisapproachrequiresnoadditionaltrainingortuning, adaptingLLMsbecomeshighlyefficientoncetheyaredeveloped. Thissomewhatinfluences theparadigmsinNLP:wenolongerneedtodevelopspecificsystemsforindividualtasksbut can instead use a single, well-trained LLM to perform different tasks by prompting it. An appealing aspect of LLM prompting arises as a result: users can easily have “customized” systems by designing their own prompts for LLMs. Given the important role played by prompting in LLMs, prompting engineering has become a very active area of research in NLP. The term prompt is used in many different ways in the literature. In this chapter, this term refers to the entire input to LLMs, and so we use the terms prompt and model input interchangeably. Beforediscussingpromptingfurther,letusfirstseeafewexampleswhere the prompts are more complex than those presented in the previous subsections. Note that thissubsectionisnotaimedatwritinghigh-qualitypromptsbutratherathighlightingsome interestingissuesinpromptingLLMs. One of the popular ways to use LLMs is to assign them a “role” played in generating responses. Forexample,LLMscanactasapsychologistwhenansweringquestions. Pleaseexplainwhatdelayedgratificationis? Note,youarearesearcherwithadeepbackgroundinpsychology. Yourown fieldofstudyisdevelopmentalpsychologyinchildren,withafocusonaspects ofcontrolandintelligence. Youareveryfamiliarwiththeconceptofdelayed gratification and have applied it to your own research. You have published papersondelayedgratificationandconductedrelatedexperiments. Delayed gratification is the process of resisting an immediate reward in anticipation of receiving a more valuable reward in the future. It is a significantconceptindevelopmentalpsychology,especiallyinrelationtohow children learn self-control and develop cognitive abilities. Research shows that the ability to delay gratification is associated with positive outcomes in academicperformance,socialcompetence,andemotionalwell-being. Studies and experiments in this area highlight the importance",
    "resisting an immediate reward in anticipation of receiving a more valuable reward in the future. It is a significantconceptindevelopmentalpsychology,especiallyinrelationtohow children learn self-control and develop cognitive abilities. Research shows that the ability to delay gratification is associated with positive outcomes in academicperformance,socialcompetence,andemotionalwell-being. Studies and experiments in this area highlight the importance of cultivating this skill earlyinlifetosupportlong-termsuccessandresilience. AnotherexampleistheuseofLLMsindetectingandcorrectingerrorssuchassyntacticor 8.1 A Brief Introduction to LLMs 421 semanticmistakesintext. ForanLLMwhichistrainedonbothcodeandnaturallanguage data,wemayuseitforcodedebugging6. FixthebugsinthisClanguageprogram. #include<stdio.h> intmain(){ printg(\"Hello,World!\") return0; } 1. Thefunctionname’printg’isincorrect. Itshouldbe’printf’. 2. Thereisamissingsemicolonattheendofthe’printf’functioncall. Hereisthecorrectedversionoftheprogram: #include<stdio.h> intmain(){ printf(\"Hello,World!\"); return0; } Also,LLMscanbefine-tunedtoactasaconversationpartner7. Hi! I’manassistant. HowcanIhelpyou? WhowontheFIFAWorldCup2022? ArgentinawontheFIFAWorldCup2022. Wherewasitheld? The2022FIFAWorldCupwasheldinQatar. HowmanytimeshasArgentinawontheWorldCup? ArgentinahaswontheFIFAWorldCupthreetimes. ... Theseexamplesandpreviousoneshaveshownthatappropriateresponsescanbegenerated via prompts involving clear instructions and questions. However, when problem solving requiresknowledgethatisnotexplicitlyspecified,LLMsmaymakemistakes,eventhoughthe instructionsaresufficientlyclearandprecise. AfamilyofchallengingtasksforLLMsinvolves arithmeticreasoningandcommonsensereasoning. Forexample,wecanaskanLLMtosolve 6Inthisexample,thecodeisnottokenizedforeasierreading. 7Tofine-tuneanLLMformulti-turndialogue,oneneedstoconsiderconversationhistoryinthecontextfor predictingtheresponseinthecurrentroundofconversation. Thismakestheactualpromptusedinresponse generationrelativelylongerthanthatusedinsingle-turndialogue. 422 Chapter 8. Generative Models primaryschoolmathproblemspresentedinnaturallanguage. Jackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5 moreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany applesdoesJackhaveleftintheend? Theansweris10. Thecorrectanswershouldbe7,sothemodeloutputisincorrect. One approach to addressing such issues is to incorporate learning into prompts, called in-contextlearningor(ICL).TheideaofICListodemonstratethewaystosolveproblemsin prompts,andconditionpredictionsonthesedemonstrations. Hereisanexamplewhereasimilar problemandthecorrespondinganswerarepresentedintheprompt(green=demonstrations). Tomhas12marbles. Hewins7moremarblesinagamewithhisfriendbut thenloses5marblesthenextday. Hisbrothergiveshimanother3marblesas agift. HowmanymarblesdoesTomhavenow? Theansweris17. Jackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5 moreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany applesdoesJackhaveleftintheend? Theansweris12. ButtheLLMstillmademistakesthistime. Areasonforthismightbethatsolvingmath problems does not only involve problem-answer mappings but also, to a larger extent, the underlyinglogicalinferenceinmultiplesteps. Amethodtoimprovetheinferenceabilitiesof LLMsischain-of-thoughtprompting(COTprompting)[Weietal.,2022c]. InCOTprompt- ing,wedecomposecomplexreasoningproblemsintomultipleproblem-solvingintermediate steps. These steps are demonstrated in prompts so that LLMs can be prompted to learn to mimictheunderlyingthoughtprocessesinsolvingtheproblems. COTpromptinghasbeen foundtobeveryusefulforachievinggoodresultsonchallengingtasks,suchastheGSM8K mathematicalreasoningbenchmark. ConsidertheabovedemonstrationexampleTomhas12marbles. Hewins7moremarbles inagamewithhisfriendbutthenloses5marblesthenextday. Hisbrothergiveshimanother 3 marbles as a gift. How many marbles does Tom have now ?. The corresponding logical reasoningstepscanbestructured: 1. Tomstartswith12marbles. 2. Hewins7moremarblesinagame,soweaddthosetohisinitialcount: 12marbles+7 marbles=19marbles. 3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19 8.1 A Brief Introduction to LLMs 423 marbles-5marbles=14marbles. 4. Hisbrothergiveshim3moremarbles,soweaddthosetohiscurrenttotal: 14marbles+ 3marbles=17marbles. 5. Afteralltheseevents,Tomhas17marbles. Wecanaddthesereasoningstepsintotheprompt,andgetthecorrectanswer. Tomhas12marbles. Hewins7moremarblesinagamewithhisfriendbut thenloses5marblesthenextday. Hisbrothergiveshimanother3marblesas agift. HowmanymarblesdoesTomhavenow? Tominitiallyhas12marbles. Afterwinningagame,hegains7more,increasing his total to 19 marbles. He loses 5 marbles the next day, bringing his total downto14marbles. Hisbrothergiftshim3moremarbles,increasinghistotal to17marbles. Therefore,Tomnowhas17marbles. Sotheansweris17. Jackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5 moreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany applesdoesJackhaveleftintheend? Jack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,leavinghimwith5apples. Hismomgiveshim5moreapples,adding5to his current total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3 apples to his friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore,Jackhas7applesleft. Sotheansweris7. This approach is also called one-shot COT prompting. Likewise, we can add more demonstrationstohaveafew-shotCOTpromptingapproach. Furthermore,aspecialkindofCOT,zero-shotCOT,introducesinstructionsthatprovoke LLMstothink,withoutpresentinganyintermediatereasoningstepsintheprompts[Kojima etal.,2022]. Aninterestingexampleisthatbyappendingasimplesentence“Let’sthinkstep bystep.” totheendofaprompt,LLMscangeneratereasoningstepstoreachthefinalanswer. 424 Chapter 8. Generative Models Jackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5 moreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany applesdoesJackhaveleftintheend? Let’sthinkstepbystep. 1. InitialQuantity: Jackstartswith7apples. 2. AfterDinner: Heeats2apples,so7−2=5applesremain. 3. HisMomGivesMore: Hismomgiveshim5moreapples,so5+5=10 applesarenowwithhim. 4. GivingtoJohn: Thenextday,Jackgives3applestohisfriendJohn, so10−3=7applesareleft. Intheend,Jackhas7applesleft. Zero-shot,one-shot,andfew-shotlearningarecommonconceptsintheareaofin-context learningforLLMsandarenotrestrictedtoCOTprompting. Broadlyspeaking,anyprompting thatinvolvesonlysimpleinstructionswithoutanydemonstrationscanbeconsideredaformof zero-shotlearning. Thiszero-shotlearningabilityemergesasLLMsarepre-trainedand/or fine-tuned. Also, one-shot and few-shot learning methods are more often considered when LLMsdonotacquirethecorrespondingzero-shotlearningability. Thesemethodsaretherefore important for in-context learning when addressing new tasks. Examples include those for performingvariousNLPtasksbydemonstratingtask-formattedsamples. Seethefollowing examplesforsentimentsentenceclassificationandphrasetranslationviafew-shotlearning. Giventhefollowingtextsnippets,classifytheirsentimentasPositive,Negative, orNeutral. Example1: “Ihadanamazingdayatthepark!” Sentiment: Positive Example2: “Theserviceattherestaurantwasterrible.” Sentiment: Negative Example3: “Ithinkit’sgoingtoraintoday.” Sentiment: Neutral Text: “Thismoviewasafantasticjourneythroughimagination.” Sentiment: Positive 8.2 Training at Scale 425 TranslatethefollowingChinesephrasesintoEnglish. Example1: “你好” Translation: “Hello” Example2: “谢谢你” Translation: “Thankyou” Phrasetotranslate: “早上好” Translation: “GoodMorning” Above, we have presented examples to illustrate the fundamental in-context learning capabilities of prompting LLMs. This section, however, does not include more advanced promptingtechniquesinordertokeepthecontentconciseandcompact. Morediscussionson promptingcanbefoundinChapter9. 8.2 Training at Scale AsafirststepindevelopingLLMs,weneedtotrainthesemodelsonlargeamountsofdata. Thetrainingtaskisitselfstandard: theobjectiveistomaximizethelikelihood,whichcanbe achievedviagradientdescent. However,aswescaleupboththemodelsizeandtheamountof data,theproblembecomesverychallenging,forexample,largemodelsgenerallymakethe trainingunstable. Inthissection,wediscussseveralissuesoflarge-scaletrainingforLLMs, includingdatapreparation,modelmodification,anddistributedtraining. Wealsodiscussthe scalinglawsforLLMs,whichhelpusunderstandtheirtrainingefficiencyandeffectiveness. 8.2.1 Data Preparation TheimportanceofdatacannotbeoverstatedinNLP.Aslargerneuralnetworksaredeveloped, thedemandfordatacontinuestoincrease.",
    "Phrasetotranslate: “早上好” Translation: “GoodMorning” Above, we have presented examples to illustrate the fundamental in-context learning capabilities of prompting LLMs. This section, however, does not include more advanced promptingtechniquesinordertokeepthecontentconciseandcompact. Morediscussionson promptingcanbefoundinChapter9. 8.2 Training at Scale AsafirststepindevelopingLLMs,weneedtotrainthesemodelsonlargeamountsofdata. Thetrainingtaskisitselfstandard: theobjectiveistomaximizethelikelihood,whichcanbe achievedviagradientdescent. However,aswescaleupboththemodelsizeandtheamountof data,theproblembecomesverychallenging,forexample,largemodelsgenerallymakethe trainingunstable. Inthissection,wediscussseveralissuesoflarge-scaletrainingforLLMs, includingdatapreparation,modelmodification,anddistributedtraining. Wealsodiscussthe scalinglawsforLLMs,whichhelpusunderstandtheirtrainingefficiencyandeffectiveness. 8.2.1 Data Preparation TheimportanceofdatacannotbeoverstatedinNLP.Aslargerneuralnetworksaredeveloped, thedemandfordatacontinuestoincrease. Forexample,developingLLMsmayrequiretrillions oftokensinpre-training(seeTable8.3),ordersofmagnitudelargerthanthoseusedintraining conventionalNLPmodels. Ingeneral,wemaywanttogatherasmuchtrainingdataaspossible. However,largertrainingdatasetsdonotmeanbettertrainingresults,andthedevelopmentof LLMsraisesnewissuesincreatingorcollectingthesedatasets. A first issue is the quality of data. High-quality data has long been seen as crucial for training data-driven NLP systems. Directly using raw text from various sources is in generalundesirable. Forexample,asignificantportionofthedatausedtotrainrecentLLMs comesfromwebscraping,whichmaycontainerrorsandinappropriatecontent,suchastoxic informationandfabricatedfacts. Also,theinternetisfloodedwithmachine-generatedcontent duetothewidespreaduseofAI,presentingfurtherchallengesforprocessingandusingweb- scrapeddata. ResearchershavefoundthattrainingLLMsonunfiltereddataisharmful[Raffel etal.,2020]. Improvingdataqualitytypicallyinvolvesincorporatingfilteringandcleaning stepsinthedataprocessingworkflow. Forexample,Penedoetal.[2023]showthatbyadopting anumberofdataprocessingtechniques,90%oftheirweb-scrapeddatacanberemovedfor 426 Chapter 8. Generative Models LLM #ofTokens Data GPT3-175B[Brownetal.,2020] 0.5T Webpages,Books,Wikipedia Falcon-180B[Almazroueietal.,2023] 3.5T Webpages,Books,Conversations, Code,TechnicalArticles LLaMA2-65B[Touvronetal.,2023a] 1.0T∼1.4T Webpages,Code,Wikipedia, Books,Papers,Q&As PaLM-450B[Chowdheryetal.,2022] 0.78T Webpages,Books,Conversations, Code,Wikipedia,News Gemma-7B[GemmaTeam,2024] 6T Webpages,Mathematics,Code Table8.3: AmountsoftrainingdatausedinsomeLLMsintermsofthenumberoftokens. LLMtraining. Inadditiontolarge-scaleweb-scrapeddata,LLMtrainingdataoftenincludes books,papers,user-generateddataonsocialmedia,andsoon. MostofthelatestLLMsare trainedonsuchcombineddatasets,whicharefoundtobeimportantforthestrongperformance oftheresultingmodels. Asecondissueisthediversityofdata. Wewantthetrainingdatatocoverasmanytypes ofdataaspossible,sothatthetrainedmodelscanadapttodifferentdownstreamtaskseasily. It has been widely recognized that the quality and diversity of training data both play very importantrolesinLLMs. Aninterestingexampleisthatincorporatingprogrammingcodeinto training data has been found to be beneficial for LLMs. The benefits are demonstrated not only inenhancing theprogramming abilitiesof LLMs, butalso inimprovingreasoning for complexproblems,especiallythoserequiringCOTprompting. Theconcept“diversity”can beextendedtoincludelanguagediversityaswell. Forexample,manyLLMsaretrainedon multi-lingualdata,andthereforewecanhandlemultiplelanguagesusingasinglemodel. While thisapproachshowsstrongabilitiesinmulti-lingualandcross-lingualtasks,itsperformanceon specificlanguageslargelydependsonthevolumeandqualityofthedataforthoselanguages. Ithasbeenshowninsomecasestoprovidepoorresultsforlow-resourcelanguages. A third issue is the bias in training data. This is not a problem that is specific to LLMs but exists in many NLP systems. A common example is gender bias, where LLMs show a preference for one gender over another. This can partly be attributed to class imbalance in the training data, for example, the term nurses is more often associated with women. In ordertodebiasthedata,itiscommonpracticetobalancethecategoriesofdifferentlanguage phenomena, such as gender, ethnicity, and dialects. The bias in data is also related to the diversityissuementionedabove. Forexample,sincemanyLLMsaretrainedandalignedwith English-centricdata, theyarebiasedtowardstheculturalvaluesandperspectivesprevalent among English-speaking populations. Increasing language diversity in training data can somewhatmitigatethebias. Anotherissuewithcollectinglarge-scaledataistheprivacyconcern. IfLLMsaretrained on data from extensive sources, this potentially leads to risks regarding the exposure of sensitive information, such as intellectual property and personal data. This is particularly 8.2 Training at Scale 427 concerninggiventhe capacityofLLMsto representpatternsfrom thedatathey aretrained on,whichmightinadvertentlyinvolvememorizingandreproducingspecificdetails. Asimple approachtoprivacyprotectionistoremoveoranonymizesensitiveinformation. Forexample, anonymizationtechniquescanbeappliedtoremovepersonallyidentifiableinformationfrom trainingdatatopreventLLMsfromlearningfromsuchdata. However,inpractice,erasingor redactingallsensitivedataisdifficult. Therefore,manyLLMs,particularlythoselaunchedfor publicservice,typicallyworkwithsystemsthatcandetectthepotentialexposureofsensitive data,orarefine-tunedtorejectcertainrequeststhatcouldleadtoinformationleakage. 8.2.2 Model Modifications Training LLMs is difficult. A commonly encountered problem is that the training process becomesmoreunstableasLLMsgetbigger. Forexample,oneneedstochooseasmalllearning rate to achieve stable training with gradient descent, but this in turn results in much longer trainingtimes. Sometimes,evenwhenthetrainingconfigurationiscarefullydesigned,training maydivergeatcertainpointsduringoptimization. ThetrainingofLLMsisgenerallyinfluenced bymanyfactors,suchasparameterinitialization,batching,andregularization. Here,wefocus oncommonmodificationsandimprovementstothestandardTransformerarchitecture,which areconsideredimportantindevelopingtrainableLLMs. 1. Layer Normalization with Residual Connections Layernormalizationisusedtostabilizetrainingfordeepneuralnetworks. Itisaprocessof subtractingthemeananddividingbythestandarddeviation. Bynormalizinglayeroutputin thisway,wecaneffectivelyreducethecovariateshiftproblemandimprovethetrainingstability. InTransformers,layernormalizationistypicallyusedtogetherwithresidualconnections. As describedinSection8.1.1,asub-layercanbebasedoneitherthepost-normarchitecture,in whichlayernormalizationisperformedrightafteraresidualblock,orthepre-normarchitecture, in which layer normalization is performed inside a residual block. While both of these architecturesarewidelyusedinTransformer-basedsystems[Wangetal.,2019a],thepre-norm architecturehasproventobeespeciallyusefulintrainingdeepTransformers. Giventhis,most LLMsarebasedonthepre-normarchitecture,expressedasoutput=LNorm(F(input))+ input. Awidely-usedformofthelayernormalizationfunctionisgivenby h−µ LNorm(h) = α· +β (8.23) σ+ϵ wherehisad-dimensionalreal-valuedvector,µisthemeanofalltheentriesofh,andσisthe correspondingstandarddeviation. ϵisintroducedforthesakeofnumericalstability. α∈Rd andβ∈Rd arethegainandbiasterms. Avariantoflayernormalization,calledrootmeansquare(RMS)layernormalization,only re-scalestheinputvectorbutdoesnotre-centerit[ZhangandSennrich,2019]. TheRMSlayer 428 Chapter 8. Generative Models normalizationfunctionisgivenby h LNorm(h) = α· +β (8.24) σ +ϵ rms whereσ rms istherootmeansquareofh,thatis,σ rms =( d 1(cid:80)d k=1 h2 k )2 1 . Thislayernormaliza- tionfunctionisusedinLLMsliketheLLaMAseries. 2. Activation Functions in FFNs InTransformers,FFNsub-layersaredesignedtointroducenon-linearitiesintorepresentation learning,andarefoundtobeusefulforpreventingtherepresentationslearnedbyself-attention fromdegeneration8 [Dongetal.,2021]. AstandardformoftheFFNsusedinthesesub-layers canbeexpressedas FFN(h) = σ(hW +b )W +b (8.25) h h f f whereW h ∈Rd×d h,b h ∈Rd h,W f ∈Rd h ×d,andb f ∈Rd aretheparameters,andd h isthe hiddensize. σ(·)istheactivationfunctionofthehiddenlayer. Acommonchoiceforσ(·)is therectifiedlinearunit(ReLU),givenby σ (h) = max(0,h) (8.26) relu In practical implementations, increasing d is helpful and thus",
    "fromdegeneration8 [Dongetal.,2021]. AstandardformoftheFFNsusedinthesesub-layers canbeexpressedas FFN(h) = σ(hW +b )W +b (8.25) h h f f whereW h ∈Rd×d h,b h ∈Rd h,W f ∈Rd h ×d,andb f ∈Rd aretheparameters,andd h isthe hiddensize. σ(·)istheactivationfunctionofthehiddenlayer. Acommonchoiceforσ(·)is therectifiedlinearunit(ReLU),givenby σ (h) = max(0,h) (8.26) relu In practical implementations, increasing d is helpful and thus it is often set to a larger h numberinLLMs. Butaverylargehiddensizeposeschallengesforbothtraininganddeploy- ment. In this case, the design of the activation function plays a relatively more important roleinwideFFNs. ThereareseveralalternativestotheReLUinLLMs. Oneoftheseisthe gaussianerrorlinearunit(GeLU)whichcanbeseenasasmoothedversionoftheReLU. Ratherthancontrollingtheoutputbythesignoftheinput,theGeLUfunctionweightsitsinput bythepercentilePr(h≤h). Herehisad-dimensionalvectorwhoseentriesaredrawnfrom thestandardnormaldistributionGaussian(0,1)9. Specifically,theGeLUfunctionisdefined tobe σ (h) = hPr(h≤h) gelu = hΦ(h) (8.27) where Φ(h) is the cumulative distribution function of Gaussian(0,1), which can be imple- mented in convenient ways [Hendrycks and Gimpel, 2016]. The GeLU function has been adoptedinseveralLLMs,suchasBERT,GPT-3,andBLOOM. Another family of activation functions which is popular in LLMs is gated linear unit 8Heredegenerationreferstothephenomenoninwhichtherankofamatrixisreducedaftersomeprocessing. 9Pr(h≤h)isaninformalnotation. Itreferstoavector,witheachentryrepresentingthepercentileforthe correspondingentryofh. 8.2 Training at Scale 429 (GLU)-basedfunctions. ThebasicformofGLUsisgivenby σ (h) = σ(hW +b )⊙(W +b ) (8.28) glu 1 1 2 2 where W ∈Rd×d, b ∈Rd, W ∈Rd×d, and b ∈Rd are model parameters. Different 1 1 2 2 choicesofσ(·)resultindifferentversionsofGLUfunctions. Forexample,ifσ(·)isdefinedto betheGeLUfunction,wewillhavetheGeGLUfunction σ (h) = σ (hW +b )⊙(W +b ) (8.29) geglu gelu 1 1 2 2 ThisactivationfunctionhasbeensuccessfullyappliedinLLMslikeGemma. Asanotherexample,considerσ(·)tobetheSwishfunctionσ (h)=h⊙Sigmoid(ch) swish [Ramachandranetal.,2017]. Then,theSwiGLUfunctionisgivenby σ (h) = σ (hW +b )⊙(W +b ) (8.30) swiglu swish 1 1 2 2 BoththePaLMandLLaMAseriesarebasedontheSwiGLUfunction. Formorediscussions ofGLUs,thereadercanrefertoShazeer[2020]’swork. 3. Removing Bias Terms Anotherpopularmodeldesignistoremovethebiastermsinaffinetransformationsusedin LLMs. Thistreatmentcanbeappliedtolayernormalization,transformationsoftheinputsto QKVattention,andFFNs. Forexample,wecanmodifyEq. (8.25)toobtainanFFNwithno biasterms FFN(h) = σ(hW )W (8.31) h f Chowdheryetal.[2022]reportthatremovingbiastermshelpsimprovethetrainingstability ofLLMs. ThismethodhasbeenusedinseveralrecentLLMs,suchasLLaMAandGemma. 4. Other Issues ManyLLMsalsoinvolvemodificationstotheirpositionalembeddingmodels. Forexample, onecanreplacesinusoidalpositionalencodingswithrotarypositionembeddingssothatthe learnedLLMscanhandlelongsequencesbetter. ThesemodelswillbediscussedinSection 8.3. NotethatwhilemodelmodificationsarecommonintrainingLLMs,thestabilityoftraining canbeimprovedinmanydifferentways. Forexample,increasingthebatchsizeasthetraining proceedshasbeenfoundtobeusefulforsomeLLMs. Ingeneral,achievingstableandefficient large-scale LLM training requires carefully designed setups, including learning schedules, optimizerchoices,trainingparallelism,mixedprecisiontraining,andsoon. Someofthese issues are highly engineered, and therefore, we typically need a number of training runs to obtainsatisfactoryLLMs. 430 Chapter 8. Generative Models 8.2.3 Distributed Training TrainingLLMsrequiressignificantamountsofcomputationalresources. Acommonapproach toimprovingtrainingefficiencyistouselarge-scaledistributedsystems. Fortunately,alongside the rise of neural networks in AI, deep learning-oriented software and hardware have been developed,makingiteasiertoimplementLLMsandperformcomputations. Forexample,one can now easily fine-tune an LLM using deep learning software frameworks and a machine with multiple GPUs. However, scaling up the training of LLMs is still challenging, and requiressignificanteffortsindevelopinghardwareandsoftwaresystemsforstableandefficient distributedtraining. Animportantconsiderationofdistributedtrainingisparallelism. Thereareseveralforms ofparallelism: dataparallelism,modelparallelism,tensorparallelism,andpipelineparallelism. Despitedifferentwaystodistributecomputationsacrossdevices,theseparallelismmethods are based on a similar idea: the training problem can be divided into smaller tasks that can beexecutedsimultaneously. TheissueofparallelismintrainingLLMshasbeenextensively studied[Narayananetal.,2021;Fedusetal.,2022b]. Herewesketchthebasicconcepts. • DataParallelism. Thismethodisoneofthemostwidelyusedparallelismmethodsfor trainingneuralnetworks. Toillustrate,considerthesimplestcasewherethestandard deltaruleisusedingradientdescent ∂L (D ) θ = θ −lr· θt mini (8.32) t+1 t ∂θ t wherethenewparametersθ isobtainedbyupdatingthelatestparametersθ witha t+1 t smallsteplr inthedirectionofthenegativelossgradient. ∂L θt (Dmini) isthegradientof ∂θt thelosswithrespecttotheparametersθ ,andiscomputedonaminibatchoftraining t sampleD . Indataparallelism,wedivideD intoN smallerbatches,denotedby mini mini {D1,...,DN}. Then,wedistributethesebatchestoN workers,eachwithacorrespond- ingbatch. Oncethedataisdistributed,theseworkerscanworkatthesametime. The gradientoftheentireminibatchisobtainedbyaggregatingthegradientscomputedby theworkers,likethis ∂L (D ) ∂L (D1) ∂L (D2) ∂L (DN) θt mini = θt + θt +···+ θt (8.33) ∂θ ∂θ ∂θ ∂θ t t t t (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) worker1 worker2 workerN Inidealcaseswheretheworkerscoordinatewellandthecommunicationoverheadis small,dataparallelismcanachievenearlyanN-foldspeed-upfortraining. • ModelParallelism. Althoughdataparallelismissimpleandeffective,itrequireseach workertoruntheentireLLMandperformthecompleteforwardandbackwardprocess. AsLLMsgrowlarger,itsometimesbecomesunfeasibletoloadandexecuteanLLM on a single device. In this case, we can decouple the LLM into smaller components andrunthesecomponentsondifferentdevices. Onesimplewaytodothisistogroup consecutivelayersinthelayerstackandassigneachgrouptoaworker. Theworkers 8.2 Training at Scale 431 operate in the order of the layers in the stack, that is, in the forward pass we process theinputfromlower-leveltoupper-levellayers,andinthebackwardpasswepropagate the error gradients from upper-level to",
    "AsLLMsgrowlarger,itsometimesbecomesunfeasibletoloadandexecuteanLLM on a single device. In this case, we can decouple the LLM into smaller components andrunthesecomponentsondifferentdevices. Onesimplewaytodothisistogroup consecutivelayersinthelayerstackandassigneachgrouptoaworker. Theworkers 8.2 Training at Scale 431 operate in the order of the layers in the stack, that is, in the forward pass we process theinputfromlower-leveltoupper-levellayers,andinthebackwardpasswepropagate the error gradients from upper-level to lower-level layers. Consider, for example, a TransformerdecoderwithLstackedblocks. Todistributethecomputationload,each block is assigned to a worker. See the following illustration for a single run of the forwardandbackwardpassesofthismodel. WorkerL B (↑) B (↓) L L ... ... ... Worker2 B (↑) B (↓) 2 2 Worker1 B (↑) B (↓) 1 1 HereB denotesthecomputationofblockl,andthesymbols↑and↓denotetheforward l andbackwardpasses,respectively. Notethatthisparallelismmethodforcestheworkers toruninsequence,soaworkerhastowaitforthepreviousworkertofinishtheirjob. Thisresultsinthedevicesbeingidleformostofthetime. Inpracticalsystems,model parallelismisgenerallyusedtogetherwithotherparallelismmechanismstomaximize theuseofdevices. • Tensor Parallelism. Parallelism can also be performed in a single computation step. Acommonexampleissplittingalargeparametermatrixintochunks, multiplyingan inputtensorwitheachofthesechunksseparately,andthenconcatenatingtheresultsof these multiplications to form the output. For example, consider the multiplication of therepresentationh∈Rd withtheparametermatrixW h ∈Rd×d h inanFFNsub-layer (seeEq. (8.25)). WecanslicethematrixW h ∈Rd×d h verticallytoasequenceofM sub-matrices (cid:104) (cid:105) W = W1 W2 ... WM (8.34) h h h h whereeachsub-matrixWk hasashapeofd×d h. ThemultiplicationofhwithW can h M h beexpressedas (cid:104) (cid:105) hW = h W1 W2 ... WM h h h h (cid:104) (cid:105) = hW1 hW2 ... hWM (8.35) h h h Wecanperformmatrixmultiplications{hW1,hW2,...,hWM}onM devicessepa- h h h rately. Asaresult,wedistributealargematrixmultiplicationacrossmultipledevices, eachofwhichmayhaverelativelysmallmemory. Fromtheperspectiveofthedesignof modernGPUs,tensorparallelismoverGPUsprovidesatwo-level,tile-basedapproach to parallel computing. First, at a higher level, we decompose a matrix multiplication intosub-matrixmultiplicationsthatcandirectlyfitintothememoryofGPUs. Then,at 432 Chapter 8. Generative Models a lower level, we execute these sub-matrix multiplications on GPUs using tile-based parallelalgorithmsthatarespecificallyoptimizedforGPUs. • Pipeline Parallelism. Above, in model parallelism, we have described a simple ap- proach to spreading groups of model components across multiple devices. But this methodisinefficientbecauseonlyonedeviceisactivatedatatimeduringprocessing. Pipelineparallelismaddressesthisissuebyintroducingoverlapsbetweencomputations on different devices [Harlap et al., 2018; Huang et al., 2019]. To do this, a batch of samplesisdividedintoanumberofmicro-batches, andthenthesemicro-batchesare processedbyeachworkerasusual. Onceamicro-batchisprocessedbyaworkerand passedtothenextone,thefollowingmicro-batchimmediatelyoccupiesthesameworker. Inotherwords,wecreateapipelineinwhichdifferentcomputationstepscanoverlapif multiplejobsaregiventothepipeline. Thefollowingshowsanillustrationofpipeline parallelismforprocessing3micro-batches. WorkerL BL,1 BL,2 BL,3 BL,1 BL,2 BL,3 ... ... ... Worker2 B2,1 B2,2 B2,3 B2,1 B2,2 B2,3 Worker1 B1,1 B1,2 B1,3 B1,1 B1,2 B1,3 HereB representstheprocessingofthek-thmicro-batchbythel-thworker. Ideally l,k wewouldlike tomaximize thenumber ofmicro-batches, and thusminimize theidle timeoftheworkers. However,inpractice,usingsmallmicro-batchesoftenreducesGPU utilizationandincreasestask-switchingcosts. Thismay,inturn,decreasetheoverall systemthroughput. Theultimategoalofparallelprocessingistoachievelineargrowthinefficiency,thatis,the numberofsamplesthatcanbeprocessedperunitoftimeincreaseslinearlywiththenumber ofdevices. However,distributedtrainingiscomplicated,andinfluencedbymanyfactorsin additiontotheparallelismmethodwechoose. Oneproblem,whichisoftenassociatedwith distributedsystems,isthecostofcommunication. Wecanthinkofadistributedsystemasa groupofnetworkednodes. Eachofthesenodescanperformlocalcomputationorpassdata to other nodes. If there are a large number of such nodes, it will be expensive to distribute andcollectdataacrossthem. Sometimes,thetimesavingsbroughtaboutbyparallelismare offsetbythecommunicationoverheadofalargenetwork. Anotherproblemwithlarge-scale distributedsystemsisthatthesynchronizationofnodesintroducesadditionalcosts. Asisoften the case, some nodes may take longer to work, causing others to wait for the slowest ones. Whilewecanuseasynchronoustrainingtohandleheterogeneityincomputationalresources, thismayleadtostalegradientsandnon-guaranteedconvergence. Moreover,asmorenodes areaddedtothenetwork,thereismorechancetohavecrashednodesduringtraining. Inthis case,weneedtoensurethatthewholesystemisfaulttolerant. Inmanypracticalsettings,to 8.2 Training at Scale 433 increasescalability,oneneedstotakeintoaccountadditionalissues,includingarchitecture design,datatransferandcomputationoverlap,loadbalancing,memorybandwidthandsoon. Training LLMs is so computationally expensive that, even though distributed training is already in use, researchers and engineers often still employ various model compression andspeed-upmethodstoimprovetrainingefficiency[Weng,2021]. Oneexampleismixed precisiontraining,inwhichlowprecisiondata(suchasFP16andFP8data)isusedforgradient computationoneachindividualnode,andsingleordoubleprecisiondata(suchasFP32/FP64 data) is used for updating the model [Micikevicius et al., 2018]. A key operation in this approachisgradientaccumulationwheregradientsneedtobeaccumulatedandsynchronized acrossnodes. However,duetothenon-associativityoffloating-pointaddition,thiscanleadto slightnumericaldifferencesinaccumulatedgradientsondifferentnodes,whichmayaffect modelconvergenceandfinalperformance. Thisproblemismoreobviousiftherearealarge numberofnodesinvolvedindistributedtraining,especiallygiventhatlow-precisionnumerical computationsmayencounteroverflowandunderflowissues,aswellasinconsistenciesacross different hardware devices. Therefore, the design of distributed systems needs to consider thesenumericalcomputationissuestoensuresatisfactoryresultsandconvergence. 8.2.4 Scaling Laws ThesuccessofLLMsrevealsthattraininglargerlanguagemodelsusingmoreresourcescan lead to improved model performance. Researchers have explained this as scaling laws of LLMs. Morespecifically,scalinglawsdescribetherelationshipsbetweentheperformanceof LLMsandtheattributesofLLMtraining,suchasthemodelsize,theamountofcomputation usedfortraining,andtheamountoftrainingdata. Forexample,Hestnessetal.[2017]show thattheperformanceofdeepneuralnetworksisapower-law-likefunctionofthetrainingdata size. Inthebeginning,whentheamountoftrainingdataisnotlarge,theperformanceofthe modelimprovesslowly. Afterward,whenmoretrainingdataisused,themodelentersaphase ofrapidperformanceimprovement,andtheperformancecurveresemblesapower-lawcurve. Ultimately,theimprovementinperformancebecomesslowagain,andmoredatadoesnotlead tosignificantgains. Figure8.3showsanexampleofsuchcurves. InNLP,atraditionalviewholdsthattheperformancegainswilldisappearatacertainpoint asthetrainingisscaledup. However,recentresultsshowthat,ifweconsidertheproblemona largerscale,scalinguptrainingisstillaveryeffectivemethodforobtainingstrongerLLMs. For example, both closed-source and open-source LLMs can benefit from more data, even thoughtrillionsoftokenshavealreadybeenusedfortraining.",
    "thesenumericalcomputationissuestoensuresatisfactoryresultsandconvergence. 8.2.4 Scaling Laws ThesuccessofLLMsrevealsthattraininglargerlanguagemodelsusingmoreresourcescan lead to improved model performance. Researchers have explained this as scaling laws of LLMs. Morespecifically,scalinglawsdescribetherelationshipsbetweentheperformanceof LLMsandtheattributesofLLMtraining,suchasthemodelsize,theamountofcomputation usedfortraining,andtheamountoftrainingdata. Forexample,Hestnessetal.[2017]show thattheperformanceofdeepneuralnetworksisapower-law-likefunctionofthetrainingdata size. Inthebeginning,whentheamountoftrainingdataisnotlarge,theperformanceofthe modelimprovesslowly. Afterward,whenmoretrainingdataisused,themodelentersaphase ofrapidperformanceimprovement,andtheperformancecurveresemblesapower-lawcurve. Ultimately,theimprovementinperformancebecomesslowagain,andmoredatadoesnotlead tosignificantgains. Figure8.3showsanexampleofsuchcurves. InNLP,atraditionalviewholdsthattheperformancegainswilldisappearatacertainpoint asthetrainingisscaledup. However,recentresultsshowthat,ifweconsidertheproblemona largerscale,scalinguptrainingisstillaveryeffectivemethodforobtainingstrongerLLMs. For example, both closed-source and open-source LLMs can benefit from more data, even thoughtrillionsoftokenshavealreadybeenusedfortraining. Withtheincreaseinthescaleofmodeltraining,LLMsexhibitnewcapabilities,knownas theemergentabilitiesofLLMs. Forexample,Weietal.[2022b]studiedthescalingproperties ofLLMsacrossdifferentmodelsizesandamountsofcomputationalresources. Theirwork showsthatsomeabilitiesemergewhenwescalethemodelsizetocertainlevel. Theappearance ofemergentabilitieshasdemonstratedtheroleofscaledtraininginenhancingtheperformance of LLMs, and it has also, to some extent, motivated researchers to continuously attempt to trainlargermodels. AslargerandstrongerLMscontinuetoappear,ourunderstandingofthe scaling laws continues to mature. This helps researchers predict the performance of LLMs 434 Chapter 8. Generative Models SlowReduction Power-lawReduction Convergence Phase Phase Phase (IrreducibleError) TrainingDatasetSize(Log-scale) )elacs-goL(srorrEtseTforebmuN Figure8.3: Ascalinglawoftesterroragainstavariableofinterest(e.g.,trainingdatasetsize) [Hestnessetal.,2017]. Thecurveofthescalinglawcanbedividedintothreephases. Atthe beginning, the number of test errors decreases slowly when more training data is used, but this only lasts for a short period. In the second phase, the number of test errors decreases drastically, andthecurvebecomesapowerlawcurve. Afterthat, theerrorreductionslows downagaininthethirdphase. Notethatthereareirreducibleerrorsthatcannotbeeliminated, regardlessoftheamountoftrainingdata. duringtrainingandestimatetheminimalcomputationalresourcesrequiredtoachieveagiven levelofperformance. To understand how model performance scales with various factors considered during training,itiscommontoexpressthemodelperformanceasafunctionofthesefactors. For example,inthesimplestcase,wecanexpressthelossorerrorofanLLMasafunctionofa singlevariableofinterest. However,therearenouniversalscalinglawsthatcandescribethis relationship. Instead,differentfunctionsareproposedtofitthelearningcurvesofLLMs. Letxbethevariableofinterest(suchasthenumberofmodelparameters)andL(x)bethe lossofthemodelgivenx(suchasthecross-entropylossontestdata). Thesimplestformof L(x)isapowerlaw L(x) = axb (8.36) whereaandbareparametersthatareestimatedempirically. Despiteitssimplicity,thisfunction hassuccessfullyinterpretedthescalingabilityoflanguagemodelsandmachinetranslation systems in terms of model size (denoted by N) and training dataset size (denoted by D) [Gordon et al., 2021; Hestness et al., 2017]. For example, Kaplan et al. [2020] found that theperformanceoftheirlanguagemodelimprovesasapowerlawofeitherN orD afteran initialtransientperiod,andexpressedtheserelationshipsusingL(N)= (cid:0) N (cid:1)−0.076 and 8.8×1013 L(D)= (cid:0) D (cid:1)−0.095 (seeFigure8.4). 5.4×1013 8.2 Training at Scale 435 5.6 4.8 4.0 3.2 2.4 105 107 109 NumberofParameters ssoLtseT L(N)=( N )−0.076 4.2 8.8·1013 3.9 3.6 3.3 3 2.7 108 109 DatasetSize ssoLtseT L(D)=( D )−0.095 5.4·1013 Figure 8.4: Test loss against model size (N) and training dataset size (D) (data points are plotted for illustrative purposes). We plot test loss as a function of N, which is defined as L(N)= (cid:0) N (cid:1)−0.076 ,andafunctionofD,whichisdefinedasL(D)= (cid:0) D (cid:1)−0.095 8.8×1013 5.4×1013 [Kaplanetal.,2020]. Animprovementtothisscalinglawistoaddanirreducibleerrortermtothepowerlaw. TheformofL(x)isthengivenby L(x) = axb+ϵ (8.37) ∞ whereϵ istheirreducibleerrorthataccountsfortheerrorduetounknownvariables,whichis ∞ presentevenasx→∞. Eq. (8.37)isoneofthemostwidelyusedformsfordesigningscaling lawsofLLMs. Forexample,Rosenfeldetal.[2020]developedascalinglawthatinvolvesboth modelscalinganddatasetscaling,likethis L(N,D) = aNb+cDd+ϵ (8.38) ∞ AnexampleofsuchformulationistheChinchillascalinglaw. Itstatesthatthetestlossper tokenisthesumoftheinverseproportionfunctionsofN andD,withanadditionalirreducible errorterm. Hoffmannetal.[2022]expressthisscalinglawas 406.4 410.7 L(N,D) = + + 1.69 (8.39) N0.34 D0.28 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) irreducibleerror modelscaling datasetscaling Allthescalinglawsmentionedabovearebasedonmonotonicfunctions. Sotheycannot coverfunctionswithinflectionpoints,suchasdoubledescentcurves. Inresponse,researchers have explored more sophisticated functions to fit the learning curves. Examples of such functionscanbefoundinAlabdulmohsinetal.[2022]andCaballeroetal.[2023]’swork. ThesignificanceofscalinglawsliesinprovidingdirectionalguidanceforLLMresearch: ifwearestillintheregionofthepowerlawcurve,usingmoreresourcestotrainlargermodels 436 Chapter 8. Generative Models isaverypromisingdirection. Whilethisresult“forces”bigresearchgroupsandcompaniesto investmoreincomputationalresourcestotrainlargermodels,whichisveryexpensive,scaling lawscontinuouslypushtheboundariesofAIfurtheraway. Ontheotherhand,understanding scaling laws helps researchers make decisions in training LLMs. For example, given the computationalresourcesathand,theperformanceofLLMsmaybepredicted. Onelastnoteonscalinglawsinthissection. ForLLMs,alowertestlossdoesnotalways imply better performance on all downstream tasks. To adapt LLMs, there are several steps suchasfine-tuningandpromptingthatmayinfluencethefinalresult. Therefore,thescaling lawsfordifferentdownstreamtasksmightbedifferentinpractice. 8.3 Long Sequence Modeling Wehavealreadyseenthat,inlarge-scaletraining,largerlanguagemodelscanbedeveloped byusingmoredataandcomputationalresources. However,scalingupcanalsooccurinother directions. For instance, in many applications, LLMs are adapted to process significantly long sequences. An interesting example is that we pre-train an LLM on extensive texts of normallengthandthenapplyittodealwithverylongtokensequences,farbeyondthelength encounteredinpre-training. HereweusePr(y|x)todenotethetextgenerationprobability where x is the context and y is the generated text. There are broadly three types of long sequencemodelingproblems. • Textgenerationbasedonlongcontext(i.e.,xisalongsequence). Forexample,we generateashortsummaryforaverylongtext. • Longtextgeneration(i.e.,yisalongsequence). Forexample,wegeneratealongstory basedonafewkeywords. • Longtextgenerationbasedonlongcontext(i.e.,bothxandyarelongsequences). Forexample,wetranslatealongdocumentfromChinesetoEnglish. Recently,NLPresearchershavebeenmoreinterestedinapplyingandevaluatingLLMs on tasks where extremely long input texts are involved. Imagine an LLM, which reads a C++ source file containing tens of thousands of lines, and outlines the functionality of the programcorrespondingtothesourcefile. Suchmodels,capableofhandlingextensivetextual contexts,aresometimescalledlong-contextLLMs.",
    "three types of long sequencemodelingproblems. • Textgenerationbasedonlongcontext(i.e.,xisalongsequence). Forexample,we generateashortsummaryforaverylongtext. • Longtextgeneration(i.e.,yisalongsequence). Forexample,wegeneratealongstory basedonafewkeywords. • Longtextgenerationbasedonlongcontext(i.e.,bothxandyarelongsequences). Forexample,wetranslatealongdocumentfromChinesetoEnglish. Recently,NLPresearchershavebeenmoreinterestedinapplyingandevaluatingLLMs on tasks where extremely long input texts are involved. Imagine an LLM, which reads a C++ source file containing tens of thousands of lines, and outlines the functionality of the programcorrespondingtothesourcefile. Suchmodels,capableofhandlingextensivetextual contexts,aresometimescalledlong-contextLLMs. Inthissectionwewillrestrictourselves tolong-contextLLMs,butthemethodsdiscussedherecanbeapplicabletootherproblems. For Transformers, dealing with long sequences is computationally expensive, as the computationalcostofself-attentiongrowsquadraticallywiththesequencelength. Thismakes itinfeasibletotrainanddeploysuchmodelsforverylonginputs. Twostrandsofresearchhave triedtoadaptTransformerstolong-contextlanguagemodeling. • The first explores efficient training methods and model architectures to learn self- attentionmodelsfromlong-sequencedata. • Theotheradaptspre-trainedLLMstohandlelongsequenceswithmodestornofine- tuningefforts. 8.3 Long Sequence Modeling 437 Here,wewilldiscusstheformerbrieflysinceChapter6extensivelycoversmanymethods inthisstrand. Wewillfocusonthelatter,highlightingpopularmethodsinrecentLLMs. We willalsodiscussthestrengthsandlimitationsoftheselong-sequencemodels. 8.3.1 Optimization from HPC Perspectives WebeginourdiscussionbyconsideringimprovementstostandardTransformermodelsfrom the perspectives of high-performance computing. Most of these improvements, though not specificallydesignedforLLMs,havebeenwidelyappliedacrossvariousdeeplearningmodels [Kimetal.,2023]. Acommonlyusedapproachistoadoptalow-precisionimplementationof Transformers. Forexample,wecanuse8-bitor16-bitfixed-pointdatatypesforarithmetic operations,insteadof32-bitor64-bitfloating-pointdatatypes. Usingtheselow-precisiondata types can increase the efficiency and memory throughput, so that longer sequences can be processedmoreeasily. AnalternativeapproachistoimproveTransformersbyusinghardware- aware techniques. For example, on modern GPUs, the efficiency of Transformers can be improvedbyusingIO-awareimplementationsoftheself-attentionfunction[Daoetal.,2022; Kwonetal.,2023]. Anotherwaytohandlelongsequencesisthroughsequenceparallelism[Lietal.,2023b; Korthikantietal.,2023]. Specifically,considerthegeneralproblemofattendingthequeryq i at the position i to the keys K and values V. We can divide K by rows and obtain a set of sub-matrices{K[1],...,K[nu]},eachcorrespondingtoasegmentofthesequence. Similarly, wecanobtainthesub-matricesofV,denotedby{V[1],...,V[nu]}. Then,weassigneachpair ofK[u] andV[u] toacomputingnode(e.g.,aGPUofaGPUcluster). Theassignednodescan runinparallel,therebyparallelizingtheattentionoperation. Recallthattheoutputoftheself-attentionmodelcanbewrittenas m−1 (cid:88) Att (q ,K,V) = α v (8.40) qkv i i,j j j=0 whereα istheattentionweightbetweenpositionsiandj. InTransformers,α isobtained i,j i,j bynormalizingtherescaledversionofthedotproductbetweenq andk . Letβ denotethe i j i,j attentionscorebetweenq andk . Wehave i j q ·k i j β = √ +Mask(i,j) (8.41) i,j d whereMask(i,j)isthemaskingvariablefor(i,j). Then,wedefinetheattentionweightα i,j tobe α = Softmax(β ) i,j i,j exp(β ) i,j = (8.42) (cid:80) exp(β ) j′ i,j′ 438 Chapter 8. Generative Models On each computing node, we need to implement these equations. Given the keys and valuesassignedtothisnode,computingthenumeratoroftheright-handsideofEq. (8.42)(i.e., exp(β ))isstraightforward,asalltherequiredinformationisstoredonthenode. However, i,j computingthedenominatoroftheright-handsideofEq. (8.42)involvesasumofexp(β ) i,j′ overallj′s,whichrequirestransferringdatatoandfromothernodes. Toillustrate,suppose thatv andk areplacedonnodeu. WecanrewriteEq. (8.42)as j j α i,j nodeu (cid:122) (cid:125)(cid:124) (cid:123) exp(β ) i,j = (8.43) (cid:88) (cid:88) (cid:88) exp(β )+···+ exp(β )+···+ exp(β ) i,j′ i,j′ i,j′ k j′∈K[1] k j′∈K[u] k j′∈K[nu] (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) node1 nodeu nodenu wherethenotationk ∈K[u] representsthatk isarowvectorofK[u]. Inastraightforward j′ j′ (cid:80) implementation, we first perform the summations { exp(β )} separately on the k j′∈K[u] i,j′ corresponding nodes. Then, we collect these summation results from different nodes to combinethemintoafinalresult. Thiscorrespondstoacollectiveoperationinthecontextof parallelprocessing. Therearemanyefficientimplementationsofsuchoperations,suchasthe all-reducealgorithms. Hencethesumofallexp(β )valuescanbecomputedusingoptimized i,j routinesincollectivecommunicationtoolkits. Giventheattentionweights{α },wethencomputetheattentionresultsusingEq. (8.40). i,j Theproblemcanbere-expressedas Att (q ,K,V) qkv i (cid:88) (cid:88) (cid:88) = α v +···+ α v +···+ α v (8.44) i,j′ j′ i,j′ j′ i,j′ j′ v j′∈V[1] v j′∈V[u] v j′∈V[nu] (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) node1 nodeu nodenu LikeEq. (8.43),Eq. (8.44)canbeimplementedasasummationprograminparallelpro- cessing. First,performtheweightedsummationsofvaluesondifferentnodessimultaneously. Then,wecollecttheresultsfromthesenodesviacollectiveoperations. Notethat, althoughthissectionprimarilyfocusesonlongsequencemodeling, muchof themotivationforsequenceparallelismcomesfromthedistributedtrainingmethodsofdeep networks,asdiscussedinSection8.2.3. Asaresult,theimplementationofthesemethodscan bebasedonthesameparallelprocessinglibrary. 8.3.2 Efficient Architectures OnedifficultyofapplyingTransformerstolongsequencesisthatself-attentionhasaquadratic timecomplexitywithrespecttothesequencelength. Moreover,akey-valuecache(orKV cache for short) is maintained during inference, and its size increases as more tokens are processed. Although the KV cache grows linearly with the sequence length, for extremely 8.3 Long Sequence Modeling 439 long input sequences, the memory footprint becomes significant and it is even infeasible to deploy LLMs for such tasks. As a result, the model architecture of long-context LLMs generallymovesawayfromthestandardTransformer,turninginsteadtothedevelopmentof moreefficientvariantsandalternatives. Oneapproachistousesparseattentioninsteadofstandardself-attention. Thisfamilyof modelsisbasedontheideathatonlyasmallnumberoftokensareconsideredimportantwhen attendingtoagiventoken,andsomostoftheattentionweightsbetweentokensarecloseto zero. Asaconsequence,wecanprunemostoftheattentionweightsandrepresenttheattention modelinacompressedform. Toillustrate,considertheself-attentionmodel Att",
    "with the sequence length, for extremely 8.3 Long Sequence Modeling 439 long input sequences, the memory footprint becomes significant and it is even infeasible to deploy LLMs for such tasks. As a result, the model architecture of long-context LLMs generallymovesawayfromthestandardTransformer,turninginsteadtothedevelopmentof moreefficientvariantsandalternatives. Oneapproachistousesparseattentioninsteadofstandardself-attention. Thisfamilyof modelsisbasedontheideathatonlyasmallnumberoftokensareconsideredimportantwhen attendingtoagiventoken,andsomostoftheattentionweightsbetweentokensarecloseto zero. Asaconsequence,wecanprunemostoftheattentionweightsandrepresenttheattention modelinacompressedform. Toillustrate,considertheself-attentionmodel Att (Q,K,V) = α(Q,K)V (8.45) qkv wheretheattentionweightmatrixα(Q,K)∈Rm×m isobtainedby QKT α(Q,K) = Softmax( √ +Mask) d   α 0 0 ... 0 0,0  α α 0 ... 0   1,0 1,1  =   α 2,0 α 2,1 α 2,2 ... 0   (8.46)   . . . . . . . . . ... . . .     α α α ... α m−1,0 m−1,1 m−1,2 m−1,m−1 (cid:104) (cid:105) Eachrowvector α ... α 0 ... 0 correspondstoadistributionofattendingthe i,0 i,i i-th token to every token of the sequence. Since language models predict next tokens only basedontheirleft-context,wenormallywritetheoutputoftheattentionmodelatpositionias   v 0 (cid:104) (cid:105) . Att qkv (q i ,K ≤i ,V ≤i ) = α i,0 ... α i,i   . .   v i i (cid:88) = α v (8.47) i,j j j=0     k v 0 0 . . whereK ≤i =  . .   andV ≤i =  . .   arethekeysandvaluesuptopositioni. k v i i (cid:104) (cid:105) In the original version of self-attention α ... α is assumed to be dense, that is, i,0 i,i (cid:104) (cid:105) mostofthevaluesarenon-zero. Insparseattention,someoftheentriesof α ... α are i,0 i,i considerednon-zero,andtheremainingentriesaresimplyignoredincomputation. Suppose G⊆{0,...,i}isthesetofindicesofthenon-zeroentries. Forlanguagemodels,theoutputof 440 Chapter 8. Generative Models thesparseattentionmodelatpositioniisgivenby (cid:88) Att (q ,K ,V ) = α′ v (8.48) sparse i ≤i ≤i i,j j j∈G Here{α′ }arenormalizedoverG. Hencetheirvaluesaredifferentfromtheoriginalattention i,j weights(infactwehaveα′ >α ). ThesparsityofthemodelisdeterminedbyhowlargeG i,j i,j is. SparseattentionmodelsdifferinthewaywedefineG. OnesimpleapproachistodefineG basedonheuristicallydesignedpatterns. Forexample,awidely-usedpatterninvolveshaving Gcoverawindowoftokenslocatednearpositioni[Parmaretal.,2018]. Whilesparseattentionreducesthecomputationthroughtheuseofsparseoperations,such modelsstillhavesignificantlimitationsaswemustkeeptheentireKVcache(i.e.,K and ≤i V ) during inference. If the sequence is very long, storing this cache will become highly ≤i memory-intensive. Toaddressthis,wecanconsideradifferentformofattentionmodelswhere theKVcacheisnotexplicitlyretained. Linearattentionisonesuchapproach[Katharopoulos etal.,2020]. Itusesakernelfunctionϕ(·)toprojecteachqueryandkeyontopointsq′ =ϕ(q ) i i andk′ =ϕ(k ),respectively. ByremovingtheSoftmaxfunctionundersuchtransformations10, i i theformoftheresultingattentionmodelisgivenby Att (q ,K ,V ) ≈ Att (q′,K′ ,V ) qkv i ≤i ≤i linear i ≤i ≤i q′µ = i i (8.49) q′ν i i whereµ andν arevariablesthatarecomputedintherecurrentforms i i µ = µ +k′T v (8.50) i i−1 i i ν = ν +k′T (8.51) i i−1 i µ andν canbeseenasrepresentationsofthehistoryuptopositioni. Abenefitofthismodel i i isthatweneednotkeepallpastqueriesandvalues. Insteadonlythelatestrepresentationsµ i andν areused. Sothecomputationalcostofeachstepisaconstant,andthemodelcanbe i easilyextendedtodealwithlongsequences. Infact,thissequentialapproachtolongsequencemodelingarisesnaturallywhenweadopt aviewpointofrecurrentmodels. Suchmodelsreadonetoken(orasmallnumberoftokens) atatime,updatetherecurrentstateusingtheseinputs,andthendiscardthembeforethenext tokenarrives. Theoutputateachstepisgeneratedbasedonlyontherecurrentstate,ratherthan onallthepreviousstates. Thememoryfootprintisdeterminedbytherecurrentstatewhich has a fixed size. Recurrent models can be used in real-time learning scenarios where data arrivesinastreamandpredictionscanbemadeatanytimestep. InNLP,applyingrecurrent 10Inthenewspaceafterthistransformation,theSoftmaxnormalizationcanbetransformedintothesimple scalingnormalization. 8.3 Long Sequence Modeling 441 modelstolanguagemodelingisoneoftheearliestsuccessfulattemptstolearnrepresentations ofsequences. AlthoughTransformerhasbeenusedasthefoundationalarchitectureinLLMs, recurrentmodelsarestillpowerfulmodels,especiallyfordevelopingefficientLLMs. More recently,recurrentmodelshavestartedtheirresurgenceinlanguagemodelingandhavebeen reconsideredasapromisingalternativetoTransformers[GuandDao,2023]. Figure 8.5 shows a comparison of the models discussed in this subsection. Since these models,alongwithothersnotmentionedhere,havebeenintensivelydiscussedinChapter6 andinrelatedsurveys[Tayetal.,2020b],adetaileddiscussionofthemisprecludedhere. 8.3.3 Cache and Memory LLMsbasedonthestandardTransformerarchitectureareglobalmodels. Theinferencefor thesemodelsinvolvesstoringtheentireleft-contextinordertomakepredictionsforfuture tokens. This requires a KV cache where the representations (i.e., keys and values) of all previously-generatedtokensarekept,andthecostofcachinggrowsastheinferenceproceeds. Above,wehavediscussedmethodsforoptimizingthiscacheviaefficientattentionapproaches, suchassparseattentionandlinearattention. Anotheridea,whichmayhaveoverlapwiththe previousdiscussion,istoexplicitlyencodethecontextviaanadditionalmemorymodel. 1. Fixed-size KV Cache A straightforward approach is to represent the keys and values using a fixed-size memory model. SupposewehaveamemoryMemwhichretainsthecontextualinformation. Wecan writetheattentionoperationatpositioniinageneralform Att(q ,Mem) = Att (q ,K ,V ) (8.52) i qkv",
    "a KV cache where the representations (i.e., keys and values) of all previously-generatedtokensarekept,andthecostofcachinggrowsastheinferenceproceeds. Above,wehavediscussedmethodsforoptimizingthiscacheviaefficientattentionapproaches, suchassparseattentionandlinearattention. Anotheridea,whichmayhaveoverlapwiththe previousdiscussion,istoexplicitlyencodethecontextviaanadditionalmemorymodel. 1. Fixed-size KV Cache A straightforward approach is to represent the keys and values using a fixed-size memory model. SupposewehaveamemoryMemwhichretainsthecontextualinformation. Wecan writetheattentionoperationatpositioniinageneralform Att(q ,Mem) = Att (q ,K ,V ) (8.52) i qkv i ≤i ≤i Inthismodel,MemissimplytheKVcache,i.e.,Mem=(K ,V ). Thusthesizeof ≤i ≤i Memisdeterminedbyi. IfwedefineMemasafixed-sizevariable,thenthecostofperforming Att(q ,Mem)willbefixed. ThereareseveralalternativewaystodesignMem. i • Oneofthesimplestmethodsistoconsiderafixed-sizewindowofpreviouskeysand values. Memisthereforegivenby Mem = (K ,V ) (8.53) [i−nc+1,i] [i−nc+1,i] where n denotes the size of the window. The notation K and V c [i−nc+1,i] [i−nc+1,i] denotethekeysandvaluesoverpositionsfromi−n +1toi.11 Thismodelcanbeseen c asatypeoflocalattentionmodel. • ItisalsopossibletodefineMemasapairofsummaryvectors,whichleadstoamore compressedrepresentationofthehistory. Asimplewaytosummarizethepreviouskeys k  v  i−nc+1 i−nc+1 11More formally, we write K = . .  and V = . . . Sometimes we [i−nc+1,i]  .  [i−nc+1,i]  .  k v i i denoteK [i−nc+1,i] by{k i−nc+1 ,...,k i }andV [i−nc+1,i] by{v i−nc+1 ,...,v i }fornotationsimplicity. 442 Chapter 8. Generative Models Att (q ,K ,V ) qkv i ≤i ≤i k 0 k 1 ··· k i−2 k i−1 k i q i v 0 v 1 ··· v i−2 v i−1 v i (a)StandardSelf-attention Att (q ,{k ,k },{v ,v }) qkv i 1 i 1 i k 0 k 1 ··· k i−2 k i−1 k i q i v 0 v 1 ··· v i−2 v i−1 v i (b)SparseAttention µ i =µ i−1 +k′T i v i ⇒ µ i Att linear (q i ,K ≤i ,V ≤i )= q q ′ i ′ i µ νi i ν i =ν i−1 +k′T i ⇒ ν i k 0 k 1 ··· k i−2 k i−1 k i q i v 0 v 1 ··· v i−2 v i−1 v i (c)LinearAttention h =f(h ,input ) i i−1 i h 0 h 1 ··· h i−3 h i−2 h i−1 h i input i (d)RecurrentModels Figure8.5: Illustrationsofself-attention,sparseattention,linearattentionandrecurrentmodels. Blueboxes=cachedstatesforproducingtheoutputatpositioni. f(·)=arecurrentcell. andvaluesistousethemovingaverageofthem. Forexample,Memcanbedefinedas theunweightedmovingaverageofthepreviousn keysandvalues c Mem = (cid:16) (cid:80)i j=i−nc+1 k j , (cid:80)i j=i−nc+1 v j(cid:17) (8.54) n n c c 8.3 Long Sequence Modeling 443 Alternatively,wecanuseaweightedversionofmovingaverage Mem = (cid:16) (cid:80)i j=i−nc+1 β j−i+nc k j , (cid:80)i j=i−nc+1 β j−i+nc v j(cid:17) (8.55) (cid:80)nc β (cid:80)nc β j=1 j j=1 j Here{β ,...,β }arethecoefficients,whichcanbeeitherlearnedasmodelparameters 1 nc ordeterminedviaheuristics. Forexample,theycanbesettoincreasingcoefficients(i.e., β <β <...<β <β )inordertogivelargerweighttopositionsthatarecloser 1 2 nc−1 nc toi. Wecanextendthemovingaveragetoincludeallthepositionsuptoi. Thisleadsto thecumulativeaverageofthekeysandvalues,givenintheform (cid:16) (cid:80)i j=0 k j (cid:80)i j=0 v j(cid:17) Mem = , (8.56) i+1 i+1 Ingeneral,thecumulativeaveragecanbewrittenusingarecursiveformula (k ,v )+i·Mem i i i−1 Mem = (8.57) i i+1 whereMem andMem denotethecumulativeaveragesofthecurrentandprevious i i−1 positions,respectively. Anadvantageofthismodelisthatweonlyneedtostoreasingle key-valuepairduringinference,ratherthanstoringallthekey-valuepairs. Notethatthe abovememorymodelsarerelatedtorecurrentmodels,andmoreadvancedtechniques have been used to develop alternatives to self-attention mechanisms in Transformers [Maetal.,2023]. • ThememoryMemcanalsobeaneuralnetwork. Ateachstep,ittakesboththeprevious outputofthememoryandthecurrentstatesofthemodelasinput,andproducesthenew outputofthememory. Thisneuralnetworkcanbeformulatedasthefunction Mem = Update(S ,Mem ) (8.58) kv pre HereMemandMem representtheoutputsofthememoryatthecurrentstepandthe pre previousstep,respectively. S isasetofkey-valuepairs,representingtherecentstates kv of the model. This formulation is general and allows us to develop various memory modelsbyselectingdifferentUpdate(·)andS configurations. Forexample,ifS kv kv onlycontainsthelatestkey-valuepair(k ,v )andUpdate(·)isdefinedasarecurrent i i cell,thenEq. (8.58)canbeexpressedasanRNN-likemodel Mem = f((k ,v ),Mem ) (8.59) i i pre wheref(·)isarecurrentcell. Recurrencecanalsobeappliedtosegment-levelmodeling forefficiencyconsideration. Asimpleapproachisthatwecandividethesequenceinto segments, and treat S as a segment. Applying recurrent models to Update(·)",
    "model. This formulation is general and allows us to develop various memory modelsbyselectingdifferentUpdate(·)andS configurations. Forexample,ifS kv kv onlycontainsthelatestkey-valuepair(k ,v )andUpdate(·)isdefinedasarecurrent i i cell,thenEq. (8.58)canbeexpressedasanRNN-likemodel Mem = f((k ,v ),Mem ) (8.59) i i pre wheref(·)isarecurrentcell. Recurrencecanalsobeappliedtosegment-levelmodeling forefficiencyconsideration. Asimpleapproachisthatwecandividethesequenceinto segments, and treat S as a segment. Applying recurrent models to Update(·) will kv resultinmemorymodelsthatoperateonsegments. Aspecialexampleisthatwedefine 444 Chapter 8. Generative Models Update(·)asanFIFOfunctionthataddsS intothememoryandremovestheoldest kv key-valuesegmentfromthememory,givenby Mem = FIFO(S ,Mem ) (8.60) kv pre Consideramemorywhichincludestwosegments,oneforcurrentsegment,andonefor theprevioussegment. Intheattentionoperation,eachpositioncanaccessthehistory key-value pairs in two closest consecutive segments. This essentially defines a local memory,butitanditsvariantshavebeenwidelyusedsegment-levelrecurrentmodels [Daietal.,2019;Hutchinsetal.,2022;Bulatovetal.,2022]. • Theabovememorymodelscanbeextendedtoinvolvemultiplememories. Anexample ofthisapproachiscompressiveTransformer[Raeetal.,2019b]. Itemploystwodistinct fixed-sizememories: oneformodelinglocalcontext(denotedbyMem),andtheother formodelingandcompressinglong-termhistory(denotedbyCMem). TheKVcache in this model is the combination of Mem and CMem. The attention function can be writtenas Att (q ,Mem,CMem) = Att (q ,[Mem,CMem]) (8.61) com i qkv i where [Mem,CMem] is a combined memory of Mem and CMem. As with other segment-levelmodels,thecompressiveTransformermodeloperatesonsegmentsofthe sequence. Each segment is a sequence of n consecutive tokens, and we denote Sk s kv as the key-value pairs corresponding to the tokens of the k-th segment. When a new segmentarrives,MemisupdatedinanFIFOfashion: weappendthen key-valuepairs c inSk toMem,andthenpopthen oldestkey-valuepairsfromMem,whichisgiven kv s by Mem = FIFO(Sk ,Mem ) (8.62) kv pre Thepoppedkey-valuepairsarethenusedtoupdatethecompressivememoryCMem. These n key-value pairs are compressed into ns key-value pairs via a compression s c network. CMemisanFIFOwhichappendsthecompressed ns key-valuepairstothe c tailofthequeue,anddropsthefirst ns key-valuepairsofthequeue. Itisgivenby c CMem = FIFO(Ck ,CMem ) (8.63) kv pre whereCk representsthesetofcompressedkey-valuepairs. Implicitinthecompressive kv Transformermodelisthatlocalcontextshouldberepresentedexplicitlywithminimal informationloss,whilelong-rangecontextcanbemorecompressed. • Wehavealreadyseenthatbothglobalandlocalcontextsareusefulandcanbemodeled using attention models. This view motivates the extension to attention models for combiningbothlocalandlong-termmemories[Ainslieetal.,2020;Zaheeretal.,2020; GuptaandBerant,2020]. Asimplebutwidely-usedapproachistoinvolvethefirstfew 8.3 Long Sequence Modeling 445 tokensofthesequenceinattention,servingasglobaltokens. Thisapproachisusually appliedalongwithothersparseattentionmodels. Anadvantageofincorporatingglobal tokens of the sequence is that it helps smooth the output distribution of the Softmax functionusedinattentionweightcomputation,andthusstabilizesmodelperformance whenthecontextsizeisverylarge[Xiaoetal.,2024]. Onedrawback,however,isthat usingafixed-sizeglobalmemorymayresultininformationloss. Whendealingwith longsequences,weneedtoenlargetheKVcacheforsufficientrepresentationsofthe context,butthisinturnincreasesthecomputationalcost. Figure 8.6 shows illustrations of the above approaches. Note that, while we focus on optimization of the KV cache here, this issue is closely related to those discussed in the previoussection. Allofthemethodswehavementionedsofarcanbroadlybecategorizedas efficientattentionapproaches,whicharewidelyusedinvariousTransformervariants. 2. Memory-based Models ThemodelingofmemoriesdiscussedabovewasbasedonupdatestotheKVcache,andthe resulting models are typically referred to as internal memories. We now consider another familyofmodels,calledexternalmemories,whichoperateasindependentmodelstoaccess large-scale contexts for LLMs. Many such models are based on memory-based methods which have been extensively discussed in machine learning [Bishop, 2006]. A common exampleisnearestneighboralgorithms: westorecontextrepresentationsinadatastore,andtry tofindthemostsimilarstoredrepresentationstomatchagivenquery. Theretrievedcontext representationsarethenusedtoimproveattentionforthisquery. Here, we consider the k-nearest neighbors (k-NN) method which is one of the most popular memory-based methods. Since our focus is language modeling in this section, we defineasampleinthedatastoreasakey-valuepaircorrespondingtosomecontextstate. Note that “context” is a broad concept here, not just a sequence prefix in text generation. One might,forexample,viewtheentiredatasetasthecontextforpredictingtokens. Thisallows ustoretrievetheclosestcontextsituationinasetofsequences,ratherthanagivensequence prefix. Althoughwewillrestrictourselvestocontextmodelingforasinglesequence,inthis subsection,wediscussarelativelymoregeneralcase. Supposewehaveasetofkeys{k }withcorrespondingvalues{v },andsupposewestore j j thesekey-valuepairsinavectordatabase12. Foreachqueryq ,wefinditsknearestneighbours i bygrowingtheradiusofthespherecenteredasq untilitcontainsk datapointsin{k }. This i j resultsinasetofkkeysalongwiththeircorrespondingvalues,denotedbyMem . Asbefore, knn we denote Mem as the local memory for the query, such as the KV cache of neighboring tokens. Our goal is to attend query q to both the local memory Mem and the long-term i memoryMem . Thereare,ofcourse,severalwaystoincorporateMemandMem into knn knn theattentionmodel. Forexample,wemightsimplycombinethemtoformasingleKVcache [Mem,Mem ],andattendq to[Mem,Mem ]viastandardQKVattention. Orwemight knn i knn 12Avectordatabase,orvectorstore,isadatabasethatprovideshighlyoptimizedretrievalinterfacesforfinding storedvectorsthatcloselymatchaqueryvector. 446 Chapter 8. Generative Models Memory Size=4×2 ··· Keys ··· Values i−7 i−6 i−5 i−4 i−3 i−2 i−1 i (a)Window-basedCache ki−3+ki−2+ki−1+ki ⇒ Memory 4 Size=1×2 vi−3+vi−2+vi−1+vi ⇒ 4 ··· Keys ··· Values",
    "memoryMem . Thereare,ofcourse,severalwaystoincorporateMemandMem into knn knn theattentionmodel. Forexample,wemightsimplycombinethemtoformasingleKVcache [Mem,Mem ],andattendq to[Mem,Mem ]viastandardQKVattention. Orwemight knn i knn 12Avectordatabase,orvectorstore,isadatabasethatprovideshighlyoptimizedretrievalinterfacesforfinding storedvectorsthatcloselymatchaqueryvector. 446 Chapter 8. Generative Models Memory Size=4×2 ··· Keys ··· Values i−7 i−6 i−5 i−4 i−3 i−2 i−1 i (a)Window-basedCache ki−3+ki−2+ki−1+ki ⇒ Memory 4 Size=1×2 vi−3+vi−2+vi−1+vi ⇒ 4 ··· Keys ··· Values i−7 i−6 i−5 i−4 i−3 i−2 i−1 i (b)MovingAverage-basedCache Memory Mem=Update(S ,Mem ) ⇒ kv pre Size=1×2 ··· Keys ··· Values i−7 i−6 i−5 i−4 i−3 i−2 i−1 i (c)RecurrentNetworkasCache Compressed Memory Memory Size=2×2 Size=4×2 ··· Keys ··· Values i−7 i−6 i−5 i−4 i−3 i−2 i−1 i (d)HybridCache(CompressedMemory+LocalMemory) Figure8.6: Illustrationsoffixed-sizeKVcachesinLLMs. Blueboxesrepresentthekeysand valuesgeneratedduringLLMinference,greenboxesrepresentthekeysandvaluesstoredor encoded in the primary memory, and orange boxes represent the keys and values stored or encodedinthecompressedmemory. 8.3 Long Sequence Modeling 447 use Mem and Mem in separate attention steps. An example of such approaches is the knn modeldevelopedbyWuetal.[2021]. Itlinearlycombinesthetwotypesofattention,givenby Att(q ,Mem,Mem ) = g⊙Att +(1−g)⊙Att (8.64) i knn local knn Att = Att(q ,Mem) (8.65) local i Att = Att(q ,Mem ) (8.66) knn i knn Hereg∈Rd isthecoefficientvector,whichcanbetheoutputofalearnedgate. Giventhek-NN-basedmemorymodeldescribedabove,theremainingtaskistodetermine whichkey-valuepairsareretainedinthedatastore. Forstandardlanguagemodelingtasks,we considerthepreviouslyseentokensinasequenceasthecontext,sowecanaddthekeysand valuesofallthesetokensintothedatastore. Inthiscase,theresultingk-NN-basedattention modelisessentiallyequivalenttoasparseattentionmodel[Guptaetal.,2021]. Alternatively,wecanextendthecontextfromonesequencetoacollectionofsequences. Forexample,wemightcollectallkey-valuepairsacrossthesequencesinatrainingdataset andaddthemtothedatastoretomodelalargercontext. Thus,LLMscanpredicttokensbased onageneralizedcontext. Aproblemwiththisapproachisthatthecomputationalcostwould belargeifmanysequencesareinvolved. Sincethesesequencesarepartofourtrainingdata, wecanbuildandoptimizeanindexforthevectorsinthedatastorebeforerunningtheLLMs. Asaresult,theretrievalofsimilarvectorscanbeveryefficient,asinmostvectordatabases. Infact,alltheabove-mentionedmethodscanbeviewedasinstancesofaretrieval-based approach. Insteadofusingretrievalresultstoimproveattention,wecanapplythisapproach inotherwaysaswell. Oneapplicationofk-NN-basedsearchisk-NNlanguagemodeling (or k-NN LM) [Khandelwal et al., 2020]. The idea is that, although it is attempting to extendthecontextusedinself-attentionbyincorporatingnearestneighborsinrepresentation learning, in practice, similar hidden states in Transformers are often highly predictive of similartokensinsubsequentpositions. Ink-NNLM,eachiteminthedatastoreisakey-value tuple (z,w), where z represents a hidden state of the LLM at a position, and w represents the corresponding prediction. A typical way to create the datastore is to collect the output vectoroftheTransformerlayerstackandthecorrespondingnexttokenforeachpositionof each sequence in a training dataset. During inference, we have a representation h given a i prefix. Giventhisrepresentation,wefirstsearchthedatastoreforkclosestmatchingdataitems {(z ,w ),...,(z ,w )}. Here{w ,...,w }arethoughtofasreferencetokensforprediction, 1 1 k k 1 k andthuscanbeusedtoguidethetokenpredictionbasedonh . Onecommonwaytomakeuse i ofreferencetokensistodefineadistributionoverthevocabularyV, (cid:104) (cid:105) Pr (·|h ) = Softmax( −d ··· −d ) (8.67) knn i 0 |V| whered equalsthedistancebetweenh andz ifw equalsthev-thentryofV,andequals v i j j 0 otherwise. We use a linear function with a coefficient λ that interpolates between the 448 Chapter 8. Generative Models retrieval-baseddistributionPr (·|h )andtheLLMoutputdistributionPr (·|h ) knn i lm i Pr(·|h ) = λ·Pr (·|h )+(1−λ)·Pr (·|h ) (8.68) i knn i lm i Then,asusual,wecanchoosethenexttokeny bymaximizingtheprobabilityPr(y|h ). i Aswithinformationretrieval(IR)systems,thedatastorecanalsomanagetextsandprovide accesstorelevanttextsforaquery. Forexample,wecanstoreacollectionoftextdocuments in a search engine with full-text indexing, and then search it for documents that match a giventext-basedquery. ApplyingIRtechniquestoLLMsleadstoageneralframeworkcalled retrieval-augmented generation (RAG). The RAG framework works as follows. We use thecontextxasthequeryandfindthek mostrelevantdocumentpieces{c ,...,c }fromthe 1 k datastoreviaefficientIRtechniques13. Thesesearchresultsarecombinedwiththeoriginal contextviaapromptingtemplateg(·)14,resultinginanaugmentedinputfortheLLM x′ = g(c ,...,c ,x) (8.69) 1 k Then,weusex′ asthecontextandpredictthefollowingtextusingthemodelPr(y|x′). One advantageofRAGisthatweneednotmodifythearchitectureofLLMs,butinsteadaugment theinputtoLLMsviaanadditionalIRsystem. Figure8.7showsacomparisonoftheuseof differentexternalmemoriesinLLMs. 3. Memory Capacity AmemorymodelinLLMs,intheformofasimplekey-valuecacheoradatastore,canbroadly beseenasanencoderofcontextualinformation. Ideally,beforewesaythatamemorymodel isrepresentativeoftheentirecontextintokenprediction,weneedtomakesurethatthemodel canaccuratelyrepresentanypartofthecontext. ThestandardKVcacheisonesuchmodel thatcompletelystoresallpasthistory. Inthiscase,themodelissaidtohaveadequatecapacity formemorizingthecontext. Inmanypracticalapplications,however,completememorization isnotrequired. Instead,thegoalistoenableLLMstoaccessimportantcontextualinformation. As a result, efficient and compressed memory models are developed, as described in this section. Notethat,thelongerthesequence,themoredifficultitbecomesforalow-capacity memorymodeltocaptureimportantcontextualinformation. Itisthereforecommonpracticeto simplyincreasethemodelcapacitywhenprocessinglongcontexts. Whilehigh-capacitymodelsaregenerallyfavorable,theyaredifficulttotrainanddeploy. Achallengingscenarioisthatthetokensarriveinastreamandthecontextcontinuouslygrows. 13Inpracticalapplications,queriesaretypicallygeneratedusingaquerygenerationsystem,whichmayexpandit withvariationsoftokensandqueryintent. 14Forexample,thetemplatecouldbe: message={*c 1*}...{*c k *} input:{*x*} output: 8.3 Long Sequence Modeling 449 g⊙Att(q ,Mem)+(1−g)⊙Att(q ,Mem ) i i knn Att(qi,Mem knn ) Att(qi,Mem) ··· q i ··· kNearest KVCache Neighbors Keys/valuesinLLM Datastore Search Keys/valuesinDatastore (a)k-NNSearchAugmentedAttention OutputDistribution DistributionPr(·) DistributionPr (·) knn ··· Att(qi,Mem) Att(q i ,Mem) ··· q i ··· kNearest KVCache Neighbors",
    "1*}...{*c k *} input:{*x*} output: 8.3 Long Sequence Modeling 449 g⊙Att(q ,Mem)+(1−g)⊙Att(q ,Mem ) i i knn Att(qi,Mem knn ) Att(qi,Mem) ··· q i ··· kNearest KVCache Neighbors Keys/valuesinLLM Datastore Search Keys/valuesinDatastore (a)k-NNSearchAugmentedAttention OutputDistribution DistributionPr(·) DistributionPr (·) knn ··· Att(qi,Mem) Att(q i ,Mem) ··· q i ··· kNearest KVCache Neighbors Keys/valuesinLLM KeysinDatastore Datastore Search PredictedTokens (b)k-NNLanguageModeling LLM c 1 = Deepnetworkis... Message: deepnetwork...machinelearning... c 2 = Machinelearningis... Whatisdeeplearning? ··· kNearest Neighbors Search InputContext: Datastore x= Whatisdeeplearning? (c)Retrieval-augmentedGeneration Figure8.7: Illustrationsofexternalmemories(ordatastores)forlanguagemodeling. 450 Chapter 8. Generative Models DevelopingLLMsforsuchtasksisdifficultasweneedtotrainTransformersonextremelylong sequences. Apossiblewaytoaddressthisdifficultyistousenon-parametricmethods,such asretrieval-basedmethods. Forexample,asdiscussedabove,wecanuseavectordatabase tostorepreviouslygeneratedkey-valuepairs,andthusrepresentthecontextbythisexternal memorymodel. Althoughthisapproachside-stepsthechallengeofrepresentinglongcontextin Transformers,buildingandupdatingexternalmemorymodelsarecomputationallyexpensive. Thesemodelsaremoreoftenusedinproblemswherethecontextisgiveninadvanceandfixed duringinference,andhenceunsuitableforstreamingcontextmodeling. In cases where the size of the context continuously grows, applying fixed-size memory modelsisacommonlyusedapproach. Forexample,inrecurrentmodels,asequenceofarbitrary lengthcanbesummarizedintoasetofhiddenstatesbywhichwehaveafixedcomputational cost per step. While recurrent models were initially found to be not very good at handling long-distancedependenciesinsequencemodelinginearlyapplicationsofdeeplearningtoNLP, recentadvancementshaveshownthattheirvariantsarenoweffectiveinmodelingextremely longsequences. [Bulatovetal.,2022;Hutchinsetal.,2022;Munkhdalaietal.,2024;Maetal., 2024]. There is no general definition of memory capacity in LLMs. A simple approach might considerhowmuchstorageisusedtoretaincontextualinformation. Forexample,memory capacitycouldbedefinedbythesizeoftheKVcacheinTransformersorthevectordatabase usedinretrieval-basedmethods. Arelatedconceptismodelcomplexity. Inmachinelearning, there are several ways to define the model complexity of a model. One of the simplest methodsisbycountingthenumberofparameters. However,itshouldbeemphasizedthatthe memorymodelsdiscussedhereprimarilyservetostoreinformation,ratherthanaddtrainable parameters. Therefore,amodelwithalargememorycapacityisnotnecessarilymorecomplex. Nevertheless,inpracticedeterminingthecapacityofamemorymodelisnotstraightforward. In general,weneedtocontrolthetrade-offbetweenmaximizingtheperformanceandcontrolling thememoryfootprint. 8.3.4 Sharing across Heads and Layers In Transformers, the KV cache is a data structure that can be dynamically adjusted along multiple dimensions, such as heads, layers, and sequence length. For example, consider an LLM with L layers. Each layer has τ attention heads, and each head produces a d - h dimensionaloutput. Duringinference,westorethekeysandvaluesforuptomtokens. The spacecomplexityofthiscachingmechanismisO(L·τ·d ·m). Aswehaveseenpreviously, h thiscomplexitycanbereducedbycachingthekeysandvaluesforfewertokens. Forexample, inslidingwindowattention,afixed-sizewindowisusedtocachethekeysandvaluesinlocal context. AndthismodelhasaspacecomplexityofO(L·τ·d ·m ),withm beingthesize h w w ofthewindow. In addition to reducing m, we can also decrease the size of the KV cache along other dimensions. A widely-used approach is to enable sharing across heads in multi-head self- attention. RecallfromSection8.1.1thatmulti-headself-attentionusesmultiplesetsofqueries, keys,andvalues(eachsetiscalledahead),eachperformingtheQKVattentionmechanismas 8.3 Long Sequence Modeling 451 usual. Thiscanbeexpressedas Output = Merge(head ,...,head )Whead (8.70) 1 τ wherehead j ∈Rd h iscomputedusingthestandardQKVattentionfunction [j] [j] [j] head =Att (q ,K ,V ) (8.71) j qkv i ≤i ≤i [j] [j] [j] Here,q ,K ,andV arethequery,keys,andvaluesthatareprojectedontothej-thfeature i ≤i ≤i sub-space. So this model can be interpreted as performing attention on a group of feature sub-spacesinparallel(seeFigure8.8(b)). TheKVcacheneedstoretainthekeysandvalues [1] [1] [τ] [τ] foralltheseheads,thatis,{(K ,V ),...,(K ,V )}. ≤i ≤i ≤i ≤i Onerefinementtothemulti-headattentionmodel,calledmulti-queryattention(MQA), istosharekeysandvaluesacrossheads,whileallowingqueriestobeuniqueforeachhead [Shazeer, 2019]. In MQA, there is a single set of keys and values (K ,V ). In addition, ≤i ≤i [1] [τ] thereareτ queries{q ,...,q },eachcorrespondingtoadifferenthead. Foreachhead,we i i have [j] head =Att (q ,K ,V ) (8.72) j qkv i ≤i ≤i Figure 8.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache wouldbeO(L·d ·m). h Groupedqueryattention(GQA)isanaturalextensiontomulti-headattentionandMQA [Ainslieetal.,2023]. InGQA,headsaredividedinton groups,eachcorrespondingtoashared g setofkeysandvalues. Hencewehaven setsofkeysandvalues{(K [1] ,V [1] ),...,(K [ng] ,V [ng] )}. g ≤i ≤i ≤i ≤i See Figure 8.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA modelcanbeexpressedas [j] [g(j)] [g(j)] head =Att (q ,K ,V ) (8.73) j qkv i ≤i ≤i The size of the KV cache of GQA is O(L·n ·d ·m). One benefit of GQA is that we can g h trade-offbetweencomputationalefficiencyandmodelexpressivenessbyadjustingn . When g n =τ,themodelbecomesthestandardmulti-headattentionmodel. Bycontrast,whenn =1, g g itbecomestheGQAmodel. Sharingcanalsobeperformedacrosslayers. Suchamethodfallsintothefamilyofshared",
    "[j] [g(j)] [g(j)] head =Att (q ,K ,V ) (8.73) j qkv i ≤i ≤i The size of the KV cache of GQA is O(L·n ·d ·m). One benefit of GQA is that we can g h trade-offbetweencomputationalefficiencyandmodelexpressivenessbyadjustingn . When g n =τ,themodelbecomesthestandardmulti-headattentionmodel. Bycontrast,whenn =1, g g itbecomestheGQAmodel. Sharingcanalsobeperformedacrosslayers. Suchamethodfallsintothefamilyofshared weight and shared activation methods, which have been extensively used in Transformers [Dehghani et al., 2018; Lan et al., 2020]. For example, one can share KV activations or attentionweightsacrosslayerstoreducebothcomputationandmemoryfootprints[Xiaoetal., 2019;Brandonetal.,2024]. Figure8.8(e)showsanillustrationofthismethod,whereaquery inalayerdirectlyaccessestheKVcacheofalower-levellayer. 452 Chapter 8. Generative Models value key query value key query (a)Single-headAttention (b)Multi-headAttention value key query value key query (c)Multi-queryAttention (d)GroupedQueryAttention value key query Layerl Sharing Layerl−1 (e)Cross-layerMulti-headAttention Figure8.8: IllustrationofQKVattentionbasedondifferentmulti-headandsharingmechanisms. (a)=single-headattention,and(b-e)=attentionwithmultipleheads. 8.3.5 Position Extrapolation and Interpolation SinceTransformerlayersareorder-insensitivetoinput,weneedsomewaytoencodepositional informationintheinputtokens. Todothis,itiscommontoaddpositionalembeddingstotoken embeddings,andthenfeedthesecombinedembeddingsintotheTransformerlayerstackas input. Inthiscase,theembeddingatpositionicanbeexpressedas e = x +PE(i) (8.74) i i wherex ∈Rddenotesthetokenembedding,andPE(i)∈Rddenotesthepositionalembedding. i 8.3 Long Sequence Modeling 453 In general, the token embedding x is a position-independent vector, and so the positional i embedding PE(i) is used to encode the positional context. A straightforward approach is to treat PE(i) as a learnable variable and train it alongside other model parameters. In this way,wecanlearnauniquerepresentationforeachposition,andthusdistinguishthetokens appearingatdifferentpositionsofasequence. Representations of positions using learned vectors can work well in tasks where the sequences at training and test times are of similar lengths. In practice, however, we often imposelengthrestrictionsonsequencesduringtrainingtopreventexcessivecomputational costs, but wish to apply the trained models to much longer sequences during inference. In thiscase,usinglearnedpositionalembeddingshasobviousdrawbacks,astherearenotrained embeddingsforpositionsthatarenotobservedinthetrainingphase. Analternativeapproachtomodelingpositionalinformationistodeveloppositionalembed- dingsthatcangeneralize: oncetrained,theembeddingmodelcanbeusedtohandlelonger sequences. Supposethatwetrainapositionalembeddingmodelonsequenceswithamaximum lengthofm ,andwewishtoapplythetrainedmodeltoasequenceoflengthm(m>>m ). l l Iftheembeddingmodelislimitedintherangeofpositionsthatwecanobservefromtraining data,thenthismodelwillsimplyfailtodealwithnewdataoutsidethatrange. SeeFigure8.9 (a)foranillustrationwherethelearnedembeddingmodelcannotmodeldatapointsoutsidethe trainingdomainifitlackstheabilitytoextrapolate. Thereareseveralapproachestomakingpositionalembeddingmodelsgeneralize. They canbegroupedintotwoclasses. • Extrapolation. The model learned on observed data points (i.e., positions) can be directlyemployedtoassignmeaningfulvaluestodatapointsbeyondtheoriginalrange. Forexample,supposewehaveaseriesofnumbers1,2,...,10,andwewanttounderstand the meaning of a new number, 15. Knowing that these numbers are natural numbers usedforordering,wecaneasilyinferthat15isanumberthatfollows10,eventhough 15 has not been observed before. Figure 8.9 (b) shows an example of this approach, whereafunctionislearnedtofitthedatapointswithinaspecificrangeandthenapplied toestimatethevaluesofdatapointsoutsidethatrange. • Interpolation. This approach maps a larger range of data points into the original observationrange. Forexample,supposewehaveamodeldesignedfornumbersinthe range [1,10]. When given a new range of [1,20], we can scale this down by dividing everynumberby2,therebyfittingallnumbersinto[1,10]. Thisscalingallowsustouse themodeltrainedontherange[1,10]todescribedatapointsintheexpandedrangeof [1,20]. SeeFigure8.9(c)foranillustrationofthisapproach. Infact,positionalembeddingsinmanysystemshaveachievedsomelevelofgeneralization. Forexample,sinusoidalencoding,themostcommonpositionalembeddingmethod,employs sineandcosinefunctionsthatcannaturallyextendtosequencesofanylength. Althoughthis approachmightseemdirectandsimple,itdoesnotperformwellwhenwesignificantlyextend thesequencesforprocessing. Inthissubsection,wewilldiscussseveralalternativemethods basedoneitherextrapolationorinterpolation. 454 Chapter 8. Generative Models 1 0 −1 0 1,024 2,048 SequenceLength (a)EncodingwithNoGeneralization eulaV 1 0 −1 0 1,024 2,048 SequenceLength (b)Extrapolation eulaV 1 0 −1 0 1,024 2,048 SequenceLength (c)Interpolation eulaV Figure8.9: Illustrationsofdifferentpositionalembeddingmethodsforarangeofpositions. Blue points represent the positions that have been observed during training, and red points represent the positions that are newly observed at test time. In sub-figure (a), the encoding modelonlymemorizesthepointsseenduringtraining,andcannotgeneralize. Insub-figures (b)and(c),themodelcangeneralizethroughextrapolationandinterpolation. 1. Attention with Learnable Biases One problem with Eq. (8.74) is that the embedding model treats each token independently andthereforeignoresthedistancebetweendifferenttokens. Acommonimprovementtothis model,calledrelativepositionalembedding,istoconsiderthepairwiserelationshipbetween tokens[Shawetal.,2018]. Thegeneralideabehindthisistoobtaintheoffsetbetweenany pairofpositionsandincorporateitintotheself-attentionmodel. Oneofthesimplestformsof self-attentionwithrelativepositionalembeddingisgivenby i (cid:88) Att (q ,K ,V ) = α(i,j)v (8.75) qkv i ≤i ≤i j j=0 q kT+PE(i,j) i j α(i,j) = Softmax( √ +Mask(i,j)) (8.76) d 8.3 Long Sequence Modeling 455 The only difference between this model and the original self-attention model is that a bias termPE(i,j)isaddedtothequery-keyproductinthisnewmodel. Intuitively,PE(i,j)canbe interpretedasadistancepenaltyforthepairofpositionsiandj. Asimovesawayfromj,the valueofPE(i,j)decreases. PE(i,j) can be defined in several different ways. Here, we consider the T5 version of relativepositionalembedding,calledtheT5bias[Raffeletal.,2020]. Foreachpairofquery q andkeyk ,theoffsetbetweenthemisdefinedtobe15 i j d(i,j) = i−j (8.77)",
    "Long Sequence Modeling 455 The only difference between this model and the original self-attention model is that a bias termPE(i,j)isaddedtothequery-keyproductinthisnewmodel. Intuitively,PE(i,j)canbe interpretedasadistancepenaltyforthepairofpositionsiandj. Asimovesawayfromj,the valueofPE(i,j)decreases. PE(i,j) can be defined in several different ways. Here, we consider the T5 version of relativepositionalembedding,calledtheT5bias[Raffeletal.,2020]. Foreachpairofquery q andkeyk ,theoffsetbetweenthemisdefinedtobe15 i j d(i,j) = i−j (8.77) AsimpledesignforthebiasPE(i,j)istosharethesamelearnablevariableforallquery- keypairswiththesameoffset,i.e.,PE(i,j)=u ,whereu isthevariablecorresponding i−j i−j totheoffseti−j. However,simplyassigningauniquevaluetoeachoffsetwillrestrictthis modeltoobservedoffsets. Wheni−j islargerthanthemaximumtrainedoffset,themodel cannotgeneralize. TheT5biasinsteadadoptsageneralizationofthismodel. Ratherthanassigningeachquery- keyoffsetauniquebiasterm,itgroupsdifferenceoffsetsinto“buckets”,eachcorresponding toonelearnableparameter. Morespecifically,thebiastermsforn +1bucketsaregivenas b follows. • For buckets 0 to n b +1 −1, each bucket corresponds to one offset, that is, bucket 0 2 ↔ offset 0, bucket 1 ↔ offset 1, bucket 2 ↔ offset 2, and so on. We express this as b(i−j)=i−j. • Forbuckets n b +1 ton ,thesizeofeachbucketincreaseslogarithmically. Forexample, 2 b thebucketnumberforagivenoffseti−j ≥ n b +1 canbedefinedas 2 n +1 log(i−j)−log(n b +1) n +1 b(i−j) = b +⌊ 2 · b ⌋ (8.78) 2 log(dist )−log(n b +1) 2 max 2 wheretheparameterdist istypicallysettoarelativelylargenumbertoindicatethe max maximumoffsetwemayencounter. • When i−j >dist , we place i−j in the last bucket. In other words, bucket n max b containsalltheoffsetsthatarenotassignedtothepreviousbuckets. Together,thesecanbeexpressedasthefunction b(i−j)   i−j 0≤i−j < n b 2 +1 = min(n b ,n b 2 +1+⌊ log lo ( g d ( is i− tm j a ) x − ) l − og lo ( g n ( b 2 n + b 1 2 + ) 1 ) ·n b 2 +1⌋) i−j ≥ n b 2 +1 (8.79) Figure 8.10 shows an illustration of these buckets. We see that in the first half of the 15Forlanguagemodeling,aqueryisonlyallowedtoattendtoitsleft-context,andsowehavei−j≥0.Inthe moregeneralcaseofself-attention,whereatokencanattendtoalltokensinthesequence,wemayhavenegative offsetswheni<j. 456 Chapter 8. Generative Models fixedbucketsize logarithmicallyincreasedbucketsize Bucket ··· ··· 0 1 2 3 14 15 16 17 18 32 Offset 0 1 2 3 14 15 16∼2021∼26 27∼33 802∼∞ (i−j) Figure8.10: Illustrationofdistributingquery-keyoffsetsintobucketsintheT5model(n =32 b anddist =1024). Boxesrepresentbuckets. Inthefirsthalfofthebuckets,weuseafixed max bucketsize. Inthesecondhalfofthebuckets,weincreasethebucketsizelogarithmically. The lastbucketcontainsallthequery-keyoffsetsthatarenotcoveredbypreviousbuckets. buckets, each bucket is associated with only one value of i−j, while in the second half, thebucketsizeincreasesasi−j grows. Thelastbucketisdesignedtohandlesequencesof arbitrarilylonglengths. AllPE(i,j)sinabucketsharethesamebiastermu . SubstitutingPE(i,j)=u b(i−j) b(i−j) intoEq. (8.76),theattentionweightforq andk becomes16 i j q kT+u i j b(i−j) α(i,j) = Softmax( √ +Mask(i,j)) (8.81) d Theparameters{u ,...,u }arelearnedascommonparametersduringtraining. Itshould 0 n b beemphasizedthatthismodelcangeneralizetolongsequences. ThisisbecausePE(i,j)swith similarquery-keyoffsetssharethesameparameter,andthissharingstrategyisparticularly important for achieving good generalization, given that large query-key offsets are rare in training. Inpractice,weoftensetn toamoderatenumber,andthusitcanhelpcontrolthe b overfittingofpositionalembeddingmodels. 2. Attention with Non-learned Biases Relativepositionalembeddingmodelsarebasedonasetoflearnedbiasesforthequery-key product in self-attention. An alternative approach is to give these biases fixed values via heuristics,ratherthantrainingthemonaparticulardataset. Onebenefitofthisheuristics-based approachisthatitdoesnotrelyonatrainingprocessandthuscanbedirectlyappliedtoany sequencesoncethebiasesareset. OneexampleofsuchanapproachisPressetal.[2022]’sapproach,calledattentionwith linearbiasesorALiBiforshort. IntheALiBiapproach,thebiastermisdefinedasthenegative 16Notethat,inRaffeletal.[2020]’sT5model,therescalingoperationforthequery-keyproductisremoved. Theattentionweightα(i,j)isthengivenby α(i,j) = Softmax(q i kT j +u b(i−j) +Mask(i,j)) (8.80) 8.3 Long Sequence Modeling 457 Entry Query-KeyBias(PE(i,j)) T5[Raffeletal.,2020] u b(i−j) ALiBi[Pressetal.,2022] −β·(i−j ) Kerple[Chietal.,2022] −β 1 (i−j )β2 (power) −β log(1+β (i−j )) (logarithmic) 1 2 Sandwich[Chietal.,2023] (cid:80)d¯/2 cos (cid:0) (i−j )/100002k/d¯(cid:1) k=1 (cid:0) (cid:1) FIRE[Lietal.,2024b] f ψ(i−j )/ψ(max(m ,i)) len Table 8.4: Query-key biases as relative positional embeddings. β, β , β , d¯ , and m are 1 2 len hyper-parameters. IntheT5model,b(i−j)denotesthebucketassignedtoi−j. IntheFIRE model,ψ(·)isamonotonicallyincreasingfunctionsuchasψ(x)=log(cx+1),andf(·)isan FFN. scaledquery-keyoffset PE(i,j) = −β·(i−j) = β·(j−i) (8.82) whereβ isthescalingfactor. Addingthistermtothequery-keyproduct,weobtainanewform ofattentionweights q kT+β·(j−i) i j α(i,j) = Softmax( √ +Mask(i,j)) (8.83) d Thismodelcanbeinterpretedasaddingafixedpenaltytoq kT wheneverj movesone i j stepawayfromi. Sowedonotneedtoadaptittoarangeofsequencelengths,andcanemploy ittomodelarbitrarilylongsequences. SeeFigure8.11foracomparisonoftheT5biasandthe",
    "β, β , β , d¯ , and m are 1 2 len hyper-parameters. IntheT5model,b(i−j)denotesthebucketassignedtoi−j. IntheFIRE model,ψ(·)isamonotonicallyincreasingfunctionsuchasψ(x)=log(cx+1),andf(·)isan FFN. scaledquery-keyoffset PE(i,j) = −β·(i−j) = β·(j−i) (8.82) whereβ isthescalingfactor. Addingthistermtothequery-keyproduct,weobtainanewform ofattentionweights q kT+β·(j−i) i j α(i,j) = Softmax( √ +Mask(i,j)) (8.83) d Thismodelcanbeinterpretedasaddingafixedpenaltytoq kT wheneverj movesone i j stepawayfromi. Sowedonotneedtoadaptittoarangeofsequencelengths,andcanemploy ittomodelarbitrarilylongsequences. SeeFigure8.11foracomparisonoftheT5biasandthe ALiBibias. In general, the scalar β should be tuned on a validation dataset. However, Press et al. [2022] found that setting β to values decreasing geometrically by a factor of 1 for multi- 2a headattentionperformswellonavarietyoftasks. Specifically,foraself-attentionsub-layer involvingn heads,thescalarforthek-thheadisgivenby head 1 β = (8.84) k 8 2k TheALiBiapproachprovidesasimpleformofrelativepositionalembeddings. Thereare othersimilarmethodsfordesigningquery-keybiasesusingtheoffseti−j. Table8.4shows a comparison of such biases. As an aside it is worth noting that the form of the right-hand sideofEq. (8.82)isverysimilartolengthfeaturesusedinconventionalfeature-basedsystems. For example, in statistical machine translation systems, such features are widely used to modelwordreorderingproblems,resultinginmodelsthatcangeneralizewellacrossdifferent translationtasks[Koehn,2010]. 458 Chapter 8. Generative Models q kT Bias(u ) i j b(i−j) q0kT 0 u 0 q1kT 0 q1kT 1 u 1 u 0 q2kT 0 q2kT 1 q2kT 2 u 2 u 1 u 0 + q3kT 0 q3kT 1 q3kT 2 q3kT 3 u 2 u 2 u 1 u 0 q4kT 0 q4kT 1 q4kT 2 q4kT 3 q4kT 4 u 3 u 2 u 2 u 1 u 0 q5kT 0 q5kT 1 q5kT 2 q5kT 3 q5kT 4 q5kT 5 u 3 u 3 u 2 u 2 u 1 u 0 q6kT 0 q6kT 1 q6kT 2 q6kT 3 q6kT 4 q6kT 5 q6kT 6 u 3 u 3 u 3 u 2 u 2 u 1 u 0 (a)TheT5bias(n =3anddist =5) b max q i kT j Bias(−β(i−j)) q0kT 0 0 q1kT 0 q1kT 1 −1β 0 q2kT 0 q2kT 1 q2kT 2 −2β −1β 0 + q3kT 0 q3kT 1 q3kT 2 q3kT 3 −3β −2β −1β 0 q4kT 0 q4kT 1 q4kT 2 q4kT 3 q4kT 4 −4β −3β −2β −1β 0 q5kT 0 q5kT 1 q5kT 2 q5kT 3 q5kT 4 q5kT 5 −5β −4β −3β −2β −β 0 q6kT 0 q6kT 1 q6kT 2 q6kT 3 q6kT 4 q6kT 5 q6kT 6 −6β −5β −4β −3β −2β −β 0 (b)TheALiBibias Figure8.11: Query-keyproductswithbiases(above=theT5biasandbelow=theALiBibias). The colorscale of thebiases ranges fromlight blue denotingsmall absolute values todeep bluedenotinglargeabsolutevalues. 3. Rotary Positional Embedding Aswithsinusoidalembeddings,rotarypositionalembeddingsarebasedonhard-codedvalues foralldimensionsofanembedding[Suetal.,2024]. Recallthatinthesinusoidalembedding model,positionsarerepresentedascombinationsofsineandcosinefunctionswithdifferent frequencies. Theseembeddingsarethenaddedtotokenembeddingstoformtheinputstothe Transformerlayerstack. Rotarypositionalembeddingsinsteadmodelpositionalcontextas rotationstotokenembeddingsinacomplexspace. Thisleadstoamodelexpressedintheform 8.3 Long Sequence Modeling 459 ofmultiplicativeembeddings e = x R(i) (8.85) i i whereR(i)∈Rd×d istherotationmatrixrepresentingtherotationsperformedonthetoken embeddingx ∈Rd. i Forsimplicity,wewillfirstconsiderembeddingswithonlytwodimensionsandreturnto adiscussionofthemoregeneralformulationlater. Supposewehavea2-dimensionaltoken (cid:104) (cid:105) embeddingx= x x . Wecanrepresentitasavectorinaplane,originatingattheorigin 1 2 (0,0) and terminating at (x ,x ). A counterclockwise rotation of this vector refers to an 1 2 operationofmovingthevectoraroundtheoriginwhilemaintainingitsmagnitude,asshownin Figure8.12(a). Thedegreeofrotationisusuallydefinedbyaspecificangle, denotedbyθ. Therotationcanbeexpressedmathematicallyintheform Ro(x,θ) = xR θ (cid:34) (cid:35) (cid:104) (cid:105) cosθ sinθ = x x 1 2 −sinθ cosθ (cid:104) (cid:105) = cosθ·x −sinθ·x sinθ·x +cosθ·x (8.86) 1 2 1 2 (cid:34) (cid:35) cosθ sinθ whereR = istherotationmatrix. Iftwoormorerotationsareperformed θ −sinθ cosθ on the same vector, we can rotate the vector further. This follows from the fact that the compositionofsuccessiverotationsisitselfarotation. Moreformally,rotatingavectorbyan angleθ forttimescanbeexpressedas Ro(x,tθ) = xR tθ (cid:104) (cid:105) = costθ·x −sintθ·x sintθ·x +costθ·x (8.87) 1 2",
    "1 2 1 2 (cid:34) (cid:35) cosθ sinθ whereR = istherotationmatrix. Iftwoormorerotationsareperformed θ −sinθ cosθ on the same vector, we can rotate the vector further. This follows from the fact that the compositionofsuccessiverotationsisitselfarotation. Moreformally,rotatingavectorbyan angleθ forttimescanbeexpressedas Ro(x,tθ) = xR tθ (cid:104) (cid:105) = costθ·x −sintθ·x sintθ·x +costθ·x (8.87) 1 2 1 2 Ifweinterprettasthepositionofatokenrepresentedbyxinasequence, thenwewill findthattheaboveequationdefinesasimplepositionalembeddingmodel. AsshowninFigure 8.12(b),westartmovingthetokenfromposition0. Eachtimewemoveonestepforward,the vectorisrotatedbytheangleθ. Uponarrivingatthepositiont,therepresentationofthetoken withpositionalcontextisgivenbyRo(x,iθ). Astherotationsdonotchangethemagnitudeof theembedding,theoriginal“meaning”ofthetokenisretained. Thepositionalinformationis injectedintotheembedding,whenitgetsrotated. Apopularwaytounderstandvectorrotationistodefineitincomplexspaces. Itiseasy (cid:104) (cid:105) totransformeachvectorx= x x inthe2DEuclideanspaceR2 toacomplexnumber 1 2 x′=x +ix inthecomplexspaceCviaabijectivelinearmap. Then,therotationofxwith 1 2 theangletθ correspondstothemultiplicationbyeitθ. Giventhateitθ =costθ+isintθ,the 460 Chapter 8. Generative Models x x 2 2 vectorx x rotatedvector xR θ xR θ θ θ θ x x 1 1 xR 2θ θ xR 3θ (a)Single-stepRotation (b)Multi-stepRotation The1cat2is3sleeping4peacefully5 x 2 in6the7warm8sunlight9.10 sleeping4 7θ cat2 7θ x 1 sleeping11 cat9 Every1afternoon2,3you4’ll5find6that7 the8cat9is10sleeping11on12my13bed14.15 (c)Anglesbetweenembeddingsoftwotokensatdifferentpositions Figure8.12: Illustrationsofvectorrotationsinaplane. Sub-figures(a)and(b)showrotationsof avectorinasinglestepandmultiplesteps,respectively. Sub-figure(c)showstheembeddings oftokenscatandsleepingintwodifferentsentences. Weshowthesesentenceswithasubscript affixed to each token to indicate its position. If we represent tokens as vectors, we can addpositionalinformationbyrotatingthesevectors. Thisrotationpreservesthe“distances” betweenthevectors. Forexample,giventhatthedistancebetweencatandsleepingisthesame inbothsentences,theanglebetweentheirembeddingsalsoremainsthesameduringrotation. rotationoperationcanbere-expressedintheform xR (cid:55)→ x′eitθ tθ = (x +ix )(costθ+isintθ) 1 2 = costθ·x −sintθ·x +i(sintθ·x +costθ·x ) (8.88) 1 2 1 2 Herewedenotethetokenrepresentationx′eitθ byC(x,tθ). Theinnerproductoftherepresen- 8.3 Long Sequence Modeling 461 tationsofthetokensatpositionstandscanbewrittenas ⟨C(x,tθ),C(y,sθ)⟩ = (x′y′)ei(t−s)θ (8.89) where y′ is the complex conjugate of y′. As can be seen, the result of this inner product involvesatermt−s,andsoitcanmodeltheoffsetbetweenthetwotokens. Nowwegobacktorepresentationsinthe2DEuclideanspace. Thedot-productofRo(x,tθ) andRo(y,sθ)iscanbewrittenasafunctionof(t−s)θ Ro(x,tθ)[Ro(y,sθ)]T = xR [yR ]T tθ sθ = xR [R ]TyT tθ sθ = xR yT (8.90) (t−s)θ Giventhisresult, ifweconsiderRo(x,tθ)andRo(y,sθ)asthequeryandthekey, thenthe self-attentionoperationwillimplicitlyinvolvethemodelingofrelativepositionalcontext. Thisrotarypositionalembeddingcanbeextendedtomulti-dimensionalembeddings. Fora (cid:104) (cid:105) d-dimensionaltokenembeddingx= x x ... x ,wecantreatitasa d-dimensional 1 2 d 2 (cid:104) (cid:105) (cid:104) (cid:105) complexvectorx′= x′ x′ ... x′ = x +ix x +ix ... x +ix ,where 1 2 d/2 1 2 3 4 d−1 d eachconsecutivepairofitemsformsacomplexnumber. Then,therotarypositionalembedding inthecomplexspaceisgivenby d/2 (cid:88) C(x,tθ) = x′eitθ k⃗e (8.91) k k k=1 where⃗e isthestandardbasisvectorwithasinglenon-zerovalueinthek-thcoordinateand k 0’selsewhere[Bidermanetal.,2021]. Although this formula involves a complicated expression, its equivalent form in the d- dimensionalEuclideanspaceisrelativelyeasytounderstand. Wecanwriteitas   R tθ1 Ro(x,tθ) = (cid:104) x 1 x 2 ... x d (cid:105)     R tθ2 ...     (8.92)   R tθ d/2 (cid:34) (cid:35) costθ sintθ (cid:104) (cid:105) k k whereR = . θ= θ ,...,θ aretheparametersforcontrollingthe tθ k −sintθ costθ 1 d/2 k k angles of rotations in different dimensions. Typically, θ k is set to 10000−2(k d −1) , which is analogoustothesettinginsinusoidalembeddings. Inapracticalimplementation,Eq. (8.92)canberewrittenintoaformthatreliessolelyon 462 Chapter 8. Generative Models theelement-wiseproductandadditionofvectors.  T  T  T  T x costθ −x sintθ 1 1 2 1  x   costθ   x   sintθ   2   1   1   1   .   .   .   .  Ro(x,tθ) =  . .  ⊙ . .  + . .  ⊙ . .  (8.93)         x  costθ  −x  sintθ   d−1  d/2  d  d/2 x costθ x sintθ d d/2 d−1 d/2 Finally,werewriteEq. (8.85)toobtaintheformoftheembeddingatpositioni e = Ro(x ,iθ) (8.94) i i 4. Position Interpolation Inpositioninterpolation,ourgoalistomapthepositionsinthenewsequencetomatchthe observed range",
    ".  (8.93)         x  costθ  −x  sintθ   d−1  d/2  d  d/2 x costθ x sintθ d d/2 d−1 d/2 Finally,werewriteEq. (8.85)toobtaintheformoftheembeddingatpositioni e = Ro(x ,iθ) (8.94) i i 4. Position Interpolation Inpositioninterpolation,ourgoalistomapthepositionsinthenewsequencetomatchthe observed range in training. Suppose the sequence length for training ranges from 0 to m . l Whenm>m attesttime,werepresentthepositionsin[0,m]suchthatourrepresentationsfit l [0,m ]. l To illustrate, consider the rotary positional embedding model described above. The (cid:104) (cid:105) embeddingofeachtokenisdescribedbyamodelRo(x ,iθ)inwhichθ= θ ,...,θ are i 1 d/2 the parameters. Ro(x ,iθ) can be cast in the form of a linear combination of two periodic i functions(seeEq. (8.93)) (cid:104) (cid:105) cosiθ = cosiθ ... cosiθ (8.95) 1 d/2 (cid:104) (cid:105) siniθ = siniθ ... siniθ (8.96) 1 d/2 θ isaexponentialfunctionofk andtakestheform k θ k = b−2(k d −1) (8.97) wherebisthebase. Theperiodofcosiθ andsiniθ is k k 2(k−1) T k = 2π·b d (8.98) Thekeyideabehindpositioninterpolationistoadjustthisperiodsothatthenewpositions canbeencodedwithintherange[0,m ]. OnewaytoachievethisistoscaleupT by m,given l k m l by T k ′ = m m ·2π·b 2(k d −1) (8.99) l Henceallpointsin[0,m]arecompressedinto[0,m ]. Thislinearscalingcanbeeasilyrealized l by modifying the inputto theembedding model[Chen etal., 2023c]. Thenewmodel with 8.3 Long Sequence Modeling 463 linearpositionalinterpolationisgivenby m Ro′(x ,iθ) = Ro(x , l iθ) (8.100) i i m Anothermethodofpositionalinterpolationistoscalethebase17. Supposethatthebasebis scaledbyλ. Wewishtheperiodofthisnewmodelinthelastdimensionofθ (i.e.,dimension d)tobeequaltothatofthelinearpositionalinterpolationmodel. Thiscanbeexpressedas 2 2(d 2−1) m 2(d 2−1) 2π·(λb) d = ·2π·b d (8.101) m l Solvingthisequation,weobtain λ = (cid:0)m(cid:1) 2(d 2 d −1) m l (cid:0)m(cid:1) d = d−2 (8.102) m l Thisgivesanembeddingmodel Ro′(x ,iθ) = Ro(x ,iθ′) (8.103) i i where (cid:104) (cid:105) θ′= (λb)− d 0 ,(λb)− d 2 ,...,(λb)−d− d 2 (8.104) Notethatscalingthebaseprovidesanon-uniformmethodforscalingtheperiodsacross differentdimensionsofθ. ThismethodhasbeenfoundtobehelpfulforextendingLLMsto longer sequences, and several improvements have been developed [Peng et al., 2024; Ding etal.,2024]. 8.3.6 Remarks Inthissection,wehavepresentedavarietyofmethodsforlong-contextlanguagemodeling. Weclosethissectionbydiscussingsomeinterestingissuesrelatedtothesemethods. 1. Need for Long Context One of the ultimate goals of long-context LLMs is that these models can precisely encode infinitecontext. Theso-calledinfinitecontextrefersmoretothefactthatanLLMcancontin- uouslyreadwords. ThismotivatesLLMsthatcanhandleextremelylongcontextorstream data. AsdiscussedinSection8.3.3,itiscommontousefixed-sizememorymodelstoprocess continuously expanding context. Many such systems are based on recurrent architectures or their variants, because they are inherently suited to model time series problems where 17This method was first proposed in https://www.reddit.com/r/LocalLLaMA/comments/ 14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/ 464 Chapter 8. Generative Models the effects of past inputs continue indefinitely. Another way to achieve infinite memory is todevelopalternativestoself-attentionmodels,forexample,onecanusecontinuous-space attentionmodelstoencodecontext,whichremovesthedependencyoncontextlength[Martins etal.,2022]. Whenstudyinglong-contextLLMs,itisnaturaltowonderwhatmechanismsmayexplain theuseoflongcontextinlanguagemodeling. Canwecompresstherepresentationofinfinite contextintoarelativelysmall-sizedmodel? Areallcontexttokensusefulforpredictingnext tokens? HowdoLLMspreparefortokenpredictionwhentheyseethecontext? Canweknow inadvancewhichcontextualinformationwillbecriticalforprediction? Generalanswersto allthesequestionsarenotobvious,buttheyinspirefollow-onresearchofexplainablemodels, andsomeinterestingresultshavebeenfound. Forexample,Deletangetal.[2024]conducted extensive experiments to show that LLMs are powerful in-context compressors. Although viewingpredictivemodelsascompressionmodelshaslongbeenstudiedinmachinelearning, it also provides insights into our understanding of the LLM scaling laws. Pal et al. [2023] andWuetal.[2024]investigatedwhetherthefeatureslearneduptothecurrentstep,though notintentionally,arealreadysufficientforpredictingtokensatthefollowingsteps. Notethat theneedforlong-contextinlanguagemodelingishighlydependentontheproblemthatwe address. A related issue is where to apply LLMs and how to evaluate them. For example, in summarization tasks we may only need to distill and focus on a few key aspects of the text,whileinretrieval-liketasksweneedto“memorize”theentirecontextsothattherelevant informationcanbeaccessed. Wewilldiscusstheevaluationissuelaterinthissubsection. 2. Pre-training or Adapting LLMs? TrainingLLMsrequiressignificantcomputationalcosts. Althoughitisstraightforwardtotrain LLMsonlongsequencedata,thetrainingbecomescomputationallyunwieldyforlargedata sets. Itiscommonpracticetopre-trainLLMsongeneraldatasets,andthenadaptthemwith modestfine-tuningeffort. Forexample,LLMswithrelativeorrotarypositionalembeddings can be directly trained on large-scale data in the pre-training phase. While the resulting modelsmayexhibitsomeabilitiestoextrapolatelengthsintheinferencephase,itmaybemore effectivetofine-tunethemonlongersequences. Ideally, we would like to pre-train LLMs with standard Transformer architectures and adapt them to new tasks. This allows us to use many off-the-shelf LLMs and efficiently adaptthemtohandlelongsequences.",
    "Althoughitisstraightforwardtotrain LLMsonlongsequencedata,thetrainingbecomescomputationallyunwieldyforlargedata sets. Itiscommonpracticetopre-trainLLMsongeneraldatasets,andthenadaptthemwith modestfine-tuningeffort. Forexample,LLMswithrelativeorrotarypositionalembeddings can be directly trained on large-scale data in the pre-training phase. While the resulting modelsmayexhibitsomeabilitiestoextrapolatelengthsintheinferencephase,itmaybemore effectivetofine-tunethemonlongersequences. Ideally, we would like to pre-train LLMs with standard Transformer architectures and adapt them to new tasks. This allows us to use many off-the-shelf LLMs and efficiently adaptthemtohandlelongsequences. However,whennewarchitecturesareadopted,itseems inevitablethatweneedtotrainthesemodelsfromscratch. Thisposespracticaldifficultiesfor developinglong-contextLLMs,aswecannotleveragewell-developed,pre-trainedmodelsand mustinsteadtrainthemourselves. Ontheotherhand,fine-tuningisstillaneffectivewayto adaptLLMswithcertainarchitecturesthataredifferentfromthoseinpre-training. Anexample ismodelsaugmentedwithexternalmemories. Inthesemodels,thepre-trainedLLMsarefixed, andthefocusisonhowtomaketheseLLMscollaboratewiththememorymodels. InRAG,for instance,itiscommontofine-tuneLLMstoimprovetheiruseofretrieval-augmentedinputs. Anotherexampleoffine-tuningLLMsforlong-contextmodelingisthatwetrainanLLMwith fullattentionmodels,andthenreplacethemwithsparseattentionmodelsinthefine-tuning 8.3 Long Sequence Modeling 465 phase. Thepre-trainedLLMprovidesinitialvaluesofmodelparametersusedinadifferent model,andthismodelisthenfine-tunedasusual. 3. Evaluating Long-context LLMs Evaluatinglong-contextLLMsisimportant,butitisanewissueinNLP.Thegeneralideais that, ifweinputalongcontexttoanLLM,thenwecancheckfromtheoutputoftheLLM whetheritunderstandstheentirecontextandmakesuseofitinpredictingfollowingtokens. In conventional research of NLP, such evaluations are often aimed at examining the ability ofNLPmodelsinhandlinglong-rangedependencies. However,thesizeofcontextsusedin recentLLMsismuchlargerthanthatusedinNLPsystemsafewyearsago. Thismotivates researcherstodevelopnewevaluationbenchmarksandmetricsforlong-contextLLMs. Oneapproachistousetheperplexitymetric. However,inspiteofitsapparentsimplicity, this method tends to reflect more on the LLMs’ ability to make use of local context rather thanglobalcontext. Itisthereforetemptingtodevelopevaluationmethodsthatarespecific to long-context LLMs. Popular methods include various synthetic tasks where artificially generated or modified data is used to evaluate specific capabilities of long-context LLMs. In needle-in-a-haystack18 and passkey retrieval tasks [Mohtashami and Jaggi, 2024; Chen etal.,2023c],forinstance,LLMsarerequiredtoidentifyandextractasmall,relevantpiece ofinformationfromalargevolumeofgiventext. TheassumptionhereisthatanLLMwith sufficientmemoryshouldrememberearlierpartsofthetextasitprocessesnewinformation. This LLM can thus pick out the relevant details, which might be sparse and hidden among muchirrelevantinformation,fromthetext. Alternatively,incopymemorytasks(orcopytasks forshort),LLMsareusedtorepeattheinputtextoraspecificsegmentmultipletimes. These taskswereinitiallyproposedtotesttheextenttowhichrecurrentmodelscanretainandrecall previouslyseentokens[HochreiterandSchmidhuber,1997;Arjovskyetal.,2016],andhave beenadoptedinevaluatingrecentLLMs[Bulatovetal.,2022;GuandDao,2023]. Another approach to evaluating long-context LLMs is to test them on NLP tasks that involve very long input sequences. Examples include long-document or multi-document summarization,long-documentquestionanswering,codecompletion,andsoon. Abenefitof thisapproachisthatitcanalignevaluationswithuserexpectations. Although many methods have been developed, there is still no general way to evaluate long-contextLLMs[Liuetal.,2024c]. Oneproblemisthatmostofthesemethodsfocuson specificaspectsofLLMs,ratherthantheirfundamentalabilitytomodelverylongcontexts. EventhoughanLLMcanpickouttheappropriatepieceoftextfromtheinput,wecannotsay thatittrulyunderstandstheentirecontext. Instead,itmightjustremembersomeimportantparts ofthecontext,orevensimplyrecalltheanswerviathemodellearnedinpre-training. Moreover, thedatausedinmanytasksissmall-scaleandrelativelypreliminary,leadingtodiscrepancies betweenevaluationresultsandactualapplicationperformance. Amoreinterestingissueisthat theresultsofLLMsareinfluencedbymanyotherfactorsandexperimentalsetups,forexample, usingdifferentpromptscanleadtoverydifferentoutcomes. Thismakesevaluationevenmore 18https://github.com/gkamradt/LLMTest_NeedleInAHaystack 466 Chapter 8. Generative Models challengingbecauseimprovementsmaynotsolelyresultfrombettermodelingoflongcontexts, andthereisariskofoverclaimingourresults. Nevertheless,manyopenquestionsremainin thedevelopmentandevaluationoflong-contextLLMs. Forexample,thesemodelsstillsuffer fromlimitationssuchasrestrictedcontextlengthandhighlatency. Studyingtheseissuesis likelytoprovevaluablefuturedirections. 8.4 Summary Inthischapter,wehavediscussedtheconceptofLLMsandrelatedtechniques. Thiscanbe consideredageneral,thoughnotcomprehensive,introductiontoLLMs,layingthefoundation forfurtherdiscussionsonmoreadvancedtopicsinsubsequentchapters. Furthermore,wehave exploredtwowaystoscaleupLLMs. Thefirstfocusesonthelarge-scalepre-trainingofLLMs, whichiscrucialfordevelopingstate-of-the-artmodels. Thesecondfocusesonmethodsfor adaptingLLMstolonginputs,includingoptimizingattentionmodels,designingmoreefficient andcompressedKVcaches,incorporatingmemorymodels,andexploringbetterpositional embeddings. ThestrengthofLLMsliesintheirabilitytobreaktheconstraintsoftrainingNLPmodels foralimitednumberofspecifictasks. Instead,LLMslearnfromlargeamountsoftextthrough thesimpletaskoftokenprediction—wepredictthenexttokeninasentencegivenitsprior tokens. Ageneralviewisthat,byrepeatingthistokenpredictiontaskalargenumberoftimes, LLMscanacquiresomeknowledgeoftheworldandlanguage,whichcanthenbeappliedto new tasks. As a result, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent tokens given prompts. This emergent ability in language models comesfromseveraldimensions,suchasscalinguptraining,modelsize,andcontextsize. Itis undeniablethatscalinglawsarecurrentlythefundamentalprincipleadoptedindeveloping largelanguagemodels,althoughsimplyincreasingmodelsizehasyettoprovesufficientfor achieving AGI. These continuously scaled LLMs have been found to show capabilities in general-purposelanguageunderstanding,generation,andreasoning. Morerecently,ithasbeen foundthatscalingupthecomputeatinferencetimecanalsoleadtosignificantimprovements incomplexreasoningtasks[OpenAI,2024]. Giventheiramazingpower,LLMshaveattractedconsiderableinterest,bothintermsof techniquesandapplications. Asaresult,theexplosionofresearchinterestinLLMshasalso ledtoavastnumberofnewtechniquesandmodels. However,wedonotattempttoprovidea comprehensiveliteraturereviewonallaspectsofLLMs,giventherapidevolutionofthefield. Nevertheless, one can still gain knowledge about LLMs from general reviews [Zhao et al., 2023;Minaeeetal.,2024]ormorefocuseddiscussionsonspecifictopics[Ruanetal.,2024]. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 9 Prompting InthecontextofLLMs,promptingreferstothemethodofprovidinganLLMwithaspecific inputorcuetogenerateadesiredoutputorperformatask. Forexample,ifwewanttheLLM totranslateasentencefromEnglishtoChinese,wecanpromptitlikethis TranslatethetextfromEnglishtoChinese. Text: Theearlybirdcatchestheworm. Translation: Prompting is crucial for LLMs because it directly influences how effectively these models understandandrespondtouserqueries. Awell-craftedpromptcanguideanLLMtogenerate moreaccurate,relevant,andcontextuallyappropriateresponses. Furthermore,thisprocesscan beiterativelyrefined. ByanalyzingtheresponsesoftheLLM,userscanadjusttheirprompts toalignmorecloselywiththeirspecificneeds. Giventheimportanceofpromptinginapplying LLMs, prompt design has become an essential skill for users and developers working with LLMs. Thisleadstoanactiveresearcharea,calledpromptengineering,inwhichwedesign effectivepromptstomakebetteruseofLLMsandenhancetheirpracticalutilityinreal-world applications. Animportantconceptrelatedtopromptingisin-contextlearning. Whenpromptingan LLM,wecanaddnewinformationtothecontext,suchasdemonstrationsofproblem-solving. ThisallowstheLLMtolearnfromthiscontexthowtosolvetheproblem. Hereisanexample of prompting LLMs with a few demonstrations of how to classify text based on sentiment polarity. 468 Chapter 9. Prompting Herearesomeexamplesoftextclassification. Example1: Wehadadelightfuldinnertogether. →Label: Positive Example2: I’mfrustratedwiththedelays. →Label: Negative Whatisthelabelfor“Thatcommentwasquitehurtful.”? Label: In-context learning is often seen as an emergent ability of LLMs that arises after pre- training. Though LLMs can be trained or tuned to perform new tasks, in-context learning provides a very efficient way to adapt these models without any training or tuning effort. PerhapsthisisoneofthemostnotablefeaturesofLLMs: theyindeedlearngeneralknowledge about the world and language during pre-training, which we can easily apply to new chal- lenges. Moreover,in-contextlearningreflectsthebroadertrendofmakingAIsystemsmore generalizableanduser-friendly. Insteadofrequiringspecializedengineerstofine-tunemodels foreveryuniquetask,userscaninteractwithLLMsinamoreintuitiveway,simplyproviding examplesoradjustingthecontextasneeded. In this chapter, we focus on prompting techniques in LLMs. We",
    "learning provides a very efficient way to adapt these models without any training or tuning effort. PerhapsthisisoneofthemostnotablefeaturesofLLMs: theyindeedlearngeneralknowledge about the world and language during pre-training, which we can easily apply to new chal- lenges. Moreover,in-contextlearningreflectsthebroadertrendofmakingAIsystemsmore generalizableanduser-friendly. Insteadofrequiringspecializedengineerstofine-tunemodels foreveryuniquetask,userscaninteractwithLLMsinamoreintuitiveway,simplyproviding examplesoradjustingthecontextasneeded. In this chapter, we focus on prompting techniques in LLMs. We begin by considering severalinterestingpromptdesignscommonlyusedinpromptengineering. Then,wediscussa seriesofrefinementstothesemethods. Finally,weexploreapproachesforautomatingprompt design. 9.1 General Prompt Design Thissectionpresentsbasicconceptsinpromptdesign,alongwithexamplesofhowtoprompt LLMsforvariousNLPtasks. Sincetheeffectivenessofpromptingishighlydependentonthe LLMsbeingused,promptsoftenvaryacrossdifferentLLMs,makingitdifficulttoprovidea comprehensivelistofpromptsforallLLMsanddownstreamtasks. Therefore,thisdiscussion is not focused on any specific LLM. Instead, the goal is to provide guiding principles for promptdesign. 9.1.1 Basics Thetermpromptisusedinmanydifferentways. Inthischapterwedefineapromptastheinput text to an LLM, denoted by x. The LLM generates a text y by maximizing the probability Pr(y|x). In this generation process, the prompt acts as the condition on which we make predictions,anditcancontainanyinformationthathelpsdescribeandsolvetheproblem. Apromptcanbeobtainedusingaprompttemplate(ortemplateforshort)[Liuetal.,2023b]. Atemplateisapieceoftextcontainingplaceholdersorvariables,whereeachplaceholdercan befilledwithspecificinformation. HerearetwotemplatesforaskingtheLLMforweekend suggestions. 9.1 General Prompt Design 469 Pleasegivemesomesuggestionsforafunweekend. If{∗premise∗},whatareyoursuggestionsforafunweekend. Inthefirsttemplate,wesimplyinstructtheLLMtoreturnsomesuggestions. Sothetem- plateisjustapieceoftextwithnovariables. Inthesecondtemplate,thevariable{∗premise∗} needstobespecifiedbytheuserstoprovideapremiseformakingsuggestions. Forexample, ifweinput premise = theweatherisnicethisweekend thenwecangenerateaprompt Iftheweatherisnicethisweekend, whatareyoursuggestionsforafunweekend. Wecanalsodesignatemplatewithmultiplevariables. Hereisanexampleinwhichwe comparethetwosentencesintermsoftheirsemanticsimilarity. Hereisasentence {∗sentence1∗} Hereisanothersentence {∗sentence2∗} Computethesemanticsimilaritybetweenthetwosentences Apopularwaytoformatpromptsistowriteeachinputoroutputina“name:content”style. Forexample,wecandescribeaconversationbetweentwopeople,namedJohnandDavid,and usetheLLMtocontinuetheconversation. Atemplateofsuchpromptsisgivenby 470 Chapter 9. Prompting John: {∗utterance1∗} David: {∗utterance2∗} John: {∗utterance3∗} David: {∗utterance4∗} John: {∗utterance5∗} David: {∗utterance6∗} John: {∗utterance7∗} David: The“name:content”formatcanbeusedtodefinethetaskthatwewanttheLLMtoperform. Forexample, giventhat“Q”and“A”arecommonlyusedabbreviationsfor“Question”and “Answer”,respectively,wecanusethefollowingtemplatetodoquestion-answering. Q:{∗question∗} A: Thisformatcanbeusedtodescribemorecomplextasks. Forexample,thefollowingisan exampleofprovidingaspecificationforatranslationtask Task: Translation Sourcelanguage: English Targetlanguage: Chinese Style: Formaltext Template: Translatethefollowingsentence: {∗sentence∗} Inpracticalsystems,itiscommontorepresentandstoresuchdatainkey-valuepairs,suchas theJSONformat1. Whentheproblemisdifficulttodescribeinanattribute-basedmanner,itismorecommon toinstructLLMswithaclearanddetaileddescription. Therearemanywaystodothis. One 1TheJSONrepresentationis { \"Task\":\"Translation\" \"Sourcelanguage\":\"English\" \"Targetlanguage\":\"Chinese\" \"Style\":\"Formaltext\" \"Template\":\"Translatethefollowingsentence:{∗sentence∗}\" } 9.1 General Prompt Design 471 exampleistoassignaroletoLLMsandprovidesufficientcontext. Thefollowingisatemplate thatinstructsanLLMtoactasanexpertandanswerquestionsfromchildren. Youareacomputerscientistwithextensiveknowledgeinthefieldofdeep learning. Pleaseexplainthefollowingcomputer-relatedconcepttoachildaround10 yearsold,usingsimpleexampleswheneverpossible. {∗concept∗} Herethetext“Youareacomputerscientist... deeplearning. ” issometimescalledsystem information,andisprovidedtohelptheLLMunderstandthecontextorconstraintsofthetask itisbeingaskedtoperform. 9.1.2 In-context Learning Learningcanoccurduringinference. In-contextlearningisonesuchmethod,whereprompts involvedemonstrationsofproblem-solving,andLLMscanlearnfromthesedemonstrations how to solve new problems. Since we do not update model parameters in this process, in- contextlearningcanbeviewedasawaytoefficientlyactivateandreorganizetheknowledge learnedinpre-trainingwithoutadditionaltrainingorfine-tuning. Thisenablesquickadaptation of LLMs to new problems, pushing the boundaries of what pre-trained LLMs can achieve withouttask-specificadjustments. In-contextlearningcanbeillustratedbycomparingthreemethods: zero-shotlearning,one- shotlearningandfew-shotlearning. Zero-shotlearning,asitsnameimplies,doesnotinvolve atraditional“learning”process. ItinsteaddirectlyappliesLLMstoaddressnewproblemsthat were not observed during training. In practice, we can repetitively adjust prompts to guide the LLMs in generating better responses, without demonstrating problem-solving steps or providingexamples. Considerthefollowingexample. SupposewewanttouseanLLMasan assistantthatcanhelpcorrectEnglishsentences. Azero-shotlearningpromptisgivenby SYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection. USER YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Shedon’tlikegoingtothepark. Output: Herethegraywordsareusedtoindicatedifferentfieldsoftheprompt. Inone-shotlearning,weextendthispromptbyaddingademonstrationofhowtocorrect sentences,therebyallowingtheLLMtolearnfromthisnewly-addedexperience. 472 Chapter 9. Prompting SYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection. DEMO YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Thereismanyreasonstocelebrate. Output: Therearemanyreasonstocelebrate. USER YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Shedon’tlikegoingtothepark. Output: Furthermore,wecanaddmoredemonstrationstoenablefew-shotlearning. SYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection. DEMO1 YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Thereismanyreasonstocelebrate. Output: Therearemanyreasonstocelebrate. DEMO2 YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Meandmyfriendgoestothegymeveryday. Output: MyfriendandIgotothegymeveryday. USER YouwillbeprovidedwithasentenceinEnglish. Thetaskis tooutputthecorrectsentence. Input: Shedon’tlikegoingtothepark. Output: Infew-shotlearning,weessentiallyprovideapatternthatmapssomeinputstothecorre- spondingoutputs. TheLLMattemptstofollowthispatterninmakingpredictions,provided thatthepromptincludesasufficientnumberofdemonstrations,althoughgenerallysmall. Itis alsopossibletousesimplerpatternstoachievethis. Forexample,onecanusethefollowing few-shotlearningpromptfortranslatingwordsfromChinesetoEnglish. 9.1 General Prompt Design 473 DEMO 现在 → now 来 → come 去 → go 男孩 → boy USER 女孩 → If the LLM is powerful enough, few-shot learning can enable it to address complex problems, such as mathematical reasoning. For example, consider the following task of summingtwonumbersandthendividingthesumbytheirproduct. DEMO 12 5 → (12+5)/(12×5)=0.283 3 1 → (3+1)/(3×1)=1.33 −9 4 → (−9+4)/(−9×4)=0.138 15 15 → (15+15)/(15×15)=0.133 USER 19 73 → Inmanypracticalapplications,theeffectivenessofin-contextlearningreliesheavilyonthe qualityofpromptsandthefundamentalabilitiesofpre-trainedLLMs. Ononehand,weneed asignificantpromptengineeringefforttodevelopappropriatepromptsthathelpLLMslearn moreeffectivelyfromdemonstrations. Ontheotherhand,strongerLLMscanmakebetteruse ofin-contextlearningforperformingnewtasks. Forexample,supposewewishtouseanLLM totranslatewordsfromInuktituttoEnglish. IftheLLMlackspre-trainingonInuktitutdata,its understandingofInuktitutwillbeweak,anditwillbedifficultforthemodeltoperformwell intranslationregardlessofhowwepromptit. Inthiscase,weneedtocontinuetrainingthe LLMwithmoreInuktitutdata,ratherthantryingtofindbetterprompts. Itmightbeinterestingtoexplorehowin-contextlearningemergesduringpre-trainingand why it works during inference. One simple understanding is that LLMs have gained some knowledge of problem-solving, but there are many possible predictions, which are hard to distinguishwhenthemodelsconfrontnewproblems. Providingdemonstrationscanguidethe LLMs to follow the “correct” paths. Furthermore, some researchers have tried to interpret in-contextlearningfromseveraldifferentperspectives,includingBayesianinference[Xieetal., 2022],gradientdescent[Daietal.,2023;VonOswaldetal.,2023],linearregression[Akyürek etal.,2023],metalearning[Gargetal.,2022],andsoon. 9.1.3 Prompt Engineering Strategies Designingpromptsishighlyempirical. Ingeneral,therearemanywaystopromptanLLMfor performingthesametask,andweneedtoperformanumberoftrial-and-errorrunstofinda satisfactoryprompt. Towritegoodpromptsmoreefficiently,onecanfollowcertainstrategies.",
    "inference. One simple understanding is that LLMs have gained some knowledge of problem-solving, but there are many possible predictions, which are hard to distinguishwhenthemodelsconfrontnewproblems. Providingdemonstrationscanguidethe LLMs to follow the “correct” paths. Furthermore, some researchers have tried to interpret in-contextlearningfromseveraldifferentperspectives,includingBayesianinference[Xieetal., 2022],gradientdescent[Daietal.,2023;VonOswaldetal.,2023],linearregression[Akyürek etal.,2023],metalearning[Gargetal.,2022],andsoon. 9.1.3 Prompt Engineering Strategies Designingpromptsishighlyempirical. Ingeneral,therearemanywaystopromptanLLMfor performingthesametask,andweneedtoperformanumberoftrial-and-errorrunstofinda satisfactoryprompt. Towritegoodpromptsmoreefficiently,onecanfollowcertainstrategies. Examplesofcommonpromptingprinciplesinclude 474 Chapter 9. Prompting • Describingthetaskasclearlyaspossible. WhenweapplyanLLMtosolveaproblem, weneedtoprovideaprecise,specific,andcleardescriptionoftheproblemandinstruct the LLM to perform as we expect. This is particularly important when we want the outputoftheLLMtomeetcertainexpectations. Forexample,supposewearecurious aboutclimatechange. AsimplepromptforaskingtheLLMtoprovidesomeinformation is Tellmeaboutclimatechange. Sincethisinstructionistoogeneral,theLLMmaygeneratearesponsethataddresses anyaspectofclimatechange,whichmaynotalignwithourspecificinterests. Inthis case,wecaninsteadusepromptsthatarespecificanddetailed. Onesuchexampleis Provide a detailed explanation of the causes and effects of climate change,includingtheimpactonglobaltemperatures,weatherpatterns, andsealevels. Also,discusspossiblesolutionsandactionsbeingtaken tomitigatetheseeffects. Nowsupposeweintendtoexplainclimatechangetoa10-year-oldchild. Wecanadjust theabovepromptfurther. Explainthecausesandeffectsofclimatechangetoa10-year-oldchild. Talk about how it affects the weather, sea levels, and temperatures. Also,mentionsomethingspeoplearedoingtohelp. Trytoexplainin simpletermsanddonotexceed500words. • GuidingLLMstothink. LLMshaveexhibitedsurprisinglygoodcapabilitiesto“think”. Acommonexampleisthatwell-developedLLMshaveachievedimpressiveperformance inmathematicalreasoningtasks,whichareconsideredchallenging. Inpromptengineer- ing,the“thinking”abilityofLLMsneedstobeactivatedthroughappropriateprompting, especially for problems that require significant reasoning efforts. In many cases, an LLM that is instructed to “think” can produce completely different results compared withthesameLLMthatisinstructedtoperformthetaskstraightforwardly. Forexample, Kojimaetal.[2022]foundthatsimplyappending“Let’sthinkstepbystep”totheendof eachpromptcanimprovetheperformanceofLLMsonseveralreasoningtasks. LLMs canbepromptedto“think”inanumberofways. OnemethodistoinstructLLMsto 9.1 General Prompt Design 475 generate steps for reasoning about the problem before reaching the final answer. For example, consider a task of solving mathematical problems. See below for a simple promptforthistask. Youareamathematician. Youwillbeprovidedwithamathproblem. Pleasesolvetheproblem. Sincesolvingmathproblemsrequiresadetailedreasoningprocess,LLMswouldproba- blymakemistakesiftheyattemptedtoworkouttheanswerdirectly. Sowecanexplicitly askLLMstofollowagivenreasoningprocessbeforecomingtoaconclusion. You are a mathematician. You will follow these detailed reasoning stepswhensolvingmathproblems. Step1: ProblemInterpretation. Themathematiciancarefullylistenstoyourqueryandunderstandsthe intricatedetailsofthemathematicalchallengeyouhavepresented. Step2: StrategyFormulation. Drawingupontheirextensiveknowledge,themathematicianchooses themosteffectivestrategytailoredtothetypeofmathproblem,whether itisalgebra,calculus,orgeometry. Step3: DetailedCalculation. Withprecisionandexpertise,themathematicianperformsthenecessary calculationsstepbystep,adheringtoallmathematicalprinciples. Step4: SolutionReview. Before providing the final answer, the mathematician meticulously checksthecalculationsforaccuracyandoffersaconciseexplanation orrationaleforthesolution. Youwillbeprovidedwithamathproblem. Pleasesolvetheproblem. {∗problem∗} AnothermethodtoguideLLMsto“think”isthroughmultipleroundsofinteractionwith LLMs. Forexample,asafirststep,wecaninstructLLMstosolvetheproblemdirectly 476 Chapter 9. Prompting Youwillbeprovidedwithamathproblem. Pleasesolvetheproblem. {∗problem∗} Now we have an initial answer to the problem. As a second step, we prompt LLMs to evaluate the correctness of the answer and, if necessary, rework it to find a better solution. You will be provided with a math problem, along with a solution. Evaluate the correctness of this solution, and identify any errors if present. Then,workoutyourownsolution. Problem: {∗problem∗} Solution: {∗solution∗} Thepromptspresentedherearecloselyrelatedtoalonglineofresearchonreasoning problemsinLLMs. Itisimpossibletoprovideacompletediscussionofallrelatedissues becausethistopiccoversalargefamilyofmethods. Butwewillseearelativelymore detaileddiscussiononhowtoimprovepromptingthroughmorereasoninginSection 9.2. • Providingreferenceinformation. Asdiscussedintheprevioussection,wecaninclude demonstrations in prompts and allow LLMs to in-context learn from these demon- strations how to perform the task. In fact, given the remarkable ability of language understanding of LLMs, we can add any type of text into the prompts and so these modelscanpredictbasedonenrichedcontexts. Inmanyapplications,wehavevariousin- formationthatisrelevanttouserqueries. InsteadofusingLLMstomakeunconstrained predictions,weoftenwantLLMstoproduceoutputsthatareconfinedtotherelevant text. OnesuchexampleisRAG,wheretherelevanttextfortheuserqueryisprovidedby callinganIRsystem,andwepromptLLMstogenerateresponsesbasedonthisprovided relevanttext. Thefollowingpromptshowsanexample. 9.1 General Prompt Design 477 Youareanexpertthatcangenerateanswerstoinputqueries. Youhave nowbeenprovidedwithaqueryandthecorrespondingcontextinfor- mation. Pleasegenerateananswerbasedonthiscontextinformation. Notethatyouneedtoprovidetheanswerinyourownwords,notjust copyfromthecontextprovided. Contextinformation: {∗IR-result∗} Query: {∗query∗} Ifthecontextinformationishighlyreliable,wecanevenrestrictLLMstoanswering usingonlytheprovidedtext. Anexamplepromptisshownasfollows Youareanexperttaskedwithgeneratinganswersfrominputqueries. Youhavebeenprovidedwithaqueryandcorrespondingcontextinfor- mation,organizedinatablewhereeachrowrepresentsausefulrecord. Pleasegenerateananswerusingonlythiscontextinformation. Ensure thatyouprovidetheanswerinyourownwords. Contextinformation: {∗table∗} Query: {∗query∗} Whendealingwithreal-worldproblems,weoftenhavepriorknowledgeandadditional information about the problems that help produce better answers. Considering such informationinpromptingisgenerallyhelpfulinimprovingtheresult. • Payingattentiontopromptformats. Ingeneral,theperformanceofLLMsishighly sensitive to the prompts we input. Sometimes a small modification to a prompt can lead to a big change in model output. An interesting example is that changing the orderofsentencesinapromptmaycauseLLMstogeneratedifferentresults. Tomake promptseasytoreadandreduceambiguity,itiscommontoformattheminawaythat ensuresclarity. Oneexampleisthatwedefineseveralfieldsforpromptsandfilldifferent informationineachfield. Anotherexampleiswecanusecode-stylepromptsforLLMs whichcanunderstandandgeneratebothnaturallanguageandcode. Seethefollowing foracode-stylepromptthatperformstranslationwhereonedemonstrationispresented. 478 Chapter 9. Prompting [English]=[Ihaveanapple.] [German]=[IchhabeeinenApfel.] [English]=[Ihaveanorange.] [German]= LLMscanreceivetextinvariousformats. Thisallowsustousecontrolcharacters,XML tags,andspecificformattingtorepresentcomplexdata. Anditisusefultospecifyhow the input and output should be formatted or structured. For example, we can delimit sectionsoftextusingquotesandpromptLLMsaccordingly(e.g.,addingasentencelike “theinputtextisdelimitedbydoublequotes”totheprompt). Above,wehavediscussedonlyafewstrategiesforwritinggoodprompts. Thereare,of course,manysuchmethods,andoneneedstodeveloptheirownthroughpractice. Interested readerscanrefertovariousonlinedocumentsformoreinformation,suchasOpenAI’smanual ontheGPTseriesmodels2. 9.1.4 More Examples Inthissubsection,weconsidermoreexamplesofpromptingLLMstoperformvariousNLP tasks. Themotivationhereisnottogivestandardpromptsforthesetasks, butrathertouse simpleexamplestoillustratehowLLMscanbepromptedtodealwithNLPproblems. 1. Text Classification Text classification is perhaps one of the most common problems in NLP. Many tasks can",
    "tags,andspecificformattingtorepresentcomplexdata. Anditisusefultospecifyhow the input and output should be formatted or structured. For example, we can delimit sectionsoftextusingquotesandpromptLLMsaccordingly(e.g.,addingasentencelike “theinputtextisdelimitedbydoublequotes”totheprompt). Above,wehavediscussedonlyafewstrategiesforwritinggoodprompts. Thereare,of course,manysuchmethods,andoneneedstodeveloptheirownthroughpractice. Interested readerscanrefertovariousonlinedocumentsformoreinformation,suchasOpenAI’smanual ontheGPTseriesmodels2. 9.1.4 More Examples Inthissubsection,weconsidermoreexamplesofpromptingLLMstoperformvariousNLP tasks. Themotivationhereisnottogivestandardpromptsforthesetasks, butrathertouse simpleexamplestoillustratehowLLMscanbepromptedtodealwithNLPproblems. 1. Text Classification Text classification is perhaps one of the most common problems in NLP. Many tasks can bebroadlycategorizedasassigningpre-definedlabelstoagiventext. Hereweconsiderthe polarity classification problem in sentiment analysis. We choose polarity classification for illustrationbecauseitisoneofthemostpopularandwell-definedtextclassificationtasks. Ina generalsetupofpolarityclassification,wearerequiredtocategorizeagiventextintooneof threecategories: negative,positive,orneutral. Belowisasimplepromptfordoingthis(for easyreading,wehighlightthetaskdescriptionintheprompt). Analyzethepolarityofthefollowingtextandclassifyitaspositive,negative, orneutral. Text: The service at the restaurant was slower than expected, which was a bit frustrating. Thepolarityofthetextcanbeclassifiedasnegative. 2See https://platform.openai.com/docs/guides/prompt-engineering/ six-strategies-for-getting-better-results. 9.1 General Prompt Design 479 Tomaketheexamplecomplete,weshowtheresponsegeneratedbytheLLM(underlinedtext). Although the answer is correct, the LLM gives this answer not in labels but in text describingtheresult. TheproblemisthatLLMsaredesignedtogeneratetextbutnottoassign labelstotextandtreatclassificationproblemsastextgenerationproblems. Asaresult,weneed anothersystemtomaptheLLM’soutputtothelabelspace(callitlabelmapping),thatis,we extract“negative”from“Thepolarityofthetextcanbeclassifiedasnegative”. Thisistrivial in most cases because we can identify label words via simple heuristics. But occasionally, LLMs may not express the classification results using these label words. In this case, the problembecomesmorecomplicated,asweneedsomewaytomapthegeneratedtextorwords topredefinedlabelwords. OnemethodtoinduceoutputlabelsfromLLMsistoreframetheproblemasaclozetask. Forexample,thefollowingshowsacloze-likepromptforpolarityclassification. Analyzethepolarityofthefollowingtextandclassifyitaspositive,negative, orneutral. Text: The service at the restaurant was slower than expected, which was a bit frustrating. Thepolarityofthetextisnegative We can use LLMs to complete the text and fill the blank with the most appropriate word. Ideally, we wish the filled word would be positive, negative, or neutral. However, LLMs arenotguaranteedtogeneratetheselabelwords. Onemethodtoaddressthisproblemisto constrainthepredictiontothesetoflabelwordsandselecttheonewiththehighestprobability. Then,theoutputlabelisgivenby label = argmaxPr(y|x) (9.1) y∈Y wherey denotesthewordfilledintheblank,andY denotesthesetoflabelwords {positive,negative,neutral}. AnothermethodofusingLLMstogeneratelabelsistoconstraintheoutputwithprompts. For example, we can prompt LLMs to predict within a controlled set of words. Here is an example. 480 Chapter 9. Prompting Analyzethepolarityofthefollowingtextandclassifyitaspositive,negative, orneutral. Text: The service at the restaurant was slower than expected, which was a bit frustrating. Whatisthepolarityofthetext? Justanswer: positive,negative,orneutral. Negative SentimentanalysisisacommonNLPproblemthathasprobablybeenwellunderstoodby LLMsthroughpre-trainingorfine-tuning. ThuswecanpromptLLMsusingsimpleinstructions toperformthetask. However,fornewclassificationproblems,itmaybenecessarytoprovide additional details about the task, such as the classification standards, so that the LLMs can perform correctly. To do this, we can add a more detailed description of the task and/or demonstrate classification examples in the prompts. To illustrate, consider the following example. Analyzethepolarityofthefollowingtextandclassifyitaspositive,negative, orneutral.Here’swhateachcategoryrepresents: Positive:Thisindicatesthatthetextconveysapositiveemotionorattitude.For example,textsexpressinghappiness,satisfaction,excitement,oradmiration areconsideredpositive. Negative:Thisreferstoatextthatexpressesanegativeemotionorattitude.It encompassesfeelingsofsadness,anger,frustration,orcriticism. Neutral:Neutralsentimentisusedtodescribetextsthatdonotexhibitclear positiveornegativeemotionsbutinsteadconveyinformational,factual,or indifferenttones. Text: The service at the restaurant was slower than expected, which was a bit frustrating. Whatisthepolarityofthetext? Negative While it seems straightforward to use LLMs for classification problems, there are still issuesthathavenotbeenwelladdressed. Forexample,whendealingwithalargenumberof categories, it remains challenging to effectively prompt LLMs. Note that if we face a very 9.1 General Prompt Design 481 difficultclassificationproblemandhaveacertainamountoflabeleddata,fine-tuningLLMsor adopting“BERT+classifier”-likearchitecturesisalsodesirable. 2. Information Extraction Many NLP problems can be regarded as information extraction problems, involving the identification or extraction of specific pieces of information from unstructured text. This informationcanincludenamedentities,relationships,events,andotherrelevantdatapoints. The goal of information extraction is to transform raw data into a format that can be easily analyzedandusedinvariousdownstreamapplications. Asinformationextractioncoversawiderangeofproblems,wecannotdiscussthemall here. Instead, we start with the task of named entity recognition — a task that has long beenaconcerninNLP.Namedentityrecognitionisaprocessthatdetectsandclassifieskey informationintextintospecificgroups. Thesekeypiecesofinformation, knownasnamed entities,typicallyincludepropernamesandarecategorizedintodistinctclassessuchaspeople, locations, organizations, dates, monetary values, and percentages. Consider first a simple exampleofextractingpersonnamesfromatext3. Identifyallpersonnamesintheprovidedtext. Text: IstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic? ForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter. “IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto Europe,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof AmericandemandfortheUK.” Thepersonnameidentifiedintheprovidedtextis: TomJenkins Then,wecanextractallnamedentities. 3Thetextisfromhttps://edition.cnn.com/travel 482 Chapter 9. Prompting Identifyandclassifyallnamedentitiesintheprovidedtextintocategories such as person names, locations, dates, and organizations. List each entity withitstypeononeline. Text: IstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic? ForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter. “IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto Europe,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof AmericandemandfortheUK.” 1. TomJenkins-PersonName 2. EuropeanTourismOrganisation-Organization 3. UK-Location 4. Europe-Location 5. 2024-Date Given these identified named entities, we can further process the text using other infor- mationextractiontasks. Forexample,wecanidentifytherelationshipsbetweenthesenamed",
    "3Thetextisfromhttps://edition.cnn.com/travel 482 Chapter 9. Prompting Identifyandclassifyallnamedentitiesintheprovidedtextintocategories such as person names, locations, dates, and organizations. List each entity withitstypeononeline. Text: IstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic? ForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter. “IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto Europe,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof AmericandemandfortheUK.” 1. TomJenkins-PersonName 2. EuropeanTourismOrganisation-Organization 3. UK-Location 4. Europe-Location 5. 2024-Date Given these identified named entities, we can further process the text using other infor- mationextractiontasks. Forexample,wecanidentifytherelationshipsbetweenthesenamed entities(callitrelationextraction). Thecorrespondingpromptisshownasfollows. Given a text and a list of named entities identified within it, analyze and describetherelationshipsbetweentheseentities.Explainhoweachentityis contextuallyrelatedtoothers. Text: IstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic? ForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter. “IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto Europe,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof AmericandemandfortheUK.” NamedEntity: TomJenkins,EuropeanTourismOrganisation,UK,Europe,2024 Tom Jenkins - European Tourism Organisation: Tom Jenkins is the CEO of the European Tourism Organisation. This indicates a professional association where Tom Jenkins likely has a significant role in directing the organization’sactivities,especiallythoserelatedtotourisminEurope. ... 9.1 General Prompt Design 483 IfLLMshavebeenfine-tunedwithinstructionfollowingforinformationextractiontasks, itisrelativelyeasytoperformvariousinformationextractiontasks. Forexample,thefollowing isaprompttemplateforinformationextraction. Youwillbeprovidedwithatext.Yourtaskisto{∗task-description∗} Text: {∗text∗} Heretask-descriptionistheinstructionforthetask. Examplesoftheinstructioninclude extractthekeywordsfromthetext. extractthekeyeventsfromthetext. detectthecoreferenceandanaphoriclinksbetweenentitiesfromthetext. andsoon. Ofcourse,wecanprovideamoredetaileddescriptionofthetasktopromptLLMs moreaccurately. 3. Text Generation InNLP,thetermtextgenerationisusedbroadlytocoverawiderangeofdifferentproblems, manyofwhichfocusongeneratingtextbasedongiventext. Theseproblemscanberoughly categorizedintotwoclasses: 1)continualwritingbasedontheinputtext(callittextcomple- tion),and2)transformationoftheinputtextintoanothertext(callittexttransformation). Here,weillustratethepromptsfortheformerandconsiderthepromptsforthelatterinthe followingdiscussion. A simple example of text completion is continuing to write a story starting with a few givensentences. Youareawriterwhoisgreatatwritingstories.Youareprovidedwithafew sentencestostartwith.Pleasecontinuethestoryfromtheprovidedsentences. Startofthestory: Onesunnyafternoon,Lilydiscoveredatiny,goldenkeyhiddenunderarock inherbackyard. Curious,shepickeditupandwonderedwhatitmightunlock. Justthen,hercat,Whiskers,begantomeowexcitedly,leadinghertowards theoldoaktreeattheedgeofthegarden. Lily followed Whiskers, her heart pounding with excitement and a touch of nervousness. The old oak tree had always seemed mysterious to her, its gnarled branches casting long shadows even on the brightest days. As they reached the tree, Whiskers darted ahead and started pawing at something buriedbeneathapileofautumnleaves... 484 Chapter 9. Prompting Likewise,wecancreateaconversationgiventheconversationalcontext. You are provided with a conversation between a tourist and a taxi driver. Pleasecompletetheconversation. Tourist: Couldyoutakemetothedowntownmuseum,please? TaxiDriver: Ofcourse! Firsttimeinthecity? Tourist: Yes,anymust-seeplacesaround? Taxi Driver: Definitely check out the botanical gardens after the museum. They’renearbyandquitebeautiful. Tourist: Great,thanksforthetip! TaxiDriver: Noproblem. Enjoyyourvisit! Anotherapplicationoftextcompletionisgeneratingtextbasedonuserrequirements. The requirementscanincludestyle,tone,length,andanyspecificcontentthatthetextshouldcon- tain. Forexample,usersmayneedaformalreport,acreativestory,orapieceofprogramming code. Hereisapromptforgeneratingafive-characterregulatedChinesepoem. Youareapoet.PleasewriteatraditionalChinesepoem.Hereistherequire- ment. Theme: Spring’srejuvenation Structure: Five-characterregulatedpoem Emotion: Convey a sense of happiness and renewal through vivid spring imagery. 春意盎然 翠柳风中舞， 红花雨后新。 山明水又绿， 天宽鸟自频。 IftheLLMistrainedtogeneratelanguageandcode,wecanpromptittoperformcode completiontasks. Hereisanexample. 9.1 General Prompt Design 485 PleasewriteaPythonfunctiontocalculatetheaverageofalistofnumbers. defcalculate_average(numbers): ifnumbers: #Checkifthelistisnotempty returnsum(numbers)/len(numbers) else: return0 #Return0ifthelistisempty 4. Text Transformation Typicaltexttransformationtasksincludemachinetranslation,summarization,textstyletransfer, andsoon. Thepromptsusedtoperformthesetasksarenotsignificantlydifferentfromthe onespresentedpreviously. Here,wepresentafewexamplestoshowcasetheseprompts. TranslatethefollowingtextfromEnglishtoSpanish. Text: Thequickbrownfoxjumpsoverthelazydog. Elrápidozorromarrónsaltasobreelperroperezoso. Summarizethefollowingarticleinnomorethan50words: Article: Inrecentyears,urbanareashavebeenfacingsignificantchallengesrelatedto sustainabilityandenvironmentalmanagement. Citiesareexperiencinghigher ratesofpollution,increasedtrafficcongestion,andgreaterdemandsoninfras- tructure. Thisgrowthhasledtonumerousenvironmentalissues,including elevatedlevelsofairandwaterpollution,increasedwasteproduction,and strainedpublicservices... Urban areas are grappling with sustainability challenges, such as rising pollution,trafficcongestion,andinfrastructuredemands... 486 Chapter 9. Prompting Rewritethistextinaformaltone. Text: Hey,what’sup? Longtimenosee! Hello,howhaveyoubeen? Ithasbeenquitesometimesincewelastmet! 5. Question Answering Thequestion-answeringformatisinherentlysimple. Foragivenquestion,thereisananswer that corresponds to it. For example, in open-domain question answering, we expect the system to return an answer in response to a user-submitted question. Prompt templates for general-purposequestionansweringcanbe: {∗question∗} Question: {∗question∗} Answer: Q:{∗question∗} A: QuestionansweringisimportantinNLPbecausemanyproblemscanbeframedasquestion- answeringtasks. Inparticular,manyrecentreasoningtasksaredefinedintheformofquestion answering. Forexample,intheMMLUbenchmark[Hendrycksetal.,2021],eachexample consistsofamultiple-choicequestion,andLLMsarerequiredtoselectthecorrectanswer. See thefollowingforanexamplepromptforansweringaquestioninthisdataset. 9.1 General Prompt Design 487 (Z,∗)isagroupwitha∗b=a+b+1foralla,binZ. Theinverseofais (A) 0 (B) −2 (C) a−2 (D) (2+a)∗−1 D Anotherwidely-usedbenchmarkistheGSM8Kdataset[Cobbeetal.,2021]. Itconsistsof thousandsofgradeschoolmathwordproblems. Foreachproblem,weuseLLMstoreturn a solution in natural language. For example, the following is a prompt used for solving a probleminGSM8K. 488 Chapter 9. Prompting DEMO Q:Jerry’stwodaughtersplaysoftballondifferentteams. Theyeach have8gamesthisseason. Eachteampractices4hoursforeverygame theyplay. Ifeachgamelastsfor2hours,howmanyhourswillJerry spendatthefieldwatchinghisdaughtersplayandpracticealtogether? A:Jerrywillspend8games*2hourspergame=≪8∗2=16≫16 hourswatchingonedaughterplayhergames. Hewillspend16∗2=≪ 16∗2=32≫32 hours watching both daughters play their games. He will spend 8 games * 4 hours of practice = ≪8∗4=32≫32 hours watching one daughter practice. He will spend 32∗2 =≪ 32∗2=64≫64 hours watching both daughters practice. He will spendatotalof32hourswatchinggames+64hourswatchingpractice =≪32+64=96≫96hours. ####96 Q:Maryboughtsixapplesfromthestore. Fromtheapplesshebought, foreachthatMaryate,sheplantedtwotreesfromtheremainingones. HowmanyapplesdidMaryeat? A:Sheplantedeighttrees. Thismeanssheusedhalfofthatamountof apples,whichis8trees/2trees/apple=≪8/2=4≫4apples. That meansthatsheplantedfourofthesixapplesshebought,leavingonly 6apples−4apples=≪6−4=2≫2applestobeeaten. ####2 USER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If BorisgivesBeck10apples,howmanyfewerapplesdoesBeckhave",
    "practice = ≪8∗4=32≫32 hours watching one daughter practice. He will spend 32∗2 =≪ 32∗2=64≫64 hours watching both daughters practice. He will spendatotalof32hourswatchinggames+64hourswatchingpractice =≪32+64=96≫96hours. ####96 Q:Maryboughtsixapplesfromthestore. Fromtheapplesshebought, foreachthatMaryate,sheplantedtwotreesfromtheremainingones. HowmanyapplesdidMaryeat? A:Sheplantedeighttrees. Thismeanssheusedhalfofthatamountof apples,whichis8trees/2trees/apple=≪8/2=4≫4apples. That meansthatsheplantedfourofthesixapplesshebought,leavingonly 6apples−4apples=≪6−4=2≫2applestobeeaten. ####2 USER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If BorisgivesBeck10apples,howmanyfewerapplesdoesBeckhave thanBorisnow? A: Boris starts with 100 apples and gives Beck 10 apples, leaving him with 100 apples −10 apples =≪100−10=90≫90 apples. Initially, Beck has 23 fewer apples than Boris, giving him a starting count of 100−23=≪100−23=77≫77 apples. After receiving10applesfromBoris,Beck’stotalcomesto77apples+10 apples=≪77+10=87≫87apples. Consequently,Becknowhas 90−87=≪90−87=3≫3fewerapplesthanBoris. ####3 Hereafew-shotpromptisadopted. TheLLMlearnsfromthesedemonstrationsofproblem- solutionpairsnotonlythewayofproblem-solvingbutalsothewayofformattingtheoutput. Forexample,thefinalresultofcalculationfollowsthe####token,and≪...≫annotatesthe detailedcalculationsteps(calledcalculationannotation)4. 4Duringprediction,acalculatorisusedwhenwesee≪...≫.Morespecifically,oncetheLLMencounters“=” ina≪...≫,thenthecalculatorcalculatestheexpressionontheleft-handsideof“=”.Thismethodhelpsreduce thecalculationerrorsmadebyLLMs. 9.2 Advanced Prompting Methods 489 9.2 Advanced Prompting Methods So far in this chapter, we have introduced the basic concepts related to LLM prompting andpresentedanumberofpromptsforNLPtasks. Wenowconsiderseveraltechniquesfor enhancingtheeffectivenessofprompting. 9.2.1 Chain of Thought We have encountered the concept of chain of thought (CoT) several times in this chapter and previous ones [Wei et al., 2022c; Chowdhery et al., 2022]. CoT methods provide a simplewaytopromptLLMstogeneratestep-by-stepreasoningforcomplexproblems,thereby approachingtasksinamorehuman-likemanner. Ratherthancomingtoaconclusiondirectly, theCoTmethodsinstructLLMstogeneratereasoningstepsortolearnfromdemonstrations ofdetailedreasoningprocessesprovidedintheprompts. ToillustrateCoT,weconsiderthe problem of algebraic calculation, as commonly described in the literature. Suppose we are givenanalgebraicproblem Calculatetheaverageofthenumbers2,4,and6. WecanconsideritasthequestionandpromptanLLMtoanswerit. Q:Pleasecalculatetheaverageofthenumbers2,4,and9. A:Theansweris6. ItseemsdifficultfortheLLMtodirectlygiveacorrectanswer. Asimpleimprovementis toadddemonstrationsofsimilarproblemsintheprompt,andthustheLLMcanlearnfrom thesedemonstrations. Q:Pleasecalculatetheaverageofthenumbers1,3,5,and7. A:Theansweris4. Q:Pleasecalculatetheaverageofthenumbers2,4,and9. A:Theansweris7. Theproblemhereisthat,althoughwehaveshownasimilarquestion-answerpair,itremains difficultfortheLLMtoreasonoutthecorrectanswer. InCoT,notonlycanLLMslearnfrom the correspondence between questions and answers but they may gain more from detailed problem-solvingstepsthatareusedtoderivetheanswers. Todothis,wecanincorporatesome reasoningstepsintotheprompttoobtainaCoTprompt. 490 Chapter 9. Prompting Q:Pleasecalculatethemeansquareofthenumbers1,3,5,and7. A:Calculatethesquareofeachnumber:12=1,32=9,52=25,and72=49. Sumthesquares,1+9+25+49=84.Thereare4numbersintotal.Divide thesumbythenumberofitems,84/4=21.Theansweris21. Q:Pleasecalculatetheaverageofthenumbers2,4,and9. A:Calculate2+4+9,whichequals15.Therearethreenumbers.Dividethe totalsumbythecount,resultingin15/3=5.Theansweris5. Herewehighlightthereasoningstepsingreen. Byprovidingadetailedreasoningprocessfor asimilarproblem,theLLMlearnstoreasonandtherebygeneratesaproblem-solvingpaththat leadstothecorrectanswer. ThereareseveralbenefitsofusingCoTprompting. First,CoTallowsLLMstodecompose complex problems into smaller, sequential reasoning steps. This somewhat mirrors human problem-solvingbehaviors,makingitparticularlyeffectivefortasksrequiringdetailed,multi- stepreasoning. Second,CoTmakesthereasoningprocessmoretransparentandinterpretable. Sinceallreasoningstepsarevisible,wecanunderstandandinterprethowaconclusionwas reached. Third,ifuserscanfollowthelogicbehindthereasoningprocess,theywillbemore likelytotrustthepredictionsofanLLM.ThisisparticularlyimportantwhenapplyingLLMs infieldslikemedicine,education,andfinance. Fourth,CoTisanin-contextlearningapproach, andthus,itisapplicabletomostwell-trained,off-the-shelfLLMs. Moreover,CoTprovides efficientwaystoadaptLLMstodifferenttypesofproblems. Itcaneveninspiremorecreative solutionsbyexploringvariousalternativereasoningpaths,whichmightnotbeobviouswhen arrivingataconclusiondirectly. ThemethoddescribedaboverequiresprovidingoneormoreexamplesofCoTreasoning, typicallycalledthefew-shotCoTmethod. Bycontrast,thezero-shotCoTmethoddoesnot require such examples. It instead prompts LLMs to reason step-by-step by incorporating specificinstructionsinprompts. Forexample,belowisazero-shotCoTprompt. Q:Pleasecalculatetheaverageofthenumbers2,4,and9. A:Let’sthinkstep-by-step. We have three numbers: 2, 4, and 9. Add these numbers together, 2+4+9=15. Determinehowmanynumbersthereare,whichinthiscase is three. The average is calculated by dividing the total sum by the number ofelements. Completingthedivisiongives15/3=5. Sotheansweris5. Following the instruction “Let’s think step by step”, the LLM is prompted to generate detailedreasoningsteps. AsdiscussedinKojimaetal.[2022]’swork,promptingwithsuch instructionsmayresultinLLMsgeneratingonlythereasoningstepswithoutaclearconclusion. Inthiscase,asecondroundofpromptingcanbeusedtoextracttheanswerfromthesereasoning 9.2 Advanced Prompting Methods 491 steps. For example, Kojima et al. [2022] create a second prompt which combines both the input and output in the first round of prompting. Using this combined input, the LLM can continueitsreasoningprocessandthengeneratethecorrectanswer. Furthermore,itispossible to prompt LLMs to reason using instructions other than “Let’s think step by step”, such as “Let’sthinklogically”and“Pleaseshowmeyourthinkingstepsfirst”. While we have illustrated CoT methods using an algebraic reasoning problem, these methodscanbeappliedtoavarietyofdifferentproblems. Typicalproblem-solvingscenarios forCoTincludemathematicalreasoning,logicalreasoning,commonsensereasoning,symbolic reasoning,codegeneration,andsoon. SeeFigure9.1formoreexamplesofapplyingCoTin varioustasks. CoTtodayisoneofthemostactivefieldsofpromptengineering. Thishasnotonlyled to improved performance for LLM prompting but has opened the door to a wide range of methodsforstudyingandverifyingreasoningcapabilitiesofLLMs. Althoughwehavefocused on the basic idea of CoT in this section, it can be improved in several ways. For example, we can consider the reasoning process as a problem of searching through many possible paths, each of which may consist of multiple intermediate states (i.e., reasoning steps). In general, we wish the search space to be well-defined and sufficiently large, so that we are more likely to find the optimal result. For this reason, an area of current LLM research is aimedatdesigningbetterstructuresforrepresentingreasoningprocesses,allowingLLMsto tacklemorecomplexreasoningchallenges. Thesestructuresincludetree-basedstructures[Yao etal.,2024],graph-basedstructures[Bestaetal.,2024],andsoon. Byusingthesecompact representationsofreasoningpaths,LLMscanexploreawiderrangeofdecision-makingpaths, analogoustoSystem2thinking5. AnotherlineofresearchfocusesonpromptingLLMswith multi-roundinteractions. Thisinvolvesdecomposingcomplexproblemsintosub-problems, verifyingandrefiningmodeloutputs,employingmodelensembling,andsoon. Notethatthese methodsandtheissuesinvolvedarenotlimitedtoCoT.Infact,theyareoftenusedasmore generalapproachestoimprovingLLMs,whileCoTcanbeseenasawaytotestthecapabilities ofLLMs. Wewillseediscussionsofsomeoftheseissuesinthefollowingsubsections.",
    "In general, we wish the search space to be well-defined and sufficiently large, so that we are more likely to find the optimal result. For this reason, an area of current LLM research is aimedatdesigningbetterstructuresforrepresentingreasoningprocesses,allowingLLMsto tacklemorecomplexreasoningchallenges. Thesestructuresincludetree-basedstructures[Yao etal.,2024],graph-basedstructures[Bestaetal.,2024],andsoon. Byusingthesecompact representationsofreasoningpaths,LLMscanexploreawiderrangeofdecision-makingpaths, analogoustoSystem2thinking5. AnotherlineofresearchfocusesonpromptingLLMswith multi-roundinteractions. Thisinvolvesdecomposingcomplexproblemsintosub-problems, verifyingandrefiningmodeloutputs,employingmodelensembling,andsoon. Notethatthese methodsandtheissuesinvolvedarenotlimitedtoCoT.Infact,theyareoftenusedasmore generalapproachestoimprovingLLMs,whileCoTcanbeseenasawaytotestthecapabilities ofLLMs. Wewillseediscussionsofsomeoftheseissuesinthefollowingsubsections. BeforeleavingourdiscussionofCoT,weshouldconsideritspracticallimitations. Oneof themistheneedfordetailed,multi-stepreasoningdemonstrationsinfew-shotCoTscenarios, whichmaybedifficulttoobtain,eitherautomaticallyormanually. Also,thereisnostandard methodforbreakingdowncomplexproblemsintosimplerproblem-solvingsteps. Thisoften heavily depends on the user’s experience. In addition, errors in intermediate steps can also affecttheaccuracyofthefinalconclusion. ForfurtherdiscussionontheprosandconsofCoT, theinterestedreadercanrefertorecentsurveysonthistopic[Chuetal.,2023;Yuetal.,2023b; Zhangetal.,2023a]. 5System1andSystem2thinking,asdescribedbyKahneman[2011],representtwodifferentmodesofcognitive processing.System1isfast,automatic,intuitive,andemotional.Thismodeofthinkingoperateseffortlesslyand quickly,andisoftenwhatguidesourdailydecisions,judgments,andimpressions.System2isslow,deliberate, andanalytical.Itisactivatedwhenweneedtoperformcomplexcomputations. 492 Chapter 9. Prompting Q: What home entertainment equipment Q:Dohamstersprovidefoodforanyani- requirescable? mals? AnswerChoices: (a)radioshack(b)sub- A: Hamsters are prey animals. Prey are station(c)television(d)cabinet foodforpredators.Thus,hamstersprovide A:Theanswermustrequirecable.Ofthe foodforsomeanimals.Sotheansweris abovechoices,onlytelevisionrequiresca- yes. ble.Sotheansweris(c). Q:CanyouuseMicrosoftOfficewithout Q:Whattypeofpersontypicallycontracts internet? illness? A:Yes,youcanuseMicrosoftOfficeappli- AnswerChoices:(a)hospital(b)head(c) cations,althoughfeatureslikecloudstor- sickperson(d)elderlyperson(e)doctor’s ageandonlinecollaborationrequireinter- office net.Sotheanswerisyes. A: Theanswermustbeapersonsuscepti- bletoillness.Ofthechoices,\"elderlyper son\"iscorrectbecausetheytypicallyhave weakerimmunesystems. Sotheansweris(d). (a)CSQA (b)StrategyQA Q:Completetherestofthesequence,mak- Q: Take the last letters of the words in ing sure that the parentheses are closed \"ElonMusk\"andconcatenatethem. properly.Input:[{ A:Thelastletterof\"Elon\"is\"n\".Thelast A:Let’sthinkstepbystep. letterof\"Musk\"is\"k\".Concatenatingthem 0:emptystack is\"nk\".Theanswerisnk. 1:[;stack:[ Q: Take the last letters of each word in 2:{;stack:[{ \"Renata Mariela Mona Kristin\" and con- Sotheansweris}]. catenatethem. Q:Completetherestofthesequence,mak- A:Thelastletterof\"Renata\"is\"a\".The ing sure that the parentheses are closed lastletterof\"Mariela\"is\"a\".Thelastlet- properly.Input:<[[ terof\"Mona\"is\"a\".Thelastletterof A:Let’sthinkstepbystep. \"Kristin\"is\"n\".Concatenatingthemis 0:emptystack \"aaan\".Theanswerisaaan. 1:<;stack:< 2:[;stack:<[ 3:[;stack:<[[ Sotheansweris]]>. (c)Dycklanguages (d)LastLetterConcatenation Figure9.1: CoTinfourdifferentreasoningtasks,includingCSQA,StrategyQA,Dycklan- guages,andLastLetterConcatenation. TheCoTpartsarehighlightedingreen. 9.2.2 Problem Decomposition We have seen that LLMs can benefit from solving a complex problem by breaking it down intosimplerproblem-solvingtasks. Suchanapproachcanbeseenasanexampleofabroader paradigmknownasproblemdecomposition,whichhasbeenextensivelyexploredanddis- 9.2 Advanced Prompting Methods 493 cussed in psychology and computer science. From the psychological perspective, complex problem-solvingreferstoaprocessofaddressingaproblemusingknowledgethathelpsover- comethebarriersoftheproblem6. Therearegenerallynostandardorclearpathstoasolution foracomplexproblem. However,itisoftenadvantageoustoemploystrategiesthatdecompose theproblem,therebymakingiteasiertotacklethecorrespondingsub-problemswithlesseffort. Forexample,considerwritingablogabouttherisksofAI.IfwesimplypromptanLLMwith theinstruction“PleasewriteablogabouttherisksofAI”,theLLMmaygenerateablogwith arbitrarystructuresandwritingstyles. Abettermethod,instead,couldbetooutlinetheblog andprovidemoredetailedinformationabouteachsection. Considerthefollowingprompt You are a blog writer. Please follow the provided outline below to write a blogabouttherisksofAI. • Introduction Introduce AI, its relevance, and the importance of understanding its risks for youth. • PrivacyConcerns DiscusshowAImightcompromisepersonalprivacythroughinteractionsonline. • Misinformation ExploreAI’sroleinspreadingmisinformationandinfluencingyoungpeople’s decisions. • Cyberbullying HighlighthowAItoolscanbeutilizedincyberbullyingandtheimpactonmental health. • TipsforSafeAIUse OfferguidelinesforresponsibleAIusageandpromotecriticalthinking. • Conclusion RecapmainpointsandencourageproactiveengagementwithAIethics. Herewegivethetitleandmajorpointsforeachsection. Then,theLLMcanusethisstructure tobreakdownthewritingtaskbyfillingincontentforthesesections. Notethatthewayto structuretheblogcanbeprovidedbyhumansorevengeneratedautomatically. Forexample, we can use the LLM to first generate the outline, and then ask it to follow this outline to completethewriting. Incomputerscience,decomposingcomplexproblemsisacommonlyusedstrategyinsoft- wareandhardwaresystemdesign. Awell-knownexampleisthedivide-and-conquerparadigm, which is often used to design algorithms for computation problems that can be reduced to simpler,moremanageableproblems. Forexample,consideraproblemofdeterminingwhether 6ArelativelyformaldefinitioncanbefoundinFrenschandFunke[2014]’sbook:complexproblem-solving occurstoovercomebarriersbetweenagivenstateandadesiredgoalstatebymeansofbehavioraland/orcognitive, multi-stepactivities. 494 Chapter 9. Prompting adocumentdiscussestherisksofAI.WecaninstructtheLLMwiththefollowingprompt. Youareprovidedwithatext. Pleasedeterminewhetheritdiscussestherisks ofAI. {∗document∗} Ifthedocumentislong,thecomputationwillbeexpensive. Alternatively,wecandivide thedocumentintorelativelyshortsegmentsandperformthesametaskoneachsegment. These segments can be processed in parallel to further reduce the computational cost. Next, we determine the relevancy of each segment to the topic of AI risks. The final output is then generatedusinganotherprompt. YourtaskistodeterminewhetheratextdiscussestherisksofAI.Thistext hasbeendividedintosegments,andyouhaveobtainedtherelevancyofeach segment to the topic of AI risks. Based on this, please provide your final result. Segment1: {∗relevancy-to-the-topic1∗} Segment2: {∗relevancy-to-the-topic2∗} Segment3: {∗relevancy-to-the-topic3∗} ... Nowletusreturntoamoregeneraldiscussionofproblemdecompositioninprompting. While problem decomposition can be applied to various NLP problems, it has been more extensively discussed and tested in reasoning tasks recently. For complex reasoning tasks, weoftenneedamulti-stepreasoningpathtoreachacorrectconclusion. WecanuseLLMs to achieve this in three different ways. First, LLMs can directly reach the conclusion. In otherwords,theycanpredictwithoutexplicitreasoningprocesses,andthereisahiddenand uninterpretablereasoningmechanism. Second,LLMsarepromptedtogenerateamulti-step reasoningpaththatleadstotheconclusion,likeCoT.However,werunLLMsjustonce,andall intermediatestepsinreasoningaregeneratedinasingleprediction. Third,webreakdownthe originalproblemintoanumberofsub-problems,whichareeitheraddressedinseparateruns ofLLMsortackledusingothersystems. Herewefocusourattentiononthethirdapproach, whichiscloselyrelatedtoproblemdecomposition. Note,however,thatamorecomprehensive discussioncouldcoveralltheseapproaches,whilethefirsttwohavebeendiscussedtosome extentinthischapter. Ageneralframeworkforproblemdecompositioninvolvestwoelements. 9.2 Advanced Prompting Methods 495 • Sub-problemGeneration. Thisinvolvesdecomposingtheinputproblemintoanumber ofsub-problems. • Sub-problemSolving. Thisinvolvessolvingeachsub-problemandderivingintermedi- ateandfinalconclusionsthroughreasoning. These two issues can be modeled in different ways, leading to various problem decom- positionmethods. Oneapproachistotreatthemasseparatestepsinatwo-stepprocess. For example,considertheblogwritingtaskdescribedatthebeginningofthissubsection. Inthe first step, we decompose the entire problem into sub-problems all at once",
    "Herewefocusourattentiononthethirdapproach, whichiscloselyrelatedtoproblemdecomposition. Note,however,thatamorecomprehensive discussioncouldcoveralltheseapproaches,whilethefirsttwohavebeendiscussedtosome extentinthischapter. Ageneralframeworkforproblemdecompositioninvolvestwoelements. 9.2 Advanced Prompting Methods 495 • Sub-problemGeneration. Thisinvolvesdecomposingtheinputproblemintoanumber ofsub-problems. • Sub-problemSolving. Thisinvolvessolvingeachsub-problemandderivingintermedi- ateandfinalconclusionsthroughreasoning. These two issues can be modeled in different ways, leading to various problem decom- positionmethods. Oneapproachistotreatthemasseparatestepsinatwo-stepprocess. For example,considertheblogwritingtaskdescribedatthebeginningofthissubsection. Inthe first step, we decompose the entire problem into sub-problems all at once (i.e., outline the blog). Inthesecondstep,wesolvethesub-problemseithersequentiallyorinanotherorder (i.e.,fillincontentforeachsectionasneeded). Thefinaloutputofthisprocesscombinesthe resultsfromsolvingeachsub-problem. Whilethismethodissimpleandstraightforward,it assumesthattheproblemiscompositional,makingitmoresuitablefortaskslikewritingand codegeneration. However,manyreal-worldproblemsrequirecomplexreasoning. Onekeycharacteristic oftheseproblemsisthatthereasoningstepsmaynotbefixed. Thereasoningpathcanvary for different problems, and each step of reasoning may depend on the outcomes of prior steps. Insuchcases,itisundesirabletousefixedsub-problemgenerationinadvance. Instead, sub-problemsshouldbegenerateddynamicallybasedontheinputproblem,and,ifpossible, generatedontheflyduringthereasoningprocess. Thismakesproblemdecompositionmore challengingcomparedwithdesigningdivide-and-conqueralgorithms. Ideally,wewouldlike tojointlydesignboththesystemsforsub-problemgenerationandsub-problemsolving. But a more practical and widely used approach is to adopt separate models for these tasks. A straightforwardwaytoachievethisistoadaptanLLMforthesetasksbyeitherpromptingor tuningthemodel. Here we consider a method based on the above idea, called least-to-most prompting [Zhou et al., 2023b]. The motivation for this method arises from the challenges of solving difficultreasoningproblems—thosethatcannotbeaddressedbysimplygeneralizingfrom afewexamples. Fortheseproblems,amoreeffectiveproblem-solvingstrategyistofollow a progressive sequence of sub-problems that systematically lead to the conclusion. More specifically,intheleast-to-mostpromptingmethod,sub-problemgenerationisperformedby promptinganLLMwithinstructionsand/ordemonstrations. Forexample,belowisa2-shot promptforsub-problemgenerationinleast-to-mostprompting. 496 Chapter 9. Prompting TASK Your task is to decompose a problem into several sub-problems. You willbegivenafewexamplestoillustratehowtoachievethis. DEMO Q:Inacommunity,5%ofthepopulationareinfants,15%arechildren, 40%areadults,and40%areseniors. Whichgroupmakesupthelargest portionofthepopulation? A:Toanswerthequestion“Whichgroupmakesupthelargestportionof thepopulation?”,weneedtoknow: “Howmanypercentareinfants?”, “How many percent are children?”, “How many percent are adults?”, “Howmanypercentareseniors?”. Q:Alice,Bob,andCharliebroughtbeadsfortheirgroupprojectintheir craft class. Alice has twice as many beads as Bob, and Bob has five timesasmanybeadsasCharlie. IfCharliehas6beads,howmanybeads cantheyusefortheircraftproject? A:Toanswerthequestion“Howmanybeadscantheyusefortheircraft project?”,weneedtoknow: “HowmanybeadsdoesBobhave?”,“How manybeadsdoesAlicehave?”. USER Q:Theenvironmentalstudyconductedfrom2015to2020revealedthat theaveragetemperatureintheregionincreasedby2.3degreesCelsius. Whatwasthedurationoftheenvironmentalstudy? A:Toanswerthequestion“Whatwasthedurationoftheenvironmental study?”,weneedtoknow: “Whendidtheenvironmentalstudystart?”, “Whendidtheenvironmentalstudyend?”. Bylearningfromtheexamples,theLLMcangeneratetwosub-problemsforanswering thenewproblem“Whatwasthedurationoftheenvironmentalstudy?” (highlightedinblue andorange). Giventhesesub-problems,wesolvethemsequentially. Foreachsub-problem, wetakeallpreviously-generatedQApairsascontext,andthenproducetheanswer. Forthe exampleabove,weneedtoanswerthefirstsub-problembypromptingtheLLM,likethis The environmental study conducted from 2015 to 2020 revealed thattheaveragetemperatureintheregionincreasedby2.3degrees Celsius. SUB-PROB1 Q:Whendidtheenvironmentalstudystart? A:Theenvironmentalstudystartedin2015. Once we have the answer to the first sub-problem, we proceed to the second one. This time,weincludeboththefirstsub-problemanditscorrespondinganswerintheinput. 9.2 Advanced Prompting Methods 497 The environmental study conducted from 2015 to 2020 revealed thattheaveragetemperatureintheregionincreasedby2.3degrees Celsius. SUB-PROB1 Q:Whendidtheenvironmentalstudystart? A:Theenvironmentalstudystartedin2015. SUB-PROB2 Q:Whendidtheenvironmentalstudyend? A:Theenvironmentalstudyendedin2020. Finally, we use the LLM to solve the original problem given the answers to all the sub- problems. The environmental study conducted from 2015 to 2020 revealed thattheaveragetemperatureintheregionincreasedby2.3degrees Celsius. SUB-PROB1 Q:Whendidtheenvironmentalstudystart? A:Theenvironmentalstudystartedin2015. SUB-PROB2 Q:Whendidtheenvironmentalstudyend? A:Theenvironmentalstudyendedin2020. FINAL Q:Whatwasthedurationoftheenvironmentalstudy? A:Thedurationoftheenvironmentalstudywas5years. The least-to-most method offers a basic approach to prompting LLMs to generate and solvesub-problemsseparately. Wecanimproveitinseveralways. Onesimpleimprovement is to apply various advanced prompting techniques, which do not require changes to the problemdecompositionframework. Forexample,wecanincorporateCoTintotheprompting toenhancethereasoningperformanceofsub-problemgenerationandsolving. Anotherimprovementistoexploremethodsforbetterdecomposingproblemsandorganiz- ingproblem-solvingpaths. Todescribetheseapproaches,wewillusethesymbolp todenote 0 theinputproblem,andusethesymbols{p ,...,p }todenotethesub-problemscorresponding 1 n top . Forleast-to-mostprompting,wedecomposep into{p ,...,p },givenby 0 0 1 n {p ,...,p } = G(p ) (9.2) 1 n 0 whereG(·)denotesthefunctionofsub-problemgeneration. Then,wesolvethesub-problems {p ,...,p }sequentially,resultinginasequenceofanswers{a ,...,a }. Foransweringthe 1 n 1 n i-thsub-problemp ,weincludeboththeoriginalproblemp andallpreviously-seenproblem- i 0 498 Chapter 9. Prompting answerpairsinthecontextforprediction. Theanswera isgivenby i a = S (p ,{p ,p ,a }) (9.3) i i i 0 <i <i wherep ={p ,...,p }anda ={a ,...,a }. S (·)denotesthefunctionthatsolvesthe <i 1 i−1 <i 1 i−1 i sub-problemp giventhecontext{p ,p ,a }. Thelaststepistogeneratetheanswertothe i 0 <i <i originalproblemp ,whichcanbeexpressedinasimilarmannertoEq. (9.3). 0 a = S (p ,{p ,a }) (9.4) 0 0 0 ≤n ≤n OnewaytorefinethismodelistomodifytheG(·)functionsothatthemodelcandynami- callygenerateanswers. Insteadofgeneratingallsub-problemsatonetime,wecangenerate eachofthemduringproblem-solving[Duaetal.,2022]. Todothis,wecanreplaceEq. (9.2) with p = G (p ,{p ,a }) (9.5) i i 0 <i <i Henceweobtainasub-problemgenerationmodelthatoperatesinastep-by-stepmanner. At each step i, we first generate the sub-problem p by prompting an LLM with the original i problemp andtheproblem-solvinghistory{p ,a }. Wethengeneratetheanswera for",
    "0 0 0 ≤n ≤n OnewaytorefinethismodelistomodifytheG(·)functionsothatthemodelcandynami- callygenerateanswers. Insteadofgeneratingallsub-problemsatonetime,wecangenerate eachofthemduringproblem-solving[Duaetal.,2022]. Todothis,wecanreplaceEq. (9.2) with p = G (p ,{p ,a }) (9.5) i i 0 <i <i Henceweobtainasub-problemgenerationmodelthatoperatesinastep-by-stepmanner. At each step i, we first generate the sub-problem p by prompting an LLM with the original i problemp andtheproblem-solvinghistory{p ,a }. Wethengeneratetheanswera for 0 <i <i i thissub-problemusingthesameoradifferentLLM,basedonthesamecontextualinformation (seeEq. (9.3)). ThismethodeffectivelyexpandsthereasoningcapacityofLLMsbyallowing themtodynamicallygenerateandsolvesub-problemsinintermediatereasoningsteps. Asa result,thereasoningpathsarenotfixedinadvance,andthemodelscanchooseandadapttheir reasoningstrategiesduringproblem-solving. Anotherwaytoimprovetheabovemodelistofocusondevelopingbettersub-problem solvers. Inourpreviousdiscussion,werestrictedS (·)toLLMsthatarepromptedtosolvethe i sub-problemp . Infact,wecanexpandthisfunctiontoanysystemthatiscapableofaddressing i the sub-problem. For example, S (·) could make calls to IR systems, thereby allowing us i to access a broaderrange ofdata forproblem-solving. Another example isusing S (·)as a i calculatortoaccuratelycomputeresultsinmathematicalproblem-solving. Ifthesub-problem p is complex and requires multiple intermediate problem-solving steps, it is also possible i to further decompose p into smaller sub-problems. For example, S (·) can be defined as a i i recursiveprogramthatgeneratesandsolvessub-problems. Thisincorporatesrecursioninto problem-solvingandallowsustoaddressproblemsbyiterativelydecomposingthem. Asa result,wecandefineahierarchicalstructureforproblem-solving[Khotetal.,2023]. Ifwegeneralizetheaboveformulationabitfurther,wecanconsideritasareinforcement learning problem. A typical method is to model a problem-solving process as a decision making process. In each step of this process, an action is taken based on the current state. Theseactionscanincludeallfunctionsforsub-problemgenerationandsolving(i.e.,G (·)and i S (·)). Thus,theactionsequencecorrespondstoaproblem-solvingpath. Sincethediscussion i ofreinforcementlearningproblemsisbeyondthescopeofthischapter,weskiptheprecise descriptionofthislearningtask. Nevertheless,developinganagentorcontrollertodetermine 9.2 Advanced Prompting Methods 499 whenandhowtogenerateandsolveasub-problemisalsoanaturalchoice. InNLP,problemdecompositionisrelatedtoalonglineofresearchonmulti-hopquestion answering[Mavietal.,2024]. Thistaskrequiresthesystemtogatherandcombineinformation frommultiplepiecesoftexttoprovideanaccurateanswertoacomplexquestion. Forexample, toanswerthequestion“WhatisthecapitalofthecountrywhereAlbertEinsteinwasborn?”, weneedtoknow“WhereAlbertEinsteinwasborn?” and“What’sthecapitalofGermany?”. Earlierworkinthisareaandrelatedoneshasinvestigatedtheissueofproblemdecomposition, thoughthemethodsmightnotbebasedonLLMs. Forexample,apopularmethodistodevelop anadditionalneuralmodeltogeneratesimplerquestionsthataddressdifferentaspectsofthe original question [Andreas et al., 2016; Talmor and Berant, 2018; Min et al., 2019]. This questiongeneratorcancreatequestionsinabatchorsequentialmanner. Broadlyspeaking,problemdecompositionisalsorelatedtothecompositionalityissuein NLP [Drozdov et al., 2022; Press et al., 2023]. For example, in semantic parsing, we map naturallanguagesentencesintostructuredmeaningrepresentationsbybreakingthemdown intoconstituentpartsandunderstandingthesentencesbasedonthemeaningsofthesepartsand therulesusedtocombinethem. Inearlystudiesofthisfield,highlycompositionalsentences were considered easier for testing systems, as it is relatively straightforward to decompose suchsentencesandcomposethemeaningsoftheirparts. However,thetaskbecomesmuch moredifficultwhenmoregeneralizationisrequiredformodelingcompositionalityinnewdata. In this case, we want systems to have improved abilities of compositional generalization. InmorerecentresearchonLLMs,thisissuehasbeenfrequentlydiscussedincompositional reasoningtasks,suchasSCAN7,asitisconsideredanimportantaspectoftestingthelanguage understandingandreasoningabilitiesofLLMs. Thisalsopresentsnewtasksfordeveloping andexaminingproblemdecompositionmethods. InLLMs,oneinterestingapplicationofproblemdecompositionistooluse. Insomecases, itisnecessarytointegrateexternaltoolsintoLLMstoaccessaccuratedatanotavailableduring trainingorfine-tuning. Forexample,LLMscanintegratewithAPIstofetchreal-timedatasuch asweatherupdates,stockmarketprices,ornewsfeeds,enablingthemtoprovideup-to-date responses to user queries. When using tools, LLM predictions might include markers that indicatewhereandhowtocallexternalAPIs. Thisrequiresdecomposingtheprobleminto sub-problems,withsomehandledbytheLLMsandothersbyexternaltools. Moredetailed discussionsonthistopicwillbepresentedinSection9.2.5. 9.2.3 Self-refinement Inmanycases,predictionsofLLMscanbeinaccurateorincorrect. GiventhatcurrentLLMs canperformtaskslikerefinementandcorrection,itmakessensetoexploremethodsforthese models to self-refine their outputs. Self-refinement is a common phenomenon in human psychologicalactivitiesanddailybehavior. Forexample,whendesigningaproduct,adesigner 7TheSCANtasks(SimplifiedversionsoftheCommAINavigationtasks)aredesignedtoevaluatetheabilityof LLMstoperformcompositionalgeneralization[LakeandBaroni,2018].Theyinvolvetranslatingnaturallanguage commandsintoasequenceofactions. Forexample,acommand“jumpoppositeleftandwalkthrice”canbe translatedintotheactionsequence“LTURNLTURNJUMPWALKWALKWALK”. 500 Chapter 9. Prompting might first create a basic prototype, then refine the design after evaluation and testing to enhanceuserexperienceandfunctionality. Therefinementcanbeiteratedseveraltimesuntil thedesignissatisfactory. Theideaofpredict-then-refinecanalsobefoundinNLP.Oneearly exampleisBrill’stagger[Brill,1992],whereaninitialPOStaggingresultofasentencecanbe iterativelyrefinedusingarule-basedsystem. Intheeraofdeeplearning,agooddealofwork onsequence-to-sequenceproblems,suchasgrammarcorrectionandtextrewriting,canalsobe seenasexamplesonthistheme. WecanpromptLLMstodoself-refinement. Considerasimpleexamplewherewefirstuse anLLMtotranslateatextfromChinesetoEnglishandthenusethesamemodeltorefinethe translation. Hereishowwecanpromptthemodeltotranslate: TranslatethefollowingtextfromChinesetoEnglish: Chinese: 一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最 新公布的研究成果认为，大约从距今5800年开始，中华大地上各个区 域相继出现较为明显的社会分化，进入文明起源的加速阶段。 English: A series of discoveries have cemented the significant role of the Hongshan culture in studies on the origins of Chinese civilization. The latest research findings suggest that, starting from around 5800 years ago, many regions acrossChinabegantoexperiencenoticeablesocialdifferentiations,entering anacceleratedphaseintheoriginofcivilization. Then,wecanpromptittorefinethetranslation: NiuTrans 9.2 Advanced Prompting Methods 501 YouareprovidedwithaChinesetextalongwithitsEnglishtranslation. Please refinethetranslationtoimproveitsaccuracyandfluencyifnecessary. Chinese: 一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最 新公布的研究成果认为，大约从距今5800年开始，中华大地上各个区 域相继出现较为明显的社会分化，进入文明起源的加速阶段。 English: AseriesofdiscoverieshavecementedthesignificantroleoftheHongshan cultureinstudiesontheoriginsofChinesecivilization. Thelatestresearch findingssuggeststhat, startingfromaround5800yearsago, manyregions acrossChinabegantoexperiencenoticeablesocialdifferentiations,entering anacceleratedphaseintheoriginofcivilization. Refinement: Aseriesofarchaeologicaldiscoverieshavecementedthesignificantrole oftheHongshancultureinstudiesontheoriginsofChinesecivilization. Thelatestresearchfindingssuggestthat,startingfromaround5800years ago,variousregionsacrossChinabegantoexperiencenoticeablesocial differentiations,enteringanacceleratedphaseintheoriginofcivilization. Here modifications to the input translation are highlighted in green. This process can be iterative: ateachstep,weusethetranslationfromthepreviousstepasinput,andpromptthe LLMtoproduceapotentiallybettertranslation. Intheaboveexample,therefinementoftheinitialmodeloutputwasachievedbyprompting theLLMwithasimpleinstructionlike“Pleaserefineit!”. However,therefinementisbased solelyontheinstruction-followingabilityoftheLLM,andthereisnoguidanceorsupervision on how and where to improve the model output. A more effective approach would be to considerfeedbackonspecificaspectsthatneedrefinement. Forexample,wecanpromptthe LLMwith“Pleasecorrectallthegrammaticalerrorsinthetranslation”,sothatthemodelcan focusmoreongrammaticalerrorcorrectionduringrefinement. Ageneralframeworkofself-refinementwithLLMsinvolvesthreesteps[Madaanetal., 2024]. • Prediction. WeuseanLLMtoproducetheinitialmodeloutput. • FeedbackCollection. Weobtainfeedbackonthemodeloutput. • Refinement. WeusetheLLMtorefinethemodeloutputbasedonthefeedback. Thelasttwostepscanberepeatedmultipletimes,whichleadstoaniterativeself-refinement process. Inthisframework,receivingaccurateanddetailedfeedbackiscriticalasitguidesthe subsequentrefinementstep. Ingeneral,feedbackcanbeobtainedinvariousways[Fernandes etal.,2023;Panetal.,2024]. Wecanmanuallyexaminethemodel’spredictionstoidentify 502 Chapter 9. Prompting issues. Also,wecanuseafeedbackmodeltoautomaticallygeneratefeedback. Forinstance,a rewardmodeltrainedonsomelabeleddatacanhelpevaluatethemodeloutputandreturna scoreindicatingitsquality. SincethefocusofthischapterisonLLMprompting,weconsiderusingLLMstoprovide feedback. To illustrate, let us consider an example of generating good responses to user questions: • First,wepromptanLLMtoanswertheinputquestion,asusual. Generatearesponsetothequestion: “Whataresomemajorenviron-",
    "Forexample,wecanpromptthe LLMwith“Pleasecorrectallthegrammaticalerrorsinthetranslation”,sothatthemodelcan focusmoreongrammaticalerrorcorrectionduringrefinement. Ageneralframeworkofself-refinementwithLLMsinvolvesthreesteps[Madaanetal., 2024]. • Prediction. WeuseanLLMtoproducetheinitialmodeloutput. • FeedbackCollection. Weobtainfeedbackonthemodeloutput. • Refinement. WeusetheLLMtorefinethemodeloutputbasedonthefeedback. Thelasttwostepscanberepeatedmultipletimes,whichleadstoaniterativeself-refinement process. Inthisframework,receivingaccurateanddetailedfeedbackiscriticalasitguidesthe subsequentrefinementstep. Ingeneral,feedbackcanbeobtainedinvariousways[Fernandes etal.,2023;Panetal.,2024]. Wecanmanuallyexaminethemodel’spredictionstoidentify 502 Chapter 9. Prompting issues. Also,wecanuseafeedbackmodeltoautomaticallygeneratefeedback. Forinstance,a rewardmodeltrainedonsomelabeleddatacanhelpevaluatethemodeloutputandreturna scoreindicatingitsquality. SincethefocusofthischapterisonLLMprompting,weconsiderusingLLMstoprovide feedback. To illustrate, let us consider an example of generating good responses to user questions: • First,wepromptanLLMtoanswertheinputquestion,asusual. Generatearesponsetothequestion: “Whataresomemajorenviron- mentalconcernstoday?” Response: Major environmental concerns today include climate change, air and water pollution, deforestation, loss of biodiversity, and ocean degradation. • Then,usingthefollowingprompt,wepromptthesameLLMagaintogetfeedbackon thegeneratedresponse. Youwillbeprovidedwithaquestionanditscorrespondingresponse. Evaluate the accuracy and depth of this response, and identify any factual inaccuracies or areas where additional details could enhance understanding. Question: Whataresomemajorenvironmentalconcernstoday? Response: Majorenvironmentalconcernstodayincludeclimatechange,airand waterpollution,deforestation,lossofbiodiversity,andoceandegrada- tion. Feedback: The response provided is accurate and highlights key environmental concerns. However, it would benefit from more specific and detailed descriptionsoftheseconcerns. Herearesuggestionsforimprovement: 1. Includespecificexamplesofenvironmentalissues. 2. Addinformationaboutmajorpollutants. 3. Specifytheregionsmostaffected. • GiventhefeedbacktheLLMproduces,wepromptittorefinetheresponse. 9.2 Advanced Prompting Methods 503 Youwillbeprovidedwithaquestion,itscorrespondingresponse,and somefeedbackonthisresponse. Question: Whataresomemajorenvironmentalconcernstoday? Response: Majorenvironmentalconcernstodayincludeclimatechange,airand waterpollution,deforestation,lossofbiodiversity,andoceandegrada- tion. Feedback: Theresponseprovidedisaccurateandhighlightskeyenvironmental concerns. However,itwouldbenefitfrommorespecificanddetailed descriptionsoftheseconcerns. Herearesuggestionsforimprovement: 1. Includespecificexamplesofenvironmentalissues. 2. Addinformationaboutmajorpollutants. 3. Specifytheregionsmostaffected. Nowrefinetheresponsebasedontheprovidedfeedback. RefinedResponse: Key environmental concerns include climate change, evidenced by rising global temperatures and severe weather patterns; significant air and water pollution in urban and industrial areas; rampant deforestation in regions like the Amazon, causing biodiversity loss; and ocean degradation, highlighted by coral reef bleaching and widespreadoverfishing. Ideally,ifastrongLLMisadopted,wewouldliketohaveitperformallthreestepswithout extratraining. Ontheotherhand,ifwehaveenoughlabeleddataforthetaskofinterest,wecan enhancetheperformanceoftheLLMusingsupervisedlearning. Forexample,wecanfine-tune theLLMtobetteradaptittorefinementtasks,oralternatively,usetask-specificmodels,which maynotnecessarilybebasedonLLMs[Wellecketal.,2023;Schicketal.,2023]. Inabroader sense, improving LLMs for self-refinement tasks can be seen as an alignment issue. For example,ithasbeenfoundthatsomeself-correctionabilitiescanbeactivatedthroughRLHF [Gangulietal.,2023]. However,discussingtheseissuesisbeyondthescopeofthischapter. FurtherdiscussioncanbefoundinChapter10. InLLMs,self-refinementisrelatedtoseveralconceptsthatrevealthepsychologicalaspects of these models, such as the ability to self-reflect. A view is that if LLMs are capable of self-reflection,theirpredictionscanbecomemoreaccurateandevenpossessself-correcting capabilities. Thisself-reflectioncanbeactivatedinvariousways,forexample,byprompting theseLLMstoengageinmorein-depthandcarefulthinking,orbyprovidingexamplesfrom 504 Chapter 9. Prompting whichthemodelscanlearnandreflect. Toillustrate,weconsiderherethedeliberate-then- generate(DTG)methodpresentedinLietal.[2023a]’swork,whereLLMsarepromptedto deliberate. InDTG,wearegivenaninitialmodeloutputwhichmaycontainerrors. LLMsare thenpromptedtoidentifytheerrortypesofthismodeloutputandprovideanimprovedoutput. BelowisatemplateofDTGpromptingforChinese-to-Englishtranslationtasks. GiventheChinesesentence: {∗source∗} TheEnglishtranslationis: {∗target∗} Pleasefirstdetectthetypeoferror,andthenrefinethetranslation. ErrorType: Weaimtofirstpredicttheerrortype(red),andthenproducearefinedtranslation(blue). Thisprocessofdeliberationisguidedbytheinstruction“Pleasefirstdetectthetypeoferror, andthenrefinethetranslation”. ItencouragesLLMstoinitiallyengageinthoughtfulanalysis and then give better results. Since error type prediction and refinement are performed in a singlerunofLLMs,thismethodincorporatesbothstepsoffeedbackandrefinementintoone process. Intheaboveprompts,weassumethattheLLMweuseisabletoreviewtheinputtranslation andcorrectlyidentifyitserrortypes. However,thisraisesnewdifficultiesasthemodelmaynot begoodatfindingerrorsintranslations. Thiswillinturnresultinextrafine-tuningorprompting engineeringefforts. Soasimplermethodistoreducetheburdenoferroridentificationanduse LLMsfordeliberationonly. Todothis, wecanreplacetheinputtranslationwitharandom translationandassignadefaulterrortype. Anexampleofsuchapromptisshownbelow. GiventheChinesesentence: 一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。 TheEnglishtranslationis: Avarietyofinnovativetechniqueshaveredefinedtheimportanceofmodern artincontemporaryculturalstudies. Pleasefirstdetectthetypeoferror,andthenrefinethetranslation. ErrorType: IncorrectTranslation Inthisexample,theinputtranslationisnotgeneratedbyLLMsbutisinsteadrandomly sampledfromthedataset. Soitissimplyanincorrecttranslationforthesourcesentence,and wecansettheerrortypeaccordingly. TheLLMsthengenerateanewtranslationbytaking both the source sentence and the incorrect translation as input. The design of this prompt 9.2 Advanced Prompting Methods 505 can also be considered as activating the learning capabilities of LLMs through “negative evidence” [Marcus, 1993], thereby enabling them to reflect and produce better outcomes throughcontrastiveanalysis. Nevertheless,thismethoddoesnotrelyonanyfeedbackandcan enhancetheperformanceofasingleLLMpredictionviasimpleprompting. NotethatwhileDTGisnon-iterative,iterativelearningandrefinementarecommonlyused in NLP. An advantage of these iterative approaches is that they mimic human learning and problem-solving,wherecontinuousfeedbackandadjustmentsleadtoprogressivelyimproved outcomes. Iterative methods can be applied to a range of LLM prompting problems. For example,inproblemdecomposition,onecanincorporatenewsub-problemsandtheirsolutions intothecontextateachstep,andthusLLMscanprogressivelyapproachthesolutionofthe originalproblem. Ontheotherhand,iterativemethodsraiseseveralissuesthatareabsentin non-iterativemethods,forexample,errorsinearlierstepsmaynegativelyimpactsubsequent problem-solving,anddeterminingwhentostopiteratingoftenrequiresadditionalengineering effort. 9.2.4 Ensembling ModelensemblingfortextgenerationhasbeenextensivelydiscussedintheNLPliterature. Theideaistocombinethepredictionsoftwoormoremodelstogenerateabetterprediction. ThistechniquecanbedirectlyapplicabletoLLMs. Forexample,wecancollectasetofLLMs andruneachofthemonthesameinput. Thefinaloutputisacombinedpredictionfromthese models. ForLLMprompting,itisalsopossibletoimproveperformancebycombiningpredictions basedondifferentprompts. SupposewehaveanLLMandacollectionofpromptsthataddress thesametask. WecanrunthisLLMwitheachofthepromptsandthencombinethepredictions. Forexample,belowarethreedifferentprompttemplatesfortextsimplification. Makethistextsimpler. {∗text∗} Condenseandsimplifythistext. {∗text∗} Rewriteforeasyreading. {∗text∗} 506 Chapter 9. Prompting Each of these prompts will lead to a different prediction, and we can consider all three predictionstogeneratethefinalone. Formally, let {x ,...,x } be K prompts for performing the same task. Given an LLM 1 K Pr(·|·), we can find the best prediction for each x using yˆ =argmax Pr(y |x ). These i i yi i i predictionscanbecombinedtoforma“new”prediction: yˆ = Combine(yˆ ,...,yˆ ) (9.6) 1 K HereCombine(·)isthecombinationmodel,whichcanbedesignedinseveraldifferentways. Forexample,wecanselectthebestpredictionbyvotingorbyidentifyingtheonethatoverlaps themostwithothers. Anothermethodformodelcombinationistoperformmodelaveraging duringtokenprediction. Letyˆ bethepredictedtokenatthej-thstepformodelcombination. j Theprobabilityofpredictingyˆ isgivenby j K (cid:88) yˆ = argmax logPr(y |x ,yˆ",
    "Pr(·|·), we can find the best prediction for each x using yˆ =argmax Pr(y |x ). These i i yi i i predictionscanbecombinedtoforma“new”prediction: yˆ = Combine(yˆ ,...,yˆ ) (9.6) 1 K HereCombine(·)isthecombinationmodel,whichcanbedesignedinseveraldifferentways. Forexample,wecanselectthebestpredictionbyvotingorbyidentifyingtheonethatoverlaps themostwithothers. Anothermethodformodelcombinationistoperformmodelaveraging duringtokenprediction. Letyˆ bethepredictedtokenatthej-thstepformodelcombination. j Theprobabilityofpredictingyˆ isgivenby j K (cid:88) yˆ = argmax logPr(y |x ,yˆ ,...,yˆ ) (9.7) j j k 1 j−1 yj k=1 TheinterestedreadercanrefertoChapter5formoredetailsofthesemethods. InensemblingforLLMprompting,itisgenerallyadvantageoustousediverseprompts sothatthecombinationcancaptureabroaderrangeofpotentialresponses. Thispracticeis commoninensemblelearning,asdiversityhelpsaverageoutbiasesanderrorsthatmaybe specific to any single model or configuration. From the Bayesian viewpoint, we can treat thepromptxasalatentvariable,giventheproblemofinterest,p. Thisallowsthepredictive distributionofygivenptobewrittenasthedistributionPr(y|x)marginalizedoverallpossible prompts (cid:90) Pr(y|p) = Pr(y|x)Pr(x|p)dx (9.8) The integral computes the total probability of y by considering all possible values of x, weighted by their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is thepriordistributionofpromptsfortheproblem. Thisisagoodmodelbecausetheintegral effectivelyaccountsfortheuncertaintyinthechoiceofx,ensuringthatthefinalpredictive distribution Pr(y|p) is robust and encompasses all potential variations and biases in the prompts. However,computingthisintegraldirectlycanbecomputationallyinfeasibledueto thepotentiallyinfinitespaceofx. Oneapproachtoaddressingthisissueistoemploymethods likeMonteCarlosampling,whichapproximatetheintegralusingamanageable,finitenumber ofprompts. WhiletheBayesiantreatmentismathematicallywell-defined,itiscommonpracticeinNLP toassumeanon-informativeoruniformpriorandfocusinsteadonconstructingasetofdiverse prompts. Consequently, the output can be computed using a straightforward combination model, as described in Eq. (9.6). The issue of creating high-quality, diverse prompts has been studied in CoT and other in-context learning areas. Most of the research focuses on 9.2 Advanced Prompting Methods 507 incorporatingavarietyofdemonstrationexamplesacrossdifferentprompts. Here,welistsome ofthesemethods. • Givenaproblem,wemanuallycreateanumberofdemonstrationsandusedifferentones fordifferentprompts. • Givenaproblem,weuseLLMstoautomaticallygeneratedemonstrationsandprompts. • Givenaprompt,wecreatedifferentpromptsbychangingtheorderofdemonstrationsin theprompt. • Givenaprompt,weuseLLMstogenerateanumberofsimilarprompts. • Givenaprompt,wetransformitintootherforms,e.g.,translatingitintootherlanguages. Of course, in practice, we can combine these methods to achieve greater diversity. An underlyingassumptionhereisthatdiversepromptscanleadtodiversemodeloutputs. This is particularly the case when the problem we deal with is relatively new and difficult. For strongerandmorerobustLLMs,thevarianceinoutputforsimilarpromptsmightnotbelarge. Inthiscase,thebenefitofinvolvingmultiplepromptscanbemodest. InadditiontoprovidingdiversepromptsforLLMs,anotherapproachistomakeuseof theinherentvarianceintheoutputsofLLMs. Onesimplewaytogeneratemultipleoutputs is to sample outputs from the hypothesis space. This is straightforward for LLMs because their search algorithms are naturally suited to producing multiple results. For example, if beamsearchisused,wecaneasilycollectallthecompletehypothesesinthebeamtogenerate multipleoutputcandidates. Also, wecanmodifythesearchalgorithmstoimplementmore powerfulsamplingmethodsandexploreawiderrangeofhigh-qualityhypotheses. Thenextquestionishowtocombinediversemodelpredictions. Thisleadsusbacktothe model combination issue as described in Eq. (9.6). Here we consider the self-consistency method,whichoutputsnotthepredictionwiththehighestprobability,butrathertheonethat bestalignswithotherpredictions[Wangetal.,2022d;2023c]. First,anLLMispromptedwith CoTasusualandgeneratesmultiplereasoningpathsbysampling. Then,thefrequencyofeach answer is counted across these reasoning paths. The final output is the answer that has the highestcount. BelowshowsthreedifferentpredictionsbypromptinganLLMwithCoT. Prediction1(Correct): Q:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe coinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads? A:Let’sthinkstepbystep. Each coin flip is an independent event with a 50% chance of landing heads (H)and50%tails(T).Considerthepossibleoutcomesforthreeflips: HHH, HHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT, HTH,andTHHarerelevant. Weareonlyinterestedinscenarioswithexactly one head: HTT, THT, and TTH. There are 3 such outcomes out of 8 total, givingaprobabilityof3/8. 508 Chapter 9. Prompting Prediction2(Correct): Q:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe coinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads? A:Let’sthinkstepbystep. Usingabinomialprobabilityformula,wherep(successorheads)is0.5,and each of the 3 trials (flips) is independent, we look for exactly one success (head). The formula for exactly k successes in n trials is given by the binomial coefficient formula (cid:0)n(cid:1) pk(1−p)n−k. Plugging in the values, we k get (cid:0)3(cid:1) (0.5)1(0.5)3−1=3·0.5·0.25=0.375. 1 Prediction3(Wrong): Q:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe coinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads? A:Let’sthinkstepbystep. Sinceeachcoinfliphasa50%chanceoflandingheadsanda50%chanceof landingtails,andwearelookingforthecasewhereonlyoneofthethreeflips results in heads, we might initially think this is simply a matter of picking whichflipistheheads. Therearethreeflips,soonemightincorrectlyassume that the chance of picking one specific outcome like this would be 1 out of 3. Thus, they might conclude that the probability of exactly one head is 1/3=33.3%. Predictions1and2correctlyidentifythethreecaseswhereexactlyoneheadisflipped, bothobtainingaprobabilityof37.5%. ThereasoninginPrediction3failstoaccountforthe totalnumberofoutcomespossiblewiththreecoinflips,thusgivingawronganswerof33.3%. Therefore,weselect37.5%asthefinalanswerbecauseitistheconsensus. Self-consistency provides a criterion for determining the best prediction in a pool of candidates. Sincethepromptandthemodelarefixedinthismethod,itisnotstrictlyaprompt ensemblingmethod. Instead,itcanbeseenasaninstanceofoutputensemblingmethods,also knownashypothesisselectionmethods,whichhavelongbeenexploredinNLP,particularlyfor textgenerationproblems[Xiaoetal.,2013]. Inthesemethods,multipleoutputsaregenerated byvaryingmodelarchitecturesorparameters. Eachoutputisthenassignedascorebysome criterion,andtheoutputsarere-rankedbasedonthesescores. Therearevariouswaystodefine the scoring function, such as measuring the agreement between an output and others,",
    "the probability of exactly one head is 1/3=33.3%. Predictions1and2correctlyidentifythethreecaseswhereexactlyoneheadisflipped, bothobtainingaprobabilityof37.5%. ThereasoninginPrediction3failstoaccountforthe totalnumberofoutcomespossiblewiththreecoinflips,thusgivingawronganswerof33.3%. Therefore,weselect37.5%asthefinalanswerbecauseitistheconsensus. Self-consistency provides a criterion for determining the best prediction in a pool of candidates. Sincethepromptandthemodelarefixedinthismethod,itisnotstrictlyaprompt ensemblingmethod. Instead,itcanbeseenasaninstanceofoutputensemblingmethods,also knownashypothesisselectionmethods,whichhavelongbeenexploredinNLP,particularlyfor textgenerationproblems[Xiaoetal.,2013]. Inthesemethods,multipleoutputsaregenerated byvaryingmodelarchitecturesorparameters. Eachoutputisthenassignedascorebysome criterion,andtheoutputsarere-rankedbasedonthesescores. Therearevariouswaystodefine the scoring function, such as measuring the agreement between an output and others, and usingastrongermodeltorescoreeachoutput8. Figure9.2showsacomparisonofdifferent 8Aninterpretationofself-consistencyistoviewitasaminimumBayesrisksearchprocess.Itsearchesforthe bestoutputbyminimizingtheBayesrisk.Morespecifically,ariskfunctionR(y,yr)isdefinedoneachpairof outputs(denotedby(y,yr)),representingthecostofreplacingywithyr.GivenasetofoutputsΩ,theriskofan 9.2 Advanced Prompting Methods 509 ensemblingmethodsforLLMs. Now, let us briefly review the methods we have discussed so far in this section, such as problem decomposition and self-refinement. It is apparent that these methods enhance decision-makingbyintroducingmore“choices”intothereasoningprocess. Tosomeextent, theyallinvolveevaluatingandprovidingfeedbackontheresultsofLLMs. Forexample,in self-refinement,weneedtooffersuggestionsforimprovingthepredictionofLLMs,andin outputensembling,weselecttheoptimaloutputfromapoolofcandidates. Inthissense,these methodsfallunderthebroadercategoryofpredict-then-verifyapproaches,wherepredictions areinitiallymade,thenverifiedandrefined. Thefundamentalproblemhereinvolvesverifying andevaluatingthereasoningresultsorintermediatesteps. Thisissueissomewhatrelatedto theproblemoftrainingrewardmodelsinRLHF,althoughRLHFaddressesadifferentaspect. In fact, the development of verifiers has been explored and implemented in reasoning with LLMs. Mostwork,ratherthandevelopingheuristic-basedinference-timealgorithms,focuses onlearningverifiersinasupervisedmanner. Astraightforwardmethodistotrainverifiersas binaryclassifiers,suchasclassifyingananswerascorrectorincorrect,althoughtheseverifiers are typically used as scoring models. Given a reasoning path for a problem, the verifiers canbeusedtoscoreeithertheentirepath(calledoutcome-basedapproaches)[Cobbeetal., 2021],oreachindividualreasoningstep(calledprocess-basedapproaches)[Uesatoetal.,2022; Lightmanetal.,2024]. 9.2.5 RAG and Tool Use RAGisgenerallyemployedwhenstandardLLMs,whichrelysolelyonpre-trainedknowledge, lack accuracy and depth in the generated text. By drawing from external databases and documents,RAGcansignificantlyimprovethequalityofresponses,ensuringtheyareboth contextuallyrelevantandfactuallycorrect. Suchanapproachisparticularlyusefulinscenarios that require high factual accuracy and up-to-date information, such as complex question answering. TheconceptofRAGhasbeenmentionedseveraltimesintheprevioussectionsandchapters. Forcompleteness,weoutlinethekeystepsinvolvedinRAGhere. • Weprepareacollectionoftextswhicharetreatedasanadditionalsourceofknowledge wecanaccess. • Weretrieverelevanttextsforagivenquery. • WeinputboththeretrievedtextsandthequeryintoanLLM,whichisthenpromptedto producethefinalprediction. Steps1and2canbeimplementedbyusinganexternalinformationretrievalsystem. For example,wecanstorethecollectionoftextsinavectordatabaseandthenretrievethemost similar texts through vector-based search techniques. Since information retrieval is not the outputy∈Ωisgivenby Risk(y) = E yr∼Pr(yr|x) R(y,yr) (cid:88) = R(y,yr)·Pr(yr|x) (9.9) yr∈Ω 510 Chapter 9. Prompting Combine/Select LLM2 Prediction3 Prompt LLM2 Prediction2 Final Prediction LLM1 Prediction1 (a)ModelEnsembling Combine/Select Prompt3 Prediction3 Prompt2 LLM Prediction2 Final Prediction Prompt1 Prediction1 (b)PromptEnsembling Combine/Select mple Prediction3 Sa Prompt LLM Prediction2 Final Prediction Prediction1 (c)OutputEnsembling Figure 9.2: Ensembling methods for LLMs. In standard model ensembling (a), multiple LLMsvaryinginarchitecturesorparametersareused. EachLLMreceivesthesameprompt andproducesaprediction. Thesepredictionsarecombinedtogeneratethefinalprediction. In prompt ensembling (b), we have one LLM and multiple prompts. The LLM produces a predictionforeachprompt,andthesepredictionsarecombinedasusual. Inoutputensembling (c),theLLMsamplesmultiplepredictionsoverthepredictionspacegivenaprompt. Itcan beseenasamethodtoboosttheperformanceoftheLLMitself. Notethattheseensembling methodscanbecombinedtoincreasethediversityofpredictions. Forexample,wecanuse bothpromptensemblingandoutputensemblingtoobtainmorediversepredictions. focusofthischapter,wewillassumethatsuchsystemsareavailableoff-the-shelfandusethem directly. HerewepresenthowtopromptLLMstomakeuseofretrievedtexts. Toillustrate,consider 9.2 Advanced Prompting Methods 511 anexampleofusingLLMstoanswerthefollowingquestion. Wherewillthe2028Olympicsbeheld? We can simply input this question into an online search engine. It will then return the relevantpiecesoftextfoundontheinternet,forexample, (Wikipedia) The 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and commonlyknownasLosAngeles2028orLA28,isanupcominginternationalmulti- sporteventscheduledtotakeplacefromJuly14-30,2028,intheUnitedStates. ... (TheSportingNews) In2028,LosAngeleswillbecomethethirdcity,followingLondonandParisrespec- tively,tohostthreeOlympicsafterhostingtheSummerGamesin1932and1984. It willalsobethefirsttimetheUnitedStateshashostedanOlympicGamessincethe 2002WinterGamesinSaltLakeCity. ... ... Wecanusetheseretrievedtextsasadditionalcontext,andpromptanLLMtogeneratea responsebasedonthesetexts. BelowisanexampleRAGprompt. Yourtaskistoanswerthefollowingquestion. Tohelpyouwiththis,relevant textsareprovided. Pleasebaseyouransweronthesetexts. Question: Wherewillthe2028Olympicsbeheld? RelevantText1: The2028SummerOlympics,officiallytheGamesoftheXXXIVOlympiad andcommonlyknownasLosAngeles2028orLA28... RelevantText2: In2028,LosAngeleswillbecomethethirdcity,followingLondonandParis respectively,tohostthreeOlympicsafter... ... The2028OlympicswillbeheldinLosAngeles. Thispromptassumesthattheprovidedtextsarerelevanttothequestionandexpectsthe LLM to generate a faithful response using these texts. However, the information retrieval system may sometimes provide irrelevant or incorrect texts, which may lead the LLM to produceanincorrectanswer. Onestraightforwardwaytoaddressthisissueistoimprovethe accuracyoftheinformationretrievalsystem. Nevertheless,aswithmostAIsystems,errors maystilloccur. Therefore,itisalsonecessarytoenhancetherobustnessoftheLLM,sothatit 512 Chapter 9. Prompting canmakereasonablepredictionsevenwhentheinputisinaccurate. Belowisanewprompt thatenablestheLLMtobemorefaithfultothefacts,andallowsittochoosenottoanswer questionswhentheinformationprovidedisinaccurate. Yourtaskistoanswerthefollowingquestion. Tohelpyouwiththis,relevant textsareprovided. Pleasebaseyouransweronthesetexts. Pleasenotethatyouranswersneedtobeasaccurateaspossibleandfaithfulto thefacts.Iftheinformationprovidedisinsufficientforanaccurateresponse, youmaysimplyoutput\"Noanswer!\". Question: Wherewillthe2028Olympicsbeheld? RelevantText1: The2024SummerOlympics,officiallytheGamesoftheXXXIIIOlympiad andbrandedasParis2024,wereaninternationalmulti-sportevent... ... Noanswer! Inthisexample,theLLMrefusestoanswerbecausetheprovidedinformationisinsufficient andirrelevanttothequestion. BothRAGandfine-tuningarecommonmethodsforadaptingLLMsusingtask-specific data. StandardRAGistraining-freeandcanbedirectlyappliedtoLLMs. Tofurtherimprove RAG,itisalsopossibletofine-tuneLLMs,thoughthiswillrequiresometrainingeffort. For example, we can fine-tune LLMs using human-labelled data to supervise them in learning to refuse to answer. Note that, while the examples shown above seem simple, RAG is not trivial. From the prompt engineering perspective, different use cases may require different prompts, though our somewhat “greedy” goal is to develop a universal prompting strategy thatcanadapttodifferenttasks. Inmanycases,weneedtocontrolhowmuchwedependon theretrievedcontexttomakepredictions. Sometimes,LLMsmustderiveresponsesstrictly fromtheprovidedtexts,whileatothertimes,theymayneedtogenerateresponsesusingtheir pre-trainedknowledgeiftheprovidedtextsareinsufficient. TherearemanyaspectsofRAG, suchasimprovementstotheretrievalsystems,thatcannotbecoveredinthischapter. Interested readerscanrefertosurveysofRAGtechniquesformoreinformation[Lietal.,2022d;Gao etal.,2023c]. OnereasonwediscussRAGhereisthatitcanbebroadlyregardedasaninstanceofthe generalproblemdecompositionframework(seeSection9.2.2). RAGdividesproblem-solving into two steps. In the first step, we collect relevant and supporting information for a given queryfromvariousknowledgesources. Inthesecondstep,weuseLLMstogenerateresponses basedonthecollectedinformation. Ifweextendtheconceptofproblemdecompositionfurther, wewillfindthatmanytasksrequiringtheuseofexternalsystemsortoolscanbetreatedas 9.2",
    "different prompts, though our somewhat “greedy” goal is to develop a universal prompting strategy thatcanadapttodifferenttasks. Inmanycases,weneedtocontrolhowmuchwedependon theretrievedcontexttomakepredictions. Sometimes,LLMsmustderiveresponsesstrictly fromtheprovidedtexts,whileatothertimes,theymayneedtogenerateresponsesusingtheir pre-trainedknowledgeiftheprovidedtextsareinsufficient. TherearemanyaspectsofRAG, suchasimprovementstotheretrievalsystems,thatcannotbecoveredinthischapter. Interested readerscanrefertosurveysofRAGtechniquesformoreinformation[Lietal.,2022d;Gao etal.,2023c]. OnereasonwediscussRAGhereisthatitcanbebroadlyregardedasaninstanceofthe generalproblemdecompositionframework(seeSection9.2.2). RAGdividesproblem-solving into two steps. In the first step, we collect relevant and supporting information for a given queryfromvariousknowledgesources. Inthesecondstep,weuseLLMstogenerateresponses basedonthecollectedinformation. Ifweextendtheconceptofproblemdecompositionfurther, wewillfindthatmanytasksrequiringtheuseofexternalsystemsortoolscanbetreatedas 9.2 Advanced Prompting Methods 513 similarproblems. OnesuchexampleistooluseinLLMs. Inmanyapplications,LLMsneedto employ external databases, APIs, andeven simulationtools togenerate accurate responses. For example, LLMscan access real-timedata fromfinancial markets toprovide up-to-date investmentadviceorintegratewithhealthcaredatabasestoofferpersonalizedmedicalinsights. ThisintegrationextendsthecapabilitiesofLLMsbyallowingthemtointeractwith, andin somecontexts,influenceorcontrolexternalsystems. Consequently,LLMsfunctionmoreas autonomousagentsratherthanmeretextgenerators[FranklinandGraesser,1996]. Theissueoftooluseisbroadandvast. Herewenarrowourdiscussiontotasksthatcanbe facilitatedbycallingexternalAPIstosolvesomeofthesub-problems[Parisietal.,2022;Gao etal.,2023b]. ConsideragaintheexampleofaskinganLLMtoanswer“Wherewillthe2028 Olympicsbeheld?”. SupposetheLLMcanaccessawebsearchtool. Wecanthenpromptthe LLMtoanswerthequestionwithwebsearch,likethis Yourtaskistoanswerthefollowingquestion. Youmayuseexternaltools, suchaswebsearch,toassistyou. Question: Wherewillthe2028Olympicsbeheld? Theinformationregardingthisquestionisgivenasfollows: {tool:web-search,query:\"2028Olympics\"} Sotheansweris: LosAngeles Here{tool: web-search, query: \"2028 Olympics\"}indicatesarequestto thewebsearchsystemusingthequery“2028Olympics”. WhentheLLMseesthisstring,it executesawebsearchandusestheresulttoreplacethestring. Then,insubsequentstepsof prediction,theLLMusesthiswebsearchresultascontexttoproducethecorrectanswer. ConsideranotherexamplewhereweasktheLLMtosolveamathematicalproblem. 514 Chapter 9. Prompting Problem: Aswimmingpoolneedstobefilledwithwater. Thepoolmeasures10meters in length, 4 meters in width, and 2 meters in depth. Calculate the volume ofthepoolincubicmetersandthendeterminehowmanylitersofwaterare neededtofillit(considering1cubicmeterequals1000liters). Solution: Tosolvethisproblem,theLLMneedstofirstcalculatethevolumeofthe poolbyusingtheformulaforthevolumeofarectangularprism: Length× Width×Depth.Therefore,Thevolumeis10m×4m×2m={tool: calculator,expression:10*4*2}m3. Next,tofindouthow manylitersofwaterareneeded,theLLMmultipliesthevolumeincubic metersby1000(since1cubicmeterequals1000liters). Thus,80×1000 ={tool:calculator,expression:80*1000} liters. Herethestring{tool: calculator, expression: 10 * 4 * 2}triggersthe invocation of a mathematical interpreter to calculate the result of the expression. Note that theresult(i.e.,80)willreplace{tool: calculator, expression: 10 * 4 * 2}andcanbereferredtointhefollowingtokenpredictions. Forexample,inthelaststepof problem-solving, 80 is used instead of {tool: calculator, expression: 10 * 4 * 2}. AkeydifferencebetweenthetooluseexampleshereandthepreviouslydiscussedRAG examplesisthatintooluse,externalfunctionscanbecalledduringinference. Incontrast,in RAG,theretrievedtextsareprovidedbeforethepredictionprocessbegins. However,fromthe languagemodelingperspective,theyareactuallydoingthesamething: beforegeneratingthe finalresult,weuseexternaltools,eithermanuallyorautomatically,toobtainsufficientand relevantcontext. Ahigh-levelinterpretationoftheseapproachesisthattheybothrelyonan “agent”thatcandeterminewhereandhowtocallexternalfunctionstogeneratethecontext necessaryforprediction. AnissuewithtooluseisthattheoriginalLLMsarenottrainedtogeneratethenecessary markersfortooluse. Therefore,weneedtofine-tunetheLLMstoadaptthemforthesetasks [Schicketal.,2024]. Asthischapterfocusesonprompting,wewillnotpresentthedetailsof thisfine-tuningprocess. Toputitsimply,wefirstneedtoannotatedata. Foreachfine-tuning example,wereplacepartsoftheoutputthatrequiretheuseofexternaltoolswithpredefined commandsormarkers. Then,weusethislabeleddatatofine-tunetheparametersoftheLLM asusual. Asaresult,theLLMcangaintheabilitytogeneratecommandsforcallingexternal tools. Duringinference,wecanexecutethesetoolusecommandsinthemodeloutputstoget assistancefromexternaltools. 9.3 Learning to Prompt 515 9.3 Learning to Prompt So far in this chapter, we have considered several basic prompting strategies and various refinementstothem. However, allthepromptswehavediscussedweredesignedmanually. Thisleadstoanumberofproblems: First,designinghigh-qualitypromptsisinherentlydifficult andrequiressubstantialmanualeffort. Forexample,extensiveexperimentationwithdifferent promptsisoftenneededtoidentifythemosteffectiveones. SincedifferentLLMsmayrespond bettertocertaintypesofprompts,developinguniversallyeffectivepromptscanbeevenmore resource-intensive. Second,manualpromptdesignreliesheavilyonhumanexpertise,which canlimitthediversityofapproachesandoverlookpotentiallyeffectivepromptsthatarenot immediately obvious to humans. Third, prompts created by humans can be complex and redundant,leadingtolongerinputsforLLMsandhighercomputationalcosts. In this section, we discuss techniques for automated prompting. These methods aim to automatically create, optimize, and represent prompts so that the downstream tasks can be addressedmoreeffectivelyandefficiently. Inparticular,weconsiderthreeissueshere. • HowcanweautomatetheprocessofdesigningandoptimizingpromptsforLLMs? • Arethereotherformsofrepresentingpromptsbeyondstrings, andhowcanwelearn suchrepresentations? • Howcanwemakepromptsmoreconciseandcompact,therebyreducingtheircomplexity andlength? Notethattherearemanysettingsinwhichwecaninvestigatetheseissues. Forexample, we might specify that prompts are developed specifically for a particular LLM, or that the developmentisindependentoftheLLMused. Thesesettingscanleadtodifferentmethods and application scenarios, but these methods may overlap in some ways. In the following discussion, we will cover several different scenarios and discuss the connections between variousmethods. 9.3.1 Prompt Optimization Giventhatpromptdesignisdifficultandlabor-intensive,itisdesirabletousemachinelearning modelstodiscovertheoptimalpromptforaspecifictask(callitautomaticpromptdesignor promptoptimization). Thisapproachcanbroadlyberegardedasaninstanceofautomated machinelearning(AutoML),whichaimstoreduceoreliminatetheneedforexpert-driven manual design of machine learning models. Although our focus here is on the design of prompts, prompts themselves are discrete structures. Therefore, designing prompts is very similartodesigningmachinelearningmodels,suchasdiscretemodelarchitectures. Perhaps oneofthemostrelatedfieldsisneuralarchitecturesearch(NAS),wherethemostoptimal neuralnetworksareidentifiedbyexploringaspaceofpossibleneuralnetworks[ZophandLe, 2016;Elskenetal.,2019a]. Ifweconsiderpromptoptimizationasasearchprocess,thenwe candescribeageneralpromptoptimizationframeworkinvolvingthefollowingcomponents: • PromptSearchSpace. Thisdefinesallpossiblepromptsthatthealgorithmscanexplore. For example, one can edit some seed prompts to generate a set of diverse candidate 516 Chapter 9. Prompting prompts. • Performance Estimation. Once a prompt is chosen, it needs to be evaluated. For example,astraightforwardwayistoinputittoanLLMandmeasureitsperformanceon avalidationset. • Search Strategy. The search process is generally the same as that used in many AI systems. At each step, the system explores a set of promising prompts in the search spaceandevaluatesthem. Thisprocesscontinuesasmorepromptsareexplored. The outcomeofthesearchisthebest-performingpromptobserveduntilthesearchstops. This is a very general framework, and different prompt optimization systems can vary in their design of each component. A widely-used approach is to use LLMs as the basis to developthesecomponents. Initially,afewpromptsareprovided. Then,thefollowingprocess isiterateduntilastoppingcriterionismet: 1)thepromptsareevaluatedonavalidationset; 2)acandidatepoolismaintainedbykeepingonlythemostpromisingprompts;and3)new promptsarecreatedbyemployingLLMstoinfersimilarpromptsfromthiscandidatepool. Onebenefitofthisapproachisthatitallowsustouseoff-the-shelfLLMstoperformthetasks mentionedabovewithouttheneedforsubstantialsystemdevelopment. Toachievethis,wecan promptorfine-tuneLLMstoadaptthemtothesetasks. HereweconsiderZhouetal.[2023c]’s methodforillustratingLLM-basedpromptoptimization. Itinvolvesthefollowingsteps. • Initialization.",
    "spaceandevaluatesthem. Thisprocesscontinuesasmorepromptsareexplored. The outcomeofthesearchisthebest-performingpromptobserveduntilthesearchstops. This is a very general framework, and different prompt optimization systems can vary in their design of each component. A widely-used approach is to use LLMs as the basis to developthesecomponents. Initially,afewpromptsareprovided. Then,thefollowingprocess isiterateduntilastoppingcriterionismet: 1)thepromptsareevaluatedonavalidationset; 2)acandidatepoolismaintainedbykeepingonlythemostpromisingprompts;and3)new promptsarecreatedbyemployingLLMstoinfersimilarpromptsfromthiscandidatepool. Onebenefitofthisapproachisthatitallowsustouseoff-the-shelfLLMstoperformthetasks mentionedabovewithouttheneedforsubstantialsystemdevelopment. Toachievethis,wecan promptorfine-tuneLLMstoadaptthemtothesetasks. HereweconsiderZhouetal.[2023c]’s methodforillustratingLLM-basedpromptoptimization. Itinvolvesthefollowingsteps. • Initialization. LetC representthepoolofthecandidatepromptsweintendtoexplore. ThefirststepistoaddinitialpromptsintoC. Wecandothisinseveralways. Asimple method is to create such prompts by hand for a given task. However, in many cases wherehumanshavelimitedknowledgeabouthowtowriteeffectivepromptsforthetask, developingpromptsbecomeschallenging. Inthesecases,itisdesirabletouseLLMs togenerateprompts. Forexample,wecandirectlyinstructLLMstoproduceprompts, providingthemwithadescriptionofthetask. YouaregivenatasktocompleteusingLLMs. Pleasewriteapromptto guidetheLLMs. {∗task-description∗} Thismethodisstraightforward,butitstillrequiresahuman-provideddescriptionofthe task. AnalternativemethodistouseLLMstogeneratepromptsgivenexamplesofthe inputandoutputofthetask. Hereisaprompttemplate. 9.3 Learning to Prompt 517 You are provided with several input-output pairs for a task. Please writeaninstructionforperformingthistask. Input: {∗input1∗} Output: {∗output1∗} Input: {∗input2∗} Output: {∗output2∗} ... Assuch,LLMscaninferthecorrespondinginstructionforthetaskfromtheprovided inputsandoutputs. • Evaluation. Once we obtain the candidate pool C, we need to evaluate the prompts in C. One method is to feed each prompt into an LLM and assess the results on the downstreamtask. Forexample,wecanevaluatetheoutputoftheLLMgivenaninput using a pre-defined metric, or alternatively, use the log-likelihood of the output as a measureofthequalityoftheprompt. • Pruning. If C contains a large number of prompts, it is reasonable to prune the unpromisingpromptswithinit,thusreducingthecomputationalburdeninsubsequent steps. Thisisastandardpruningproblem. Giventheevaluationscoreforeachprompt,a simplemethodistokeeponlyacertainpercentageofthepromptsanddiscardtherest. • Expansion. Expansionisakeyoperationinsearchalgorithmsusedtoexploredifferent statesinthesearchspace. Theexpansionoperationherecanbedefinedasafunction C′ = Expand(C,f) (9.10) whereC′ isthesetofnewpromptsgeneratedfromC usingthemodelf. Ifweconsider f asanLLM,wecanperformtheexpansionoperationbyinstructingf togeneratenew andrelevantpromptsbasedonC. Belowisanexample. BelowisapromptforanLLM.Pleaseprovidesomenewpromptsto performthesametask. Input: {∗prompt∗} Then, we replace C with C′. The steps of evaluation, pruning and expansion can be repeated,andsowecangraduallyexploreawiderrangeofprompts. Inpromptoptimization,theexpansionstepplaysakeyrole,asitdefineshowweexplore thesearchspace,andourgoalistofindoptimalresultswithminimaleffort. Oneimprovement tothisstepistotreattheproblemasaparaphrasingtask. Asimplemethodistoapplyoff-the- shelfparaphrasingsystems,eitherbasedonLLMsorothermodels,totransforminputprompts 518 Chapter 9. Prompting intosemanticallyequivalentforms[Jiangetal.,2020]. Alternatively,wecandefinespecificedit operations,suchasinsertionsandmodifications,foreachtoken. Agivenpromptcanbeedited intonewpromptsbyapplyingtheseoperations[Prasadetal.,2023]. Also,furtherevaluation andpruningcanbeappliedtofilteroutlow-qualityprompts. Inadditiontoframingprompt generationasaparaphrasingproblem,wecanimprovethequalityofpromptsduringexpansion bylearningfromfeedback[Pryzantetal.,2023]. Thisapproachissomewhatrelatedtothe self-refinementissuediscussedinSection9.2.3. AnLLMcanbeusedtogeneratefeedbackon aninputprompt, whichisthenrevisedbasedonthisfeedback. Thisfeedback-and-revision cycle can be repeated multiple times until the result converges or the desired outcome is achieved. Anotherapproachtopromptoptimizationistoapplyclassicoptimizationtechniques. For example,theproblemcanbeframedasanevolutionarycomputationproblem,whereprompts aretreatedascandidatesthatevolvegenerationbygenerationastheoptimizationprogresses [Guo et al., 2024]. Since many powerful optimization algorithms have been developed in relatedfields,theycanbedirectlyappliedtothisproblem. In practice, we might be tempted to use existing LLM APIs to implement the steps describedabove. Suchanapproach,however,wouldbestronglydependentontheinference andin-contextlearningabilitiesoftheLLMs. IftheseLLMsarenotstrongandlackadaptation tothetasks,theymayintroduceerrorsintosearch,forexample,generatingincorrectprompts duringexpansion. Insuchcases, itispreferabletotrainmodelsthatarebettersuitedtothe tasks. Oneapproachinthisresearchdirectionappealstoreinforcementlearning,whichhas beenwidelyusedinsolvingdiscretedecisionmakingandoptimizationproblems. Forexample, Dengetal.[2022]developedapromptgeneratorbyintegratinganFFN-basedadaptorintoan LLM.Thepromptgeneratoristrainedasatypicalpolicynetwork, butonlytheparameters oftheadaptorareupdatedwhiletheremainingparametersofthemodelarekeptunchanged. Duringtraining,therewardisobtainedbytestingthegeneratedpromptsusinganotherLLM, similartotheevaluationmethodasdiscussedabove. Oncethetrainingiscomplete,theprompt generatoristhenemployedtogeneratenewprompts. Note that, in our discussion here, prompts are simply seen as sequences of tokens, and the output of prompt optimization is such a sequence. However, in a strict sense, prompts have complex structures and include different fields such as user input, instruction, and demonstration. While our discussed approaches are mostly general, much work in prompt optimizationhasfocusedonlearningbetterinstructionsforprompting. Specifically,thegoal is to generate instructions that effectively guide LLMs based on a given task. Of course, theconceptofpromptoptimizationcanalsobeextendedtolearningotherpartsofprompts. For example, there has been substantial research interest in learning to select or generate demonstrationsinCoT[Liuetal.,2022;Rubinetal.,2022;Zhangetal.,2023b]. Oneofthe differencesbetweenlearninginstructionsandlearningdemonstrationsisthatgeneratinghigh- qualitydemonstrationsusingLLMsisrelativelyeasyandthefocusoflearningdemonstrations is typically on how to sample appropriate demonstrations from a pool of candidates. In contrast, the difficulty in learning instructions is partly because pre-trained LLMs are not suited to predict the quality of instructions, and testing these instructions on downstream 9.3 Learning to Prompt 519 tasksiscomputationallyexpensive. Thismakestheoptimizationmethodscostlytoapply,and exploringawidevarietyofinstructionsposessignificantchallenges. 9.3.2 Soft Prompts Althoughdevelopingnaturallanguageprompts,eithermanuallyorautomatically,isastraight- forwardandwidelyappliedapproach,itpresentssomeproblems. Oneproblemisthatnatural languagepromptscanbecomplexandlengthy,resultinginsignificantcomputationalburdens whenprocessedviaLLMs. Inmanyapplications,usersmayneedtoperformataskrepeatedly, andinputtingthesamelongpromptintotheLLMsalargenumberoftimesisclearlyinefficient. Anotherproblemisthatwhilepromptsaretypicallyrepresentedasdiscretetokensequences (callthemhardprompts)inregularLLMinput,theLLMsencodethemaslow-dimensional real-valuedvectors. Thisraisesthequestionofwhethertherearemorecompactandefficient waystorepresentprompts. In this subsection, we introduce the concept of soft prompts, which can be viewed as hidden,distributedrepresentationsofprompts. WhenpromptingLLMs,weareconcernedwith communicatingtasksorquestionstoelicitthedesiredresponses. Wecandefinehardprompts asexplicit,predefinedtextsequencesthatusersinputdirectlyintoLLMstoguidetheresponses. Incontrast,wecanthinkofsoftpromptsasimplicit,adaptablepromptingpatternsembedded withinLLMs. Unlikehardprompts,whichareexpressedinnaturallanguageandshouldbe understandableforhumans,softpromptsareencodedinaformatthatismorecomprehensible tothemodelratherthantohumans. Toillustrate,considerasimpleprompt TranslatethesentenceintoChinese. Consideritdone! Here, the instruction “Translate the sentence into Chinese” can be seen as",
    "Oneproblemisthatnatural languagepromptscanbecomplexandlengthy,resultinginsignificantcomputationalburdens whenprocessedviaLLMs. Inmanyapplications,usersmayneedtoperformataskrepeatedly, andinputtingthesamelongpromptintotheLLMsalargenumberoftimesisclearlyinefficient. Anotherproblemisthatwhilepromptsaretypicallyrepresentedasdiscretetokensequences (callthemhardprompts)inregularLLMinput,theLLMsencodethemaslow-dimensional real-valuedvectors. Thisraisesthequestionofwhethertherearemorecompactandefficient waystorepresentprompts. In this subsection, we introduce the concept of soft prompts, which can be viewed as hidden,distributedrepresentationsofprompts. WhenpromptingLLMs,weareconcernedwith communicatingtasksorquestionstoelicitthedesiredresponses. Wecandefinehardprompts asexplicit,predefinedtextsequencesthatusersinputdirectlyintoLLMstoguidetheresponses. Incontrast,wecanthinkofsoftpromptsasimplicit,adaptablepromptingpatternsembedded withinLLMs. Unlikehardprompts,whichareexpressedinnaturallanguageandshouldbe understandableforhumans,softpromptsareencodedinaformatthatismorecomprehensible tothemodelratherthantohumans. Toillustrate,considerasimpleprompt TranslatethesentenceintoChinese. Consideritdone! Here, the instruction “Translate the sentence into Chinese” can be seen as a hard prompt, denoted by the token sequence c ...c . By feeding these tokens into an LLM, they are 1 5 transformedintoasequenceofreal-valuedvectorsh ...h ,eachcorrespondingtoatoken. We 1 5 canroughlythinkofh ...h asasoftprompt,asillustratedinFigure9.3. 1 5 Whiletheaboveexampleshowsthatsoftpromptscanbegeneratedbytransforminghard prompts, there is not necessarily a direct correspondence between them. In fact, we do not even need to interpret soft prompts using meaningful text. They are instead simply hidden statesinLLMsandcanbelearnedasstandardparametersofthemodelsthroughcontinuous optimization. Such a treatment allows us to explore prompting methods beyond text. As anotherbenefit,softpromptsprovidedense,low-dimensional,andlearnablerepresentations forencodinghowweguideLLMstogeneratespecificoutputs. Thetrainingandapplication oftheserepresentationsrequiresignificantlylowercomputationalcoststhanthoserequired forprocessinglonghardprompts. ThisapproachwouldbeofgreatpracticalvalueinLLM inferenceapplicationswherethesamepromptisrepeatedlyused. 520 Chapter 9. Prompting ... ... ... ... ... ... ... ... ... ... ... ... SoftPrompt ... h h h h h h h h h h ... j j+1 j+2 j+3 j+4 j+5 j+6 j+7 j+8 j+9 Transformer ... Translate this into Chinese . I have a cat . ... HardPrompt(Instruction) Figure9.3: Illustrationofhardandsoftprompts. Herethehardpromptistheinstructionwe inputtotheLLMforperformingthetask. TheLLMencodesthisinstructionasusual,andthe intermediaterepresentationscorrespondingtotheinstructioncanbeviewedassomesortof softprompt. 1. Adapting LLMs with Less Prompting OneobviouswaytoadaptanLLMforaparticulartaskistosimplyfine-tunethemodelusing labeleddata. ThisleadstoavarietyofLLMalignmentmethods,suchassupervisedfine-tuning, whichupdatethemodelparametersbyaligningtheresponsestogivenpromptswithsupervision signals. Fine-tunedLLMsembedtask-relatedinformationinmodelparameters,andthusthese modelscanrespondcorrectlywhendealingwithsimilarpromptswiththoseinfine-tuning. Ifwetakethisideafurther,wecanexpectLLMstoabsorbtheknowledgeaboutprompting ofataskasmuchaspossibleduringfine-tuning. Consequently, thepromptinginformation ispartiallycapturedinthemodelparameters,andthefine-tunedLLMscanperformthetask withlessprompting. Hereweconsiderasimpleformofprompt,whereonlyaninstruction (denotedbyc)andauserinput(denotedbyz)areincluded. Apromptcanbeexpressedusing thefollowingtuple x = (c,z) (9.11) Given a set of prompt-response pairs D = {(x,y)}, the objective of fine-tuning is to minimizethetotallossincurredoverthisset. Apopularmethodistominimizethenegative log-likelihood(i.e.,maximizethelog-likelihood)withrespecttothemodelparametersθ: (cid:88) θˆ = argmax logPr (y|x) θ θ (x,y)∈D (cid:88) = argmax logPr (y|c,z) (9.12) θ θ (x,y)∈D 9.3 Learning to Prompt 521 wherePr (·|·)istheprobabilitypredictedbyanLLMwiththeparametersθ9. θ In general, the instruction in each fine-tuning example should follow the guideline of promptdesign,forexample,agoodinstructionshouldbeasclearaspossibleandprovidea detaileddescriptionofthetask. However,themethoddescribedintheaboveequationdoesnot restricttheinstructiontoanyparticularform. ThisflexibilityallowsustoinstructLLMsin anywaythatwewant. ConsideranexamplewhereweintendtoinstructLLMstotranslatean EnglishsentenceintoChinese. Ofcourse,asmentionedearlierinthischapter,wecanprompt LLMsusingtheinstruction TranslatethefollowingsentencefromEnglishtoChinese. Ifwewanttheinstructiontobesimpler,wemayrephraseitintoasimplerform TranslatethisintoChinese. Even,wecandefinetheinstructionasasinglephrase Translate! With certain fine-tuning effort, we can adapt LLMs to follow any of these instructions. Fromanefficientpromptingperspective,therearecomputationaladvantagesinsimplifying instructionsinprompting. Forexample,wecanusesimpleinstructionslike“Translate!” to performtasksthatwouldtypicallyrequiremorecomplexanddetailedinstructions. Thiscan make subsequent prompting during inference much easier. On the other hand, fine-tuning LLMswithoverlysimplifiedinstructionsmaybeharmfultothegeneralizationofthemodels. Sincesimplifiedinstructionscanleadtoalossofinformation,itismorelikelythattheLLMs willoverfitthefine-tuningdataandfailtogeneralizebeyondthoseinstructions. Inscenarios involvingbothcomplexandsimplifiedinstructionsforfine-tuning,thisproblemismoresevere because the labeled data available for fine-tuning is usually limited, and accommodating a varietyofinstructionsiscostly. AnalternativewaytoadaptLLMsforsimplifiedinstructionsisthroughknowledgedistilla- tion. Asanexample,weconsiderthecontextdistillationmethod[Snelletal.,2022]. Thegoal ofthismethodistolearnastudentmodelthatcanmakeuseofsimplifiedinstructionsfrom a well-trained instruction-following teacher model. Figure 9.4 shows an illustration of this approach. Buildingtheteachermodelfollowsastandardfine-tuningprocess: wefirstcollecta certainamountofdatathatincludesinstructions,userinputs,andcorrectresponses,andthen wecontinuetotrainapre-trainedmodelwiththisdataset. Forbuildingthestudentmodel,we needtoconstructanewdatasetD′ whereeachsampleisatupleconsistingofaninstruction,a correspondingsimplifiedinstruction,andauserinput,denotedbyx′=(c,c′,z). Knowledge distillationisperformedbyminimizingalossfunctiondefinedontheoutputsoftheteacher 9Inpractice,weinitializeθwiththeparametersobtainedfrompre-training,andthenadjustθmoderatelyto ensurethattheresultsafterfine-tuningdonotdeviatetoomuchfromthepre-trainedresults. 522 Chapter 9. Prompting TeacherModel: FullContext + UserInput Prt(y|c,z) c z y Loss StudentModel: SimplifiedContext + UserInput Prs(y|c′,z) c′ z y Figure 9.4: Illustration of context distillation [Snell et al., 2022]. The teacher model is a standardLLM,whichtakesboththecontextandtheuserinputasmodelinputandproducesa predictionasmodeloutput. Then,wesimplifythecontext(e.g.,simplifyingtheinstructionin prompting)andusethestudentmodeltomakepredictionsbasedonthesimplifiedcontextand theuserinput. Thestudentmodelistrainedbyminimizingthelossbetweenthepredictions producedbythetwomodels. andstudentmodels (cid:88) θˆ = argmin Loss(Prt(·|·), Prs(·|·), x′) (9.13) θ θ x′∈D′ wherePrt(·|·)denotesthepre-trainedteachermodel,andPrs(·|·)denotesthestudentmodel θ withtheparametersθ. TokeepthenotationsimplewewillwriteLoss(Prt(·|·), Prs(·|·), x) θ asLossforshort. Acommonly-usedlossisthesequence-levelloss,whichhasthebasicform: (cid:88) Loss = Prt(y|c,z)logPrs(y|c′,z) (9.14) θ y But this function is computationally infeasible because it requires summing over an exponentiallylargenumberofoutputs. Avariantofthismethodistotrainthestudentmodel usingoutputsgeneratedbytheteachermodel. Foreachsample,weusetheteachermodelto produceanoutputyˆ=argmax logPrt(y|c,z). Thenweconsideryˆ asthetargetforlearning, y andthelossfunctionisgivenby Loss = logPrs(yˆ|c′,z) (9.15) θ Alternatively,wecanminimizethedistancesbetweentheprobabilitydistributionsoutputted bythetwomodels[Askelletal.,2021]. Forexample,thelossfunctioncanbedefinedasthe KLdivergencebetweenthetwooutputdistributions Loss = KL(Pt ||Ps) (9.16) θ 9.3 Learning to Prompt 523 where Pt = Prt(·|c,z) (9.17) Ps = Prs(·|c′,z) (9.18) θ θ Although we have restricted ourselves to knowledge distillation for instructions, the approachesdiscussedherearegeneral. Bylearningfromtheoutputsoftheteachermodel,the knowledgeinpromptingcanbedistilledintotheparametersofthestudentmodel.",
    "logPrt(y|c,z). Thenweconsideryˆ asthetargetforlearning, y andthelossfunctionisgivenby Loss = logPrs(yˆ|c′,z) (9.15) θ Alternatively,wecanminimizethedistancesbetweentheprobabilitydistributionsoutputted bythetwomodels[Askelletal.,2021]. Forexample,thelossfunctioncanbedefinedasthe KLdivergencebetweenthetwooutputdistributions Loss = KL(Pt ||Ps) (9.16) θ 9.3 Learning to Prompt 523 where Pt = Prt(·|c,z) (9.17) Ps = Prs(·|c′,z) (9.18) θ θ Although we have restricted ourselves to knowledge distillation for instructions, the approachesdiscussedherearegeneral. Bylearningfromtheoutputsoftheteachermodel,the knowledgeinpromptingcanbedistilledintotheparametersofthestudentmodel. Therefore, thedistilledmodelcanbeconsideredasencodingsomesortofsoftprompt. Thismethodcan beappliedtomanyotherproblemsinpromptlearning,suchascompressinglongcontextsand learningsoftpromptsasspecificcomponentsofLLMs. 2. Learning Soft Prompts for Parameter-efficient Fine-tuning UpdatingallparametersisacommonmethodforadaptingLLMstotasksofinterest. Although fine-tuningisconsideredcomputationallycheaperthanpre-training,itisstillcostlytoapply inpractice. Thisissuemotivatesthedevelopmentofparameter-efficientfine-tuningmethods, whichaimtominimizethenumberofparametersthatneedtobeupdated. Oneapproach,knownasprefixfine-tuning,istoappendaseriesoftrainablevectors,or prefixes,atthebeginningoftheinputofeachTransformerlayer[LiandLiang,2021]. These prefixescanbethoughtofassoftpromptsthatserveasadditionalcontexttoguidethebehavior ofthemodelunderspecifictasks. Duringfine-tuning,weneedonlytolearntheprefixesfor embeddingtask-specificknowledge. Thus,thismethodisefficientbecauseitonlymodifiesa smallpartofthemodelratherthanadjustingtheentiresetofmodelparameters. Specifically,lettheinputofalayeratdepthlbedenotedbyHl =hlhl...hl . Theoutput 0 1 m ofthelayercanbeexpressedas Hl+1 = Layer(Hl) (9.19) Inprefixfine-tuning,weextendthesequencehlhl...hl byaddingafewvectorsatthe 0 1 m beginning,whichwedenoteasplpl...pl . HenceHl canbewrittenintheform 0 1 n Hl =pl pl ...pl hl hl ...hl (9.20) 0 1 n 0 1 m (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) trainable previouslayeroutput Theoutputofthelayeristhelastm+1representations. H l+1 = Layer(Hl)[−m−1:] = hl+1hl+1...hl+1 (9.21) 0 1 m where [−m−1 :] denotes the slicing operation that extracts the last m+1 elements of a l+1 sequence. GivenH ,theinputofthenextlayercanbeexpressedinthesameformofEq. 524 Chapter 9. Prompting (9.20): Hl+1 = pl+1 pl+1 ...pl+1 H l+1 0 1 n = pl+1 pl+1 ...pl+1hl+1hl+1...hl+1 (9.22) 0 1 n 0 1 m Hereeachp ∈Rdcanbeseenasalearnableparameter. Duringtraining,plpl...pl aretrained i 0 1 n asusual,andtheparametersoftheoriginalTransformermodelarekeptfixed. Figure9.5showsanillustrationofprefixfine-tuningforatranslationtask. Here,onlythe prefix vectors pl and pl are updated by receiving the error gradients from the output (i.e., 0 1 theChinesetranslation). Byadjustingthesevectorsforthetranslationtask,themodeladapts accordingly. This makes pl and pl serve as prompts which activate the LLM to perform 0 1 thetaskwithoutneedingexplicitinputpromptslike“Translatethefollowingsentencefrom EnglishtoChinese”. Attesttime,weprependtheoptimizedpl andpl tothelayer,andthe 0 1 LLMwillthentranslatetheinputsentence. Notethatprefixfine-tuningintroducesadditional L×n×dparameters,whereListhenumberoflayers,nisthenumberofprefixes,anddis thedimensionalityofeachprefix. However,thisnumberismuchsmallercomparedtothetotal numberofparametersintheLLM,makingthefine-tuningprocesshighlyefficient. Whileprefixfine-tuningissimple,itstillrequiresmodificationstoLLMs. Alternatively, separatingsoftpromptsfromtheLLMsallowsustopreservetheoriginalmodelarchitecture, makingitmoreefficientfordeploymentacrossdifferenttaskswithouttheneedtoadjustthe coremodel. Onesuchmethodisprompttuning[Lesteretal.,2021]. Likeprefixfine-tuning, prompttuningincorporatestrainablevectorssothatLLMscanadapttogiventasksbyadjusting thesevectors. However,prompttuningdiffersinthatitmodifiesonlytheembeddinglayer. Recall that in LLMs each input token z is represented by an embedding e . These i i embeddingsaregenerallylearnedthroughatokenembeddingmodelandarethenusedasthe realinputstotheLLMs,replacingthesymbolicallyrepresentedtokens. Inprompttuning,a number of pseudo embeddings p ...p are added at the beginning of the token embedding 0 n sequence. SotheactualinputtotheLLMscanbeexpressedas p p ...p e e ...e 0 1 n 0 1 m (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) trainable tokenembeddings Notethatapseudoembeddingneedsnottocorrespondtoanytokeninnaturallanguage. Instead theseembeddingscanbeseenas“softpromptembeddings”thatservetoconditiontheLLMs. Bytrainingsoftpromptembeddingsontask-specificdata,theylearntointeractadaptivelywith the token embeddings e ...e and guide the behavior of LLMs. Since prompt tuning does 0 m notchangetheunderlyingparametersofpre-trainedLLMs,itisconsideredalightweightand efficientmethodoffine-tuning,improvingtask-specificperformancewhilemaintainingtheir generalizationcapabilities. SeeFigure9.6foranillustrationofprompttuning. Sincep p ...p isitselfasequence,wecanemploysequencemodelstobetterrepresent 0 1 n it. Forexample,aTransformermodelcanencodethissequence,andtheresultingrepresentation canthenbeusedastheinputtotheLLM.Inotherwords,wecandevelopanadditionalmodel for encoding soft prompts. Another way to improve prompting is by combining soft and 9.3 Learning to Prompt 525 Loss Loss ··· ··· ··· ··· ··· Layerl+1 pl+1 pl+1 hl+1 hl+1 hl+1 hl+1 hl+1 0 1 0 1 3 4 5 Layerl trainableprefixes pl pl hl hl hl hl hl 0 1 0 1 2 3 4 Layerl−1 pl−1 pl−1 hl−1 hl−1 hl−1 hl−1 hl−1 0 1 0 1 3 4 5 ··· ··· ··· ··· ··· Look out ! 小心 ! SoftPrompt UserInput LLMPrediction Figure9.5: Illustrationofprefixfine-tuningforatranslationtask(Lookout! → 小心!). For eachlayer,weaddtwoprefixespl andpl atthebeginning. TheLLMistrainedtominimize 0 1 thelossonthepredictionsgiventheinput. Duringthisprocess,onlytheprefixesareoptimized whiletherestoftheparametersremainfixed. Therefore,themodelcanadapttothegiventask inaveryefficientmanner. Atinferencetime,theLLMworkswithoptimizedprefixes,andcan performthetaskwithouttheneedofexplicithardprompts. hardprompts,therebytakingadvantageofbothtypes[Liuetal.,2023c]. Intheembedding sequence,wecanarrangeorinterspersetheseprompts. Thiswouldresultindifferentprompt patterns. Forexample,asimplepatternthatusesbothtwotypesofpromptis SoftPrompt HardPrompt UserInputandResponse p 0 p 1 ··· p n q 0 q 1 ··· q m′ e 0 e 1 ··· e m c 0 c 1 ··· c m′ z 0 z 1 ··· z m wherec ...c denotesthehardpromptandq ...q denotesthecorrespondingembedding 0 m′ 0 m′ sequence. HerewehaveconsideredmethodsforinsertingsoftpromptsinLLMs. Butweskipthe detailsoftrainingthesesoftpromptsandassumethatthereaderisfamiliarwiththestandard supervised learning process, that is, maximizing the",
    "p n q 0 q 1 ··· q m′ e 0 e 1 ··· e m c 0 c 1 ··· c m′ z 0 z 1 ··· z m wherec ...c denotesthehardpromptandq ...q denotesthecorrespondingembedding 0 m′ 0 m′ sequence. HerewehaveconsideredmethodsforinsertingsoftpromptsinLLMs. Butweskipthe detailsoftrainingthesesoftpromptsandassumethatthereaderisfamiliarwiththestandard supervised learning process, that is, maximizing the likelihood of the correct model output 526 Chapter 9. Prompting Loss Loss ··· ··· ··· ··· ··· ··· ··· Layerl+1 Layerl Layerl−1 ··· ··· ··· ··· ··· ··· ··· trainableprompt p 0 p 1 e 0 e 1 e 2 e 3 e 4 embeddings Look out ! 小心 ! SoftPrompt UserInput LLMPrediction Figure9.6: Illustrationofprompttuningforatranslationtask(Lookout! → 小心!). Instead ofusingfixedtextualprompts,softpromptsarelearnableembeddingsthatareaddedatthe beginning of the embedding sequence. During fine-tuning, only these prompt embeddings are optimized to efficiently adapt the LLM to the given task. Once optimized, the prompt embeddingsareusedtoinstructtheLLMtoperformthetaskasnewdataarrives. giventhemodelinput. Infact,learningsoftpromptscanberelatedtomanyissuesinLLM fine-tuning. Forexample,ifweconsideritasacontextcompressionproblem,wecanapply theknowledgedistillationmethodsdescribedpreviously. InMuetal.[2024]’swork,prompts are compressed and represented as a few pseudo tokens, which are appended to each input sequence. Theembeddingsofthesepseudotokensareoptimizedtomimicthepredictionsofa standard-promptedmodel. Inotherwords,thepromptingknowledgeisdistilledfromateacher modelintothepseudotokens. Broadly speaking, many parameter-efficient fine-tuning methods can be thought of as learningsomesortofsoftprompt[Lialinetal.,2023]. Whenwefine-tuneapartofanLLM foratask,thisprocesscanessentiallybeseenasinjectingtask-relatedpromptinginformation intothatspecificpartofthemodel. Anotherwidely-usedapproachtoparameter-efficientfine- tuningistoaddanadaptorlayerbetweentheexistingmodellayers. Thisapproachallowsusto fine-tuneonlytheadaptorlayeronspecifictaskswithoutalteringtheunderlyingarchitecture orretrainingtheentiremodel. Inthissense,adaptorlayerscanbeviewedassoftpromptsthat encode prompting and task-related information and interact with the original LLM to help it adapt. To summarize, Figure 9.7 shows a comparison of different methods of using soft promptsinLLMs. 9.3 Learning to Prompt 527 LLM LLM (a)SoftPromptsasPrefixes (b)SoftPromptsasInputs(Embeddings) Layer LLM LLM (c)Fine-tuningPartsoftheModel rotpadA (d)Fine-tuningtheAdaptor Figure9.7: IllustrationsofusingsoftpromptsinLLMs. Heretunablesoftpromptsareshown inblue,andcomponentswhoseparametersarefixedduringfine-tuningareshowningray. In sub-figure (a), soft prompts are prefixes appended to each layer of the LLM. In sub-figure (b),softpromptsareusedasinputembeddingsfortheLLM.Insub-figures(c)and(d),soft promptsarebroadlytreatedascomponentsofthemodelthatarefine-tunedfortaskadaptation. 3. Learning Soft Prompts with Compression Another approach to learning soft prompts is from the perspective of compression. As a simple example, consider the problem of approximating a long context using a continuous representation[Wingateetal.,2022]. Supposewehaveauserinputzanditscontextc(suchas longinstructionsanddemonstrations). Nowwewanttodevelopacompressedrepresentation ofthecontext,denotedbyσ,suchthatthepredictionbasedonzandσ isascloseaspossible tothepredictionbasedonzandc. Thisgoalcanbeexpressedintheform σˆ = argmins(yˆ,yˆ ) (9.23) σ σ where yˆ = argmax Pr(y|c,z) and yˆ = argmax Pr(y|σ,z) are the LLM predictions y σ yσ giventhefullcontextandthecompressedcontext,respectively. Thefunctions(·,·)typically represents a loss or similarity measure, aiming to minimize the difference in predictions betweenthetwocontextrepresentations. Onegeneralframeworkforachievingthisisknowledgedistillation,whereyˆ andyˆ can σ be seen as the predictions of the teacher model and the student model, respectively. This formalizationlinksourdiscussiontothecontextdistillationproblemdiscussedearlier. The training objective can be obtained by analogy with Eqs. (9.15) and (9.16). For example, a simpletrainingobjectiveisgivenby σˆ = argmaxlogPr(yˆ|σ,z) (9.24) σ 528 Chapter 9. Prompting Alternatively,wecanminimizetheKLdivergencebetweentheoutputdistributions,giving σˆ = argmin KL(Pr(·|c,z)|| Pr(·|σ,z)) (9.25) σ ThedifferencewiththemodelsinEqs. (9.15)and(9.16)isthatherethecompressedcontext isrepresentedasreal-valuedvectors(callthempromptembeddings),ratherthanasnormal tokens. Byapplyingtheabovemethods,wedistillthecontextfromthetokensequencecinto the embeddings σ. Note that the teacher model Pr(·|c,z) and the student model Pr(·|σ,z) maynotsharethesamearchitectureormodelsettings. Inpractice,wegenerallywishforthe teachermodeltobestronger,whilethestudentmodelshouldbesmallerandmoreefficient. While compressing full context into continuous representations is a straightforward ap- proach to learning soft prompts, it requires a teacher model that can deal with long input sequences. Inmanycases,however,thecontextissolongthatapplyinganLLMistoocostlyor infeasible. Modelinglonginputsequencescanfallunderthebroadfamilyofefficientmethods for long-context LLMs. Many techniques have been developed to address this issue. For example,onecanuseafixed-sizeKVcachetostorethepastinformationateachstepduring inference. EfficientTransformerarchitecturesandlong-contextLLMshavebeenintensively discussedinthisbook. Formoredetaileddiscussionsofthesetopics,interestedreaderscan refertoChapters6and8. Therearealsomethodsspecificallydesignedtocompresslongcontextintosoftprompts. Here we consider Chevalier et al. [2023]’s method as an example. The basic idea is that welearnsoftpromptsgraduallybyaccumulatingthefixed-sizecontextrepresentationover the context sequence. Given a long context, we first divide it into a number of segments z1,...,zK. Wethenprocessthesesegmentsinsequence,eachtimegeneratingarepresentation ofthecontextwehaveprocessedsofar,denotedbyσ<i+1. Todothis,afewsummarytokens ⟨g ⟩,...,⟨g ⟩ are introduced. At each step, we take a segment zi =zi...zi , along with the",
    "al. [2023]’s method as an example. The basic idea is that welearnsoftpromptsgraduallybyaccumulatingthefixed-sizecontextrepresentationover the context sequence. Given a long context, we first divide it into a number of segments z1,...,zK. Wethenprocessthesesegmentsinsequence,eachtimegeneratingarepresentation ofthecontextwehaveprocessedsofar,denotedbyσ<i+1. Todothis,afewsummarytokens ⟨g ⟩,...,⟨g ⟩ are introduced. At each step, we take a segment zi =zi...zi , along with the 1 κ 1 mi previouscontextrepresentationσ<i andthesummarytokens⟨g ⟩,...,⟨g ⟩asinput,anduse 1 κ anLLMtoproducethecorrespondinghiddenrepresentationsequenceatthelastTransformer layer. AnexampleofthisprocessisillustratedinFigure9.8. Here σ<i is essentially a memory. The model operates in an RNN fashion. Each time wetakeasegmentandupdatethismemorybyencodingboththepreviousmemorystateand thesegment. Therefore,theσ<i producedatthelastsegmentisarepresentationoftheentire contextsequence. TheTransformermodelforlearningtheserepresentationscanbeastandard LLMbutweneedtofine-tuneittoadapttothiscontextrepresentationtask. Notethatherewesimplyconsiderpromptandcontextassimilarterms,eventhoughthey arenotthesame. Althoughwearesomewhat“misusing”theconceptprompt,wecanoften viewitasatypeofcontext. Fromthisperspective,themethodsdiscussedherecanbeapplied togeneraltextcompressionproblems. 9.3.3 Prompt Length Reduction Whilesoftpromptsprovidedense,hiddenrepresentations,theyarenotdirectlyinterpretable. Thelackofinterpretabilitycanbeasignificantbarrierforuserstryingtounderstandhowtheir 9.3 Learning to Prompt 529 h< 1 i h< 1 i h 1 h 2 h 3 h 4 σ 1 <i+1 σ 2 <i+1 SoftPrompts attheCurrentStep Transformer Layers SoftPrompts σ 1 <i σ 2 <i ei 1 ei 2 ei 3 ei 4 ⟨e 1 ⟩ ⟨e 2 ⟩ atStepi−1 z 1 i z 2 i z 3 i z 4 i ⟨g 1 ⟩ ⟨g 2 ⟩ Figure9.8: Illustrationofcompressingacontextsegmentintosoftprompts(κ=2andm =4). i The input to the LLM includes the soft prompts from the previous step (σ<i and σ<i), the 1 2 tokens of the segment (z ,z ,z , and z ), and the summary tokens (⟨g ⟩ and ⟨g ⟩). Given 1 2 3 4 1 2 these, theLLMoperatesasusual. WethenextracttheoutputsatthelastTransformerlayer thatcorrespondtothesummarytokens. Theseoutputscanbeviewedasthesoftpromptsthat accumulateduptothissegment. inputsinfluenceLLMoutputs. Moreover,althoughsoftpromptsareefficientforfine-tuning anddeployment,theyareinflexibleanddonotallowforeasyadjustmentswithoutextensive fine-tuningormodification. Thisinflexibilitycanlimittheirutilityindynamicenvironments wherepromptchangesarefrequentlyneeded. Onealternativewaytodevelopefficientpromptsistosimplifythetextusedforprompting. Forexample,belowisapromptforansweringquestionsonhealthcareandfinance. The task involves developing a language model capable of understanding and responding to user inquiries across various domains, with a particular emphasisonhealthcareandfinance. Consideringthebroadrangeofpotential queries, from the specifics of medical diagnoses to the nuances of finan- cialregulations,themodelmustensureacomprehensiveunderstandingand accurateresponses. Question: What are the best practices for using artificial intelligence in diagnosing cardiovasculardiseases? Wecansimplifythetaskdescriptionbydeletingtheunimportantparts. 530 Chapter 9. Prompting Thetaskinvolvesdevelopingalanguagemodelcapableofunderstanding andrespondingtouserinquiriesacrossvariousdomains,withaparticular emphasis on healthcare and finance. Considering the broad range of potential queries, from the specifics of medical diagnoses to the nuances of financial regulations, The model must ensure a comprehensive under- standingandaccurateresponses. Wecanalsoparaphraseitasashortertext. Thetaskinvolvesdevelopingalanguagemodelfocusedonhealthcareand finance, capable of understanding and accurately responding to a wide rangeofuserinquiries. ThisproblemcanbeviewedasaclassicNLPissue—textsimplification. Sothemethods used can be general and not restricted to the problem of simplifying prompts. There are many ways to achieve this. One simple method is to define some heuristics and identify redundant words that can be eliminated without losing essential information. For example, wecanexamineeachtokeninasequenceintermsofitscontributiontotheoverallmeaning andremovethosethatprovideminimalvalue[Lietal.,2023c;Jiangetal.,2023b]. Another methodinvolvesframingtheproblemasasequence-to-sequencetask. Withlabeleddatafor textsimplification,wecantrainanencoder-decodermodeltotransformeachinputtextinto itssimplifiedform. Inaddition,giventhatmanyLLMshavebeenfine-tunedandalignedto performtextsimplificationtasks,itisstraightforwardtousethesemodelstosimplifyprompts. For example, we can prompt an LLM to simplify a text under certain constraints, such as limitingthelengthofthesimplifiedtext. 9.4 Summary Inthischapter,wehavediscussedavarietyofissuesrelatedtoLLMprompting. Ourdiscussion hasfocusedmainlyontwoaspects: • HowtodesignbasicpromptstoguidethepredictionsofLLMsandrefinetheseprompts formoreeffectiveandefficientproblem-solving? • Howtoautomatethedesignandrepresentationofprompts? Solutionstotheseissuesinvolvebothgeneralpromptdesignsandmoreadvancedtechniques, suchasCoTandpromptlearning,whichhavebeenexploredextensivelyinrecentresearch. InNLP,promptingcanbeviewedasatechnologythathasevolvedalongwithLLMs,and inasense,ithasopenedthedoortothepracticalapplicationofthesemodelsinanimpressive rangeofproblemdomains. Infact,ifweexpandtheconceptofpromptstosomeextent,itcan betracedbacktotheearlydaysofmachinelearningandNLP.Forexample,manyNLPsystems use hand-crafted features and templates to “prompt” specific tasks. Imagine developing a featuretoindicatewhetheratextisformalorinformal. Wecanfeedthisfeatureintoamachine translationsystemtoconditionthetranslationonthetypeoftheinputtext. 9.4 Summary 531 The widespread use of the modern concept of prompts began with the rise of large pre- trained models in the field of NLP. Initially, these models, such as BERT, were adapted to specificdownstreamtasksmainlythroughfine-tuning. However,researcherssoondiscovered thatbydesigningspecific\"prompts\"—addingcertainwordsorsentencestotheinput—the models could be triggered to respond to specific tasks without extensive fine-tuning. This motivated the NLP community to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre-trainingprocedure.",
    "models, such as BERT, were adapted to specificdownstreamtasksmainlythroughfine-tuning. However,researcherssoondiscovered thatbydesigningspecific\"prompts\"—addingcertainwordsorsentencestotheinput—the models could be triggered to respond to specific tasks without extensive fine-tuning. This motivated the NLP community to develop and apply universal foundation models that can be prompted to address various tasks without changing the underlying architecture and the pre-trainingprocedure. Promptingapproacheswerefirstexperimentedwithsmallermodelsandlaterdemonstrated impressivecapabilitieswithlargemodelslikeGPT-3,whichcouldgeneratehigh-qualitytext inresponsetosimplepromptsacrossvarioustasks. Aspromptingtechnologyevolved,prompt engineering emerged as a critical area of research. As discussed in this chapter, it broadly involves designing effective prompts to maximize model performance, encompassing both hand-craftedandautomaticallygeneratedprompts. Morerecentresearchhasexploredhowto enhancetheeffectivenessofpromptingthroughtechniqueslikefew-shotlearning,zero-shot learning, and CoT reasoning, enabling LLMs to work effectively across a wide range of scenarios. A general discussion of prompting can be very broad, and we cannot cover all details in this chapter. For more advanced techniques of prompting, the reader can refer to recentsurveys. Topicsincludein-contextlearning[Li,2023;Dongetal.,2022],CoT[Chu etal.,2023;Yuetal.,2023b;Zhangetal.,2023a],efficientprompting[Changetal.,2024], andgeneralpromptengineering[Liuetal.,2023d;Chenetal.,2023a]. Notethatalthoughwewouldideallyliketodevelopgeneralpromptingmethodswithout adjustingmodelarchitecturesandparameters,theresultsofpromptinggenerallydependheavily onthequalityandsizeofthegivenLLMs. Forstrongermodels,suchascommercializedonline LLMs,simplepromptsmaybesufficienttoinstructthesemodelstoperformtaskscorrectly. In thiscase,promptengineeringisrelativelyeasy,thoughwestillneedcertaineffortstomake LLMs work properly. By contrast, if the LLMs are not powerful enough, we may need to carefullydesignthepromptstoachievethedesiredresults. Inmanycases,fine-tuningisstill necessarytoadaptthemodelstosophisticatedpromptingstrategies. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 10 Alignment Alignment is not a new concept in NLP, but its meaning varies across different domains and over time. In traditional NLP, the term alignment typically refers to the tasks that link correspondingelementsintwosets,suchasaligningwordsbetweenaChinesesentenceand anEnglishsentence. AsLLMsbecomeincreasinglyimportantinNLPresearch,thistermis morebroadlyusedtorefertoaligningmodeloutputswithhumanexpectations. Theproblem thatalignmentaddressesisthattheoutputofamodelmaynotalignwiththespecificgoals or contexts intended by users. For example, pre-trained LLMs may not be able to follow userinstructionsbecausetheywerenottrainedtodoso. AnotherexampleisthatLLMsmay generateharmfulcontentorperpetuatebiasesinherentintheirtrainingdata. Thisposesnew challengesinensuringthatLLMoutputsarenotonlyaccurateandrelevant,butalsoethically soundandnon-discriminatory. Simplypre-trainingLLMscanresultinavarietyofalignmentproblems. Ourultimategoal istoresolveormitigatealltheseproblemstoensureLLMsarebothaccurateandsafe. There isaninterestingissuehere: sincelargelanguagemodelsaretrainedonvastamountsofdata, wehavereasontobelievethatifwehavesufficientdatacoveringavarietyoftasksandaligned withhumanpreferences,pre-trainingcouldmakeLLMsaccurateandsafeenough,perhaps eveneliminatingtheneedforalignment. However,therealityisthatitisnearlyimpossible togatherdatathatencompassesalltasksoradequatelyrepresentshumanpreferences. This makes it difficult to achieve model alignment through pre-training alone, or at least, at this stage,alignmentremainsaverynecessaryandcriticalstepinthedevelopmentofLLMs. Inthischapter,wewillfocusonalignmentmethodsforLLMs. Wewillbeginbydiscussing thegeneralalignmenttasks. Thenwewillconsidertwowidely-usedapproaches,knownas instructionalignmentandhumanpreferencealignment,respectively. Theformerresortsto supervisedfine-tuningtechniquesandguidestheLLMstogenerateoutputsthatadhereclosely touserinstructions. Ontheotherhand, thelattertypicallyreliesonreinforcementlearning techniques,wheretheLLMsaretrainedbasedonfeedbackfromhumans. Whilethesemethods are motivated by different goals, they are commonly used together to develop well-aligned LLMs. 534 Chapter 10. Alignment 10.1 An Overview of LLM Alignment Alignment can be achieved in several different ways. We need different methods for LLM alignmentbecausethisproblemisitselfcomplicatedandmultifaceted,requiringablendof technicalconsiderations. Hereweconsiderthreewidely-usedapproachestoaligningLLMs. Thefirstapproachistofine-tuneLLMswithlabeleddata. Thisapproachisstraightforward asitsimplyextendsthepre-existingtrainingofapre-trainedLLMtoadaptittospecifictasks. An example of this is supervised fine-tuning (SFT), in which the LLM is further trained on a dataset comprising task-specific instructions paired with their expected outputs. The SFTdatasetisgenerallymuchsmallercomparedtotheoriginaltrainingset,butthisdatais highly specialized. The result of SFT is that the LLM can learn to execute tasks based on userinstructions. ThesetaskscaneitherbeonespreviouslyencounteredinSFT,ornewtasks similar to those. For example, by fine-tuning the LLM with a set of question-answer pairs, themodelcanrespondtospecificquestions,evenifnotdirectlycoveredintheSFTdataset. Thismethodprovesparticularlyusefulwhenitisrelativelyeasytodescribetheinput-output relationshipsandstraightforwardtoannotatethedata. Thesecondapproachistofine-tuneLLMsusingrewardmodels. Onedifficultyinalignment isthathumanvaluesandexpectationsarecomplexandhardtodescribe. Inmanycases,even forhumansthemselves,articulatingwhatisethicallycorrectorculturallyappropriatecanbe challenging. Asaresult,collectingorannotatingfine-tuningdataisnotasstraightforwardas itiswithSFT.Moreover, aligningLLMsisnotjustataskoffittingdata, orinotherwords, thelimitedsamplesannotatedbyhumansareofteninsufficienttocomprehensivelydescribe these behaviors. What we really need here is to teach the model how to determine which outputsaremoreinlinewithhumanpreferences,forexample,wenotonlywanttheoutputs to be technically accurate but also to align with human expectations and values. One idea istodeveloparewardmodelanalogoustoahumanexpert. Thisrewardmodelwouldwork byrewardingtheLLMwheneveritgeneratesresponsesthatalignmorecloselywithhuman preferences,muchlikehowateacherprovidesfeedbacktoastudent. Toobtainsuchareward model,wecantrainascoringfunctionfromhumanpreferencedata. Thetrainedrewardmodel isthenusedasaguidetoadjustandrefinetheLLM.ThisframestheLLMalignmenttaskasa reinforcementlearningtask. Theresultingmethods,suchasreinforcementlearningfrom humanfeedback(RLHF),havebeendemonstratedtobeparticularlysuccessfulinadapting LLMstofollowthesubtletiesofhumanbehaviorandsocialnorms. Thethirdapproachistoperformalignmentduringinferenceratherthanduringtraining or fine-tuning. From this perspective, prompting in LLMs can also be seen as a form of alignment, but it does not involve training or fine-tuning. So we can dynamically adapt an LLMtovarioustasksatminimalcost. Anothermethodtodoalignmentatinferencetimeisto rescoretheoutputsofanLLM.Forexample,wecoulddevelopascoringsystemtosimulate human feedback on the outputs of the LLM (like a reward model) and prioritize those that receivemorepositivefeedback. Thethreemethodsmentionedabovearetypicallyusedinsequenceoncethepre-training is complete: we first perform SFT, then RLHF, and then prompt the LLM in some way 10.2 Instruction Alignment 535 Pre-training Alignment Stage Stage Instruction HumanPreference Pre-training Alignment Alignment Prompting (e.g.,SFT) (e.g.,RLHF) Training&Fine-tuning Inference Figure10.1: Schematicillustrationofthepre-train-then-alignmethodfordevelopingLLMs.",
    "outputs of the LLM (like a reward model) and prioritize those that receivemorepositivefeedback. Thethreemethodsmentionedabovearetypicallyusedinsequenceoncethepre-training is complete: we first perform SFT, then RLHF, and then prompt the LLM in some way 10.2 Instruction Alignment 535 Pre-training Alignment Stage Stage Instruction HumanPreference Pre-training Alignment Alignment Prompting (e.g.,SFT) (e.g.,RLHF) Training&Fine-tuning Inference Figure10.1: Schematicillustrationofthepre-train-then-alignmethodfordevelopingLLMs. Inthepre-trainingstage,wetrainanLLMonvastamountsofdatausingnexttokenprediction. Then,inthealignmentstage,wealigntheLLMtouserinstructions,intents,andpreferences. Thisincludesinstructionalignment,humanpreferencealignment,andprompting. during inference. This roughly divides the development of LLMs into two stages — the pre-training stage and the alignment stage. Figure 10.1 shows an illustration of this. Since promptingtechniqueshavebeenintensivelydiscussedinthepreviouschapter,wewillfocus onfine-tuning-basedalignmentmethodsintherestofthischapter. 10.2 Instruction Alignment OnefeatureofLLMsisthattheycanfollowthepromptsprovidedbyuserstoperformvarious tasks. Inmanyapplications,apromptconsistsofasimpleinstructionanduserinput,andwe wanttheLLMtofollowthisinstructiontoperformthetaskcorrectly. ThisabilityofLLMsis alsocalledtheinstruction-followingability. Forexample,belowisapromptwherewewant theLLMtoextractkeypointsandprovideaconcisesummaryforalengthyarticle. Instruction Summarizethistextinthreesentences. Input DaylightSavingsTime(DST)-theprocessofmovingclocks forwardbyonehourinthesummer-wasstartedinGermanyin 1916. DuringWorldWarOneitwasawaytosave... Output ThistaskrequirestheLLMtounderstandtheinstruction“Summarizethistextinthreesen- tences”andperformthesummarizationaccordingly. However,LLMsaretypicallytrainedfor next-tokenpredictionratherthanforgeneratingoutputsthatfollowinstructions. Applyinga pre-trainedLLMtotheaboveexamplewouldlikelyresultinthemodelcontinuingtowrite theinputarticleinsteadofsummarizingthemainpoints. Thegoalofinstructionalignment 536 Chapter 10. Alignment (orinstructionfine-tuning)istotunetheLLMtoaccuratelyrespondtouserinstructionsand intentions. Therestofthissectionwilldiscusssomeissuesrelatedtoinstructionalignment, includingfine-tuningLLMstofollowinstructions,generatingorcollectinginstructiondata, andgeneralizinginstructionalignment. 10.2.1 Supervised Fine-tuning OnestraightforwardapproachtoadaptingLLMstofollowinstructionsistofine-tunethese modelsusingannotatedinput-outputpairs[Ouyangetal.,2022;Weietal.,2022a]. Unlike standardlanguagemodeltraining,herewedonotwishtomaximizetheprobabilityofgener- atingacompletesequence,butrathermaximizetheprobabilityofgeneratingtherestofthe sequencegivenitsprefix(i.e.,generatingtheoutputgiventheinput). Thisapproachmakes instructionfine-tuningabitdifferentfrompre-training. Letx=x ...x beaninputsequence 0 m (e.g.,instruction+userinput)andy=y ...y bethecorrespondingoutputsequence. TheSFT 1 n dataisacollectionofsuchinput-outputpairs(denotedbyS),whereeachoutputisthecorrect responseforthecorrespondinginputinstruction. Forexample,belowisanSFTdataset x(instruction+userinput) y(output) Summarizethefollowingarticle. {∗summary∗} Article: Inrecentyears,solarenergyhasseen unprecedentedgrowth,becomingthefastest-growing... Analyzethesentimentofthefollowingreview. Positive Review: Iabsolutelylovedthenewdiningexperience. Thefoodwasdivineandtheservicewasimpeccable. TranslatethefollowingsentenceintoFrench. Lapratiqueaide Sentence: practiceindeedhelps. effectivement. Extractthemainfinancialfiguresfromthefollowing Revenue: $10million, earningsreport. ProfitMargin: 15% Report: Thecompanyreportedarevenueof$10million inthefirstquarterwithaprofitmarginof15%... Classifythefollowingemailasspamornotspam. Spam Text: Congratulations! You’vewona$500giftcard. Clickheretoclaimnow. Provideasolutiontothefollowingtechnicalissue. First,checkfor... Issue: mycomputerisrunningslowandoftenfreezes. wheretheinstructionsarehighlighted. Thisdatasetcontainsinstructionsandthecorresponding outputsforseveraldifferentNLPproblems,andsowecanfine-tuneanLLMtohandlemultiple taskssimultaneously. InSFT,weaimtomaximizetheprobabilityofthecorrectoutputgiventheinput. Consider anLLMwithpre-trainedparametersθˆ . Thefine-tuningobjectivecanthenbeformulatedas: (cid:88) θ˜ = argmax logPr (y|x) (10.1) θˆ+ θˆ+ (x,y)∈D 10.2 Instruction Alignment 537 Input Output Loss=0 Loss̸=0 x 1 x 2 x 3 y 1 y 2 x 1 x 2 x 3 y 1 y 2 x x x x y x x x x y 0 1 2 3 1 0 1 2 3 1 (a)ForwardPass (b)BackwardPass Figure10.2: Illustrationofsupervisedfine-tuningforLLMs. Weconcatenatetheinputandthe outputintoasinglesequence. Duringtheforwardpass,weruntheLLMasusual. Duringthe backwardpass,wecomputethelossonlyfortheoutputpartandsimplysetthelossforthe inputpartto0. whereθ˜ denotestheparametersoptimizedviafine-tuning,andθˆ+representsanadjustmenttoθˆ . Herewewillomitthesuperscript+anduseθ torepresentθˆ+ tokeepthenotationuncluttered. Butthereadershouldkeepinmindthatthefine-tuningstartsfromthepre-trainedparameters ratherthanrandomlyinitializedparameters. TheobjectivefunctionlogPr (y |x,y )iscomputedbysummingthelog-probabilities θ i <i ofthetokensiny,conditionalontheinputxandalltheprevioustokensy : <i n (cid:88) logPr (y|x) = logPr (y |x,y ) (10.2) θ θ i <i i=1 Thisformulationisequivalenttominimizingthecross-entropyloss. Notethatminimizingtheconditionallog-probabilitylogPr (y|x)isnotastandardlan- θ guagemodeltrainingproblem. Ifweconcatenatexandyasasinglesequence,amoregeneral form of language modeling is based on the joint log-probability logPr (x,y), that is, we θ minimizethelossoveralltokensofthesequenceseq =[x,y]. Wecanwritetheprobability x,y ofthissequenceusingthechainrule logPr (seq ) = logPr (x,y) θ x,y θ = logPr (x)+logPr (y|x) (10.3) θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) setto0 losscomputation Therearetwotermsontheright-handsideoftheequation. Wecansimplysetthefirstterm logPr (x)to0,focusingsolelyonthesecondtermlogPr (y|x)forlosscomputation. Asa θ θ result,thetrainingcanbeimplementedusingstandardLLMs. Forthesequenceseq ,wefirst x,y runtheforwardpassasusual. Then,duringthebackwardpass,weforcethelosscorresponding toxtobezero. Figure10.2showsanillustrationofthisprocess. BytakinglogPr (seq )astheobjectivefunction,wecandescribeSFTusingaregular θ x,y 538 Chapter 10. Alignment formoflanguagemodeltraining: (cid:88) θ˜ = argmax logPr (seq ) (10.4) θ x,y θ (x,y)∈D Theproblemweconsideredaboveisfundamentallyasingle-roundpredictionproblem, wheretheLLMgeneratesaresponsebasedonasingleinputwithoutanyfurtherinteractionor feedbackfromtheuser. Theinputisprocessed,andtheoutputisgeneratedinonego. Thisis typicalinscenarioswhereasinglequestionisasked, andasingleanswerisprovided, with nofollow-upquestionsorclarifications. However,inpractice,wesometimeshavetohandle multi-roundpredictionproblems,forexample,anLLMengagesinadialogueovermultiple turns. Inthissetting,theLLMnotonlygeneratesresponsesbasedontheinitialinputbutalso incorporatessubsequentinputsthatmightrefineorexpandonearlierinteractions. Forexample, wecanusetheLLMtoactasahealthcareassistantchatbotandhaveaconversationwiththe user,likethis User I’vebeenfeelingverytiredlately. Chatbot I’msorrytohearthat. Besidesfeelingtired,haveyounoticedany othersymptoms? User Yes,I’malsoexperiencingheadachesfrequently. Chatbot Howlonghavethesesymptomsbeengoingon? User Aboutaweeknow. Chatbot Itmightbegoodtocheckinwithahealthcareprofessional. Would youlikehelpsettingupanappointment? User Yes,please. Canitbeafterworkhours? Chatbot Sure,Icanarrangethat. ThereareslotsavailablenextWednesday andThursdayafter5PM.Whichdayworksbetterforyou? ... In this task, there are several rounds of conversation, each involving the generation of a response based on the user’s request or question and the conversational history. Suppose wehaveK roundsofconversation,denotedby{x1,y1,x2,y2,...,xK,yK}. Herexk andyk denotetheuserrequestandtheresponse,respectively,foreachroundk. Thelog-probability of generating the response can be written as logPr (yk|x1,y1,...,xk). Our goal is then to θ maximizethesumoftheselog-probabilities K (cid:88) θ˜ = argmax logPr (yk|x1,y1,...,xk) (10.5) θ θ k=1 10.2 Instruction Alignment 539 Astraightforwardimplementationofthisinvolvescalculatingtheconditionalprobability foreachk. However,itrequiresrunningtheLLMK times,eachtimewithanincreasedconver- sationalhistorytomakepredictions. Amoreefficientmethodistoperformlosscomputation of all responses in a single run of the LLM. To do this, we represent the conversation as a sequence",
    "logPr (yk|x1,y1,...,xk). Our goal is then to θ maximizethesumoftheselog-probabilities K (cid:88) θ˜ = argmax logPr (yk|x1,y1,...,xk) (10.5) θ θ k=1 10.2 Instruction Alignment 539 Astraightforwardimplementationofthisinvolvescalculatingtheconditionalprobability foreachk. However,itrequiresrunningtheLLMK times,eachtimewithanincreasedconver- sationalhistorytomakepredictions. Amoreefficientmethodistoperformlosscomputation of all responses in a single run of the LLM. To do this, we represent the conversation as a sequence seq =[x1,y1,...,xK,yK] (or seq for short). The log-probability of x1,y1,...,xK,yK thissequenceisgivenby logPr (seq) = logPr (x1,y1,...,xK,yK) θ θ = logPr (x1)+logPr (y1|x1)+···+ θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) setto0 losscomputation logPr (xK|x1,y1,...,yK−1)+ θ (cid:124) (cid:123)(cid:122) (cid:125) setto0 logPr (yK|x1,y1,...,xK) (10.6) θ (cid:124) (cid:123)(cid:122) (cid:125) losscomputation The trick here is that we ignore the loss for generating user inputs (i.e., logPr (x1),..., θ logPr (xK|x1,y1,...,yK−1)), as illustrated in Figure 10.3. Hence we only compute the θ probabilitiesofgeneratingtheresponsesgiventheirconversationalhistories,inotherwords, thevalueontheright-handsideofEq. (10.6)isactuallyequaltothevalueontheright-hand sideofEq. (10.5). AswithEq. (10.4),thetrainingofthismulti-roundpredictionmodelcanbe achievedbymaximizingtheloglikelihoodoveratrainingdatasetD: (cid:88) θ˜ = argmax logPr (seq) (10.7) θ θ seq∈D WhileimplementingtheSFTmethodsintroducedaboveseemstrivialastheyarefunda- mentally the same as regular language model training, there are still issues that need to be consideredinpractice. Forexample, • SFTrequireslabeleddata. ThismakesSFTquitedifferentfrompre-training,whereraw text is used as training data and is readily available. As in other supervised machine learningproblems,dataannotationandselectioninSFTarenotsimpletasks. Ingeneral, wewishtodevelopSFTdatathatisbothsubstantialinquantityandhighinquality,and thisdatashouldbehighlyrelevanttothetaskstheLLMwillperform. Ontheotherhand, there is a need to fine-tune LLMs with less data to minimize computational and data constructioncosts. Often,thequalityofLLMsishighlydependentonthedatausedin SFT.Thus,suchdatamustbecarefullydevelopedandexamined. Aswewillseeinlater subsections,SFTcanbemoreefficientandeffectivethroughmoreadvancedtechniques fordataconstruction. • SFT is still computationally expensive for LLMs due to their large size. As a result, maintainingandupdatingsuchmodelsisresource-intensive. Forexample,applyinggra- dientupdatestobillionsofparameterswithinanLLMrequiressignificantcomputational power and memory. This often requires high-performance computing environments, 540 Chapter 10. Alignment Loss=0 Loss̸=0 Loss=0 Loss̸=0 Pr (x1) Pr (y1|x1) Pr (x2|x1,y1) Pr (y2|x1,y1,x2) θ θ θ θ x1 y1 x2 y2 ··· User:I’vebeenfeelingverytiredlately. Chatbot:I’msorrytohearthat.Besidesfeelingtired, haveyounoticedanyothersymptoms? User:Yes,I’malsoexperiencingheadachesfrequently. Chatbot:Howlonghavethesesymptomsbeengoingon? . . . Figure10.3: Illustrationofsupervisedfine-tuningforconversationalmodels. HeretheLLM actsasachatbottorespondtoeachrequestbasedontheconversationalhistory. Theconversa- tionprogressesbyalternatingbetweentheuserandthechatbot. InSFT,wetreattheentire conversation as a sequence, just like in standard LLMs, but compute the loss only for the responsesoftheLLM. whicharecostlytooperate. Toaddressthesechallenges,variousoptimizationstrategies, suchaspruning,quantization,andtheuseofmoreefficienttrainingalgorithms,have beenexplored. Inparticular, there hasbeensignificant interestin parameter-efficient fine-tuningmethodswhicharedesignedtomaintainstate-of-the-artperformancewithout theneedforextensivecomputationalresources. WehaveseeninChapter9thatapplying techniqueslikesoftpromptscanmakethefine-tuningprocessmoreefficient. Forfurther discussiononparameter-efficientmethods,thereadercanrefertorelatedpapersonthis issue[Houlsbyetal.,2019;Huetal.,2022;Hanetal.,2024]. • SFTcanberegardedasapost-trainingstepfollowingpre-training. Itisaseparatetraining phasedesignedtopreservetheadvantagesoftheinitialpre-trainingwhileincorporating new adjustments. This may seem paradoxical because updating a pre-trained LLM with further data potentially causes the model to forget some of its prior knowledge. Imagine a scenario where we have a large amount of SFT data and extensively fine- tunetheLLM.Inthiscase,theLLMcouldoverfitthedata,whichinturnmayreduce generalization performance or cause catastrophic forgetting. A common strategy to mitigatethisissueistoemployregularizationandearlystoppingtechniques. Another 10.2 Instruction Alignment 541 practicalapproachistouseasmallerlearningratetogentlyadjusttheweightsofthe LLM.Inaddition,fine-tuningwithdatafromdiversesourcesandproblemdomainscan alsobebeneficial. Nevertheless,inpractice,theSFTstepisoftencarefullyexamined andrequiressubstantialengineeringandexperimentaleffortstooptimize. 10.2.2 Fine-tuning Data Acquisition Fine-tuningdataissoimportantthatmuchrecentworkinLLMhasfocusedondeveloping variousdatasetsforinstructionfine-tuning. Aswithmostworkinmachinelearning,thereare generallytwoapproachestodataacquisition—manualdatagenerationandautomaticdata generation. 1. Manually Generated Data Onestraightforwardmethodistorecruithumanannotatorstocreateinput-outputpairsforthe tasksofinterest. UnlikedataannotationinconventionalNLP,suchastextclassification,where annotatorssimplyassignlabelstocollectedtextsaccordingtoguidelines,creatingfine-tuning dataforLLMsrequiresmorestepsandeffort,makingitthusmorechallenging. Supposewe wanttoobtainfine-tuningdatafortheEnglish-to-Chinesemachinetranslationtask. Thefirst step is to write a prompt template to describe the task and format the problem clearly. For example, Instruction TranslatethetextfromEnglishtoChinese. UserInput {∗text∗} Output {∗translation∗} Then,wecollectpairsofsourceandtargettexts(i.e.,Chinesetextsandthecorresponding translations), and replace the variables {∗text∗} and {∗translation∗} to generate the fine- tuningsamples. Forexample,givenapairofEnglishandChinesesentences How’stheweathertoday? → 今天天气怎么样？ {∗text∗} {∗translation∗} wecangenerateafine-tuningsampleusingtheprompttemplate,likethis Instruction TranslatethetextfromEnglishtoChinese. UserInput How’stheweathertoday? Output 今天天气怎么样？ 542 Chapter 10. Alignment Thatis, x = TranslatethetextfromEnglishtoChinese.\\nHow’stheweathertoday? y = 今天天气怎么样？ Wecanusethis(x,y)pairtofine-tunetheLLM,asdescribedintheprevioussubsection. Onedifficultyhereisthattherearemany,manydifferentwaystowriteprompttemplates forthesametask,anddifferentpeoplemayproduceprompttemplateswithvaryingqualities andcomplexities. Sometimes,wemaywriteprompttemplateswithoverlycomplexorverbose instructions. Sometimes, we may not even know exactly what the target task is and how to describe it. A widely-adopted strategy is to create prompt templates for existing NLP tasks, given that there have been so many well-established NLP problems and benchmarks [Bach et al., 2022; Wang et al.,",
    "Sometimes,wemaywriteprompttemplateswithoverlycomplexorverbose instructions. Sometimes, we may not even know exactly what the target task is and how to describe it. A widely-adopted strategy is to create prompt templates for existing NLP tasks, given that there have been so many well-established NLP problems and benchmarks [Bach et al., 2022; Wang et al., 2022e; Mishra et al., 2022]. In this case, annotators can be giventhe originaltaskdescription andmanyexamples. Then, they canusetheir own ways to express how to prompt the LLM to perform the tasks. Note that, while such a method can ease the process of creating and writing prompts, we still need annotation frameworks and crowdsourcing systems to manage the work and conduct quality control. For example, we generally need to design annotation guidelines and a unified format for writing prompt templates,especiallywhenmanyannotatorsarecontributingtothesametask. Oneadvantage of inducing prompts from existing NLP tasks is that, once the prompt templates have been developed,itiseasytogeneratepromptsusingtheannotatedsamplesintheoriginaltasks. For example,givenabilingualdatasetforEnglish-to-Chinesetranslation,wecaneasilycreatea numberoffine-tuningexamplesbyfillingtheslotsintheabovetemplatewiththesentence pairsinthisdataset. Anotherapproachistodirectlyusethenaturallyexistingdataavailableontheinternet. A commonexampleisbycollectingquestion-and-answerpairsfromQAwebsitestofine-tune LLMsforopen-domainQAtasks[Joshietal.,2017]. ManybenchmarksinQAarebuiltin thiswaybecausetherearesomanytypesofquestionsthatitisimpossibletothinkofthemall byasmallgroupofpeople. Instead,usingdatafromthosewebsitescanensurethattheLLM fine-tuningdataisatagoodoracceptablelevelintermsofquantityandquality. In addition to employing existing resources, another straightforward way to develop a fine-tuningdatasetistocrowdsourcethedata. Asimpleapproachistoallowuserstoinput anyquestion,afterwhichresponsesareeithermanuallygivenorautomaticallygeneratedby anLLMandthenmanuallyannotatedandcorrected. Itisthuspossibletocapturerealuser behaviorandconsequentlygatherinputsandoutputsforalargenumberof“new”problems thattraditionalNLPtasksdonotcover. Anissuerelatedtotheconstructionofthefine-tuningdatasetsisthatweusuallywantthe data to be as diverse as possible. Many studies have found that increasing the diversity of fine-tuningdatacanimprovetherobustnessandgeneralizationabilityofLLMs. Forthisreason, there has been considerable interest in involving more diverse prompts and tasks in LLM fine-tuniningdatasets. Wewillprovidefurtherdiscussiononthegeneralizationoffine-tuning inSection10.2.4. 10.2 Instruction Alignment 543 2. Automatically Generated Data Onelimitationofmanualdatagenerationisthatthequalityanddiversitylargelydependon human experience and creativity. Therefore, if we want LLMs to handle a broad range of tasks,thatis,toeffectivelyexecuteanyinstruction,relyingonhuman-annotateddataforLLM fine-tuningisofteninefficient. Moreover,thecoverageofsuchdatacanbelimited,andthe datamayevencontainbiasesintroducedbytheannotatorsthemselves. Analternativeapproach istogeneratedataautomatically. Forexample,wecancollectanumberofquestionsthrough crowdsourcing,andemployawell-tunedLLMtogenerateanswerstothequestions. These question-answerpairsarethenusedasfine-tuningsamplesasusual. Thismethod,thoughvery simple,hasbeenextensivelyappliedtogeneratelarge-scalefine-tuningdataforLLMs. The above way of producing synthetic fine-tuning data is similar to those used in data augmentationforNLP.IfwehaveanLLM,wecanproduceapredictioninresponsetoany input. Repeating this process for different inputs allows us to create a sufficient number of fine-tuningsamples. Suchamethodisparticularlyusefulforfine-tuningnewLLMsusinga well-tunedLLM.However,onedisadvantageofthisapproachisthatitreliesonhuman-crafted orcollectedinputsfordatageneration,whichmayturnouttobeinappropriateforgeneralizing LLMs. InmanyLLMapplications,asignificantchallengearisesfromthebroadrangeofusers’ questions and requests, many of which are not covered in existing NLP tasks and datasets. Inthesecases,itbecomesnecessarytogeneratenotonlythepredictionsbutalsotheinputs themselves. Hereweconsiderself-instructasanexampletoillustratehowtogenerateLLMfine-tuning samples[Wangetal.,2023e;Honovichetal.,2023]. TheideaisthatwecanpromptanLLM tocreateanewinstructionbylearningfromotherinstructions. Giventhisinstruction,theLLM canthenfillinotherfields(suchastheuserinput)andproducethepredictions. Figure10.4 showsaschematicillustrationofself-instruct. Herewegiveabriefoutlineofthekeysteps involved. • The self-instruct algorithm maintains a pool of tasks. Initially it contains a number of seed hand-crafted tasks, each with an instruction and input-output sample. As the algorithmproceeds,LLM-generatedinstructionsandsampleswillbeaddedtothispool. • Ateachstep,asmallnumberofinstructionsaredrawnfromtheinstructionpool. For example, we can randomly select a few human-written instructions and a few LLM- generatedinstructionstoensurediversity. 544 Chapter 10. Alignment Initializethetaskpoolwithanumberofinstructions andcorrespondinginput-outputsamples. Initialization Sample1: (Instruction,User-input,Output) Sample2: (Instruction,User-input,Output) TaskPool ··· Drawafewinstructionsfromthepool Sampling Instructiona sampling Instruction b TaskPool Instructionc PrompttheLLMtogenerateanewinstructionbasedon Instruction thedrawninstructions. Generation You are provided several different instructions for performing some tasks. Please generate an instruction based on these. Task 1:Instructiona Task 2:Instructionb Task 3:Instructionc New Task:Instructionnew Giventhenewly-generatedinstructionandafew Sample input-outputsamples,generateanewsample. Generation You are provided with a set of input-output samples tasks, each composed of an instruction, a user input, and an output. Please generate a new sample based on these. Sample 1:Samplea Sample 2:Sampleb New Sample:InstructionnewUser-inputnewOutputnew Filteroutinvalidandlow-qualitysamples. Addtheremainingsamplesintothepool. Filtering Figure10.4: Illustrationofself-instruct[Wangetal.,2023c]. Thismethodmaintainsapoolof instructionsandcorrespondinginput-outputsamples. Initially,thepoolcontainsanumberof hand-craftedinstructionsandsamples. Eachtime,wedrawafewinstructionsfromthepool. An LLM is then prompted to generate new instructions and samples based on those drawn. Finally,thenewly-generatedinstructionsandsamplesarefilteredandaddedtothepool. • The selected instructions are then used as demonstration examples. Thus, the LLM canin-contextlearnfromtheseexamplesandproduceanewinstruction. Belowisan exampletemplateforpromptingtheLLM. 10.2 Instruction Alignment 545 You are provided several different instructions for performing some tasks. Pleasegenerateaninstructionbasedonthese. Task1: {instruction1} Task2: {instruction2} Task3: {instruction3} Task4: {instruction4}",
    "to generate new instructions and samples based on those drawn. Finally,thenewly-generatedinstructionsandsamplesarefilteredandaddedtothepool. • The selected instructions are then used as demonstration examples. Thus, the LLM canin-contextlearnfromtheseexamplesandproduceanewinstruction. Belowisan exampletemplateforpromptingtheLLM. 10.2 Instruction Alignment 545 You are provided several different instructions for performing some tasks. Pleasegenerateaninstructionbasedonthese. Task1: {instruction1} Task2: {instruction2} Task3: {instruction3} Task4: {instruction4} NewTask: • Giventhegeneratedinstruction,theLLMisthenpromptedtocompletethesampleby fillingintheremaininginputfieldsandgeneratingthecorrespondingoutput. Belowisa prompttemplate. Youareprovidedwithasetofinput-outputsamples,eachcomposed ofaninstruction, auserinput, andanoutput. Pleasegenerateanew samplebasedonthese. Sample1: {instruction1} Input: {user-input1} Output: {output1} Sample2: {instruction2} Input: {user-input2} Output: {output2} NewSample: {new-instruction} • Thisnewly-generatedsampleisexaminedbysomeheuristicrules(suchasfilteringout samples or instructions that are similar to those already in the pool). If it passes, the sampleandinstructionareaddedtothepool. This generation process can be repeated many times to obtain a sufficient number of fine-tuningsamples. Notethat,above,wejustshowsimpleprompttemplatesforgenerating instructionandfine-tuningsamples. Ofcourse,wecandevelopbettertemplatestogenerate morediverseandaccurateinstructionandfine-tuningsamples. Forexample,forcertaintasks liketextclassification,theLLMmaytendtoproducebiasedpredictions,forexample,most generatedsamplesbelongtoasingleclass. Insuchcases,wecanadjusttheorderofgeneration ofdifferentfields. Morespecifically,wecanspecifytheoutput(i.e.,theclass)withsomeprior, and prompt the LLM to generate user input given both the instruction and the output. This 546 Chapter 10. Alignment methodresemblesinputinversion,wheretheLLMgeneratestheinputbasedonthespecified output[Longpreetal.,2023]. UsingLLM-generatedinstructionsandfine-tuningsampleshasbeenacommonmethod fordevelopingLLMs,especiallygiventhatmanuallydevelopingsuchdataissoexpensivethat mostresearchgroupscannotaffordit. Inseveralwell-tunedLLMs,theirfine-tuningdatasets includeacertainamountofsyntheticdata,whichhasproveduseful[Ouyangetal.,2022;Taori etal.,2023;Chiangetal.,2023b]. Therehavebeenfurtherstudiesonsyntheticdatageneration forLLMfine-tuning. Forexample,onecangeneratemorediverseinstructionsbyintroducing evolutionary algorithms [Xu et al., 2024], or use synthetic data as supervision signals in a moreadvancedfine-tuningprocess[Chenetal.,2024b]. Morerecently,therehasalsobeen considerableinterestinusingsyntheticdatainthepre-trainingstage[Gunasekaretal.,2023; Allaletal.,2024]. Inmanyapplications,areal-worldscenarioisthat,givenatask,wecancollectorannotate a relatively small amount of fine-tuning data, for example, we can recruit experts to create questions for QA tasks in a specific domain. But the quantity and diversity of this data are in general not sufficient. In this case, we can use self-instruct techniques to generate more diverse question-answer pairs, and thus augment the fine-tuning data. This provides a way of bootstrapping the LLM starting from a seed set of fine-tuning samples. Note that using self-generated data is a common practice and has long been applied in NLP. For example, thisapproachhasbeensuccessfullyusedinparsingandmachinetranslation[Charniak,1997; Sennrichetal.,2016a]. 10.2.3 Fine-tuning with Less Data Withtheincreasingprominenceofinstructionfine-tuning,therehasbeenasurgeindemandfor large-scale,high-qualityfine-tuningdata. Forexample,theFLANfine-tuningdataset,whichis compiledfrom1,836tasks,contains15millionsamples[Longpreetal.,2023]. Fine-tuning LLMswithsuchlargedatasetsistypicallyacomputationallyexpensivetask,especiallygiven thatupdatingthelargenumberofparametersinLLMsisresource-intensive. Oneapproach formitigatingthisissueistoexploreefficientmodeltrainingmethods,forexample,onecan useparameter-efficientmethodstoupdateonlyasmallportionofthemodel. However,many fine-tuningdatasetscontainalargeamountofsyntheticdata,whereerrorsandbiasesarestill inevitable. Anotherapproachtoefficientfine-tuningistoconsideronlythemostrelevantandimpactful examplesforfine-tuning. Wecanthusreducetheamountofdatathatneedstobeprocessed whilestillmaintainingthequalityofthemodelupdates. Thereareseveralmethodstoachieve this. Forexample,Zhouetal.[2023a]builtaninstruction-followingdatasetcontainingonly 1,000 samples by carefully crafting the prompts and collecting samples from a variety of NLP tasks. They showed that the LLaMa 65B model fine-tuned with this dataset could be competitivewithorevenbetterthanmodelswithmuchmorefine-tuningeffort. Thissuggests thatLLMscanbeadaptedtorespondtodiversetaskswithoutnecessarilyneedingfine-tuning onalltypesofinstruction-followingdata. Chenetal.[2024a]developedasystembasedon the GPT-3.5 model to assess the quality of each instruction-following sample. Therefore, 10.2 Instruction Alignment 547 they could select high-quality samples from existing datasets, showing better fine-tuning performance with fewer fine-tuning samples. Researchers have also developed methods to eitherselectorfilteroutdatausingheuristics[Zhaoetal.,2024;Geetal.,2024],ortoprioritize datathatmoresignificantlyinfluencesthefine-tuningprocess[Xiaetal.,2024]. Infact,most of these methods can be seen as instances of larger families of data selection and filtering methods. Anditisoftenthecasethatusinghigherquality(butmaybeless)dataisbeneficial fortrainingNLPmodels. Thediscoveriesininstructionfine-tuningsomewhatdifferfromtraditionalviewsinNLP: the ability of models to handle complex problems can be activated with a small amount of annotateddata,ratherthanrequiringmassiveamountsofsuperviseddataforextensivetraining. Onepossibleexplanationisthattheabilityofgeneratingcorrectresponsesgiveninstructions has been learned during pre-training, but such instruction-response mappings are not with high probabilities during inference. Fine-tuning can slightly adjust the models to get them to follow instructions, requiring significantly less training effort than pre-training. This is closely related to what is known as the superficial alignment hypothesis, which suggests thatlearningprimarilyoccursduringpre-training,andthesubsequentfine-tuningoralignment phasedoesnotsignificantlycontributetotheunderlyingknowledgebaseofanLLM[Zhou et al., 2023a]. Since the core abilities and knowledge of the model are already established frompre-training,effectivefine-tuningforalignmentwithuserneedscanbeachievedwith relativelysmalltrainingfine-tuningeffort. Thisimpliesthepossibilityoffine-tuningLLMs withverylittledata. Inanotherdirection,itmaynotbenecessarytorestrictfine-tuningtopaired instruction-responsedata. Forexample,Hewittetal.[2024]foundthatinstruction-following can be implicitly achieved by fine-tuning LLMs only on responses, without corresponding instructions. Aconceptrelatedtothediscussionhereissampleefficiency. Amachinelearningmethod iscalledsampleefficientifitcanlearneffectivelyfromasmallnumberoftrainingexamples. In this sense, instruction fine-tuning is sample efficient compared with pre-training. From theperspectiveofmachinelearning,sample-efficientmethodscanbeseenasefficientways to sample the space of data, and are",
    "of the model are already established frompre-training,effectivefine-tuningforalignmentwithuserneedscanbeachievedwith relativelysmalltrainingfine-tuningeffort. Thisimpliesthepossibilityoffine-tuningLLMs withverylittledata. Inanotherdirection,itmaynotbenecessarytorestrictfine-tuningtopaired instruction-responsedata. Forexample,Hewittetal.[2024]foundthatinstruction-following can be implicitly achieved by fine-tuning LLMs only on responses, without corresponding instructions. Aconceptrelatedtothediscussionhereissampleefficiency. Amachinelearningmethod iscalledsampleefficientifitcanlearneffectivelyfromasmallnumberoftrainingexamples. In this sense, instruction fine-tuning is sample efficient compared with pre-training. From theperspectiveofmachinelearning,sample-efficientmethodscanbeseenasefficientways to sample the space of data, and are advantageous as they make optimal use of scarce data. Therefore, sampling-based learning techniques, such as many reinforcement learning algo- rithms,canbenefitfromthesesampleefficientapproaches. Forexample,inhumanpreference alignment, we can either efficiently sample preference data via reward models [Liu et al., 2024b]orimprovethesamplingefficiencyinpolicylearning[Wangetal.,2024]. 10.2.4 Instruction Generalization InmanymachinelearningandNLPproblems,trainingamodeltogeneralizeisafundamental goal. Forexample,intextclassification,weexpectourmodeltocorrectlyclassifynewtexts that were not seen during training. However, generalization poses additional challenges in instructionfine-tuning. Weexpectinstruction-fine-tunedLLMstonotonlygenerateappropriate responses for different inputs within a task but also to accurately perform various tasks as described by different instructions. To illustrate this issue, consider an LLM Pr(y|c,z), wherecisaninstruction,zisauserinput,andyisthecorrespondingmodeloutput(i.e.,the 548 Chapter 10. Alignment response). Supposethattheperformanceofthismodelisevaluatedintermsofametric,written asPerformance(Pr(y|c,z))orP(c,z,y)forshort. Informally,whenwesaythismodelcan generalizewithinagiventask(indicatedbytheinstructionc∗),wemeanthattheremaybea valueϵsuchthattheaverageperformanceonnewinputsisabovethisvalue: 1 (cid:88) P(c∗,z′,y′)>ϵ (10.8) |Z| z′∈Z whereZ isthesetofnewinputs,andz′ andy′ areaninputinthissetandthecorresponding output,respectively. Likewise,wecansaythatthismodelcangeneralizeacrosstasksiftheaverageperformance overallinstruction-inputpairsisabovesomeϵ: 1 (cid:88) P(c′,z′,y′)>ϵ (10.9) |D| (c′,z′)∈D whereD isthesetofnewinstruction-inputpairs. Here,weneedtodealwithvariationsintwodimensions: instructionanduserinput. This makesthegeneralizationproblemverycomplex,because,intuitively,amodelneedstolearn from a vast number of tasks and different input-output pairs associated with each task to achievegoodgeneralization. Aswehavediscussedseveraltimesinthisbook,achievingsuch generalizationincursmuchlowercostthanpre-training. Ingeneral,fine-tuningLLMswith instruction-responsedatatosomeextentcanleadtomodelsyieldinginstructionfollowingon newtasks. Nevertheless,itistypicallybelievedthatcertaineffortsarestillneededtoadapt LLMstomakethemunderstandandexecuteinstructionsbroadly. One way to generalize instruction fine-tuning is to increase the diversity of the fine- tuningdata. Inearlierstudiesoninstructionfine-tuning,researchersdevelopedmanydatasets, covering a wide variety of NLP tasks and different instructions for each task [Wang et al., 2022e; Sanh et al., 2022; Longpre et al., 2023]. By transforming these tasks into a unified format,onecanfine-tuneanLLMwithasufficientlylargenumberofsamples,forexample, therehavebeenseveralinstructionfine-tuningdatasetsthatinvolveover100NLPtasksand 1M samples. However, these early datasets mostly focus on existing academic problems, butnotthosethatuserswanttodealwithinreal-worldapplications. Muchrecentworkhas shifted focus to addressing new and more practical problems. For example, there has been considerableinterestinconstructingdatasetsthatcontainlargeandcomplicateddemonstrations andresponsesfromSOTAmodelstorealuserqueries[Wangetal.,2023d;Teknium,2023]. Perhapstheuseoflargeanddiversefine-tuningdatasetshasitsoriginsinattemptstoscale LLMsindifferentdimensions. Indeed,scalinglawshavebeenusedbroadlytomotivatethe developmentofawiderangeofdifferentinstruction-fine-tunedLLMs. Anditisreasonableto scaleinstructionfine-tuningtomakeanLLMfollowbroadinstructions. Fromtheperspective ofLLMalignment,however,scalinginstructionfine-tuningmightnotbeefficienttoachieve generalization. One problem is that instruction fine-tuning relies on supervised learning that learns to 10.2 Instruction Alignment 549 generalize and perform tasks based on instruction-response mappings. However, such an approachdoesnotcapturesubtleorcomplexhumanpreferences(e.g.,tone,style,orsubjective quality)becausethesearehardtoencodeasexplicitinstruction-responsedata. Moreover,the generalizationperformanceisboundedbythediversityandqualityoftheinstruction-response dataset. Given these limitations, we would instead like to employ preference models as an additional fine-tuning step following instruction fine-tuning, so the LLMs can generalize further(seeSection10.3). Anotherviewisthatsomeinstruction-responsemappingsmayalreadybelearnedduring pre-training,andsothepre-trainedLLMshaveencodedsuchmappings. However,sincewe often do not know exactly what data is used in the pre-training, it is hard to judge whether we need to learn such mappings in the fine-tuning. A related question is whether out-of- distribution generalization is primarily achieved during pre-training or fine-tuning. While directlyansweringthisquestionisbeyondthescopeofthischapter,ithasbeenshownthatpre- trainingonlargeanddiversedatasetsiseffectiveinimprovingout-of-distributionperformance [Hendrycksetal.,2020;Radfordetal.,2021;Gunasekaretal.,2023]. Thisraisesaninteresting problem: ifanLLMhasbeenwellpre-trainedatscale,fine-tuningmaynotbeasessentialfor out-of-distributiongeneralization,sincethemodelmayhavealreadyencounteredsufficient distributional variation. This prompts researchers to fine-tune LLMs with modest effort or toexplorenewmethodstoachieveinstruction-following. Asdiscussedintheprevioussub- section,forexample,instructionfollowingcanbeyieldedbyfine-tuningonasmallnumberof carefullyselectedinstruction-responsepairs[Zhouetal.,2023a],orevenbyusingmethods thatarenotexplicitlydesignedtodoso[KungandPeng,2023]. Theabovediscussionprovidestwodifferentstrategies: onerequiresscalingupfine-tuning datasets for larger diversity, the other requires small but necessary fine-tuning datasets for efficient LLM adaptation. However, in practice, involving diverse instructions often helps. In many cases, we need to adapt our LLM for specific purposes. But the LLM, which has possibly encoded broad instruction-following mappings during pre-training, might tend to behaveasageneral-purposeinstructionexecutorevenwithmodestfine-tuning. Aninteresting phenomenonisthatwhenfine-tuningonmathdata,theresultingLLMmightnotspecializein mathoutputs. Instead,thismodelmightrespondnormallytogeneralinstructions,forexample, itcouldgeneratepoetryifinstructedtodoso[Hewitt,2024]. Thisisnotabadthing, butit showsthatLLMsmaynoteasilychangetheirnatureoffollowinggeneralinstructions. Inthis case,additionaladaptationswithmorediversedatamayhelpadjustthewaytheLLMfollows instructions,particularlyforthosetasksweaimtoaddress. 10.2.5 Using Weak Models to Improve Strong Models So farwe haveexplored a variety ofinstruction fine-tuningmethods basedon labeled data. One of the limitations of many such methods is that they require the data to be annotated by humans or",
    "behaveasageneral-purposeinstructionexecutorevenwithmodestfine-tuning. Aninteresting phenomenonisthatwhenfine-tuningonmathdata,theresultingLLMmightnotspecializein mathoutputs. Instead,thismodelmightrespondnormallytogeneralinstructions,forexample, itcouldgeneratepoetryifinstructedtodoso[Hewitt,2024]. Thisisnotabadthing, butit showsthatLLMsmaynoteasilychangetheirnatureoffollowinggeneralinstructions. Inthis case,additionaladaptationswithmorediversedatamayhelpadjustthewaytheLLMfollows instructions,particularlyforthosetasksweaimtoaddress. 10.2.5 Using Weak Models to Improve Strong Models So farwe haveexplored a variety ofinstruction fine-tuningmethods basedon labeled data. One of the limitations of many such methods is that they require the data to be annotated by humans or generated by strong LLMs, which can provide accurate supervision signals in fine-tuning. However, in many cases, the LLM we have in hand is already strong (or at leastisadvantageousinspecificaspectsofproblemsolving),andthusitisnoteasytofinda superiormodelforsupervision. Evenforhumanexperts,whentheproblembecomescomplex, 550 Chapter 10. Alignment providingcorrectanddetailedanswersmightbedifficult,orsometimesinfeasible. Forexample, whenfacedwithanextremelylongdocument,theexpertswouldfinditchallengingtoidentify anyinconsistencies,subtlebiases,ormissingkeypointswithoutconductinganexhaustiveand time-consumingreview. Onemayaskatthispoint: canweuseweakLLMstosupervisestrongLLMs? Thisseems tobeasignificantchallenge,butitmayreflectafuturescenariowhereweneedtosupervise AIsystemsthataresmarterthanhumansoranyotherAIsystems[Burnsetal.,2023b]. The problemofusingsmaller,lesscomplexmodelstoimprovethetrainingoflarger,morecomplex modelsisalsocalledtheweak-to-stronggeneralizationproblem. Whiletherehavenotbeen matureapproachestoweak-to-stronggeneralization,usingsmallermodelstoassiststronger modelshasindeedprovenusefulinseveralareasofLLMs. For instruction fine-tuning, one of the simplest ways of applying weak LLMs is to use thesemodelstogeneratesyntheticfine-tuningdata. SupposewehaveacollectionofinputsX, whereeachinputincludesaninstructionandauserinputifnecessary. Foreachx∈X,weuse aweakLLMPrw(·)togenerateapredictionyˆ =argmax Prw(y|x). Then,thestrongLLM y Prs(·)canbetrainedonthesegeneratedpredictions(seeEq. (10.1)): θ (cid:88) θ˜ = argmax logPrs(yˆ|x) (10.10) θ θ x∈X whereθ isthemodelparameters. Theaboveformtransformsthefine-tuningproblemintoaknowledgedistillationproblem, inotherwords,wedistillknowledgefromtheweakmodeltothestrongmodel. Consequently, we can employ various knowledge distillation methods to achieve this goal. However, ex- plaining weak-to-strong fine-tuning from the perspective of knowledge distillation is not straightforward. Amajorconcernisthatthestrongmodelmaymerelyimitateoroverfitthe errorsoftheweakmodelandfailtogeneralize. Forexample,thefine-tunedstrongmodelstill cannotsolvedifficultproblemsthattheweakmodelcannotaccuratelypredict. Fortunately, preliminaryexperimentsinthislineofresearchhaveshownpositiveandpromisingresults. For example,Burnsetal.[2023a]foundthatfine-tuningthestrongpre-trainedGPT-4modelwith GPT-2-levelsupervisioncouldimprovegeneralizationacrossseveralNLPtasks. Tomeasure howtheweakmodelimprovesthegeneralizationofthestrongmodel,wedefinethefollowing terms: • WeakPerformance(P ). Thisisthetest-setperformanceoftheweakmodel,which weak canberegardedasthebaselineperformance. • Weak-to-strongPerformance(P ). Thisisthetest-setperformanceofthe weak→strong strongmodelthatisfine-tunedwiththeweakmodel. • StrongCeilingPerformance(P ). Thisisthetest-setperformanceofthestrong ceiling modelthatisfine-tunedwithgroundtruthdata. Forexample,wefine-tunethestrong modelwithhuman-annotatedpredictionsandtaketheresultingmodelasaceiling. 10.2 Instruction Alignment 551 Then,theperformancegaprecovered(PGR)canbedefinedas (cid:110) P −P (cid:111) weak→strong weak PGR = max 0, (10.11) P −P ceiling weak Thismetricmeasureshowmuchoftheperformancegapbetweentheceilingmodeland the weak model can be recovered by the weak-to-strong model. A PGR of 1 indicates that theweak-to-strongfine-tuningcancompletelyclosestheperformancegap,whereasaPGR of 0 indicates no improvement. In Burns et al. [2023a]’s work, it is shown that PGR can bearound0.8on22NLPclassificationtasks. Itshouldbenotedthat,whilethepotentialof weak-to-strongfine-tuningispromising,achievingsubstantialweak-to-stronggeneralization remainsachallenginggoalthatneedsfurtherinvestigation[Aschenbrenner,2024]. Fine-tuning LLMs with weak supervision is just one choice for using small models to improve large models. Although this section primarily focuses on fine-tuning LLMs, we also mention other methods here to give a more complete discussion (see Figure 10.5 for illustrationsofthesemethods). • Instead of using small models to generate synthetic data, it is also straightforward to incorporateknowledgedistillationlossbasedonthesemodels. Forexample,asimple lossfunctionthatmeasuresthedifferencebetweenthesmallandlargemodelscanbe definedas: Loss = KL(Prw(·|x)||Prs(·|x)) (10.12) kd θ Then, we can add this loss to the original loss of language modeling, and yield the followingtrainingobjective (cid:88) θ˜ = argmax logPrs(y|x)−λ·Loss (10.13) θ kd θ (x,y)∈D whereD isthesetofinputandoutputpairs,andλisthecoefficientoftheinterpolation. Thismethodcanbeemployedineitherthepre-trainingorfine-tuningphase. Wecan adjust λ to control how much the small model influences the training. For example, we can gradually decrease λ to make the training rely more on the original language modelinglossasthelargemodelbecomesmorecapable. • Another approach to involving small models in LLM pre-training and fine-tuning is to use them to do data selection or filtering. Given a sequence, we can compute the likelihoodorcross-entropyusingasmallmodel. Thesequantitiescanthenbeusedas criteriaforselectingorfilteringdata. Forexample, sequenceswithlowlikelihoodor highcross-entropymightbeexcludedfromthetrainingset,astheyarelessalignedwith thesmallmodel’slearneddistribution. Conversely,sequenceswithhighlikelihoodor lowcross-entropycanbeprioritized,ensuringthatthetrainingfocusesonmorerelevant orhigh-qualitydata. • Ensemblelearningisasimpleandeffectivewaytobuildastrongmodelbycombining 552 Chapter 10. Alignment LMLoss KDLoss SmallModel ComputeLoss&Train ComputeLoss&Train SmallModel yˆ y Input Predict LargeModel LargeModel Dataset Dataset =⇒ =⇒ x yˆ x y x x (a)Fine-tuningondatageneratedbyasmallmodel (b)Fine-tuningwithKDLossfromasmallmodel (weak-to-stronggeneralization) (weak-to-stronggeneralization) Dataset y ComputeLoss&Train SmallModel Data y CombinationModel Selection SmallModel1 SmallModel2 SmallModel3 LargeModel Dataset =⇒ x y x x x x (c)Dataselectionwithasmallmodel (d)Ensembleofmultiplesmallmodels IfStep1isnotsatisfactory,gotoStep2 Step1 Step2 (cheap) (expensive) y2 y1 SmallModel LargeModel x x (e)Cascading(atinferencetime) Figure 10.5: Illustrations of using small models to improve large models in LLMs. One approachinvolvesusingsmallermodelsforthefine-tuningorpre-trainingoflargermodels. This includes generating synthetic data (a), incorporating",
    "CombinationModel Selection SmallModel1 SmallModel2 SmallModel3 LargeModel Dataset =⇒ x y x x x x (c)Dataselectionwithasmallmodel (d)Ensembleofmultiplesmallmodels IfStep1isnotsatisfactory,gotoStep2 Step1 Step2 (cheap) (expensive) y2 y1 SmallModel LargeModel x x (e)Cascading(atinferencetime) Figure 10.5: Illustrations of using small models to improve large models in LLMs. One approachinvolvesusingsmallermodelsforthefine-tuningorpre-trainingoflargermodels. This includes generating synthetic data (a), incorporating auxiliary loss (b), and selecting appropriatedata(c). Anotherapproachinvolvescombiningsmallmodelsandlargemodels. Thisincludeslearningastrongmodelbyaggregatingmultiplesmallmodels(d),andcascading smallmodelswithlargemodels(e). multiple weak models. Applying this technique to LLMs is straightforward. We can aggregatedistributionspredictedbymultiplesmallmodelsorspecializedsubmodels, 10.3 Human Preference Alignment: RLHF 553 andderivethefinalpredictionfromtheaggregatedresults. Thisaggregationcanbedone usingmethodssuchasmajorityvoting,weightedaveraging,orstacking. • Small models can also be employed at inference time to improve overall efficiency. Supposewehavealargemodelthatisslowbutmoreaccurate,andasmallmodelthatis fastbutlessaccurate. Inmodelcascading,thesmallmodelfirstprocessestheinputdata, quicklygeneratingpreliminaryresults. Iftheseresultsmeetcertainpre-definedcriteria, theycanbedirectlyused. However,iftheinitialresultsarenotsufficientlygood,the inputisthenpassedtothelarger,moreaccuratemodeltoproduceabetterresult. This approachsignificantlyreducescomputationalcostsandlatency,asthesmallmodelcan effectivelyhandlemanyinputswithoutaccesstothelargemodel. 10.3 Human Preference Alignment: RLHF Sofarinthischapter,wehavefocusedonfine-tuningLLMsusinginput-outputpairedlabeled data. ThisapproachallowsustoadaptLLMsforinstruction-followingviasupervisedlearning. Inmanyapplications,however,LLMsarerequirednotonlytofollowinstructionsbutalsoto act in ways that are more aligned with human values and preferences. Consider a scenario whereauserasksanLLMhowtohackintoacomputersystem. IftheLLMisnotappropriately aligned,itmayrespondbyprovidingdetailsonhowtoperformthisillegalactivity. Instead, a more desirable response might be to advise the user against engaging in illegal activities andofferageneraloverviewoftheconsequencesofsuchactions. Thedifficultyinachieving thisisthattheethicalnuancesandcontextualconsiderationsrequiredforanLLMtorespond appropriately in such scenarios are not always straightforward to encode into a fine-tuning dataset. What’s even more challenging is that, often, humans themselves cannot precisely expresstheirownpreferences. Inthissection,wediscussanalternativeLLMfine-tuningmethod,calledreinforcement learning from human feedback or RLHF for short [Christiano et al., 2017; Stiennon et al., 2020]. The basic idea behind RLHF is that LLMs can learn from comparisons of model outputsusingrewardmodels(seeFigure10.6). Todothis,wecanrecruithumanexpertswho indicate their preferences between pairs of outputs generated by the LLM. This preference data is used to train a reward model that can predict the perceived quality of LLM outputs. Oncetrained,therewardmodelprovidesfeedbackbyassigningscorestonewoutputsthatthe LLMgeneratesinresponsetotheinputs. TheLLMusesthesescorestoupdateitsparameters throughreinforcementlearningalgorithms. Intherestofthissection,wewillfirstintroduce thebasicknowledgeofreinforcementlearningtofacilitatethediscussion,andthendiscuss methodsfortrainingrewardmodelsandaligningLLMswiththesemodels. 10.3.1 Basics of Reinforcement Learning Webeginbylookingatsomebasicconceptsofreinforcementlearning. Notethatthenotation usedhereslightlydiffersfromthatusedintheprevioussectionsandchaptersbecausewewant tomakeourdescriptionmoreconsistentwiththoseinthereinforcementlearningliterature. 554 Chapter 10. Alignment y Objective(MLE): Gold-standardPredictions LLM (none-hotdistributions) maxPr(y|x) PredictedTokenDistributions where (ntokendistributions) x:input x y:gold-standardoutput (a)Supervisedfine-tuning(maximizingthepredictionprobabilitygiventheinput) train Generatemultiple Humanpreferencedata RewardModel outputsviasampling yy 21 Objective(RLLossMinimization): Predictiony 1 minL(x,{y ,y },r) LLM 1 2 Predictiony 2 where L(·):lossfunction x r(·):rewardmodel (b)ReinforcementLearningfromHumanFeedback Figure 10.6: Supervised fine-tuning vs. reinforcement learning from human feedback. In supervisedfine-tuning,weoptimizetheLLMbymaximizingtheprobabilityoftheprediction giventheinput. Inreinforcementlearningfromhumanfeedback,wefirsttrainarewardmodel onhumanpreferencedata(oneachpairofpredictions,evaluatorsareaskedtochoosewhich onetheyprefer). Then,weusethisrewardmodeltosupervisetheLLMduringfine-tuning. Nevertheless,wewillshowhowthisnotationcorrespondstothelanguagemodelingnotation. Thereaderwhoisalreadyfamiliarwithreinforcementlearningtechniquesmayskiporskim thissubsection. A general reinforcement learning framework describes how an agent interacts with a dynamic environment. This interaction is modeled as a sequence of actions taken by the agentinresponsetothestateoftheenvironment. Ateachtimestep,theagentobservesthe current state, chooses an action based on its policy, performs the action, and then receives feedback from the environment in the form of a reward and a new state. This sequence of observe-act-receivefeedbackisrepeateduntiltheagentachievesitsgoal. Areinforcementlearningsysteminvolvesseveralcomponents: • Agent. Thisisthelearnerordecision-makerinreinforcementlearning. Inthecontextof LLMs,itcanbeseenastheLLMitself. • Environment. This includes everything external to the agent with which the agent interacts. But the environment in LLMs is less about a physical or virtual space and moreabouttheframeworkwithinwhichtheagent(e.g.,anLLM)receivesfeedbackand 10.3 Human Preference Alignment: RLHF 555 learns. • State(s). Astaterepresentsthecurrentsituationoftheenvironment. Givenasequence oftokensforlanguagemodeling,astateatatimestepcanbeviewedasthetokenswe observedsofar,thatis,thecontexttokenswetaketopredictthenexttoken. Forexample, wecandefine(x,y )asthestatewhenpredictingthenexttokenatthetimestept. <t • Action(a). Actionsrepresentpossibledecisionstheagentcanmake. Wecanseethem aspossiblepredictedtokensinthevocabulary. • Reward(R). Therewardisthefeedbackfromtheenvironmentthatevaluatesthesuccess ofanaction. Forexample,r(s,a,s′)denotestherewardtheagentreceivesfortakingthe actionaatthestatesandmovingtothenextstates′. Ifthestate-actionsequenceisgiven, wecandenotetherewardatthetimesteptasr =r(s ,a ,s ). Alsonotethatifthe t t t t+1 decision-makingprocessisdeterministic,wecanomits becauseitcanbedetermined t+1 bys anda . Insuchcases,wecanuser(s ,a )asshorthandforr(s ,a ,s ). t t t t t t t+1 • Policy (π). For an LLM, a policy is defined as the probability distribution over the tokensthattheLLMpredicts,giventheprecedingcontexttokens. Formally,thiscanbe expressedas π(a|s)=Pr(y |x,y ) (10.14) t <t whereacorrespondstothetokeny ,andscorrespondstothecontext(x,y ). Figure t <t 10.7 illustrates how an LLM can be treated as a",
    ",s ). t t t t t t t+1 • Policy (π). For an LLM, a policy is defined as the probability distribution over the tokensthattheLLMpredicts,giventheprecedingcontexttokens. Formally,thiscanbe expressedas π(a|s)=Pr(y |x,y ) (10.14) t <t whereacorrespondstothetokeny ,andscorrespondstothecontext(x,y ). Figure t <t 10.7 illustrates how an LLM can be treated as a policy in the reinforcement learning framework. • Value Function (V and Q). A state-value function (or value function, for short) assessestheexpecteddiscountedreturn(i.e.,accumulatedrewards)foranagentstarting fromaparticularstatesandfollowingaspecificpolicyπ. Itisdefinedas: V(s) = E (cid:104) r(s 0 ,a 0 ,s 1 )+γr(s 1 ,a 1 ,s 2 )+γ2r(s 2 ,a 2 ,s 3 )+··· (cid:12) (cid:12)s 0 =s,π (cid:105) = E (cid:104) r 0 +γr 1 +γ2r 2 +··· (cid:12) (cid:12)s 0 =s,π (cid:105) ∞ = E (cid:104)(cid:88) γtr t (cid:12) (cid:12)s 0 =s,π (cid:105) (10.15) t=0 where γ ∈[0,1] is the discount factor that adjusts the importance of future rewards, s =sindicatesthattheagentstartswiththestates,andtheexpectationEisperformed 0 overallpossibletrajectories(i.e.,state-actionsequences). Similarly,anaction-value function (or Q-value function) measures the expected return starting from a state s takinganactionaandthereafterfollowingapolicyπ,givenby ∞ Q(s,a) = E (cid:104)(cid:88) γtr t (cid:12) (cid:12)s 0 =s,a 0 =a,π (cid:105) (10.16) t=0 wherea =aindicatesthattheactiontakenattheinitialstateisa. 0 556 Chapter 10. Alignment Feedback Actionat ValueFunctions V(s )andQ(s ,a ) y1 y2 ... yt t t t Policy(LLM) x0 x1 ... xm y1 ... yt−1 RewardModel R(s ,a ) t t Statest(xandy<t) Figure 10.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a tokeny giventhemodelinputxandthepreviously-generatedtokensy . Thisprocesscan t <t beframedasareinforcementlearningproblem,wherey servesastheaction,(x,y )asthe t <t state,andthepredicteddistributionPr(y |x,y )asthepolicy. Oncey ispredicted,theLLM t <t t inputsboth(x,y )andy totherewardmodel,whichevaluateshoweffectivelythechosen <t t tokencontributestoachievingthedesiredtextualoutcome. Thisevaluationgeneratesreward scoreswhichareusedtocomputethevaluefunctionsV(s )andQ(s ,a ). Thesefunctions t t t thenprovidefeedbacktotheLLMandguidethepolicytraining. The goal of reinforcement learning is to learn a policy that maximizes the cumulative reward (or return) the agent receives over the long run. Given a state-action sequence τ ={(s ,a ),...,(s ,a )}1,thecumulativerewardoverthissequencecanbewrittenas 1 1 T T T (cid:88) R(τ) = r (10.17) t t=1 Theexpectationofthiscumulativerewardoveraspaceofstate-actionsequencesisgiven intheform (cid:104) (cid:12) (cid:105) J(θ) = E R(τ)(cid:12)π θ τ∼D (cid:88) = Pr (τ)R(τ) θ τ∈D T (cid:88) (cid:88) = Pr (τ) r (10.18) θ t τ∈D t=1 whereτ ∼Dindicatesthatτ isdrawnfromthestate-actionsequencespaceD,andthesubscript 1Weassumethestate-actionsequencebeginswiths 1anda 1,ratherthans 0anda 0,toalignwiththenotation commonlyusedinthischapter,wherethepredictionytypicallystartsfromy 1.Ofcourse,itisalsocommonto denoteastate-actionsequenceas{(s 0 ,a 0 ),...,(s T ,a T )}or{(s 0 ,a 0 ),...,(s T−1 ,a T−1 )}intheliterature. But thisvariationinnotationdoesnotaffectthediscussionofthemodelspresentedhere. 10.3 Human Preference Alignment: RLHF 557 θ indicatestheparametersofthepolicy. J(θ)isalsocalledtheperformancefunction. ThenthetrainingobjectiveistomaximizeJ(θ): θ˜ = argmaxJ(θ) (10.19) θ Now, we have a simple reinforcement learning approach: 1) we sample a number of state-actionsequences;then,2)weevaluateeachsequenceusingtheperformancefunction; then,3)weupdatethemodeltomaximizethisperformancefunction. IfwetakeEq. (10.18) and use gradient descent to optimize the policy, this approach would constitutes a form of policygradientmethods[Williams,1992]. NotethatinmanyNLPproblems,suchasmachinetranslation,rewardsaretypicallysparse. For instance, a reward is only received at the end of a complete sentence. This means that r =0forallt<T,andr isnon-zeroonlywhent=T. Ideally,onemightpreferfeedbackto t t beimmediateandfrequent(dense),andthusthetrainingofthepolicycanbeeasierandmore efficient. Whileseveralmethodshavebeenproposedtoaddresssparserewards,suchasreward shaping,wewillcontinueinourdiscussiontoassumeasparserewardsetup,wherethereward isavailableonlyuponcompletingtheprediction. The model described in Eqs. (10.17-10.19) establishes a basic form of reinforcement learning, and many variants and improvements of this model have been developed. Before showing those more sophisticated models, let us take a moment to interpret the objective functionJ(θ)fromtheperspectiveofpolicygradient. Ingradientdescent,weneedtocompute thegradientofJ(θ)withrespecttoθ: (cid:80) ∂J(θ) ∂ Pr (τ)R(τ) = τ∈D θ ∂θ ∂θ (cid:88) ∂Pr θ (τ) = R(τ) ∂θ τ∈D (cid:88) ∂Pr θ (τ)/∂θ =",
    "variants and improvements of this model have been developed. Before showing those more sophisticated models, let us take a moment to interpret the objective functionJ(θ)fromtheperspectiveofpolicygradient. Ingradientdescent,weneedtocompute thegradientofJ(θ)withrespecttoθ: (cid:80) ∂J(θ) ∂ Pr (τ)R(τ) = τ∈D θ ∂θ ∂θ (cid:88) ∂Pr θ (τ) = R(τ) ∂θ τ∈D (cid:88) ∂Pr θ (τ)/∂θ = Pr (τ) R(τ) θ Pr (τ) θ τ∈D (cid:88) ∂logPr θ (τ) = Pr (τ) R(τ) (10.20) θ ∂θ τ∈D Insomecases,wewillassumethateverysequenceinDisequallyprobable(i.e.,Pr (τ)= θ ∂logPr (τ) 1/|D|). InthiscasewecansimplifyEq. (10.20)andneedonlyconsidertheterms θ ∂θ andR(τ): ∂J(θ) 1 (cid:88) ∂logPr θ (τ) = R(τ) (10.21) ∂θ m ∂θ τ∈D OneadvantageofthisresultisthatR(τ)doesnotneedtobedifferentiable,whichmeansthat wecanuseanytypeofrewardfunctioninreinforcementlearning. 558 Chapter 10. Alignment Bytreatingthegenerationofthesequenceτ asaMarkovdecisionprocess,wecanfurther ∂logPr (τ) derive θ ,andobtain: ∂θ T ∂logPr θ (τ) ∂ (cid:89) = log π (a |s )Pr(s |s ,a ) θ t t t+1 t t ∂θ ∂θ t=1 T T ∂ (cid:88) ∂ (cid:88) = logπ (a |s )+ logPr(s |s ,a ) (10.22) θ t t t+1 t t ∂θ ∂θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) t=1 t=1 policy dynamics where the gradient is decomposed into two parts: the policy gradient and the dynamics gradient. Thepolicycomponent,logπ (a |s ),determinesthelog-probabilityoftakingaction θ t t a givenstates ,anditisparameterizedbyθ. Thedynamicscomponent,logPr(s |s ,a ), t t t+1 t t represents the log-probability of transitioning to state s from state s after taking action t+1 t a . In typical reinforcement learning settings, the dynamics are not directly influenced by t thepolicyparametersθ,andthus,theirderivativesareoftenzero. Inthiscase,therefore,Eq. (10.22)canbesimplifiedto: T ∂logPr θ (τ) ∂ (cid:88) = logπ (a |s ) (10.23) θ t t ∂θ ∂θ t=1 Inotherwords,weonlyconcentrateonoptimizingthepolicywithoutconcerningourselves withtheunderlyingdynamics. SubstitutingEq. (10.23)intoEq. (10.21),andexpandingR(τ),wethenobtain T T ∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:88) (cid:17) = logπ (a |s ) r (10.24) θ t t t ∂θ |D| ∂θ τ∈D t=1 t=1 Whilethispolicygradientapproachisstraightforward,itsuffersfromtheproblemthatthe varianceoftheestimatedgradientscanbeveryhigh,makingthelearningprocessnoisyand inefficient. Onereasonforthishighvarianceproblemisthatrewardscanvarygreatlyacross differentstepsorscenarios. Imaginethatinasequenceofactiondecisions,therewardmodel tendstoassignsmallrewardstogoodactions(e.g.,R =2)andlargepenaltiestopooractions t (e.g.,R =−50). Suchvaryingrewardscalesforgoodandpooractionscanresultinavery t lowtotalrewardfortheentiresequence,evenifitincludesgoodactions. One simple method for reducing the variance of the gradient is to set a baseline b and subtractitfrom (cid:80)T r ,resultingin (cid:80)T r −b.2 Here,thebaselinecanbeinterpretedasa t=1 t t=1 t referencepoint. Bycenteringtherewardsaroundthisbaseline,weremovesystematicbiasesin 2Infact,theuseofabaselinebdoesnotchangethevarianceofthetotalrewards (cid:80)T t=1 rt. However,itis importanttonotethatwhileintroducingabaselinedoesnotaltertheoverallvarianceoftherewards,ithelpsreduce thevarianceofthegradientestimates.Thisisbecausesubtractingthebaselinefromthetotalrewardseffectively reducesfluctuationsaroundtheirmean,whichmakesthegradientestimatesmorestable.Ingeneral,theoperation (cid:80)T t=1 rt−bcenterstherewardsaroundzero(e.g.,bisdefinedastheexpectedvalueof (cid:80)T t=1 rt),whichcanlead toreducedvarianceintheproduct (cid:80)T t=1 logπ θ (at|st)( (cid:80)T t=1 rt−b). 10.3 Human Preference Alignment: RLHF 559 therewardsignal,makingtheupdatesmorestableandlesssensitivetoextremefluctuationsin individualrewards. Thispolicygradientmodelwithabaselinecanbegivenby T T ∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:17)(cid:16)(cid:88) (cid:17) = logπ (a |s ) r −b θ t t t ∂θ |D| ∂θ τ∈D t=1 t=1 T T 1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:17)(cid:105) = logπ (a |s ) r −b θ t t k |D| ∂θ τ∈D t=1 k=1 T t−1 T 1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:88) (cid:17)(cid:105) = logπ (a |s ) r + r −b (10.25) θ t t k k |D| ∂θ τ∈D t=1 k=1 k=t Herewewrite (cid:80)T r asthesumoftwoterms (cid:80)t−1 r and (cid:80)T r todistinguishbetween k=1 k k=1 k k=t k therewardsaccruedbeforeandaftertheactionattimestept. NotethatinMarkovdecision processes,thefutureisindependentofthepastgiventhepresent. Therefore,theactiontaken attimesteptcannotinfluencetherewardsreceivedbeforet,orinotherwords,therewards priortotarealready“fixed”bythetimetheactionattischosen. Theterm (cid:80)t−1 r doesnot k=1 k contributetothegradientandcanbeomitted,leadingtoasimplifiedversionofEq. (10.25) T T ∂J(θ) 1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:17)(cid:105) = logπ (a |s ) r −b (10.26) θ t t k ∂θ |D| ∂θ τ∈D t=1 k=t Alsonotethatremoving (cid:80)T r canfurtherreducethevarianceofthegradient. k=t k There are many ways to define the baseline",
    "priortotarealready“fixed”bythetimetheactionattischosen. Theterm (cid:80)t−1 r doesnot k=1 k contributetothegradientandcanbeomitted,leadingtoasimplifiedversionofEq. (10.25) T T ∂J(θ) 1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:17)(cid:105) = logπ (a |s ) r −b (10.26) θ t t k ∂θ |D| ∂θ τ∈D t=1 k=t Alsonotethatremoving (cid:80)T r canfurtherreducethevarianceofthegradient. k=t k There are many ways to define the baseline b. Here we consider the value function of thestates ,thatis,theestimatedvalueofbeinginstates : V(s )=E(r +r +···+r ). t t t t t+1 T Hencewehave T (cid:88) A(s ,a ) = r −b t t k k=t T (cid:88) = r −V(s ) (10.27) k t k=t where (cid:80)T r representstheactualreturnreceived,andV(s )representstheexpectedreturn. k=t k t A(s ,a )(orA forshort)iscalledtheadvantageattimestept,whichquantifiestherelative t t t benefitoftheactiona comparedtotheexpectedvalueoffollowingthepolicyfromthestate t s onward. t ByusingtheadvantagefunctionA(s ,a ),thegradientofJ(θ)canbewrittenintheform t t T ∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:17) = logπ (a |s )A(s ,a ) (10.28) θ t t t t ∂θ |D| ∂θ τ∈D t=1 560 Chapter 10. Alignment Thisoptimizationobjectivecorrespondstotheadvantageactor-critic(A2C)methodin reinforcementlearning[Mnihetal.,2016]. Inthismethod,theactoraimsatlearningapolicy. It updates the policy parameters using Eq. (10.28) to help focus more on actions that are likely to improve performance. The critic, on the other hand, updates its estimation of the valuefunction,whichisusedtocalculatetheadvantagefunctionA(s ,a ),thusservingasthe t t evaluatorofthepolicybeinglearnedbytheactor. IntheA2Cmethod,A(s ,a )istypicallyexpressedasthedifferenceoftheaction-value t t functionQ(s ,a )andthestate-valuefunctionV(s ) t t t A(s ,a ) = Q(s ,a )−V(s ) (10.29) t t t t t Atfirstglance,thismodelmayseemchallengingtodevelopbecauseitrequirestwoseparatesub- modelstocalculateQ(s ,a )andV(s )respectively. Fortunately,consideringthatQ(s ,a ) t t t t t canbedefinedasthereturnr +V(s ),wecanrewriteEq. (10.29)as t t+1 A(s ,a ) = r +V(s )−V(s ) (10.30) t t t t+1 t oralternatively,introducethediscountfactorγ toobtainamoregeneralform A(s ,a ) = r +γV(s )−V(s ) (10.31) t t t t+1 t A(s ,a )=r +γV(s )−V(s )isalsocalledthetemporaldifference(TD)error. What t t t t+1 t weneedistotrainacriticnetworkforthevaluefunctionV(s ),andthenuseittocomputethe t advantagefunction3. Uptothispoint,wehavespentconsiderablespacediscussingthebasicsofreinforcement learning,especiallyonhowtoderivetheoptimizationobjectivefortheA2Cmethod. However, reinforcement learning is a vast field, and many technical details cannot be covered here. Theinterestedreadercanrefertoreinforcementlearningbooksformoredetails[Suttonand Barto, 2018; Szepesvári, 2010]. Nevertheless, we now have the necessary knowledge to furtherdiscussRLHF.Inthesubsequentsubsections,wewillreturntothediscussiononLLM alignment,demonstratinghowtousetheA2Cmethodforaligningwithhumanpreferences. 10.3.2 Training Reward Models Wehaveshownthatrewardmodelsplayaveryimportantroleinthegeneralreinforcement learningframeworkandformthebasisforcomputingvaluefunctions. Wenowconsiderthe problemoftrainingtheserewardmodels. InRLHF,arewardmodelisaneuralnetworkthatmapsapairofinputandoutputtoken 3Thetraininglossforthevaluenetwork(orcriticnetwork)inA2Cisgenerallyformulatedasthemeansquared errorbetweenthecomputedreturnrt+γV(s t+1 )andthepredictedstatevalueV(st). Supposethatthevalue networkisparameterizedbyω.Thelossfunctionisgivenby 1 (cid:88)(cid:0) (cid:1)2 Lv(ω) = rt+γVω(s t+1 )−Vω(st) (10.32) M whereM isthenumberoftrainingsamples,forexample,forasequenceofT tokens,wecansetM =T. 10.3 Human Preference Alignment: RLHF 561 Reward(Scalar) Wr LinearMap Representation hx0 hx1 hx2 ··· hxm hy1 hy2 ··· h last atEachPosition TransformerDecoder(LLM) x 0 x 1 x 2 ··· xm y 1 y 2 ··· yn (LastToken⟨EOS⟩) Figure10.8: ArchitectureoftherewardmodelbasedonTransformer. Themaincomponentof thismodelisstillanLLM.WeusetheTransformerdecoderasthesequencerepresentation model. Weextracttherepresentationofthelastpositionofthedecoderastherepresentation of the entire sequence [x,y]. We then map this representation to a scalar through a linear transformation,whichservesastherewardscorefor[x,y]. sequencestoascalar. Givenaninputxandanoutputy,therewardcanbeexpressedas r = Reward(x,y) (10.33) whereReward(·)istherewardmodel. rcanbeinterpretedasameasureofhowwelltheoutput yalignswiththedesiredbehaviorgiventheinputx. Asdiscussedintheprevioussubsection, bothxandyareassumedtocompletetexts. Thismeansthattherewardmodelevaluatesthe relationshipbetweeninputsandoutputsthatprovidefullsemanticcontent. Forexample,when applyingtherewardmodel,itassignsavalueof0(oranotherpredeterminedvalue)ateach positiontintheoutputsequencey=y ...y . Onlyatthefinalposition,whent=n,doesthe 1 n rewardmodelgeneratetheactualrewardscore. Tokeepthenotationuncluttered,wewilluse r(x,y)todenotetherewardmodelReward(x,y)fromhereon. Therearemanywaystoimplementtherewardmodel. Onesimpleapproachistobuildthe rewardmodelbasedonapre-trainedLLM.Morespecifically,wecanconcatenatexandyto formasingletokensequenceseq =[x,y]. Werunapre-trainedLLMonthissequence,as x,y usual,andateachposition,weobtainarepresentationfromthetop-mostTransformerlayer. Then,wetaketherepresentationatthelastposition(denotedbyh )andmapittoascalar last vialineartransformation: r(x,y) = h W (10.34) last r where h is a d-dimensional vector, and Wr is a d×1 linear mapping matrix. This last architectureoftherewardmodelisillustratedinFigure10.8. Totraintherewardmodel,thefirststepistocollecthumanfeedbackonasetofgenerated outputs. Givenaninputx,weusetheLLMtoproducemultiplecandidateoutputs{y ,...,y }. 1 N 562 Chapter 10. Alignment Humanfeedbackcanbeobtainedinseveralways: • PairwiseComparison(PairwiseRanking). Giventwodifferentoutputs,humanexperts selectwhichoneisbetter. • Rating. Human experts provide a score or rating to each output. This score is often a continuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars, or 1-10",
    "Totraintherewardmodel,thefirststepistocollecthumanfeedbackonasetofgenerated outputs. Givenaninputx,weusetheLLMtoproducemultiplecandidateoutputs{y ,...,y }. 1 N 562 Chapter 10. Alignment Humanfeedbackcanbeobtainedinseveralways: • PairwiseComparison(PairwiseRanking). Giventwodifferentoutputs,humanexperts selectwhichoneisbetter. • Rating. Human experts provide a score or rating to each output. This score is often a continuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars, or 1-10 points). In some cases, the rating might be binary, indicating a “yes/no” or “positive/negative”preference. • ListwiseRanking. Humanexpertsareaskedtorankororderthegivensetofpossible outputs. Here we consider pairwise comparison feedback as it is one of the simplest and most common forms of human feedback used in RLHF. In this setting, each time, two outputs (y ,y )arerandomlydrawnfromthecandidatepool{y ,...,y }. Humanexpertsarethen a b 1 N presented with these pairs and asked to decide which output they prefer based on specific criteria, such as clarity, relevance, and accuracy. The human feedback can be encoded as a binarylabel,y ≻y forapreferencefory ,andy ≻y forapreferencefory . a b a b a b One simple and widely used model for describing such pairwise comparisons is the Bradley-Terrymodel[BradleyandTerry,1952]. Itisaprobabilisticmodelthatestimatesthe probabilitythatoneitemispreferredoveranother. Adaptingthismodeltothenotationused here,wecanwritetheprobabilitythaty ispreferredovery intheform a b er(x,ya) Pr(y ≻y |x) = a b er(x,ya)+er(x,y b ) er(x,ya)−r(x,y b ) = er(x,ya)−r(x,y b )+1 = Sigmoid(r(x,y )−r(x,y )) (10.35) a b Whentrainingtherewardmodel,wewanttomaximizethispreferenceprobability. Aloss functionbasedontheBradley-Terrymodelisgivenby L (ϕ) = −E (cid:2) logPr (y ≻y |x) (cid:3) (10.36) r (x,ya,y b )∼Dr ϕ a b where(x,y ,y )isdrawnfromahuman-annotateddatasetD consistingofpreferencepairs a b r ofoutputsandtheircorrespondinginputs. ϕrepresentstheparametersoftherewardmodel, whichincludesboththeparametersoftheTransformerdecoderandthelinearmappingmatrix W . In practice, assuming (x,y ,y ) is uniformly sampled from D , we can replace the r a b r expectationwithasummation 1 (cid:88) L (ϕ) = − logPr (y ≻y |x) (10.37) r ϕ a b |D | r (x,ya,y b )∈Dr Thegoaloftrainingtherewardmodelistofindtheoptimalparametersϕˆ thatminimize 10.3 Human Preference Alignment: RLHF 563 thislossfunction,givenby ϕˆ = argminL (ϕ) (10.38) r ϕ SincetherewardmodelitselfisalsoanLLM,wecandirectlyreusetheTransformertraining proceduretooptimizetherewardmodel. ThedifferencefromtrainingastandardLLMisthat weonlyneedtoreplacethecross-entropylosswiththepairwisecomparisonlossasdescribed inEq. (10.37). Afterthetrainingoftherewardmodel,wecanapplythetrainedrewardmodel r (·)tosupervisethetargetLLMforalignment. ϕˆ Itisworthnotingthatalthoughwetraintherewardmodeltoperformpairwiseranking, weapplyittoscoreeachinput-outputpairindependentlyduringthealignmentprocess. The pairwise ranking objective ensures that the reward model is sensitive to subtle differences betweenoutputs,butwerelyonthecontinuousscoresproducedbytherewardmodeltoguide theoptimizationoftheLLM.Anadvantageofthisapproachisthatwecanchoosefromor combinevariousrankinglossfunctions,andstillapplytheresultingrewardmodelsinthesame way as we have done in this subsection. This consistency ensures a unified framework for aligningtheLLM,regardlessofthespecificrankinglossusedduringrewardmodeltraining. 10.3.3 Training LLMs Havingobtainedtherewardmodel,wethentrainthepolicy(i.e.,theLLM)viatheA2Cmethod. RecallfromSection10.3.1thatastate-actionsequenceortrajectoryτ canbeevaluatedbythe utilityfunction T (cid:88) U(τ;θ) = logπ (a |s )A(s ,a ) (10.39) θ t t t t t=1 where A(s ,a ) is the advantage of taking the action a given the state s . An estimate of t t t t A(s ,a )isdefinedastheTDerrorr +γV(s )−V(s ),wherethevaluefunctionV(s )is t t t t+1 t t trainedwiththerewardmodel. Giventhisutilityfunction,theA2C-basedlossfunctioncanbewrittenintheform L(θ) = −E (cid:2) U(τ;θ) (cid:3) τ∼D T = −E (cid:2)(cid:88) logπ (a |s )A(s ,a ) (cid:3) (10.40) τ∼D θ t t t t t=1 whereD isaspaceofstate-actionsequences. Asusual, thegoaloftrainingthepolicyisto minimizethislossfunction θ˜ = argminL(θ) (10.41) θ If we map the problem back to the language modeling problem and adopt the notation 564 Chapter 10. Alignment fromLLMs,thelossfunctioncanbewrittenas: L(θ) = −E (cid:2) U(x,y;θ) (cid:3) (10.42) (x,y)∼D where T (cid:88) U(x,y;θ) = logπ (y |x,y )A(x,y ,y ) (10.43) θ t <t <t t t=1 Hereπ (y |x,y )=Pr (y |x,y )istheLLMparameterizedbyθ. θ t <t θ t <t In general, we do not have a human annotated input-output dataset D in RLHF, but a datasetcontaininginputsonly. Theoutputs,inthiscase,aretypicallythepredictionsmadeby theLLM.Thelossfunctionisthendefinedas L(θ) =",
    "(cid:88) U(x,y;θ) = logπ (y |x,y )A(x,y ,y ) (10.43) θ t <t <t t t=1 Hereπ (y |x,y )=Pr (y |x,y )istheLLMparameterizedbyθ. θ t <t θ t <t In general, we do not have a human annotated input-output dataset D in RLHF, but a datasetcontaininginputsonly. Theoutputs,inthiscase,aretypicallythepredictionsmadeby theLLM.Thelossfunctionisthendefinedas L(θ) = −E E (cid:2) U(x,y;θ) (cid:3) (10.44) x∼D y∼π (·|x) θ whereD denotestheinput-onlydataset,andy∼π (·|x)denotesthattheoutputyissampled θ bythepolicyπ (·|x). θ The above formulation provides a basic form of the A2C method for LLMs. Improved versionsofthismodelaremorecommonlyusedinRLHF.Inthefollowingdiscussion,wewill stillusethereinforcementlearningnotationtosimplifythepresentationandwillgetbackthe languagemodelingnotationlater. Onecommonimprovementofpolicygradientmethodsistouseimportancesamplingto refinetheestimationofU(τ;θ). Thiscanbewrittenas T (cid:88) π θ (a t |s t ) U(τ;θ) = A(s ,a ) (10.45) t t π (a |s ) θ t t t=1 ref Here we replace the log-probability logπ (a |s ) with the ratio π θ (at|st) . θ denotes the θ t t π θref (at|st) ref parametersofthepreviouspolicy(suchasaninitialmodelfromwhichwestartthetraining). So π θ (at|st) , alsocalledtheratiofunction, canbeinterpretedasthelog-probabilityratio π θref (at|st) betweenthecurrentpolicyπ andthepreviouspolicyπ (callitthereferencepolicy). By θ θ ref using the ratio function we reweight the observed rewards based on the likelihood of the actionsunderthecurrentpolicyversusthereferencepolicy. When π θ (at|st) >1,theaction π θref (at|st) a ismorefavoredbythecurrentpolicycomparedtothereferencepolicy. Bycontrast,when t π θ (at|st) <1,theactiona islessfavoredbythecurrentpolicy4. π θref (at|st) t A problem with the model presented in Eq. (10.47) (as well as in Eq. (10.39)) is that thevarianceinthegradientestimatesisoftenhigh,makingthelearningprocessunstable. To 4Consideramoregeneralcasewherewewishtoevaluatethepolicyusingitsexpectedreward(alsoseeEq. (10.18)) (cid:104) (cid:105) J(θ) = E τ∼πθ R(τ) (10.46) Hereτ ∼π meansthatthesequenceτ isgeneratedbythepolicyπ .Alternatively,wecanwriteJ(θ)inanother θ θ 10.3 Human Preference Alignment: RLHF 565 mitigatethisissue,techniquessuchasclippingareoftenemployedtoboundtheimportance weightsandpreventlargeupdates. Aclippedversionoftheutilityfunction(alsocalledthe clippedsurrogateobjectivefunction)isgivenby T (cid:88) (cid:16) π θ (a t |s t ) (cid:17) U (τ;θ) = Clip A(s ,a ) (10.49) clip t t π (a |s ) θ t t t=1 ref (cid:16) π θ (a t |s t ) (cid:17) (cid:16) π θ (a t |s t ) (cid:0) π θ (a t |s t ) (cid:1) (cid:17) Clip = min ,bound ,1−ϵ,1+ϵ (10.50) π (a |s ) π (a |s ) π (a |s ) θ t t θ t t θ t t ref ref ref Here the function bound( π θ (at|st) ,1−ϵ,1+ϵ) constrains the ratio function to the range π θref (at|st) [1−ϵ,1+ϵ]. Afurtherimprovementtotheabovemodelistoconsidertrustregionsinoptimization [Schulman et al., 2015]. In reinforcement learning, a large update to the policy can lead to instability,wheretheagentmaystartperformingworseafteranupdate. Areasonableideaisto optimizethemodelinthetrustregion,whichreferstoaregionaroundthecurrentparameter estimatewherethemodeliswell-behaved. Oneapproachtoincorporatingtrustregionsisto imposeaconstraintonthesizeofthepolicyupdate,ensuringthatthecurrentpolicydoesnot deviatetoosignificantlyfromthereferencepolicy. Thiscanbeachievedbyaddingapenalty basedonsomeformofdivergencebetweenthecurrentandreferencepoliciestotheobjective function. Asimpleformofsuchapenaltyisgivenbythedifferenceinthelog-probabilityof thesequenceτ underthecurrentpolicyversusthereferencepolicy: Penalty = logπ (τ)−logπ (τ) (10.51) θ θ ref form (cid:104) Pr (τ) (cid:105) J(θ) = E τ∼πθref Pr θr θ ef (τ) R(τ) (10.47) It is not difficult to find that the right-hand sides of these equations are essentially the same since (cid:104) (cid:105) (cid:104) (cid:105) E τ∼πθref P P rθ r r θ e ( f τ (τ ) ) R(τ) = (cid:80) τ Pr θref (τ) P P rθ r r θ e ( f τ (τ ) ) R(τ)= (cid:80) τ Pr θ (τ)R(τ)=E τ∼πθ R(τ) Notethatthisequivalenceholdsonlywhentheexpectationisperformedovertheentiresequencespace. In practice,however,weoftenonlysamplearelativelysmallnumberofsequencesusingapolicyinpolicylearning. Asaresult,thesamplingmethoditselfmatters.Eq.(10.47)offersaninterestingmannertoseparatethesampling andrewardcomputationprocesses: wefirstuseabaselinepolicy(withθ )tosampleanumberofsequences, ref andthenusethetargetpolicy(withθ)tocomputetheexpectedreward.Inthisway,weseparatethepolicyused forcollectingthedata,andthepolicyusedforcomputingthegradient.Thisapproachavoidstheneedtodirectly samplefromthepolicyweareevaluating,whichcanbebeneficialincaseswheregeneratingsequencesfrom (cid:104) (cid:105) thetargetpolicyisexpensiveordifficult.Inreinforcementlearning,E τ∼πθref P P rθ r r θ e ( f τ (τ ) ) R(τ) isoftencalleda surrogateobjective. (cid:104) (cid:105) Eq. (10.47)canalsobeinterpretedfromapolicygradientperspective. ForE τ∼πθref P P rθ r r θ e (",
    "Pr θ (τ)R(τ)=E τ∼πθ R(τ) Notethatthisequivalenceholdsonlywhentheexpectationisperformedovertheentiresequencespace. In practice,however,weoftenonlysamplearelativelysmallnumberofsequencesusingapolicyinpolicylearning. Asaresult,thesamplingmethoditselfmatters.Eq.(10.47)offersaninterestingmannertoseparatethesampling andrewardcomputationprocesses: wefirstuseabaselinepolicy(withθ )tosampleanumberofsequences, ref andthenusethetargetpolicy(withθ)tocomputetheexpectedreward.Inthisway,weseparatethepolicyused forcollectingthedata,andthepolicyusedforcomputingthegradient.Thisapproachavoidstheneedtodirectly samplefromthepolicyweareevaluating,whichcanbebeneficialincaseswheregeneratingsequencesfrom (cid:104) (cid:105) thetargetpolicyisexpensiveordifficult.Inreinforcementlearning,E τ∼πθref P P rθ r r θ e ( f τ (τ ) ) R(τ) isoftencalleda surrogateobjective. (cid:104) (cid:105) Eq. (10.47)canalsobeinterpretedfromapolicygradientperspective. ForE τ∼πθref P P rθ r r θ e ( f τ (τ ) ) R(τ) ,the gradientatθ=θ isgivenby ref ∂ ∂ θ E τ∼πθref (cid:104) P P r θ r r θ e ( f τ (τ ) ) R(τ) (cid:105)(cid:12) (cid:12) (cid:12) θ=θref = E τ∼πθref (cid:104)∂Pr θ (τ ∂ ) θ | θ=θrefR(τ) (cid:105) (10.48) Theright-handsideisastandardformusedinpolicygradientmethods,meaningthatwecomputethedirection oftheparameterupdateatthepointθ=θ ontheoptimizationsurface. ref 566 Chapter 10. Alignment Inpractice,thispenaltycanbeapproximatedbyconsideringonlythepolicyprobabilitiesand ignoringthedynamics. Thisgives T T (cid:88) (cid:88) Penalty = logπ (a |s )− logπ (a |s ) (10.52) θ t t θ t t ref t=1 t=1 By including this penalty in the optimization objective, we encourage the current policy to remainclosetothereferencepolicy,limitingverylargeupdatesthatcoulddestabilizelearning. Wecanincorporatethispenaltyintotheclippedsurrogateobjectivefunction,andobtain U (τ;θ) = U (τ;θ)−βPenalty (10.53) ppo-clip clip whereβ istheweightofthepenalty. Thistrainingmethodiscalledproximalpolicyoptimiza- tion(PPO),whichisoneofthemostpopularreinforcementlearningmethodsusedinLLMs andmanyotherfields[Schulmanetal.,2017]. NowwecanwritetheobjectiveoftrainingLLMsintheformofPPO. U(x,y;θ) = U (x,y;θ)−βPenalty (10.54) ppo-clip where T (cid:88) (cid:16) π θ (y t |x,y <t ) (cid:17) U (x,y;θ) = Clip A(x,y ,y ) (10.55) ppo-clip <t t π (y |x,y ) θ t <t t=1 ref Penalty = logPr (y|x)−logPr (y|x) θ θ ref T T (cid:88) (cid:88) = logPr (y |x,y )− logPr (y |x,y ) (10.56) θ t <t θ t <t ref t=1 t=1 Althoughthenotationhereappearsabittedious,theideaofPPOissimple: wedevelopan objectivebycombiningtheclippedlikelihoodratioofthetargetandreferencepolicieswithan advantagefunction,andthenimposeapenaltythatensurespolicyupdatesarenottoolarge. ThePPO-basedRLHFisillustratedinFigure10.9. To summarize, implementing RLHF requires building four models, all based on the Transformerdecoderarchitecture. • RewardModel(r (·)whereϕdenotestheparameters). Therewardmodellearnsfrom ϕ human preference data to predict the reward for each pair of input and output token sequences. ItisaTransformerdecoderfollowedbyalinearlayerthatmapsasequence (theconcatenationoftheinputandoutput)toareal-valuedrewardscore. • ValueModelorValueFunction(V (·)whereω denotestheparameters). Thevalue ω function receives reward scores from the reward model and is trained to predict the expectedsumofrewardsthatcanbeobtainedstartingfromastate. Itisgenerallybased onthesamearchitectureastherewardmodel. 10.3 Human Preference Alignment: RLHF 567 HumanpreferencedataD ={(x,y ,y )} r a b ToLearn Minimizingthelossbasedon RewardModel RewardModel theBradley-Terrymodel r (x,y) ϕ Training m ϕ in− |D 1 r| (x,ya, (cid:80) yb)∈Dr logσ(r ϕ (x,ya)−r ϕ (x,y b )) Fixed ToLearn ToLearn RefModel LLMPolicy ValueFunction Pr (y |x,y ) Pr (y |x,y ) V (x,y ) θold t <t θ t <t ω <t Evaluatethestate-actionpairusingtheadvantage functionortheTDerror(basedonthereward modelandthevaluefunction) x1 x2 ··· y1 y2 ··· yt Actionyt (sampledwithPrθold ) LLMPolicy PolicyTraining x0 x1 ··· xm y1 ··· yt−1 State(x,y<t) LLMPolicy ValueFunction MinimizingtheclippedPPOloss MinimizingtheMSEbetweenthe withthepenalty computedreturnandthepredicted statevalue min−(cid:80) (cid:80)T min 1 (cid:80) (cid:80)T θ x∈D,y∼Prθold (·|x) t=1 ω M x∈D t=1 (cid:104) Clip (cid:0) P P rθ r o θ ld (y ( t y | t x | , x y , < y< t) t) (cid:1) At− (cid:2) rt+γVω(x,y<t+1)−Vω(x,y<t) (cid:3)2 (cid:0) β· logPr θ (yt|x,y<t)− (cid:1)(cid:105) logPr θold (yt|x,y<t) ∗∗rt=r(x,y<t+1)denotestherewardreceivedasstept. ∗∗Atdenotestheadvantageatstept,andcanbedefinedasrt+γVω(x,y<t+1)−Vω(x,y<t) Input-onlydataD={x} Figure10.9: IllustrationofRLHF.Thefirststepistocollecthumanpreferencedataandtrain therewardmodelusingthisdata. Oncetherewardmodelisoptimized,alongwiththereference model,weproceedtotrainboththepolicyandthevaluefunction. Ateachpredictionstep,we computethesumofthePPO-basedlossandupdatetheparametersofthepolicy. Thisrequires accesstotherewardmodel,thereferencemodel,andthevaluefunctionathand. Atthesame time,weupdatetheparametersofthevaluefunctionbyminimizingtheMSEloss. 568 Chapter 10. Alignment • ReferenceModel(π (·)=Pr (·)whereθ denotestheparameters). Thereference θ θ ref ref ref modelisthebaselineLLMthatservesasastartingpointforpolicytraining. InRLHF,it representsthepreviousversionofthemodeloramodeltrainedwithouthumanfeedback. It is used to perform sampling over the space of outputs and contribute to the loss computationforpolicytraining. • TargetModelorPolicy(π (·)=Pr (·)whereθ denotestheparameters). Thispolicy θ θ governshowtheLLMdecidesthemostappropriatenexttokengivenitscontext. Itis trainedunderthesupervisionofboththerewardmodelandthevaluemodel. Inpractice,thesemodelsneedtobetrainedinacertainorder. First,weneedtoinitialize them using some other models. For example,",
    "ReferenceModel(π (·)=Pr (·)whereθ denotestheparameters). Thereference θ θ ref ref ref modelisthebaselineLLMthatservesasastartingpointforpolicytraining. InRLHF,it representsthepreviousversionofthemodeloramodeltrainedwithouthumanfeedback. It is used to perform sampling over the space of outputs and contribute to the loss computationforpolicytraining. • TargetModelorPolicy(π (·)=Pr (·)whereθ denotestheparameters). Thispolicy θ θ governshowtheLLMdecidesthemostappropriatenexttokengivenitscontext. Itis trainedunderthesupervisionofboththerewardmodelandthevaluemodel. Inpractice,thesemodelsneedtobetrainedinacertainorder. First,weneedtoinitialize them using some other models. For example, the reward model and the value model can be initialized with a pre-trained LLM, while the reference model and the target model can beinitializedwithamodelthathasbeeninstructionfine-tuned. Notethat,atthispoint,the referencemodelisreadyforuseandwillnotbefurtherupdated. Second,weneedtocollect humanpreferencedataandtraintherewardmodelonthisdata. Third,boththevaluemodel andthepolicyaretrainedsimultaneouslyusingtherewardmodel. Ateachpositioninanoutput tokensequence,weupdatethevaluemodelbyminimizingtheMSEerrorofvalueprediction, andthepolicyisupdatedbyminimizingthePPOloss. 10.4 Improved Human Preference Alignment In the previous section, we reviewed the basic concepts of reinforcement learning and the generalframeworkofRLHF.Inthissection,wewilldiscusssomerefinementsofRLHFand alternativemethodstoachievehumanpreferencealignment. 10.4.1 Better Reward Modeling InSection10.3.2,wehighlightedthetaskoflearningfromhumanpreferencesaswellasthe use of pairwise ranking loss for training reward models. Here we consider more methods forrewardmodeling. Ourdiscussionwillberelativelygeneral,andsincetherewardmodel iswidelyusedinmanyreinforcementlearningproblems, itwillbeeasyforustoapplythe methodsdiscussedheretoRLHFandrelatedapplications. 1. Supervision Signals The training of reward models can broadly be seen as a ranking problem, where the model learns to assign scores to outputs so that their order reflects the preferences indicated by humans. Thereareseveralmethodstotrainarewardmodelfromtheperspectiveofranking. One approach is to extend pairwise ranking to listwise ranking. For each sample in a dataset, we can use the LLM to generate multiple outputs, and ask human experts to order theseoutputs. Forexample,givenasetoffouroutputs{y ,y ,y ,y },onepossibleorderof 1 2 3 4 themcanbey ≻y ≻y ≻y . Averysimplemethodtomodeltheorderingofthelistis 2 3 1 4 toaccumulatethepairwisecomparisonloss. Forexample,wecandefinethelistwiselossby 10.4 Improved Human Preference Alignment 569 accumulatingthelossoverallpairsofoutputs: (cid:104) 1 (cid:88) (cid:105) L = −E logPr(y ≻y |x) (10.57) list (x,Y)∼Dr N(N−1) a b ya∈Y,y b ∈Y ya̸=y b whereY isalistofoutputs,andN isthenumberofoutputsinthelist. Pr(y ≻y |x)canbe a b definedusingtheBradley-Terrymodel,thatis,Pr(y ≻y |x)=Sigmoid(r(x,y )−r(x,y )). a b a b HereweomittheϕsuperscriptonthePr(·)tokeepthenotationuncluttered. AnextensiontotheBradley-Terrymodelforlistwiserankingcouldinvolvearankingmech- anismthattakesintoaccounttheentirelistofoutputsratherthanjustpairwisecomparisons. OnesuchmodelisthePlackett-Lucemodel,whichgeneralizestheBradley-Terrymodelto handlemultipleitemsinaranking[Plackett,1975]. InthePlackett-Lucemodel,foreachitem inalist,wedefinea“worth”forthisitemthatreflectsitsrelativestrengthofbeingchosenover otheritems. Fortherewardmodelingproblemhere,theworthofyinthelistY canbedefined as α(y) = exp(r(x,y)) (10.58) ThentheprobabilityofselectingyfromY isgivenby α(y) Pr(yisselected|x,Y) = (cid:80) α(y′) y′∈Y exp(r(x,y)) = (10.59) (cid:80) exp(r(x,y′)) y′∈Y Suppose Y˚ is an ordered list y ≻y ≻···≻y . The overall log-probability of this j1 j2 jN ordered list can be defined as the sum of the conditional log-probabilities at each stage of selection,givenby logPr(Y˚|x) = logPr(y ≻y ≻···≻y |x) j1 j2 jN = logPr(y |x,{y ,y ,...,y })+ j1 j1 j2 jN logPr(y |x,{y ,...,y })+ j2 j2 jN ···+ logPr(y |x,{y }) jN jN N (cid:88) = logPr(y |x,Y˚ ) (10.60) j k ≥k k=1 whereY˚ representsthesubsetofthelistofoutputsthatremainunselectedatthek-thstage, ≥k i.e., Y˚ = {y ,...,y }. Given the log-probability logPr(Y˚|x), we can define the loss ≥k j k jN functionbasedonthePlackett-Lucemodelby L = −E (cid:2) logPr(Y˚|x) (cid:3) (10.61) pl (x,Y˚)∼Dr 570 Chapter 10. Alignment There are also many other pairwise and listwise methods for modeling rankings, such as RankNet [Burges et al., 2005] and ListNet [Cao et al., 2007]. All these methods can be categorizedintoalargefamilyoflearning-to-rankapproaches,andmostofthemareapplicable totheproblemofmodelinghumanpreferences. However,discussingthesemethodsisbeyond thescopeofthischapter. Interestedreaderscanrefertobooksonthistopicformoredetails [Liu,2009;Li,2011]. Inadditiontopairwiseandlistwiseranking,usingpointwisemethodstotrainrewardmodels offers an alternative way to capture human preferences. Unlike methods that focus on the relativerankingsbetweendifferentoutputs,pointwisemethodstreateachoutputindependently. Forexample,humanexpertsmightassignascoretoanindividualoutput,suchasaratingona five-pointscale. Theobjectiveistoadjusttherewardmodelsothatitsoutputsalignwiththese scores. Asimplewaytoachievepointwisetrainingisthroughregressiontechniqueswhere therewardofeachoutputistreatedasatargetvariable. Letφ(x,y)bethescoreassignedtoy givenxbyhumans. Pointwiserewardmodelscanbetrainedbyminimizingalossfunction, oftenbasedonmeansquarederrororotherregressionlosses,betweenthepredictedreward r(x,y)andtheactualhumanfeedbackφ(x,y). Forexample,thelossfunctioncouldbe L = −E(cid:2) φ(x,y)−r(x,y) (cid:3)2 (10.62) point Whilepointwisemethodsareconceptuallysimplerandcandirectlyguidetherewardmodel topredictscores,theymightnotalwaysbethebestchoiceinRLHF.Aproblemisthatthese methodsmaystrugglewithhighvarianceinhumanfeedback,especiallywhendifferentexperts provide inconsistent scores for similar outputs. Because they focus on fitting to absolute scores rather than relative differences, inconsistencies in scoring can lead to poor model performance. Moreover, fitting to specific scored outputs",
    "therewardofeachoutputistreatedasatargetvariable. Letφ(x,y)bethescoreassignedtoy givenxbyhumans. Pointwiserewardmodelscanbetrainedbyminimizingalossfunction, oftenbasedonmeansquarederrororotherregressionlosses,betweenthepredictedreward r(x,y)andtheactualhumanfeedbackφ(x,y). Forexample,thelossfunctioncouldbe L = −E(cid:2) φ(x,y)−r(x,y) (cid:3)2 (10.62) point Whilepointwisemethodsareconceptuallysimplerandcandirectlyguidetherewardmodel topredictscores,theymightnotalwaysbethebestchoiceinRLHF.Aproblemisthatthese methodsmaystrugglewithhighvarianceinhumanfeedback,especiallywhendifferentexperts provide inconsistent scores for similar outputs. Because they focus on fitting to absolute scores rather than relative differences, inconsistencies in scoring can lead to poor model performance. Moreover, fitting to specific scored outputs might discourage generalization, particularlygiventhattrainingdataisoftenverylimitedinRLHF.Incontrast,methodsthat considerrelativepreferencescanpromotethelearningofmoregeneralizedpatternsofsuccess andfailure. Nevertheless,therearescenarioswherepointwisemethodsmightstillbesuitable. For example, in tasks where training data is abundant and the costs of obtaining accurate, consistentannotationsarelow,pointwisemethodscanproveeffective. Infact,tomakethesupervisionsignalfortrainingtherewardmodelmorerobust,wecan alsointroduceadditionalregularizationtermsintotraining. Forexample,ifweconsiderthe firsttermU (x,y;θ)inEq. (10.54)asatypeofgeneralizedreward,thenthesecondterm ppo-clip (i.e.,thepenaltyterm)canbeviewedasaformofregularizationfortherewardmodel,except thatherethegoalistotrainthepolicyratherthantherewardmodel. Anotherexampleisthat Eisensteinetal.[2023]developaregularizationtermbasedonthesquaredsumofrewards, andaddittothepairwisecomparisonlossinRLHF: L = L +(−E (cid:2) r(x,y )+r(x,y ) (cid:3)2 ) reg pair (x,ya,y b )∼Dr a b = −E (cid:2) logPr (y ≻y |x) (cid:3) (x,ya,y b )∼Dr ϕ a b −E (cid:2) r(x,y )+r(x,y ) (cid:3)2 (10.63) (x,ya,y b )∼Dr a b 10.4 Improved Human Preference Alignment 571 Optimizingwiththisregularizationtermcanhelpmitigatetheunderdeterminationofreward models5. 2. Sparse Rewards vs. Dense Rewards AsdiscussedinSection10.3,therewardsinRLHFareverysparse: theyareobservedonly attheendofsequences,ratherthancontinuouslythroughoutthegenerationprocess. Dealing with sparse rewards has long been a concern in reinforcement learning, and has been one ofthechallengesinmanypracticalapplications. Forexample,inrobotics,itoftenneedsto shapetherewardfunctiontoeaseoptimizationratherthanrelyingsolelyonend-of-sequence rewards. Variousmethodshavebeendevelopedtoaddressthisissue. Onecommonapproach is reward shaping, where the original function is modified to include intermediate rewards, thereby providing more immediate feedback. Also, one can adopt curriculum learning to sequentiallystructuretasksinawaythatthecomplexitygraduallyincreases. Thiscanhelp modelstomastersimplertasksfirst,whichpreparesthemformorecomplexchallengesastheir skillsdevelop. Therearemanysuchmethodsthatcanmitigatetheimpactofsparserewards, suchasMonteCarlomethodsandintrinsicmotivation. Mostofthesemethodsaregeneraland thediscussionofthemcanbefoundinthebroaderliteratureonreinforcementlearning,such asSuttonandBarto[2018]’sbook. Although we do not discuss methods for mitigating sparse rewards in detail here, an interesting question arises: why are sparse rewards so successful in RLHF? Recall from Section 10.3.1 that the supervision signal received at each time step t is not the reward for thecurrentaction,butrathersomeformoftheaccumulatedrewardsfromtuntilthelasttime step. Suchsupervisionsignalsaredenseoverthesequence,becausetherewardobtainedatthe endofthesequencecanbetransferredbacktothattimestep,regardlessofwhichtimestep it is. In other words, the sparse rewards are transformed into the dense supervision signals. Furthermore,fromtheperspectiveofrewardshaping,Ngetal.[1999]showthattherewardat tcanbedefinedas r′(s ,a ,s ) = r(s ,a ,s )+f(s ,a ,s ) (10.64) t t t+1 t t t+1 t t t+1 wherer′(·)isthetransformedrewardfunction,r(·)istheoriginalrewardfunction,andf(·) istheshapingrewardfunction. Toensuretheoptimalityofthepolicyunderthetransformed rewardfunction,theshapingrewardfunctioncanbegivenintheform f(s ,a ,s ) = γΦ(s )−Φ(s ) (10.65) t t t+1 t+1 t whereΦ(s)iscalledthepotentialvalueofthestates. IfwedefineΦ(s)asthecommonvalue functionasinEq. (10.15)andsubstituteEq. (10.65)intoEq. (10.64),weobtain r′(s ,a ,s ) = r(s ,a ,s )+γV(s )−V(s ) (10.66) t t t+1 t t t+1 t+1 t 5Amodeliscalledunderdeterminediftherearemultiplealternativesetsofparametersthatcanachievethe sameobjective. 572 Chapter 10. Alignment Itisinterestingtoseethatthisfunctionisexactlythesameastheadvantagefunctionusedin PPO.Thisrelatesadvantage-basedmethodstorewardshaping: theadvantageisessentiallya shapedreward. On the other hand, one of the reasons for adopting end-of-sequence rewards lies in the natureoftheRLHFtasks. Unliketraditionalreinforcementlearningenvironmentswherethe agent interacts with a dynamic environment, RLHF tasks often involve complex decision- makingbasedonlinguisticorotherhigh-levelcognitiveprocesses. Theseprocessesdonotlend themselveseasilytofrequentandmeaningfulintermediaterewardsbecausethequalityand appropriatenessoftheactionscanonlybefullyevaluatedafterobservingtheirimpactinthe largercontextoftheentiresequenceortask. Inthiscase,therewardsignalsbasedonhuman feedback,thoughverysparse,aretypicallyveryinformativeandaccurate. Consequently,this sparsity,togetherwiththehighinformativenessandaccuracyofthehumanfeedback,canmake thelearningbothrobustandefficient. 3. Fine-grained Rewards Formanyapplications,ourobjectivewillbemorecomplexthanmerelyevaluatinganentire text. For example, in sentiment analysis, we often do not just determine the sentiment of a text,butneedtoanalyzethesentimentinmoredetailbyassociatingitwithspecificaspects ofatopicdiscussedinthetext. Considerthesentence\"Thecameraofthephoneisexcellent, butthebatterylifeisdisappointing.\"Inthisexample, wewouldneedtoseparatelyanalyze thesentimentsexpressedaboutthecameraandthebattery. Suchanalysis,knownasaspect- basedsentimentanalysis,helpsprovideafiner-grainedunderstandingofthecustomerreview comparedtogeneralsentimentanalysis. Fortheproblemofrewardmodeling,weoftenneedtomodeldifferentpartsofasequence as well. A simple and straightforward way to do this is to divide a sequence into different segmentsandthencomputetherewardforeachsegment[Wuetal.,2023b]. Supposethatan outputtokensequenceycanbedividedinton segments{y¯ ,...,y¯ }bysomecriterion. We s 1 ns canusetherewardmodeltoevaluateeachofthesesegments. Bytakingx,yandy¯ asinput k totherewardmodel,therewardscoreforthek-thsegmentisgivenby rk = r(x,y,y¯ ) (10.67) k Thentherewardscorefortheentireoutputsequenceisgivenby (cid:88) ns r(x,y) = r(x,y,y¯ ) (10.68) k k=1 Herer(x,y)canbeusedtotrainthepolicyasusual. A problem with this model is that training reward models at the segment level is not as straightforward as learning from human preferences on entire texts, as it is difficult to obtainsegment-levelhumanpreferencedata. Forrating-likeproblems(e.g.,werateasegment accordingtoitslevelofmisinformation),onesimpleapproachistoassignaratingscoreto eachsegmentandtraintherewardmodelusingpointwisemethods. Forexample,wecanusea 10.4 Improved Human Preference Alignment 573 strongLLMtoratethesequencesy¯ ...y¯ andy¯ ...y¯ ,andobtainthescoress(y¯ ...y¯ ) 1 k−1 1",
    "with this model is that training reward models at the segment level is not as straightforward as learning from human preferences on entire texts, as it is difficult to obtainsegment-levelhumanpreferencedata. Forrating-likeproblems(e.g.,werateasegment accordingtoitslevelofmisinformation),onesimpleapproachistoassignaratingscoreto eachsegmentandtraintherewardmodelusingpointwisemethods. Forexample,wecanusea 10.4 Improved Human Preference Alignment 573 strongLLMtoratethesequencesy¯ ...y¯ andy¯ ...y¯ ,andobtainthescoress(y¯ ...y¯ ) 1 k−1 1 k 1 k−1 and s(y¯ ...y¯ ). We can then define the score of the segment y¯ as the difference between 1 k k s(y¯ ...y¯ )ands(y¯ ...y¯ ) 1 k 1 k−1 s(y¯ ) = s(y¯ ...y¯ )−s(y¯ ...y¯ ) (10.69) k 1 k 1 k−1 Usingthesesegment-levelscores,wecantraintherewardmodelwitharegressionloss function L = −E (cid:2) s(y¯ )−r(x,y,y¯ ) (cid:3)2 (10.70) rating y¯ k k k Sometimes,alignmentcanbetreatedasaclassificationproblem,forexample,weassess whether a segment has ethical issues. In this case, the segment can be labeled as ethical or unethical,eitherbyhumansorusingadditionalclassifiers. Giventhelabelofthesegment,we cantraintherewardmodelusingsomeclassificationlossfunction. Forexample,supposethat r(x,y,y¯ )=1ifthesegmentisclassifiedasunethical,andr(x,y,y¯ )=−1otherwise6. The k k hingelossoftrainingbinaryclassificationmodelsisgivenby L = max(0,1−r(x,y,y¯ )·rˆ) (10.71) hinge k whererˆ∈{1,−1}denotesthegroundtruthlabel. The remaining issue here is how to split y into segments. One approach is to define a fixed-lengthsegmentation,whereyisdividedintoequal-lengthchunks. However,thismay notalwaysbeideal,asthecontentofthesequencemaynotalignwellwithfixedboundaries. Analternativeapproachistosegmentybasedonspecificlinguisticorsemanticcues,suchas sentenceboundaries,topicshifts,orothermeaningfulstructuresinthetext. Suchasegmenta- tioncanbeachievedbyusinglinguisticsegmentationsystemsorpromptingLLMstoidentify naturalbreaksinthesequence. Anotherapproachistousedynamicsegmentationmethods basedonthecomplexityofthesequence. Forexample,segmentscouldbedefinedwherethere isasignificantchangeintherewardscore,whichmightcorrespondtoshiftsinthetaskbeing modeled. 4. Combination of Reward Models Arewardmodelcanbeviewedasaproxyfortheenvironment. Sincethetrueenvironmentis oftentoocomplexorunknown,developingaperfectproxyfortheenvironmentisgenerallynot possible. Asaresult,over-aligningLLMswiththisimperfectproxymightleadtodecreased performance, known as the overoptimization problem [Stiennon et al., 2020; Gao et al., 2023a]7. We can also explain this through Goodhart’s law, which states: when a measure 6Toallowtherewardmodeltooutputcategories,wecanreplacethelinearlayerdescribedinSection10.3.2 withaSoftmaxlayer. 7Thisproblemisalsocalledrewardhackingorrewardgaming[Krakovnaetal.,2020;Skalseetal.,2022; Panetal.,2022],whichreferstothephenomenonwheretheagentattemptstotricktherewardmodelbutfails toalignitsactionswiththetrueintendedobjectivesofthetask. Imagineastudentwhoisassignedhomework andisrewardedwithpointsorpraiseforcompletingit.Thestudentmightthenfindwaystofinishthehomework 574 Chapter 10. Alignment becomesatarget,itceasestobeagoodmeasure[Goodhart,1984]. Addressingtheoveroptimizationproblemisnoteasy,andthereisnomaturesolutionyet. The ideal approach might be to develop an oracle reward model that can perfectly capture thetrueobjectivesofthetaskandpreventtheagentfrom“tricking”. However,creatingsuch amodelisextremelydifficultduetothecomplexityofthereal-worldenvironment, aswell as the challenge of defining all the relevant factors that contribute to the desired outcome. Instead,amorepracticalapproachistocombinemultiplerewardmodels,therebyalleviating themisalignmentbetweenthetrainingobjectiveandthetrueobjectivethatarisesfromusinga single,specificrewardmodel[Costeetal.,2024]. Given a set of reward models, combining them is straightforward, and in some cases, we can simply treat this problem as an ensemble learning problem. A simple yet common approachistoaveragetheoutputsofthesemodelstoobtainamorepreciserewardestimation: K 1 (cid:88) r = w ·r (x,y) (10.72) combine k k K k=1 where r (·) is the k-th reward model in the ensemble, w is the weight of r (·), and K k k k is the number of reward models. This combined reward can then be used to supervise the trainingofapolicy. Infact,therearemanywaystocombinedifferentmodels,forexample, one can make predictions using Bayesian model averaging or develop a fusion network to learn to combine the predictions from different models. Alternatively, one can frame this taskasamulti-objectiveoptimizationproblem,andusemultiplerewardmodelstotrainthe policy simultaneously. These methods have been intensively discussed in the literature on optimizationandmachinelearning[Miettinen,1999;Bishop,2006]. Inadditiontomodelcombinationmethods,anotherimportantissueishowtocollector construct multiple different reward models. One of the simplest approaches is to employ ensemblelearningtechniques,suchasdevelopingdiverserewardmodelsfromdifferentsubsets of a given dataset or from various data sources. For RLHF, it is also possible to construct reward models based on considerations of different aspects of alignment. For example, we candeveloparewardmodeltoevaluatethefactualaccuracyoftheoutputandanotherreward modeltoevaluatethecompletenessoftheoutput. Thesetwomodelsarecomplementaryto each other, and can be combined to improve the overall evaluation of the output. Another approachistoemploydifferentoff-the-shelfLLMsasrewardmodels. Thisapproachissimple andpractical,astherehavebeenalotofwell-developedLLMsandwejustneedtousethem withnoorlittlemodification. Aninterestingissue,thoughnotcloselyrelatedtothediscussion here, arises: can an LLM that aligns with other LLMs outperform those LLMs? Probably notatfirstglance. Inpart,thisisbecausethetargetLLMmerelyimitatesotherLLMsbased on limited supervision and thus cannot capture well the nuances of the behaviors of these supervisors. However,giventhestronggeneralizationabilityofLLMs,thisapproachcan,in fact, be quite beneficial. For example, using open-sourced or commercial LLMs as reward withminimalefforttomaximizethereward,suchascopyingandpastingsolutionsfromtheinternetorprevious assignments,ratherthansolvingtheproblemsthemselves. 10.4 Improved Human",
    "arises: can an LLM that aligns with other LLMs outperform those LLMs? Probably notatfirstglance. Inpart,thisisbecausethetargetLLMmerelyimitatesotherLLMsbased on limited supervision and thus cannot capture well the nuances of the behaviors of these supervisors. However,giventhestronggeneralizationabilityofLLMs,thisapproachcan,in fact, be quite beneficial. For example, using open-sourced or commercial LLMs as reward withminimalefforttomaximizethereward,suchascopyingandpastingsolutionsfromtheinternetorprevious assignments,ratherthansolvingtheproblemsthemselves. 10.4 Improved Human Preference Alignment 575 ValueFunction Preference Data y a ≻ y b trainingwithMLE RewardModel Training withPPO Policy (a)RLHF(PPO) Preference Data y ≻ y trainingwithMLE a b Policy (b)DPO Figure10.10: StandardRLHF(PPO)vs. DPO.InRLHF,thehumanpreferencedataisused to train a reward model, which is then employed in training the policy as well as the value function. InDPO,theuseofhumanpreferencedataismoredirect,andthepolicyistrainedon thisdatawithouttheneedforrewardmodeltraining. modelshasdemonstratedstrongperformanceinaligningLLMs,evenachievingstate-of-the-art resultsonseveralpopulartasks[Lambertetal.,2024]. 10.4.2 Direct Preference Optimization Althoughlearningrewardmodelsisastandardstepinreinforcementlearning,itmakesthe entiretrainingprocessmuchmorecomplexthansupervisedtraining. Trainingareliablereward model is itself not an easy task, and a poorly trained reward model can greatly affect the outcomeofpolicylearning. Wenowconsideranalternativealignmentmethod,calleddirect preferenceoptimization(DPO),whichsimplifiesthetrainingframeworkbyeliminatingthe needtoexplicitlymodelrewards[Rafailovetal.,2024]. Thismethoddirectlyoptimizesthe policybasedonuserpreferences,ratherthandevelopingaseparaterewardmodel. Asaresult, we can achieve human preference alignment in a supervised learning-like fashion. Figure 10.10showsacomparisonofthestandardRLHFmethodandtheDPOmethod. BeforederivingtheDPOobjective,letusfirstreviewtheobjectiveofpolicytrainingused inRLHF.AsdiscussedinSection10.3.3,thepolicyistypicallytrainedbyoptimizingaloss function with a penalty term. The DPO method assumes a simple loss function where the qualityoftheoutputygiventheinputxisevaluatedbytherewardmodelr(x,y). Thetraining 576 Chapter 10. Alignment objectiveisthusgivenby θ˜ = argminE E (cid:2) −r(x,y)+β(logπ (y|x)−logπ (y|x)) (cid:3) (10.73) x∼D y∼π θ (·|x) θ θ ref θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) loss penalty Note that in this optimization problem, only the term π (y|x) depends on the target policy θ π (·). Boththerewardmodelr(x,y)andthereferencemodelπ (y|x)areassumedtobe θ θ ref fixedgivenxandy. ThisisastrongassumptioncomparedwithPPO,butaswillbeshown later,itsimplifiestheproblemandcrucialforderivingtheDPOobjective. Sinceθisthevariablewewanttooptimize,werearrangetheright-handsideofEq. (10.73) toisolateπ (y|x)asanindependentterm: θ θ˜ = argminE E (cid:2) βlogπ (y|x)−βlogπ (y|x)−r(x,y) (cid:3) x∼D y∼π θ (·|x) θ θ ref θ = argminE E (cid:2) logπ (y|x)− (cid:0) logπ (y|x)+ 1 r(x,y) (cid:1)(cid:3) x∼D y∼π θ (·|x) θ θ ref β θ = argminE E (cid:2) logπ (y|x)−logπ (y|x)exp (cid:0)1 r(x,y) (cid:1)(cid:3) (10.74) x∼D y∼π θ (·|x) θ θ ref β θ (cid:124) (cid:123)(cid:122) (cid:125) dependentonθ (cid:124) (cid:123)(cid:122) (cid:125) notdependentonθ Thisequationdefinestheobjectivefunctionasthedifferencebetweenthelog-probability distribution function of y and another function of y. This form of the objective function seems not “ideal”, as we usually prefer to see the difference between two distributions, so that we can interpret this difference as some kind of divergence between the distributions. A simple idea is to convert the second term (i.e., logπ (y|x)exp(1r(x,y))) into a log- θ ref β probability distribution over the domain of y. If we treat π (y|x)exp(1r(x,y)) as an θ ref β unnormalizedprobabilityofy,wecanconvertitintoanormalizedprobabilitybydividingit byanormalizationfactor: (cid:88) (cid:0)1 (cid:1) Z(x) = π (y|x)exp r(x,y) (10.75) θ ref β y Hencewecandefineaprobabilitydistributionby π (y|x)exp (cid:0)1r(x,y) (cid:1) π∗(y|x) = θ ref β (10.76) Z(x) 10.4 Improved Human Preference Alignment 577 WethenrewriteEq. (10.74)as θ˜ = argminE E (cid:104) logπ (y|x)−log π θ ref (y|x)exp (cid:0) β 1r(x,y) (cid:1) x∼D y∼π (·|x) θ (cid:1) θ Z(x θ (cid:105) −logZ(x) (cid:104) (cid:105) = argminE E logπ (y|x)−logπ∗(y|x)−logZ(x) x∼D y∼π (·|x) θ θ θ (cid:20) (cid:104) (cid:105) = argminE E logπ (y|x)−logπ∗(y|x) x∼D y∼π (·|x) θ θ θ (cid:21) −E (cid:2) logZ(x) (cid:3) y∼π (·|x) θ (cid:104) (cid:105) = argminE KL (cid:0) π (·|x)||π∗(·|x) (cid:1) − logZ(x) (10.77) x∼D θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) KLdivergence constantwrt.θ SincelogZ(x)isindependentofθ,itdoesnotaffecttheresultoftheargmin operation, θ andcanberemovedfromtheobjective. Nowweobtainanewtrainingobjectivewhichfinds theoptimalpolicyπ byminimizingtheKLdivergencebetweenπ (·|x)andπ∗(·|x) θ θ (cid:104) (cid:105) θ˜ = argminE KL (cid:0) π (·|x)||π∗(·|x)",
    "(cid:21) −E (cid:2) logZ(x) (cid:3) y∼π (·|x) θ (cid:104) (cid:105) = argminE KL (cid:0) π (·|x)||π∗(·|x) (cid:1) − logZ(x) (10.77) x∼D θ θ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) KLdivergence constantwrt.θ SincelogZ(x)isindependentofθ,itdoesnotaffecttheresultoftheargmin operation, θ andcanberemovedfromtheobjective. Nowweobtainanewtrainingobjectivewhichfinds theoptimalpolicyπ byminimizingtheKLdivergencebetweenπ (·|x)andπ∗(·|x) θ θ (cid:104) (cid:105) θ˜ = argminE KL (cid:0) π (·|x)||π∗(·|x) (cid:1) (10.78) x∼D θ θ Clearly,thesolutiontothisoptimizationproblemisgivenby π (y|x) = π∗(y|x) θ π (y|x)exp (cid:0)1r(x,y)) = θ ref β (10.79) (cid:1) Z(x Giventhisequation,wecanexpresstherewardr(x,y)usingthetargetmodelπ (y|x),the θ referencemodelπ (y|x),andthenormalizationfactorZ(x): θ ref (cid:18) (cid:19) π (y|x) θ r(x,y) = β log +logZ(x) (10.80) π (y|x) θ ref This is interesting because we initially seek to learn the policy π (·) using the reward θ modelr(x,y),buteventuallyobtainarepresentationoftherewardmodelbasedonthepolicy. GiventherewardmodeldefinedinEq. (10.80),wecanapplyittotheBradley-Terrymodelto 578 Chapter 10. Alignment calculatethepreferenceprobability(alsoseeSection10.3.2): Pr (y ≻y |x) = Sigmoid(r(x,y )−r(x,y )) θ a b a b (cid:18) (cid:16) π (y |x) (cid:17) θ a = Sigmoid β log +logZ(x) − π (y |x) θ a ref (cid:19) (cid:16) π (y |x) (cid:17) θ b β log +logZ(x) π (y |x) θ b ref (cid:18) (cid:19) π (y |x) π (y |x) θ a θ b = Sigmoid βlog −βlog (10.81) π (y |x) π (y |x) θ a θ b ref ref Thisformulaiselegantbecauseitconvertsthedifferenceinrewardsintothedifference inratiofunctions,andwedonotneedtocalculatethevalueofZ(x). Adirectresultisthat we no longer need a reward model, but only need the target policy and reference model to calculatetheprobabilityofpreferences. Finally,wecantrainthetargetpolicybyminimizing thefollowingDPOlossfunction L (θ) = −E (cid:2) logPr (y ≻y |x) (cid:3) (10.82) dpo (x,ya,y b )∼Dr θ a b TheformofthislossfunctionisverysimilartothatusedintrainingrewardmodelsinRLHF (seeEq. (10.36)). Butitshouldbenotedthatthelossfunctionheredependsontheparameters ofthepolicy(i.e.,θ)ratherthantheparametersoftherewardmodel(i.e.,ϕ). ThemainadvantageofDPOliesinitssimplicityandefficiency. TheDPOobjectiveisvery straightforward—itdirectlyoptimizesforpreference-basedfeedback,ratherthanrelyingon separatelydevelopedrewardmodels. Moreover,DPOisgenerallymoresample-efficient,asit learnsfromafixeddatasetwithouttheneedforthecomputationallyexpensivesamplingprocess usedinPPO.ThismakesDPOapopularmethodforhumanpreferencealignment,especially whendevelopingandapplyingrewardmodelsviareinforcementlearningischallenging. DPO can broadly be viewed as an offline reinforcement learning method, where the trainingdataispre-collectedandfixed,andthereisnoexploration. Incontrast,onlinerein- forcementlearningmethodslikePPO,whichrequireexploringnewstatesthroughinteraction withtheenvironment(usingtherewardmodelasaproxy),alsohavetheiruniqueadvantages. Oneofthebenefitsofonlinereinforcementlearningisthatitallowstheagenttocontinuously adapttochangesintheenvironmentbylearningfromreal-timefeedback. Thismeansthat, unlikeofflinemethods,onlinemethodsarenotconstrainedbythestaticnatureofpre-collected dataandcandiscovernewproblem-solvingstrategies. Inaddition,explorationcanhelpthe agentcoverawiderrangeofstate-actionpairs,thusimprovinggeneralization. Thiscouldbe animportantadvantageforLLMs,asgeneralizationisconsideredacriticalaspectinapplying suchlargemodels. 10.4.3 Automatic Preference Data Generation Althoughlearningfromhumanpreferencesisaneffectiveandpopularmethodforaligning LLMs,annotatingpreferencedataiscostly. Usinghumanfeedbackdoesnotonlyfacesthe problem of limited scalability, but it may also introduce bias because human feedback is 10.4 Improved Human Preference Alignment 579 inherently subjective. As a result, one can turn to AI feedback methods to address these scalabilityandconsistencyissueswithoutthelimitationsassociatedwithhumanannotators. Aswithdatagenerationforinstructionfine-tuning,generatingpreferencedatausingLLMs is straightforward. Given a set of inputs, we first use an LLM to generate pairs of outputs. Then,weprompttheLLMtolabelthepreferencebetweeneachpairofoutputs,alongwith itscorrespondinginput. BelowisanexampleofpromptingtheLLMtogenerateapreference labelforapairofconsumerserviceresponses. Consideracustomerservicescenariowhereacustomerposesarequest. You willreviewtworesponsestothisrequest. Pleaseindicatewhichresponseis preferred. Notethatagoodresponseshouldbecourteous,clear,andconcise. Itshouldaddressthecustomer’sconcerndirectly,providehelpfulinformation orasolution,andmaintainapositivetone. Request: Hello,Inoticedthatmyorderhasn’tarrivedyet,thoughitwasscheduledto arriveseveraldaysago. Couldyoupleaseupdatemeonitsstatus? Thank you! ResponseA: I’mverysorryforthedelayandunderstandhowdisappointingthiscanbe. We’redoingourbesttosortthisoutquicklyforyou. ResponseB: Hey,stuffhappens! Yourpackagewillgettherewhenitgetsthere,noneedto stress. ResponseAispreferred. Oncewecollectsuchpreferencelabels,wecanusethem,alongwiththeoutputpairand input,totraintherewardmodel. Ofcourse,wecanconsiderdemonstratingafewexamples orusingadvancedpromptingtechniques,suchasCoT,toimprovelabelingperformance. For example, we can include in the prompt an example showing how and why one of the two responsesispreferredbasedonaCoTrationale. Inadditiontopreferencelabels,wecanalsoobtaintheprobabilityassociatedwitheach label[Leeetal.,2023]. Asimplemethodistoextracttheprobabilitiesforthelabeltokens, suchas“A”and“B”,fromtheprobabilitiesoutputbytheLLM.WecanthenusetheSoftmax functionorothernormalizationtechniquestore-normalizetheseprobabilitiesintoadistribution over the labels. These probabilities of preferred labels can serve as pointwise supervision signalsfortrainingtherewardmodel,asdiscussedinSection10.4.1. For data generation, although it is easy to scale up, it is often necessary to ensure the dataisaccurateanddiverse. Here,thedataqualityanddiversityissuesinvolvenotonlythe 580 Chapter 10. Alignment labelingofpreferencesbutalsotheinputsandoutputsofthemodel. Therefore,weoftenneed touseavarietyoftechniquestoobtainlarge-scale,high-qualitydata. Forexample,onecan generatediversemodeloutputsandannotationsbyusingdifferentLLMs,prompts,in-context demonstrations,andsoon[Cuietal.,2024]. Duboisetal.[2024]reportthatthevariabilityin pairwisepreferencedataisimportantfortrainingLLMsfromeitherhumanorAIfeedback. WhilelearningfromAIfeedbackishighlyscalableandgenerallyobjective,thismethod ismoresuitedtowell-definedtaskswhereobjectiveperformancemetricsareavailable. By contrast,learningfromhumanfeedbackismoreadvantageouswhenaligningAIsystemswith humanvalues,preferences,andcomplexreal-worldtasksthatrequireunderstandingofsubtle orsubjectivecontext. ThesemethodscanbecombinedtotrainLLMsthatbenefitfromboth humaninsightsandthescalabilityofAIfeedback. 10.4.4 Step-by-step Alignment So far, our discussion of alignment has primarily focused on the use of reward models for evaluatingentireinput-outputsequencepairs. Thesemethodscanbeeasilyadaptedtoscenarios wherethecorrectnessofanoutputcanbeexaminedbycheckingwhetherthedesiredresultis included. Forexample,inthetaskofcalculatingamathematicalexpression,arewardmodel canprovidepositivefeedbackiftheansweriscorrect,andnegativefeedbackiftheansweris wrong. However, in many problems that requirecomplexreasoning, simply examining the correctnessofthefinalresultisinsufficientforlearning. Imagineastudentwhoisonlygiven thefinalanswertoachallengingmathproblem. Knowingwhetherthefinalanswerisrightor wrongdoesnothelpthestudentfigureoutwheretheywentwrongandhowtocalculatethe correctanswer. Abetterapproachwouldbetoguidethestudentwithastep-by-stepbreakdown oftheproblem-solvingprocessandencourageunderstandingoftheunderlyingconceptsand logicbehindthesesteps. InChapter9,westudiedCoTmethodstopromptLLMstoexplicitlywriteoutintermediate stepsorthereasoningprocessneededtoreachaconclusionorsolveaproblem. Wesawthat breakingdownaproblemintosmallerpartscouldmakeiteasiertounderstandthesolution path and increase the accuracy of the output. These methods can be naturally extended to the alignment",
    "of reward models for evaluatingentireinput-outputsequencepairs. Thesemethodscanbeeasilyadaptedtoscenarios wherethecorrectnessofanoutputcanbeexaminedbycheckingwhetherthedesiredresultis included. Forexample,inthetaskofcalculatingamathematicalexpression,arewardmodel canprovidepositivefeedbackiftheansweriscorrect,andnegativefeedbackiftheansweris wrong. However, in many problems that requirecomplexreasoning, simply examining the correctnessofthefinalresultisinsufficientforlearning. Imagineastudentwhoisonlygiven thefinalanswertoachallengingmathproblem. Knowingwhetherthefinalanswerisrightor wrongdoesnothelpthestudentfigureoutwheretheywentwrongandhowtocalculatethe correctanswer. Abetterapproachwouldbetoguidethestudentwithastep-by-stepbreakdown oftheproblem-solvingprocessandencourageunderstandingoftheunderlyingconceptsand logicbehindthesesteps. InChapter9,westudiedCoTmethodstopromptLLMstoexplicitlywriteoutintermediate stepsorthereasoningprocessneededtoreachaconclusionorsolveaproblem. Wesawthat breakingdownaproblemintosmallerpartscouldmakeiteasiertounderstandthesolution path and increase the accuracy of the output. These methods can be naturally extended to the alignment of LLMs, that is, we supervise the model during the intermediate steps of reasoning. ConsiderareasoningtaskwhereanLLMproducesasequenceofreasoningsteps y={y¯ ,...,y¯ }forthegiveninput. Theresultofthereasoningisassumedtobeincludedin 1 ns thelaststepy¯ ,andcanbeeasilyverified. Forthisreasoningproblem,Uesatoetal.[2022] ns categorizeLLMfine-tuningapproachesintotwoclasses: • Outcome-basedApproaches. Supervisionoccursonlywhentheendresultisverified. Thisisastandardmethodforlearningfromhumanfeedbackwehavediscussedinthis chapter. For example, the LLM is optimized to maximize some form of the reward r(x,y). • Process-basedApproaches. Supervisionisinvolvedinallintermediatestepsinaddition tothelaststep. Todothis,weneedtodevelopamodeltogiveasupervisionsignalat eachstep,anddeveloplossfunctionsthatcanmakeuseofsuchsupervisionsignals. 10.4 Improved Human Preference Alignment 581 Question(fromTheInternationalJuniorMathOlympiad): Jessicahasalotofappsonherphone. Thenumberofappsshehasisdivisibleby5. Ifshe downloaded10moreapps,shewouldstillhavelessthan50apps. Whatisthemaximum originalnumberofappsJessicacouldhave? Response1: Response2: y¯ 1: xmustbeamultipleof5,i.e., y¯ 1: Thenumberofappsisdivisibleby5, x x x≡0(mod5). soxmustbeamultipleof5. y¯ 2: Afterdownloading10apps,thetotal y¯ 2: Ifshedownloads10moreapps,her totalnumberofappsshouldstillbeless newtotalwillbex+10,andthismust x x than50: x+10<50. Solvingthis belessthan50. Therefore,x+10<50. inequality: x<40. Simplifyingthisgivesx<40. y¯ 3: Sincexmustbeamultipleof5and y¯ 3: xmustbelessthanorequalto problematic x x lessthan40,thelargestvalueofxis35. thelargestmultipleof5,whichis40. y¯ 4: Verifytheresult. Ifx=35,thenafter y¯ 4: Butx+10shouldnotbemore problematic downloading10apps,Jessicawould thanorequalto50. Soweneedto x x have: 35+10=45whichsatisfy subtract5from40. 45<50. y¯ 5: Therefore,thefinalresultis35. x y¯ 5: Themaximumoriginalnumberof x correct appsis35. correct Figure 10.11: Two LLM responses to a math problem. In response 1, both the final result andallthereasoningstepsarecorrect. Inresponse2,thefinalresultiscorrect,butthereare mistakesinthereasoningprocess(highlightedinred). Foroutcome-basedapproaches,both responsesareconsideredcorrect. Forprocess-basedapproaches,themistakesinresponse2 canbeconsideredinrewardmodeling. Figure10.11showstwoLLMoutputsforanexamplemathproblem. AlthoughtheLLM gives the correct final answer in both cases, it makes mistakes during the problem-solving processinthesecondoutput. Outcome-basedapproachesoverlookthesemistakesandgive positivefeedbackfortheentiresolution. Bycontrast,process-basedapproachescantakethese mistakesintoaccountandprovideadditionalguidanceonthedetailedreasoningsteps. Animportantissueforprocess-basedapproachesisthatweneedtogetstep-levelfeedback duringa(potentially)longreasoningpath. Wecancollectorgeneratereasoningpathscorre- spondingtoproblemsfromexistingdatasets. Humanexpertsthenannotateeachstepinthese pathsforcorrectness. TheseannotationscanbeusedtodirectlytrainLLMsorasrewardsin reward modeling. However, in practice, richer annotations are often introduced [Lightman et al., 2024]. In addition to the correct and incorrect labels, a step can also be labeled as 582 Chapter 10. Alignment neutraltoindicatethatwhilethestepmaybetechnicallycorrect,itmightstillbeproblematic withintheoverallreasoningprocess. Furthermore,toimprovetheefficiencyofdataannotation, techniquessuchasactivelearningcanbeemployed. Identifyingobviouserrorsusuallydoes notsignificantlycontributetolearningfromreasoningmistakes. Instead,annotatingstepsthat themodelconfidentlyconsiderscorrectbutareactuallyproblematicisoftenmoreeffective. Givenasetofstep-levelannotatedreasoningpathsandcorrespondinginputs,wecantrain arewardmodeltoprovidefeedbackforsupervisingpolicylearning. Therewardmodelcanbe treatedasaclassificationmodel,andsoitsarchitecturecanbeaTransformerdecoderwitha Softmaxlayerstackedontop. Atstepk,therewardmodeltakesboththeproblemdescription (denotedbyx)andthereasoningstepsgeneratedsofar(denotedbyy¯ )asinputandoutputs ≤k aprobabilitydistributionoverthelabelset{correct,incorrect}or{correct,incorrect,neutral}. Thenthelearnedrewardmodelisusedtoevaluatereasoningpathsbyassessingthecorrectness ofeachstep. Asimplemethodtomodelcorrectnessistocountthenumberofstepsthatare classifiedascorrect,givenby (cid:88) ns r(x,y) = δ(correct,C(x,y¯ )) (10.83) ≤k k=1 where C(x,y¯ ) denotes the label with the maximum probability. We can also use log- ≤k probabilitiesofclassificationtodefinetherewardoftheentirepath (cid:88) ns r(x,y) = logPr(correct|x,y¯ ) (10.84) ≤k k=1 wherePr(correct|x,y¯ )denotestheprobabilityofthecorrectlabelgeneratedbythereward ≤k model. Therewardscorer(x,y)canthenbeusedtotrainthepolicyinRLHFasusual. While we restrict our discussion to math problems, the approaches described here are general and can be applied to a wide variety of tasks that involve multi-step reasoning and decision-making. Moreover,wecanconsidervariousaspectswhenassessingthequalityofa step, rather thanjust itscorrectness. For example, indialogue systems, responsesmust not onlybeaccuratebutalsocontextuallyappropriateacrossmultipleturnsofconversation. Ifa modelprovidesacorrectresponsebutfailstomaintaincoherenceinthecontextoftheongoing dialogue, step-level feedback could help the model identify and correct such discrepancies. Alsonotethattheprocess-basedapproachesarerelatedtothefine-grainedrewardmodeling approachesdiscussedinSection10.4.1. Alltheseapproachesessentiallyaimtoprovidemore detailedsupervisiontoLLMsbybreakingtheiroutputsintosmaller,moremanageablesteps. However,process-basedfeedbackfocusesmoreonevaluatingthecorrectnessofastepbased onitsprecedingsteps,whiletheapproachesinSection10.4.1emphasizeevaluatingeachstep independently. TheideaofaligningLLMsstepbystephasgreatapplicationpotential,especiallyconsider- ingtherecentshifttowardsmorecomplexreasoningtasksintheuseofLLMs. Forexample, boththeGPT-o1andGPT-o3modelsaredesignedwithmoreadvancedreasoningtechniques (such as long internal CoT) to solve challenging problems like scientific and mathematical 10.4 Improved Human Preference Alignment 583 reasoning[OpenAI,2024]. Thesetasksoftenrelyonlongandcomplexreasoningpaths,and therefore,itseemsessentialtointroducedetailedsupervisionsignalsinthereasoningprocess. Moreover, from a practical perspective, effective supervision on long reasoning paths not only improves reasoning performance, but it also helps the model eliminate redundant or unnecessaryreasoningsteps,therebyreducingreasoningcomplexityandimprovingefficiency. 10.4.5 Inference-time Alignment In this section we explored a variety of methods to align models with human preferences and annotations. However, one of the significant limitations of many such methods is that LLMsmustbefine-tuned. ForRLHFanditsvariants,trainingLLMswithrewardmodelscan becomputationallyexpensiveandunstable,leadingtoincreasedcomplexityandcostswhen applyingtheseapproaches. Inthiscase,wecanconsideraligningmodelsatinferencetime, thusavoidingtheadditionalcomplexityandeffortinvolved. Onesimplewaytoachieveinference-timealignmentistousetherewardmodeltoselect thebestonefromN alternativeoutputsgeneratedbytheLLM,amethodknownasBest-of-N sampling(BoNsampling). WecanconsiderBoNsamplingasaformofreranking. Infact, rerankingmethodshavebeenwidelyusedinNLPtasks,suchasmachinetranslation,foralong time. Theyaretypicallyappliedinsituationswheretrainingcomplexmodelsiscostly. Insuch cases,directlyrerankingtheoutputsallowsfortheincorporationofthesecomplexmodelsata verylowcost8. In the BoN sampling process, the LLM takes the input sequence x and generates N differentoutputsequences{yˆ ,...,yˆ }:",
    "and annotations. However, one of the significant limitations of many such methods is that LLMsmustbefine-tuned. ForRLHFanditsvariants,trainingLLMswithrewardmodelscan becomputationallyexpensiveandunstable,leadingtoincreasedcomplexityandcostswhen applyingtheseapproaches. Inthiscase,wecanconsideraligningmodelsatinferencetime, thusavoidingtheadditionalcomplexityandeffortinvolved. Onesimplewaytoachieveinference-timealignmentistousetherewardmodeltoselect thebestonefromN alternativeoutputsgeneratedbytheLLM,amethodknownasBest-of-N sampling(BoNsampling). WecanconsiderBoNsamplingasaformofreranking. Infact, rerankingmethodshavebeenwidelyusedinNLPtasks,suchasmachinetranslation,foralong time. Theyaretypicallyappliedinsituationswheretrainingcomplexmodelsiscostly. Insuch cases,directlyrerankingtheoutputsallowsfortheincorporationofthesecomplexmodelsata verylowcost8. In the BoN sampling process, the LLM takes the input sequence x and generates N differentoutputsequences{yˆ ,...,yˆ }: 1 N {yˆ ,...,yˆ } = argTopN[Pr(y|x)] (10.85) 1 N y wheretheargTopNoperationreturnsthetop-N outputsthatmaximizethefunctionPr(y|x). Theseoutputscanbegeneratedinavarietyofways,dependingonthesearchalgorithmusedby themodel(e.g.,samplingorbeamsearch). OncetheN-bestoutputcandidatesaregenerated, therewardmodelisusedtoevaluateandselectthebestone: yˆ = max{r(x,yˆ ),...,r(x,yˆ )} (10.86) best 1 N It is worth noting that the result of BoN sampling is also influenced by the diversity of theN-bestlist. Thisisacommonissuewithmostrerankingmethods. Typically,wewishthe N-bestoutputcandidatestohaverelativelyhighqualitybutbesufficientlydifferentfromeach other. Inmanytextgenerationsystems,theN-bestoutputsareverysimilar,oftendifferingby 8Rerankingmethodscanalsohelpusexplorewhatareknownasmodelerrorsandsearcherrors,althoughthese issuesarenotoftendiscussedinthecontextofLLMs. Forexample,supposewehaveanoldmodelandanew, morepowerfulmodel.WecanusethenewmodeltoselectthebestoutputfromtheN-bestlistoftheoldmodel astheoracleoutput.Theperformancedifferencebetweentheoracleoutputandthetop-1outputoftheoriginal N-bestlistreflectstheperformancegainbroughtbythenewmodel.Iftheperformancegainissignificant,wecan saythattheoldmodelhasmoremodelerrors.Ifthegainissmall,itmayindicatethattheissueliesinsearcherrors, asthebestcandidateswerenotfound. 584 Chapter 10. Alignment justoneortwowords. ThediversityissueisevenmorechallenginginLLMs,astheN-best outputsgeneratedbyanLLMcanbedifferentintheirwordings,yettheirsemanticmeanings areoftenquitesimilar. Inpractice, onecanadjustthemodelhyperparametersand/oradopt differentLLMstogeneratemorediverseoutputcandidatesforreranking. Nevertheless,aswith manypracticalsystems,weneedtomakeatrade-offbetweenselectinghigh-qualitycandidates andensuringsufficientvariationinthegeneratedoutputs. BoNsamplingcanbeusedfortrainingLLMsaswell. Acloselyrelatedmethodisrejection sampling. Inthismethod,wefirstselectthe“best”outputsfromtheN-bestlistsviathereward model,andthentaketheseselectedoutputstofine-tunetheLLM.Inthisway,wecanintroduce humanpreferencesintothetrainingofLLMsviaamuchsimplerapproachcomparedtoRLHF. ManyLLMshaveadoptedrejectionsamplingforfine-tuning[Nakanoetal.,2021;Touvron etal.,2023b]. 10.5 Summary Inthischapter,wehaveexploredarangeoftechniquesforaligningLLMs. Inparticular,we havediscussedfine-tuningmethodsthatenableLLMstofollowinstructionsandalignthem withhumanpreferences. Oneofthebenefitsoffine-tuningLLMsiscomputationefficiency. Unlike pre-training based on large-scale neural network optimization, fine-tuning is a post- trainingstepandsoislesscomputationallyexpensive. Moreover,itisbettersuitedtoaddress problems that are not easily solved in pre-training, such as human value alignment. The widespreadattentiontothealignmentissuehasalsoledtoasurgeofresearchpapersonthis topic, which has posed challenges in writing this chapter, as it is difficult to cover all the latesttechniques. However,wehavetriedtoprovidearelativelydetailedintroductiontothe fundamentalapproachestoalignment,suchasinstructionfine-tuningandRLHF. WhilewehavefocusedonLLMalignmenttechniquesinthischapter,thetermAIalignment isawide-rangingconcept. Itgenerallyreferstotheprocessofensuringthatthebehaviorofan AIsystemalignswithhumanvalues,goals,andexpectations. TheideaofAIalignmentcanbe tracedbacktotheearlydaysofAI.AwidelyciteddescriptionofAIalignmentcomesfrom anarticlebythemathematicianandcomputerscientistNorbertWiener[Wiener,1960]. The quoteisasfollows Ifweuse,toachieveourpurposes,amechanicalagencywithwhoseopera- tionwecannotefficientlyinterfere... wehadbetterbequitesurethatthe purposeputintothemachineisthepurposewhichwereallydesire. At that time, AI alignment was a distant concern for researchers. But today, it greatly influences thedesign ofvariousAI systems. Forexample, in robotics, alignmentis critical toensuringthatautonomousrobotssafelyinteractwithhumansandtheirenvironments. In autonomousdriving,carsmustnotonlyfollowtrafficlawsbutalsomakecomplex,real-time decisionsthatprioritizehumansafety,avoidaccidents,andnavigateethicaldilemmas. IncurrentAIresearch,alignmentisusuallyachievedbydevelopingasurrogateobjective thatisanalogoustotherealgoalandsteeringtheAIsystemtowardsthisobjective. However, 10.5 Summary 585 designingtheobjectiveofAIalignmentisverydifficult. Onereasonisthathumanvaluesare diverseandoftencontext-dependent,makingitdifficulttodistillthemintoasingle,universally applicableobjectivefunction. Also,thecomplexityofreal-worldenvironments,wherevalues and goalsoften conflictor evolve over time, furthercomplicates alignment efforts. Even if wecoulddefineanappropriateobjective,AIsystemsmayfindunintendedwaystoachieveit, leadingto“misaligned”outcomesthatstilltechnicallysatisfytheobjectivebutinaharmfulor counterproductiveway. These challenges have motivated and are motivating AI research towards more aligned systems,eitherthroughdevelopingnewmechanismsforperceivingtheworldormoreefficient and generalizable methods to adapt these systems to given tasks. More importantly, as AI systemsbecomemorepowerfulandintelligent,especiallygiventhatrecentadvancesinLLMs haveshownremarkablecapabilitiesindealingwithmanychallengingproblems,theneedfor AI alignment has become more urgent. Researchers have started to be concerned with AI safetyandwarnthecommunitythattheyneedtodevelopandreleaseAIsystemswithgreat cautiontopreventthesesystemsfrombeingmisaligned[Russell,2019;Bengioetal.,2024]. https://github.com/NiuTrans/NLPBook https://niutrans.github.io/NLPBook Chapter 11 Inference Once we have pre-trained and fine-tuned an LLM, we can apply it to make predictions on newdata. Thisprocessiscalledinference,inwhichtheLLMcomputestheprobabilitiesof differentpossibleoutputsgivenaninput,andselectstheoutputthatmaximizestheprobability. Theinferenceproblemisgenerallyexpressedinthefollowingform: yˆ = argmaxPr(y|x) (11.1) y where x is the input sequence, y is a possible output sequence, and yˆ is the best output sequence. ThisisperhapsoneofthemostwidelyadoptedformulasinNLP,anddatesbacktotheearly days of speech recognition and machine translation systems based on probabilistic models. Although for some applications, such as predicting a token using a very small language model,solvingthisoptimizationproblemseemstrivial,formostsituationsthecomputational challengesarisefrombothcalculatingPr(y|x)andperformingtheargmaxoperation. The problemswethereforewishtoaddressinthischapterinvolve: 1)computingtheprediction probabilityefficientlygivenatrainedLLM,and2)devisinganefficient(suboptimal)search foryˆ. At a high level, these are fundamental issues in artificial intelligence, which have been extensivelystudied. Somanywell-establishedtechniquescanbedirectlyapplied,forexample, onecanusegreedysearchalgorithmstoimplementanefficientinferencesystem. Ontheother hand,model-specificoptimizations,suchasefficientattentionmodelsforTransformers,canbe consideredtofurtherimproveefficiency. But,inmanypracticalapplications,westillneedto makeatrade-offbetweenaccuracyandefficiency,bycarefullycombiningvarioustechniques. TheimportanceoftheinferenceprobleminLLMsalsoliesinthefactthatmanyapplication scenariosrequireprocessingextremelylongsequences. Recentstudieshavefoundthatinjecting additionalpromptsandcontextualinformation,suchaslongchain-of-thoughtprompts,during inferencecansignificantlyimprovetheperformanceofLLMs. Thisprovidesanewapproach toscalingLLMs: betterresultscanbeachievedbyincreasingthecomputeatinferencetime. Forinstance,throughinference-timescaling,OpenAI[2024]’so1andDeepseek[2025]’sR1 588 Chapter 11. Inference systemshavedemonstratedimpressiveperformanceoncomplexreasoningandprogramming tasks. This, in turn, has encouraged the NLP field to focus more on the issue of efficient inference. Inthischapter,wewillintroducebasicconceptsandalgorithmsofLLMinference,includ- ingprefilling-decodingframeworks,search(decoding)algorithms,andevaluationmetricsof inferenceperformance. WewillthenpresentmethodsforimprovingtheefficiencyofLLM inference, covering a range of techniques for speeding up the system and compressing the model. Finally, we will discuss inference-time scaling, which is considered an important applicationofinferenceoptimization. 11.1 Prefilling and Decoding Inthissection,wepresenttheprefilling-decodingframework,themostcommonapproachfor interpretingandimplementingLLMinferenceprocesses. Wefirstintroducethenotationand backgroundknowledge,andthendescribethedetailsoftheframework,suchasthedecoding algorithmsforLLMinference. 11.1.1 Preliminaries AlthoughwehavedescribedLLMsmanytimesinthisbook,webeginbybrieflydefiningthe notationtofacilitatethesubsequentdiscussion,andtomakethischapterself-contained. x: Theinputtokensequence. Itisconceptuallyequivalenttoa“prompt”, which includes instructions, user inputs, and any additional context intendedasinputtotheLLM.xcomprisesm+1tokens,denotedby x ...x ,wherex isthestartsymbol⟨SOS⟩. 0 m 0 y: The output token sequence, also called the response to the input. y comprisesntokens,denotedbyy ...y",
    "important applicationofinferenceoptimization. 11.1 Prefilling and Decoding Inthissection,wepresenttheprefilling-decodingframework,themostcommonapproachfor interpretingandimplementingLLMinferenceprocesses. Wefirstintroducethenotationand backgroundknowledge,andthendescribethedetailsoftheframework,suchasthedecoding algorithmsforLLMinference. 11.1.1 Preliminaries AlthoughwehavedescribedLLMsmanytimesinthisbook,webeginbybrieflydefiningthe notationtofacilitatethesubsequentdiscussion,andtomakethischapterself-contained. x: Theinputtokensequence. Itisconceptuallyequivalenttoa“prompt”, which includes instructions, user inputs, and any additional context intendedasinputtotheLLM.xcomprisesm+1tokens,denotedby x ...x ,wherex isthestartsymbol⟨SOS⟩. 0 m 0 y: The output token sequence, also called the response to the input. y comprisesntokens,denotedbyy ...y . 1 n y : Theoutputtokensthatprecedepositioni,thatis,y =y ...y . <i <i 1 i−1 Pr(y|x): TheprobabilityofgeneratingygivenxusingtheLLM.IftheLLMis parameterizedbyθ,wecanwriteitasPr (y|x). θ [x,y]: The concatenated token sequence of x and y. That is, [x,y] = x ...x y ...y . Occasionally,weusethenotationseq torepresent 0 m 1 n x,y [x,y]. Pr([x,y]): Theprobabilityofgeneratingthetokensequence[x,y]usingtheLLM. AsdescribedinEq. (11.1),thegoalofLLMinferenceistomaximizePr(y|x). Modeling this conditional probability is common in NLP. At first glance, it seems to be a sequence- to-sequenceproblem,wherewetransformasequenceintoanotherusingencoding-decoding models. However,wearenotdiscussingsequence-to-sequenceproblemsorencoding-decoding architectures. Instead,asdiscussedinearlierchapters,thismodelingproblemcanbeaddressed 11.1 Prefilling and Decoding 589 byusingdecoder-onlymodels. Todothis,wecaninterpretthelog-scaleprobabilitylogPr(y|x) asthedifferencebetweenlogPr([x,y])andlogPr(x) logPr(y|x) = logPr([x,y])−logPr(x) (11.2) where logPr([x,y]) and logPr(x) can be obtained by running the LLM on the sequences [x,y]andx,respectively. Forexample,wecancalculatetheprobabilityofgeneratingxusing thechainrule logPr(x) = logPr(x ...x ) 0 m (cid:2) (cid:3) = log Pr(x )Pr(x |x )···Pr(x |x ...x ) 0 1 0 m 0 m−1 m (cid:88) = logPr(x )+ logPr(x |x ) 0 j <j (cid:124) (cid:123)(cid:122) (cid:125) j=1 =0 m (cid:88) = logPr(x |x ) (11.3) j <j j=1 Inotherwords, wecalculatethetokenpredictionlog-probabilityateachpositionofx, and sumalltheselog-probabilities. In common implementations of LLMs, however, we do not need to compute the log- probabilityoftheinputsequence,butusetheLLMtodirectlycomputethelog-probabilityof theoutputsequenceinthefollowingform n (cid:88) logPr(y|x) = logPr(y |x,y ) (11.4) i <i i=1 where [x,y ] represents the context for predicting y . We use Pr(y |x,y ) to denote <i i i <i Pr(y |[x,y ]),followingthecommonlyusednotationintheliterature. i <i Now,wehavetwosub-problemsinaddressingtheinferenceissuedescribedinEq. (11.1): • ModelComputation: wemodelPr(y |x,y )andcomputeitinanefficientmanner. i <i • Search: wefindtheoptimal(orsub-optimal)outputsequenceintermsoflogPr(y|x). The second sub-problem is a classic issue in NLP. We will show in Section 11.1.3 that there are several well-studied algorithms that can be applied to efficiently search the space ofpossibleoutputsequences. Thefirstsub-problemrequiresalanguagemodeltoproducea distributionoveravocabularyV givenasequenceofcontexttokens. Wecandothisbytraining aTransformerdecoder,whichoutputsthedistribution Pr(·|x,y ) = Softmax(HWo) (11.5) <i m+i H = Dec([x,y ]) (11.6) <i HereDec(·)producesasequenceofrepresentations,eachcorrespondingtoapositionofthe input sequence. So, if we input [x,y ] to the LLM, H is an i′×d matrix, where d is the <i 590 Chapter 11. Inference Pr(·|x) Pr(·|x,y1) ··· Pr(·|x,y<i) SoftmaxLayer decoderoutput ··· ··· FFN ··· ··· Self-attention Llayers ··· ··· LinearMapping ··· ··· EmbeddingLayer x 0 x 1 ··· xm y 1 ··· y i−1 Figure11.1: Thedecoder-onlyarchitectureforLLMs. Thedecoderconsistsofanembedding layerandastackofTransformerlayers. IneachTransformerlayer,theinputpassesthrougha linearmapping,aself-attentionnetwork,andanFFN.Theoutputofthedecoderisasequence ofrepresentationsthataretakenasinputtoaSoftmaxlayer,whichgeneratesadistributionof tokensforeachposition. dimensionality of each representation, and i′ =m+i is the number of context tokens. We canthenuseaSoftmaxlayertotransformtheserepresentationsintodistributionsoftokens. Wo ∈Rd×|V| is the linear mapping matrix of the Softmax layer, and HWo transforms the d-dimensionalrepresentationsinHintothe|V|-dimensionalrepresentations. Theuseofthe subscriptm+iindicatesthattheSoftmaxfunctionisperformedonlyontherepresentationat positionm+i. SeeFigure11.1foranillustrationofthisarchitecture. Dec(·) is a Transformer decoding network that consists of an embedding network and anumberofstackedself-attentionandFFNnetworks. WewillnotdiscussTransformersin detailhere,asreaderscaneasilylearnaboutthesemodelsfromtheliterature. However,itis worthpointingoutthatthedifficultyofinferenceisinpartfromtheuseoftheself-attention mechanisminTransformers. Recallthatageneralformofsingle-headself-attentionisgiven by q KT Att (q ,K,V) = Softmax( i√ ′ )V (11.7) qkv i′ d whereq ∈Rd isthequeryatthepositioni′ (i.e.,positionofy ),andKandV∈Ri′×d are i′ i thekeysandvaluesuptoi′,respectively. Ateachstepduringinference,wecalltheself-attentionfunctionAtt (·),followedby qkv 11.1 Prefilling and Decoding 591 anFFN,togeneratead-dimensionalrepresentationthatintegratesinformationfromboththe currenttokenanditsleftcontext. ThisprocessisrepeatedthroughLlayersofself-attention andFFN,formingastackofTransformerlayers. TheoutputoftheL-thlayerinthisstackis thefinalrepresentation. Each time, the model attends position i′ to all previous positions, which results in 2i′ vectorproducts(i′ timesforq i′ KT andi′ timesfortheproductofSoftmax( q i√′KT )andV). d Hence, generating a sequence of length len has a time complexity of O(L×len2)",
    "qkv 11.1 Prefilling and Decoding 591 anFFN,togeneratead-dimensionalrepresentationthatintegratesinformationfromboththe currenttokenanditsleftcontext. ThisprocessisrepeatedthroughLlayersofself-attention andFFN,formingastackofTransformerlayers. TheoutputoftheL-thlayerinthisstackis thefinalrepresentation. Each time, the model attends position i′ to all previous positions, which results in 2i′ vectorproducts(i′ timesforq i′ KT andi′ timesfortheproductofSoftmax( q i√′KT )andV). d Hence, generating a sequence of length len has a time complexity of O(L×len2) for the self-attentionnetwork. Clearly,theinferenceofthismodelisslowforlongsequencesdueto itsquadratictimecomplexitywithrespecttosequencelength. Therefore,manyimprovements toTransformersandalternativemodelshavefocusedonefficientmethodsthatarefasterthan thisquadratictimecomplexity,suchassparseattentionmechanismsandlinear-timemodels. A detaileddiscussionofefficientTransformerscanbefoundinthepreviouschapters,andthis sectionwillfocusonthestandardTransformerarchitecture. Note that in self-attention, the queries, keys, and values of a layer are linear mappings from the same input (i.e., the output of the previous layer). Once a new key-value pair is generated,itisrepeatedlyusedinsubsequentinferencesteps. Ratherthanregeneratingthese key-valuepairsduringinference,amoredesirablewayistostoretheminastructure,called thekey-valuecache,ortheKVcache. Thus,(K,V)canstraightforwardlybeconsidereda KVcache. Thiscacheisupdatedasfollows K = Append(K,k ) (11.8) i′ V = Append(V,v ) (11.9) i′ where(k ,v )isthenewlygeneratedkey-valuepairatpositioni′,andAppend(a,b)denotes i′ i′ afunctionthatappendsarowvectorbtoamatrixa. Figure11.2showshowaTransformer decoderworkswithaKVcache. Finally,theprocessofcomputinglogPr(y|x)issummarizedasfollows: 1. Weconcatenatexandyintoasequence[x,y]. Foreachpositioni′ ofthissequence,we performthefollowingsteps. (a) We compute the embedding of the token at position i′, and feed the resulting embeddingasaninitialrepresentationintothestackofTransformerlayers. (b) In each Transformer layer, we pass the input representation through the self- attentionnetworkfirstandthenthroughanFFN.Intheself-attentionnetwork,the inputrepresentationistransformedintoq ,k ,andv . Then,weupdatetheKV i′ i′ i′ cache (K,V) using k and v (see Eqs. (11.8-11.9)). Then, we compute the i′ i′ outputoftheattentionmodelbyattendingq to(K,V)(seeEq. (11.7)). i′ (c) Ifi′ >m(i.e., i=i′−m≥0), wetaketheoutputoftheTransformerstackand computethetokenpredictionprobabilityPr(y |x,y )viatheSoftmaxlayer(see i <i Eq. (11.5)). 2. Whenreachingtheendofthesequence,weobtainlogPr(y|x)bysumminglogPr(y |x,y ) i <i overi∈[1,n](seeEq. (11.4)). 592 Chapter 11. Inference attention k 1 k 2 ··· k i′−1 k i′ q i′ query v 1 v 2 ··· v i′−1 v i′ KVCache(positions1toi′−1) value key v i′ k i′ Linearmaps Input Inputatpositioni′ (a)UpdatingtheKVCacheatPositioni′ attention k 1 k 2 ··· k i′−1 k i′ k i′+1 q i′+1 query v 1 v 2 ··· v i′−1 v i′ v i′+1 KVCache(positions1toi′) value key v i′+1 k i′+1 Linearmaps Input Inputatpositioni′+1 (b)UpdatingtheKVCacheatPositioni′+1 Figure11.2: IllustrationoftheKVcache. WeupdatetheKVcacheataposition,performthe attentionoperation,andthenmovetothenextpositiontorepeattheprocess. 11.1 Prefilling and Decoding 593 11.1.2 A Two-phase Framework Aswehaveseen,languagemodelingisastandardautoregressiveprocess,whereeachtokenis generatedoneatatime,conditionedontheprevioustokens. ForTransformers,thisrequiresthe modeltomaintainaKVcachethatstorespastrepresentations,andattendthenewlygenerated representation to this KV cache. If we think of the model Pr(y|x) from the perspective of computingtheKVcache,itisnaturaltodivideinferenceintotwophases: • Prefilling. The prefilling phase computes the KV cache for the input sequence x. It iscalledprefillingbecausethemodelpreparesandstoresthekey-valuepairsforeach token in the input before the actual inference begins. The process of prefilling in an LLMcanbeexpressedas cache = Dec (x) (11.10) kv whereDec (·)isthedecodingnetwork(i.e.,thesameasDec(·)),butitreturnstheKV kv cacheinself-attentioninsteadoftheoutputrepresentations. cacheisalist,givenby cache = {cache1,...,cacheL} (11.11) wherecachel representsthekey-valuepairsforthel-thlayer. • Decoding. ThedecodingphasecontinuesgeneratingtokensbasedontheKVcache,as illustratedinFigure11.2. Whenanewtokenisinputintothedecoder,weupdatetheKV cacheineachlayerbyaddingthenewkey-valuepair. Theupdatedcacheisthenused forself-attentioncomputation. Thetokengenerationstopswhensomestoppingcriterion ismet,suchaswhenthegeneratedtokenistheendsymbol. Thegoalofdecodingisto findthebestpredictedsequence,whichisgivenby yˆ = argmaxPr(y|cache) (11.12) y HereweusePr(y|cache)insteadofPr(y|x)toemphasizethatthedecodingprocess actuallyreliesontheKVcacheratherthanx. TheprefillinganddecodingprocessesareillustratedinFigure11.3. Notethatboththese processesareautoregressive. However,asshowninTable11.1,theydifferinseveralaspects, whichleadtoverydifferentimplementationsinpractice. Inessence,whiletheunderlyingmodelofprefillingisbasedontokenprediction,itcan beconsideredanencodingprocess. Thisisbecauseourgoalisnottogeneratetokens,butto build acontext representation(i.e., the KVcache)forthe subsequentstepsin thedecoding phase. Inthissense,itissimilartoBERT,whereweencodetheinputsequenceintoasequence of contextualized token representations. On the other hand, unlike BERT which generates bidirectionalsequencerepresentations,prefillingisbasedonstandardlanguagemodelingtasks, andisthusunidirectional. Notethat, sincetheentiresequencexisinputintothemodelall atonce,allqueriescanbepackedtogetherandtheself-attentionoperationisperformedonx 594 Chapter 11. Inference Prefilling Decoding Goal Setupinitialcontextx. Continuegeneratingtokens yaftertheinitialinput. All-at-onceVisibility Tokensinxarepresented Tokensinyarepresented allatonce. sequentially,thatis, predictingatokenrequires waitingfortheprevious tokenstobepredictedfirst. ContextUse Buildthecontextor Usethecachedkey-value encodedrepresentationof pairs(fromprefilling)to theinput. generatefurthertokens. ResourceLimitation Compute-bound Memory-bound ComputationalCost High VeryHigh Table11.1: PrefillingvsDecoding. inparallel. LetQbethequeriesthatarepackedintoonematrix. Theself-attentionmodelin prefillingcanbedefinedas QKT Att (Q,K,V) = Softmax( √ +Mask)V (11.13) qkv d whereQ,K,V∈Rd×(m+1). Mask∈R(m+1)×(m+1) isamaskthatensuresthateachtoken onlyattendstoitselfandthetokensthatprecedeitinthesequence. Itisrepresentedbysetting thevaluesinthemaskcorrespondingtofuturetokenstoalargenegativenumber,forexample, for the query q and the key k , we set the value of the entry (i,j) to −∞ if i<j. One i j advantageofprocessingthesequencewithasingleself-attentioncomputationisthatwecan make better use of the parallel computing capabilities of modern GPUs, and so speed up prefilling. In general, the prefilling process",
    "isamaskthatensuresthateachtoken onlyattendstoitselfandthetokensthatprecedeitinthesequence. Itisrepresentedbysetting thevaluesinthemaskcorrespondingtofuturetokenstoalargenegativenumber,forexample, for the query q and the key k , we set the value of the entry (i,j) to −∞ if i<j. One i j advantageofprocessingthesequencewithasingleself-attentioncomputationisthatwecan make better use of the parallel computing capabilities of modern GPUs, and so speed up prefilling. In general, the prefilling process is considered compute-bound. This is because merging multiple computational operations into one operation reduces the number of data transfersandtheperformancebottleneckusuallycomesfromthecomputationalcapacityrather thanmemorybandwidth. Decodingisastandardleft-to-righttextgenerationprocess. Thetokensequenceisgener- atedautoregressivelybypredictingonetokenatatimebasedontheKVcache. Eachtimeanew tokenisgenerated,weneedtoattendittoprevioustokens,followingEq. (11.7). Therefore, thedecodingprocessismemory-boundduetoitsfrequentaccesstotheKVcache. Thecost of decoding grows significantly as more tokens are generated. In most cases, decoding is computationallymoreexpensivethanprefilling. Notethatthisisnotjustbecause,indecoding, theLLMgeneratestokensonebyoneandrepeatedlyupdatestheKVcache. Aswewillseein thefollowingsubsection,wemayneedtoexploremultipledifferenttokensequencesduring decoding,whichmakestheproblemmorecomplexandincreasesitscostfurther. 11.1 Prefilling and Decoding 595 ··· queries q0 q1 ··· qm−1 keys k0 k1 ··· km−1 values v0 v1 ··· vm−1 ··· reyaLaninoitnetta-fleS TransformerDecoder ··· ··· ··· EmbeddingLayer x 0 x 1 ··· x m−1 Processedallatonce (a)Prefilling ··· queries qm qm+1 ··· qm+n keys k0 k1 ··· km−1 km km+1 ··· km+n values v0 v1 ··· vm−1 vm vm+1 ··· vm+n ··· reyaLaninoitnetta-fleS Pr(y1|x)Pr(y2|x,y1) ··· Pr(yn|x,y<n) SoftmaxLayer TransformerDecoder ··· ··· ··· EmbeddingLayer xm y 1 ··· y n−1 Processedstepbystep(nsteps) (b)Decoding(atthen-thstep) Figure11.3: Illustrationoftheprefillinganddecodingprocesses. Inprefilling,theentireinput sequenceisprocessedtogetherandtheKVcacheisfilled. Indecoding,theLLMgeneratesthe outputsequencestepbystepbasedontheprefilledKVcache. 596 Chapter 11. Inference 11.1.3 Decoding Algorithms So far our discussion of LLM inference has primarily focused on the model computation problem, that is, how to compute Pr(y|x). Now we turn to the discussion of the search problem. Theproblemcanbestatedas: givenanLLMPr(y|x),howdoweefficientlysearch for the best output sequence yˆ given the input sequence x (or the generated KV cache)? Naively,wecanconsideralloftheoutputsequences,computethepredictionprobabilityfor each, and then select the output sequence having the highest probability. This method can guaranteethegloballyoptimalsolution,butdirectexhaustivesearchisimpracticalforLLMs asthenumberofpossibleoutputsequencesgrowsexponentiallywiththelengthofy. Inpractice,variousheuristicsearchalgorithms,suchasgreedysearchandsampling-based search,arecommonlyemployedtoapproximatethesolution. Eachofthesemethodsoffers trade-offsbetweensearchqualityandcomputationalefficiency. Thesearchproblem,therefore, becomesabalancingactbetweenexplorationandexploitation, wherethegoalistofindan efficientstrategythatproduceshigh-qualityoutputswithoutexploringtheentirespace. Beforegivingamoredetaileddiscussionofthesemethods,letusfirstinformallydefine whatasearchspaceisandhowitisrepresented. InLLMinference,wedefineahypothesisasa tupleofinputandoutputsequences. Sincexisfixedduringinference,wecansimplyconsider eachhypothesisasanoutputsequence. Thesearchspace,denotedbyY,isthenthesetofall possiblehypotheses(i.e.,outputsequences)thatthemodelcangenerate. Thesearchproblem forLLMinferencecanbere-expressedas yˆ = argmaxPr(y|x) (11.14) y∈Y In NLP, Y is commonly represented in a tree data structure to facilitate search. Figure 11.4showsanexampleofthesearchtreeresultingfromasmallvocabulary. Inthisexample, a node represents a prefix subsequence that can be shared by many sequences. The search startswiththerootofthetree,whichcanberegardedasthebeginningofallsequencesthat canbegenerated1. Eachchildnodeextendstheprefixofitsparentnodebyaddingonetoken fromthevocabularytothesequence,alongwiththeprobabilityofpredictingthetokengiven the prefix. This process continues as each node further branches out into additional child nodes,eachrepresentinganewpossibleextensionofthesequencewithanothertoken. The searchtreethusgrowsdeeperandwider,representinganever-increasingnumberofpotential sequencesasmoretokensareappended. Thisstructureallowsustoefficientlytraversethrough possiblesequences,evaluatingeachintermsofthelog-probabilityaccumulatedoverthepath from the root to that node. For example, in Figure 11.4, the path from the root to the node 17 corresponds to the output sequence “Cats are playful.”. The prediction log-probability logPr(y|x)isthesumofthelog-probabilitiesofallthenodesonthispath. Ingeneral,thesearchtreeisorganizedaslevels,whereeachlevelconsistsofallnodesthat arethesamedistancefromtherootnode. Thus,abreadth-firstsearchoverthetreeessentially performsleft-to-rightgenerationoftokens. Nodesinthesamelevelcorrespondtosequences 1Here,sincethepredictionsinLLMsarebasedonx,wecanthinkoftherootasarepresentationofx. 11.1 Prefilling and Decoding 597 Path: node0→node3→node9→node11→node17 Output: catsareplayful. Probability: node0→0 node3→logPr(\"cats\"|x) node9→logPr(\"are\"|x,\"cats\") node11→logPr(\"playful\"|x,\"catsare\") node17→logPr(\".\"|x,\"catsareplayful\") 16 playful 1 6 17 playful playful . 2 7 11 18 . . playful cats 0 3 8 12 19 root cats cats . are 4 9 13 20 are are cats is 5 10 14 is is are 15 is Figure11.4: Asearchtreefordecoding. Ateachnode,weexpandthetreebyconsideringall possibletokens,eachleadingtoanewnoderepresentingapotentialcontinuationofthetext. Here we highlight a path through nodes 0, 3, 9, 11, and 17. The path represents the output sequence “cats are playful.”, whose log-probability can be computed by accumulating the log-probabilitiesofthesenodes. of the same length. As the search progresses, new tokens are appended to these sequences, expandingthemincrementally. LetY bethesetofthesequencesthattheLLMgeneratesatstepi. Y canbeobtainedby i i expandingeachsequenceinY withallpossiblenexttokensinthevocabularyV,givenin i−1 thefollowingrecursiveform Y = Y ×V (11.15) i i−1 whereY ×V denotestheCartesianproductofY andV (i.e.,eachsequenceinY is i−1 i−1 i−1 concatenatedwitheachtokeninV). NotethatifasequenceinY iscomplete(e.g.,ending",
    "computed by accumulating the log-probabilitiesofthesenodes. of the same length. As the search progresses, new tokens are appended to these sequences, expandingthemincrementally. LetY bethesetofthesequencesthattheLLMgeneratesatstepi. Y canbeobtainedby i i expandingeachsequenceinY withallpossiblenexttokensinthevocabularyV,givenin i−1 thefollowingrecursiveform Y = Y ×V (11.15) i i−1 whereY ×V denotestheCartesianproductofY andV (i.e.,eachsequenceinY is i−1 i−1 i−1 concatenatedwitheachtokeninV). NotethatifasequenceinY iscomplete(e.g.,ending i−1 withthe⟨EOS⟩token),itwillnotbeexpandedanyfurther. LetΨ(Y )bethesetofallcomplete i sequencesinY . Then,thesearchspacecanbeexpressedas i (cid:91) (cid:91) (cid:91) Y = Ψ(Y ) Ψ(Y ) ··· Ψ(Y ) (11.16) 1 2 nmax wheren isthemaximumlengthofasequence. max Mostdecodingalgorithmsfollowthislevel-by-levelsearchprocess. However,Y consists 598 Chapter 11. Inference of an exponentially large number of sequences, and a direct search in such a vast space is computationallyinfeasible. Therefore,practicaldecodingalgorithmsoftenrelyonstrategies toprunethesearchspaceandavoidexploringlow-qualitysequences. Forexample,ateach decodingstep,Y canbeobtainedinthefollowingway i Y = Prune(Y ×V) (11.17) i i−1 wherePrune(·)isafunctionthatselectivelyremovessequenceslesslikelytoresultinhigh- quality outcomes. In general, we expect that |Y |<<|Y |·|V|. Thus we can drastically i i−1 reducethenumberofsequencesunderconsiderationateachstep,ensuringthatthecomputa- tionalloaddoesnotgrowexponentiallywiththesequencelength. Next, we will introduce these decoding algorithms. Some of them have already been discussedinsequence-to-sequencemodels(seeChapter5),whileothersaremorecommonly usedinLLMs. 1. Greedy Decoding Greedysearch(orgreedydecoding)isoneofthemostwidelyuseddecodingmethodsinNLP, particularlyintextgenerationtaskslikemachinetranslation. Theideabehindgreedysearch is straightforward: at each step in generation, it selects the next token that has the highest prediction probability. For each sequence y=y ...y ∈Y ×V, we can evaluate it using 1 i i−1 logPr(y|x). Thislog-probabilitycanbeeasilycomputedbynotingthat logPr(y|x) = logPr(y ...y |x) 1 i = logPr(y |x) + logPr(y |x,y ) (11.18) <i i <i (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) accumulateduptotheparentnode newlycomputedforthecurrentnode Herethefirsttermisthesumofthelog-probabilitiesofthepathfromtheroottotheparent node, which has been computed in the previous decoding steps. At step i, we only need to computethesecondtermwhichisthestandardtokenpredictionlog-probabilityproducedby theLLM. The“best”tokenatstepiisthenchosenas ytop1 = argmaxlogPr(y ...y |x) i 1 i yi∈V (cid:2) (cid:3) = argmax logPr(y |x)+logPr(y |x,y ) <i i <i yi∈V (cid:124) (cid:123)(cid:122) (cid:125) fixedwrt.yi = argmaxlogPr(y |x,y ) (11.19) i <i yi∈V Thus,the“best”sequencegenerateduptostepiisgivenby ytop1 = y ...y ytop1 (11.20) 1 i−1 i 11.1 Prefilling and Decoding 599 Finally,Y containsonlythissequence i Y = {ytop1} (11.21) i ThegreedychoiceinonedecodingstepisillustratedinFigure11.5(a). Greedysearch offers computational efficiency and simplicity in implementation for LLM inference. Its primary disadvantage, however, lies in its suboptimal nature — high-quality sequences are likelyprunedatearlystagesofdecoding. Therefore,greedysearchisappealingfortasksthat demandspeedandsimplicity. Fortasksthatrequirebettersearchresults,alternativestrategies suchasbeamsearch,whichexploresmultiplepotentialpathssimultaneously,arepreferable. 2. Beam Decoding Beamsearch(orbeamdecoding)isanaturalextensionofgreedysearch. Insteadofselecting thesinglemostprobabletokenateachstep,beamsearchmaintainsafixednumberofthebest candidatesateachstep,knownasthe“beamwidth”. SeeFigure11.5(b)foranillustrationof beamsearch. LetK bethebeamwidth. Givenaparentnode,whichcorrespondstotheprefixy ...y , 1 i−1 wecanselectthetop-K nexttokensby {ytop1,...,ytopK} = argTopK Pr(y |x,y ) (11.22) i i i <i yi∈V whereargTopKisafunctionthatranksthepredictionprobabilitiesofallpossiblenexttokens andselectsthetopK candidates. Giventhesetokens,thetop-K sequencesforstepiaregiven by ytop1 = y ...y ytop1 (11.23) 1 i−1 i . . . ytopK = y ...y ytopK (11.24) 1 i−1 i Then,wecandefineY as i Y = {ytop1,...,ytopK} (11.25) i WecanadjustthebeamwidthK tobalancesearchefficiencyandaccuracy. Butaverylarge beamwidthmightnotbehelpful. Inmanypracticalapplications,selectingarelativelysmall numberforK,suchasK =2orK =4,isoftensufficienttoachievesatisfactoryperformance inLLMinference. 3. Sampling-based Decoding Bothgreedyandbeamsearchgeneratedeterministicoutputs,thatis,givenanLLM,theoutput ofthemodelwillalwaysbethesameeverytimeitprocessesthesameinput. Thedeterministic natureofgreedyandbeamsearchensurespredictabilityandreliabilityinapplicationswhere 600 Chapter 11. Inference ①Expansion ②Ranking ③Output(1-best) 5 . 6 cute Pr=.34 ok =⇒ 6 cute 6 7 cute on Pr=.32 pruned 4 7 9 is on sick Pr=.21 pruned 8 8 are are Pr=.12 pruned 9 sick 5 . Pr=.01 pruned (a)Greedysearch ①Expansion ②Ranking ③Output(K-best) 5 . 6 cute Pr=.34 ok =⇒ 6 cute 6 cute 7 on Pr=.32 ok =⇒ 7 on beamwidth(K)=3 4 is 7 on 9 sick Pr=.21 ok =⇒ 9 sick 8 8 are are Pr=.12 pruned 9 sick 5 . Pr=.01 pruned (b)Beamsearch ①Expansion ②Ranking ③Selection&Sampling ④Output 5 . 6 cute Pr=.34 ok =⇒ 6 cute Pr=.39 pruned 6 cute 7",
    "7 on Pr=.32 ok =⇒ 7 on beamwidth(K)=3 4 is 7 on 9 sick Pr=.21 ok =⇒ 9 sick 8 8 are are Pr=.12 pruned 9 sick 5 . Pr=.01 pruned (b)Beamsearch ①Expansion ②Ranking ③Selection&Sampling ④Output 5 . 6 cute Pr=.34 ok =⇒ 6 cute Pr=.39 pruned 6 cute 7 on Pr=.32 ok =⇒ 7 on Pr=.36 ok =⇒ 7 on 4 is 7 on 9 sick Pr=.21 ok =⇒ 9 sick Pr=.25 pruned 8 8 are are Pr=.12 pruned selecttop-khypotheses(k=3), renormalizetheirproababilities, 9 sick 5 . Pr=.01 pruned andselectoneviasampling. (c)Top-kSampling ①Expansion ②Ranking ③Selection&Sampling ④Output 5 . 6 cute Pr=.34 ok =⇒ 6 cute Pr=.51 pruned 6 cute 7 on Pr=.32 ok =⇒ 7 on Pr=.49 ok =⇒ 7 on 4 7 9 is on sick Pr=.21 pruned selecttop-rankedhypotheses whoseprobabilitysum≥p=0.6, 8 8 are are Pr=.12 pruned renormalizetheirproababilities, andselectoneviasampling. 9 sick 5 . Pr=.01 pruned (d)Top-pSampling Figure 11.5: Illustrations of greedy decoding, beam decoding, top-k decoding and top-p decodingmethods(inonedecodingstep). 11.1 Prefilling and Decoding 601 consistentoutcomesarecritical,suchasinformaldocumentgeneration,wherevaryingoutputs could cause confusion or errors. On the other hand, one disadvantage of these methods is the lack of diversity and flexibility. For example, in creative tasks like story generation or conversational agents, generic or repetitive outputs generated by deterministic systems are oftenlessengaging. ToaddvariationintoLLMoutputs,wecanusesampling-baseddecodingmethods. There aretwocommonlyusedmethods. • Top-k Sampling. Thismethodselectsthenexttokenfromthetop-k mostlikelycandi- datesateachstepofthegenerationprocess[Fanetal.,2018]. LetV betheselection i poolfortop-k sampling. Wecandefineitas V = {ytop1,...,ytopk} (11.26) i i i where {ytop1,...,ytopk} are the top-k tokens selected based on their prediction prob- i i abilities (see Eq. (11.22)). Once the selection pool is determined, we recompute the prediction probability distribution over V . One of the simplest ways to do this is to i renormalizetheirprobabilities: Pr(y |x,y ) i <i Pr(y |x,y ) = (11.27) i <i (cid:80) Pr(y |x,y ) yj∈Vi j <i Alternatively,wecancalculatethedistributionbyusingtheSoftmaxfunction: exp(u ) Pr(y |x,y ) = yi (11.28) i <i (cid:80) exp(u ) yj∈Vi yj whereu isthelogitfortokeny . Then,wesampleatokeny¯ fromthisdistribution: yi i i y¯ ∼ Pr(y |x,y ) (11.29) i i <i Thecorrespondingsequenceisy¯ =y ...y y¯,andY isgivenby 1 i−1 i i Y = {y¯} (11.30) i • Top-p Sampling. This sampling method, also known as nucleus sampling, follows a procedure similar to that of top-k sampling. Instead of drawing from a fixed size candidatepool,itselectsthenexttokenfromthesmallestsetoftokensthattogetherhave acumulativeprobabilityhigherthanapredefinedthresholdp[Holtzmanetal.,2020b]. Inthiswaywepreventthepredictionfromchoosingfromlow-probabilitytokensinthe longtailthatcouldleadtoincoherentornonsensicaloutputs. Toobtainthecandidate poolinthetop-psamplingmethod,wecansortalltokensbytheirpredictedprobabilities. Then,startingwiththetokenwiththehighestprobability,wecontinuetoaddtokensto thecandidatepooluntilthecumulativeprobabilityofthetokensinthepoolreachesor exceedsp(wedenotethesizeofthecandidatepoolatthispointask ). Thecandidate p 602 Chapter 11. Inference (a)β=0.1 (b)β=0.8 (c)β=2.0 Figure11.6: HistogramestimatesofthedistributionsgeneratedbytheSoftmaxfunctionwith differentvaluesofthetemperatureparameterβ. poolcanthenbeexpressedas V = {ytop1,...,y topkp} (11.31) i i i Thesubsequentsteps,suchastherenormalizationofthedistributionandsampling,are thesameasinthetop-k samplingmethod(seeEqs.(11.27-11.30)). SeeFigure11.5(c-d)forillustrationsofthetop-kandtop-psamplingmethods. Bylimiting thechoicestoasmallersetofhigh-probabilitytokens,thesemethodsstrikeabalancebetween randomness and coherence. They allow for more diverse outputs while still maintaining a reasonable level of relevance and fluency. However, the value of k or p must be carefully chosen: ifk orpistoosmall,theoutputmaystillbeoverlydeterministic(morelikegreedy decoding),andifk orpistoolarge,theLLMmightproducedegenerateoutputs. Inordertofurthercontroltherandomnessofthetokenselectionprocess,therenormalized distribution Pr(·) is typically obtained by using the Softmax function with the temperature parameter,givenby exp(u /β) Pr(y |x,y ) = yi (11.32) i <i (cid:80) exp(u /β) yj∈Vi yj Hereβ isatemperatureparameterβ thatcontrolsthesharpnessoftheprobabilitydistribution derivedfromlogits. InFigure11.6,weshowsimpleexamplesinvolvingdistributionsgenerated by the above function with different temperatures. When the temperature is set to a higher value,theresultingprobabilitydistributionbecomesmoreuniform,asthedifferencesbetween thelogitsarediminished. Thismeansthateachtokeninthecandidatepoolhasamoreequal chance of being selected, leading to greater diversity in the generated output. By contrast, whenthetemperatureissettoalowervalue,thedistributionbecomessharper,makingthehigh- probabilitytokensevenmorelikelytobechosen,whichoftenresultsinmoredeterministic outputs. Forexample,ifwesetpto1andβ toaverysmallnumber(approachingzero),the top-psamplingmethodwillbecomeequivalenttothegreedysearchmethod. 4. Decoding with",
    "exp(u /β) yj∈Vi yj Hereβ isatemperatureparameterβ thatcontrolsthesharpnessoftheprobabilitydistribution derivedfromlogits. InFigure11.6,weshowsimpleexamplesinvolvingdistributionsgenerated by the above function with different temperatures. When the temperature is set to a higher value,theresultingprobabilitydistributionbecomesmoreuniform,asthedifferencesbetween thelogitsarediminished. Thismeansthateachtokeninthecandidatepoolhasamoreequal chance of being selected, leading to greater diversity in the generated output. By contrast, whenthetemperatureissettoalowervalue,thedistributionbecomessharper,makingthehigh- probabilitytokensevenmorelikelytobechosen,whichoftenresultsinmoredeterministic outputs. Forexample,ifwesetpto1andβ toaverysmallnumber(approachingzero),the top-psamplingmethodwillbecomeequivalenttothegreedysearchmethod. 4. Decoding with Penalty Terms One common improvement to decoding methods in text generation is to modify the search objective. Forexample,onecanreplacemaximumaposteriori(MAP)decodingwithminimum 11.1 Prefilling and Decoding 603 Bayesrisk(MBR)decoding[KumarandByrne,2004b],wherethefocusshiftsfromselecting thesinglemostprobableoutputtochoosinganoutputthatminimizestheexpectedriskovera distributionofpossibleoutputs. MoredetailsonMBRdecodingcanbefoundinChapter5. Hereweexploremethodsthatincorporatepenaltytermsintodecoding. Thesemethodsoffera simplebuteffectivewaytomakedecodingmorecontrollable. Recall from Eq. (11.14) that the goal of decoding is to maximize the likelihood of the outputsequence. Withpenaltyterms,theobjectiveisextendedtoincludeadditionalfactors that penalize or reward certain behaviors in the generated text. A general form of the new objectiveisgivenby (cid:2) (cid:3) yˆ = argmax Pr(y|x)−λ·Penalty(x,y) (11.33) y∈Y wherePenalty(x,y)isafunctionthatquantifiesthedegreetowhichthegeneratedsequence yviolatescertainconstraintsorexhibitsundesirablebehaviorsgiventheinputx. Thedesign ofPenalty(·)isveryflexible,thusallowingustoincorporateawiderangeofconstraintsor priorknowledgeintoit. Below,wepresentsomecommontypesofpenaltyfunctions. • RepetitionPenalty. Arepetitionpenaltydiscouragesthemodelfromgeneratingrepeti- tiveorredundanttext. Thepenaltyfunctionmightmeasurethefrequencyofrepeated tokensorphrasesinthegeneratedsequenceandimposeapenaltyproportionaltotheir occurrence. • Length Penalty. A length penalty ensures that the generated sequence adheres to a desired length. For example, in text summarization tasks, the penalty function could penalizeoutputsthataretooshortortoolong. • Diversity Penalty. A diversity penalty promotes variation in the generated text. For example,inbeamsearch,wecanmeasurethesimilaritybetweengeneratedhypotheses, andencouragethemodeltoexploredifferenthypotheses. • Constraint-based Penalty. A constraint-based penalty enforces specific constraints related to the content or style of the generated text. For example, in machine transla- tion, the penalty function could penalize outputs that deviate from a desired tone or terminology. Ingeneral,wecanconsiderPenalty(x,y)asafunctionthatdefinesthecostofgenerating the surface form of the output sequence y given the input sequence x. Alternatively, this functioncanbedefinedtoassessthehiddenstatesofanLLMwhengeneratingy. Forexample, Su et al. [2022] develop a penalty term that calculates the maximum distance between the representationofthepredictedtokenandtherepresentationsofthepreviouslygeneratedtokens. Therefore, the search objective will penalize degenerated outputs, such as texts with many repetitions. ThemethoddescribedinEq. (11.33)isgeneralandcanbeeasilyadaptedtodifferentsearch algorithms. Forexample,ingreedysearch,wecankeepthesinglesequencethatmaximizes Pr(y|x)−λ·Penalty(x,y)ateachdecodingstep;insampling-basedsearch,wecanrankand 604 Chapter 11. Inference selectthetop-rankedsequencesbasedonPr(y|x)−λ·Penalty(x,y)toformthecandidate pool. 5. Speculative Decoding Speculative decoding stems from the concept of speculative execution, where a system makeseducatedguessesaboutfutureactionsandperformstheminadvance. Iftheguessis correct,theresultsareimmediatelyavailable,whichspeedsupprocessing. InthecaseofLLM inference,supposewehavetwomodels. Oneisasmaller,fastermodel(calleddraftmodel), andtheotheristhefull,moreaccuratemodel(calledverificationmodel). Thesetwomodels representtwobaselinesinLLMinference: thedraftmodelisefficientbutnotveryaccurate; theverificationmodelisusuallytheonewewanttorun, butitisveryslow. Givenaprefix, wefirstusethedraftmodeltospeculativelypredictasequenceoflikelyfuturetokens. This is a standard autoregressive decoding process, but it is still fast in practice due to the high efficiencyofthedraftmodel. Then,theverificationmodelevaluatesthespeculatedtokensin parallel. Itcheckswhetherthepredictedtokensarecorrectorneedtobeadjusted. Notethat, sincewecandealwiththesetokensallatonce,theverificationcanbedoneinasinglestepfor allthetokenssimultaneously,ratherthaninatoken-by-tokenmanner. Ifthespeculatedtokens arecorrect,theyareaccepted,andtheprocesscontinueswiththenextsetoftokens. Ifthey areincorrect,theincorrectspeculationsarediscarded,andtheverificationmodelisusedto generatethecorrecttokens. Tobemorespecific, letusseethespeculativedecodingmethodpresentedinLeviathan etal.[2023]’swork. Inthismethod,thedraftmodelisasmalllanguagemodel,denotedby Pr (y |x,y ),whiletheverificationmodelisanormalLLM,denotedbyPr (y |x,y ). The q i <i p i <i goalisthat,givenaprefix,weusethedraftmodeltoautoregressivelypredictuptoτ tokens. Theverificationmodelisthenemployedtogeneratethelasttokenatthepointwhereerrors begintooccurinthespeculativepredictions. Figure11.7illustratesonestepinthisdecoding process. Thespeculativedecodingalgorithmcanbesummarizedasfollows. • Giventheprefix[x,y ],weusethedraftmodeltopredictthenextτ consecutivetokens, ≤i denotedby{yˆ ,...,yˆ }. Thisisatoken-by-tokengenerationprocess,givenby i+1 i+τ yˆ = argmaxPr (y |x,y ,yˆ ...yˆ ) (11.34) i+t q i+t ≤i i+1 i+t−1 yi+t • Weevaluate{yˆ ,...,yˆ }usingtheverificationmodel,thatis,wecompute{Pr (yˆ |x,y ) i+1 i+τ p i+1 ≤i ,...,Pr (yˆ |x,y ,yˆ ...yˆ )}. Notethatwecancomputetheseprobabilitiesin p i+τ ≤i i+1 i+τ−1 parallel,andsothisverificationstepisefficient. • Wedeterminethemaximumnumberofacceptedspeculatedtokens. Inordertokeepthe notationuncluttered,wedenotePr (yˆ |x,y ,yˆ ...yˆ )andPr (yˆ |x,y ,yˆ ...yˆ ) q i+t ≤i i+1 i+t−1 p i+t ≤i i+1 i+t−1 simplybyq(yˆ )andp(yˆ ),respectively. Wethendefinethat,ifq(yˆ )≤p(yˆ ), i+t i+t i+t i+t thenweacceptthisspeculation. Bycontrast,ifq(yˆ )>p(yˆ ),werejectthisspecula- i+t i+t tionwithprobability1− p(yˆi+t) . Startingfromyˆ ,themaximumnumberofaccepted q(yˆi+t) i+1 11.1 Prefilling and Decoding 605 Context(x,y ) yˆ yˆ yˆ yˆ yˆ <i i+1 i+2 i+3 i+4 i+5 predict DraftModelPr (·) q (a)Predictthenextτ tokensgiventhecontextusingthedraftmodel(τ =5) EvaluationModelPr (·) p evaluate Context(x,y ) yˆ yˆ yˆ yˆ yˆ <i i+1 i+2 i+3 i+4 i+5 DraftModelPr (·) q (b)Evaluatethepredictedtokensusingtheevaluationmodel EvaluationModelPr (·) p accepted rejected Context(x,y )",
    "Decoding 605 Context(x,y ) yˆ yˆ yˆ yˆ yˆ <i i+1 i+2 i+3 i+4 i+5 predict DraftModelPr (·) q (a)Predictthenextτ tokensgiventhecontextusingthedraftmodel(τ =5) EvaluationModelPr (·) p evaluate Context(x,y ) yˆ yˆ yˆ yˆ yˆ <i i+1 i+2 i+3 i+4 i+5 DraftModelPr (·) q (b)Evaluatethepredictedtokensusingtheevaluationmodel EvaluationModelPr (·) p accepted rejected Context(x,y ) yˆ yˆ yˆ yˆ yˆ <i i+1 i+2 i+3 i+4 i+5 DraftModelPr (·) q (c)Determinethenumberofacceptedtokens EvaluationModelPr (·) p Context(x,y <i ) yˆ i+1 yˆ i+2 yˆ i+3 y¯ i+4 DraftModelPr (·) q (d)Predictanewtokenfollowingtheacceptedtokensusingtheevaluationmodel Figure11.7: Illustrationofonestepofspeculativedecoding. Thegoalistopredictasmany nexttokensaspossibleusingthedraftmodel. Therearefoursub-steps. Giventhecontext,we firstusethedraftmodeltopredictthenextτ tokens. Then,weevaluatethesepredictionsin parallelusingtheevaluationmodel. Next,wedeterminethemaximumnumberofpredicted tokens that can be accepted. Finally, we use the evaluation model to predict a new token followingtheseacceptedtokens. 606 Chapter 11. Inference consecutivespeculatedtokensisdefinedas (cid:26) (cid:27) p(yˆ ) i+t n = min t−1|1≤t≤τ,r > (11.35) a t q(yˆ ) i+t wherer isavariabledrawnfromtheuniformdistributionU(0,1). t • Givenn ,wekeepthespeculatedtokens{yˆ ,...,yˆ }. Wethenusetheverification a i+1 i+na modeltomakeanewpredictionati+n +1 a y¯ = argmaxPr (y |x,y ,yˆ ...yˆ ) (11.36) i+na+1 p i+ns+1 ≤i i+1 i+ns yi+ns+1 • Above, we have described one step of speculative decoding. The result sequence (includingboththecontextandpredictedtokens)isillustratedasfollows [x, y ] yˆ ...yˆ y¯ <i i+1 i+na i+na+1 Context natokens Onetoken predictedusing predictedusing thedraftmodel theverificationmodel Oncewehavefinishedthisstep,weaddthepredictedtokens{yˆ ,...,yˆ ,y¯ } i+1 i+na i+na+1 tothecontext,andrepeattheaboveprocess. Inpractice,weusuallywishtouseasmallerdraftmodelsothatpredicting{yˆ ,...,yˆ } i+1 i+na wouldbecomputationallycheaper. Butaverysmalldraftmodelislessaccurateandcanresult in smaller n . We therefore need to carefully select the draft model to make the trade-off a betweenthecomputationalefficiencyandaccuracy. 6. Stopping Criteria StoppingcriteriaareacriticalcomponentofLLMinference. Theytypicallyinvolverulesor conditions that specify when the model should stop generating text during decoding. Most LLMsaretrainedtogenerateanend-of-sequencetoken(e.g.,⟨EOS⟩or⟨/s⟩)tosignaltheend ofthegeneratedtext. Sooneofthesimpleststrategiesisthatthegenerationprocessstopswhen thistokenisproduced. Forbeamsearch,whichexploresmultiplehypothesessimultaneously, theprocesscancontinueuntilagivennumberofcompletesequenceshavebeengenerated. Inpracticalapplications,itwillgenerallybeundesirabletogenerateverylongsequences, andsoweneedtoreducethedecodingcostandunnecessaryverbosity. Onecommonly-used stoppingcriterionisthemaximumlengthoftheoutput. Themodelstopsgeneratingtextonce ithasproducedapredeterminednumberoftokens. Alternatively,wecanstopthedecoding basedontherealcost,suchasthecomputationalresourcesortimeconstraints. Forexample, inreal-timeapplicationslikechatbots,decodingmayneedtostopafteracertaintimelimitto ensureresponsiveness. Another approach is to design stopping criteria based on the behavior of LLMs. For example,decodingcanbestoppediftheprobabilityofpredictingthenexttokenfallsbelowa certainthreshold. Inadditiontoprobability-basedstopping,arepetitiondetectionmodulecan 11.1 Prefilling and Decoding 607 beimplementedtotriggerthemodeltostopifitbeginsrepeatingtokensorphrasesbeyonda predefinedlimit. Thishelpspreventredundantorincoherentoutputs. 11.1.4 Evaluation Metrics for LLM Inference EvaluatingtheperformanceofLLMsduringinferenceinvolvesavarietyofmetricstoassess how well these models meet desired standards, such as accuracy, robustness, usability, and efficiency. AswithmostNLPsystems,wecanevaluateLLMsusingaccuracy-basedmetrics, suchasperplexityandF1score. Wecanalsoexaminetheirrobustnessbytestinghowwellthey handleambiguousorchallenginginputs,includingadversarial,perturbed,orout-of-distribution data. Additionally, usability can be assessed by measuring how well the generated outputs alignwithuserexpectationsintermsoffluency,coherence,relevance,anddiversity. Human evaluatorscanratethenaturalnessofthetextorassesswhethertheresponsesarecontextually appropriateandlogicallyconsistent. Ethicalandfairnessmetricscanalsobeincludedtoensure LLMsavoidperpetuatingbiasesorgeneratingharmfulcontent. Alloftheevaluationmetricsmentionedaboveessentiallyfocusonassessingthequalityof theoutputs. GiventhehighcostofdeployingandapplyingLLMs,efficiencymetricsarealso veryimportantforpractitioners. Belowaresomecommonlyusedefficiencymetrics[Nvidia, 2025]: • RequestLatency. Thismetricmeasuresthetotaltimetakenfromwhenarequestissent totheLLMuntilthecompleteresponseisreceived. Thisincludesthetimetakenfordata transmission,processingbythemodel,andthereturnoftheoutputtotheuser. • Throughput. Itreferstothenumberoftokensorrequeststhemodelcanprocessper second. • TimetoFirstToken(TTFT).Thismetricmeasuresthetimeittakesfromthebeginning of a request being sent to the generation of the first token of the response. If data transmission does not consume too much time, then TTFT is mainly the time for prefillingandpredictingthefirsttoken. • Inter-token Latency (ITL). This metric refers to the time taken to generate each subsequenttokenafterthefirstone. Itreflectstheefficiencyofthedecodingprocess. • TokensPerSecond(TPS).Thismetricquantifiesthenumberoftokensthatthemodel cangeneratepersecond. • ResourceUtilization. Thisinvolvesmeasuringthecomputationalresourceusage(e.g., CPUandGPUutilization)andmemoryconsumptionofthemodelduringinference. Inadditiontothesemetrics,energyefficiencyandcostefficiencyarepracticalconsidera- tionsfordeployingLLMsatscale. Energyefficiencymeasurestheamountofelectricalpower consumedbythemodelduringinference. Costefficiency,ontheotherhand,evaluatesthetotal expensesrelatedtodeployingandmaintainingthemodel. Ingeneral,choosingtherightevaluationmetricsdependsonthespecifictaskandapplica- tion. Whilequality-focusedmetricsareessentialforassessingLLMs,efficiencymetricsare equally crucial for their effective deployment in real-world applications. A comprehensive 608 Chapter 11. Inference evaluation framework should include both sets of metrics to accurately estimate an LLM’s performanceandpracticality. 11.2 Efficient Inference Techniques In practical applications, we often wish a system to be as efficient as possible. For LLM inference,thistypicallyinvolvestwotypesofimprovements: reducingmemoryrequirements and accelerating the system. For example, we can modify the Transformer architecture to avoidmemoryexplosionwhenprocessingverylonginputsequences. Anotherexampleisthat wecan compressinput sequencestoreduce computationaloverheadwhile preservingtheir semanticinformation. Inaddition,techniqueslikequantizationandpruningcanbeemployed tofurtheroptimizememoryusageandinferencespeed. Efficientinferenceisawide-rangingtopicthatoverlapswithseveralsub-fieldsofLLMs, suchasarchitecturedesignandmodelcompression. Mostofthesetopicshavebeencoveredin previouschapters. Forexample,inChapter6,wediscussedefficientTransformerarchitectures; in Chapter 8, we discussed long-context LLMs; and in Chapter 9, we discussed prompt compressionmethodsforreducingpromptlength. Inthissection,wefocusontechniquesthat arecommonlyusedinLLMdeploymentandserving. 11.2.1 More",
    "LLM inference,thistypicallyinvolvestwotypesofimprovements: reducingmemoryrequirements and accelerating the system. For example, we can modify the Transformer architecture to avoidmemoryexplosionwhenprocessingverylonginputsequences. Anotherexampleisthat wecan compressinput sequencestoreduce computationaloverheadwhile preservingtheir semanticinformation. Inaddition,techniqueslikequantizationandpruningcanbeemployed tofurtheroptimizememoryusageandinferencespeed. Efficientinferenceisawide-rangingtopicthatoverlapswithseveralsub-fieldsofLLMs, suchasarchitecturedesignandmodelcompression. Mostofthesetopicshavebeencoveredin previouschapters. Forexample,inChapter6,wediscussedefficientTransformerarchitectures; in Chapter 8, we discussed long-context LLMs; and in Chapter 9, we discussed prompt compressionmethodsforreducingpromptlength. Inthissection,wefocusontechniquesthat arecommonlyusedinLLMdeploymentandserving. 11.2.1 More Caching In real-world applications, it is common practice to store frequent requests and their corre- sponding responses in a cache. When a new request hits the cache, the system can retrieve theresponsedirectlyfromthecacheinsteadofrecomputingtheresult. Onestraightforward implementation is a key-value datastore (e.g., a hash table) that maps input sequences to theirLLM-generatedoutputsequences. Inthesimplestcase,wecancollectfrequentqueries, generatetheirresponsesusingtheLLM,andstorethesequery-responsepairsinthedatastore. Thiscreatesabasicsequence-levelcachingmechanismthatallowsthesystemtobypassLLM computationwhentheinputsequenceexactlymatchesacachedquery. A straightforward extension of the caching mechanism is to cache prefixes and their correspondinghiddenstates. GivenaninputsequencexinadatasetD,wecanprocessitasin thestandardprefillingphase. Thus,weobtainasequenceofprefixesandtheircorresponding KVcachestates: x (x ) ⇒ cache 0 <1 <1 x x (x ) ⇒ cache 0 1 <2 <2 ... x x ...x (x ) ⇒ cache 0 1 m−1 <m <m where cache denotes the KV cache for the prefix x (see also Eq. (11.10)). All these <i <i mappingscanbestoredintheprefixcacheforefficientreuse. When processing a new sequence that shares a common prefix with a previously seen sequenceinD,wecanloadthecorrespondingcachedhiddenstatesinsteadofrecomputing 11.2 Efficient Inference Techniques 609 them. Specifically, if a new input x′ has x (i.e., x′ =x for some k ≤m), we can <k <k <k initializetheKVcachewithcache andonlycomputethehiddenstatesfortheremaining <k tokensx′ . ≥k Asusual,wecanmaintainakey-valuedatastorethatmapsfrequentlyencounteredprefixes to their precomputed KV caches. The lookup can be performed using a hash of the prefix tokens, allowing constant-time access to the cached states. Care must be taken to manage memoryusage,asstoringallpossibleprefixesmaybeinfeasibleforlargedatasets. Practical systemsoftenemployleastrecentlyused(LRU)cachingmethodsorotherstrategiestobalance betweencomputationalsavingsandmemoryconstraints. 11.2.2 Batching BatchinginLLMinferencereferstotheprocessofprocessingmultipleinputsequencessimul- taneouslyasagroup(calledabatch)ratherthanoneatatime. BecausemodernGPUsexcelat parallelprocessing,batchingallowsthemtocomputemultiplesequencesinasingleforward pass,keepingthehardwarefullyoccupied. Therefore,whenservingLLMsatscale,batching isimportantforimprovingcomputationalefficiencyandmaximizinghardwareutilization2. To illustrate the idea of batching, Figure 11.8 (a-b) show simple examples with batch sizesof1and4,respectively. Whenusingabatchsizeof1(i.e.,withoutbatching),theGPU processesoneinputsequenceatatime. Thus,theprocessingissequential: thenextsequence must wait for the current computation to finish. By contrast, when using a batch size of 4, the GPU can process four sequences simultaneously in a single forward pass. As the input sequencesvaryinlength,weneedtostandardizetheirlengthusingpaddingtechniques. Here weuseleftpadding,whichaddsdummytokenstothebeginningsofshortsequences,soallthe sequences in the batchwouldhave the same length forprefilling. For decoding, tokens are generatedsimultaneouslyforallthesesequences,andthegenerationprocesscontinuesuntil thelongestsequencereachescompletion. Theaboveexamplesimplyatrade-offbetweenthroughputandlatency, whichisavery importantconsiderationindesigningandimplementingLLMinferencesystems. Ifwechoose a smaller batch size, the latency would be lower, as fewer tokens need to be processed in a singlerunofinference. Imaginethatwehaveonlyonesequence. Theresultbecomesavailable immediatelyaftergenerationcompletes,withnoadditionalcomputationaloverhead. However, thislow-latencyadvantagecomesatthecostofunderutilizingparallelcomputingresources, asthe parallelism ofGPUsremainslargelyidleduring sequential processing. Onthe other hand, if we use a larger batch, we can make better use of the parallelism, as GPUs can be occupiedbylarge-scalematrixcomputations. Asaresult,wecanprocessmoretokensinthe sameperiodoftimeandthethroughputisimproved. However,sincetheresultisobtainedonly whenthelasttokeninthebatchispredicted,thelatencywouldbehigher. Inpractice, weusuallyprefertouseaslightlylargerbatch, buttrytofillthebatchwith sequences of similar lengths to reduce the number of padding tokens and improve device utilization. Forexample,wecangrouptheincominguserrequestsinashortperiodoftimeinto 2See https://docs.nvidia.com/deeplearning/performance/ dl-performance-gpu-background/index.html#understand-perfforasimpleevaluation. 610 Chapter 11. Inference prefilling decoding prefilling decoding pad 1 1 1 1 1 1 1 1 1 pad pad 2 2 2 2 2 2 2 pad pad pad 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 (a)batchsize=1 (b)batchsize=4 transfertheKVcache engine1 engine2 prefilling decoding prefilling decoding pad 1 1 1 1 1 1 1 1 1 1 1 pad 1 1 1 1 1 1 1 1 1 pad 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 3 3 3 3",
    "prefilling decoding prefilling decoding pad 1 1 1 1 1 1 1 1 1 1 1 pad 1 1 1 1 1 1 1 1 1 pad 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 4 4 5 5 5 5 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 4 4 4 5 5 5 5 5 6 6 6 6 6 6 (c)batchsize=4(similarsequencelengths) (d)disaggregationofprefillinganddecoding Figure 11.8: Illustrations of basic batching methods. We use a 2D layout to illustrate the batch,whereeachsquarerepresentsatoken. Redsquaresindicatetokensintheprefillingstage, blue squares represent tokens in the decoding stage, green squares denote padding tokens, andgraysquarescorrespondtomeaninglesstokens. Subfigures(a)and(b)comparethecases where the batch size is 1 and 4, respectively. Subfigure (c) shows the strategy of grouping sequenceswithsimilarlengthsintothesamebatch. Subfigure(d)illustratesthedisaggregation ofprefillinganddecoding. Inthisapproach,wecanmakebetteruseoftheparallelismofGPUs by concatenating multiple short sequences into a single long sequence for joint processing. Thisallowsustomaximizethenumberoftokensprocessedinabatchwhileminimizingthe number of padding tokens. However, as a trade-off, we need to copy the KV cache to the decodingengineandreorganizeitaftertheprefillingphase,whichintroducesadditionaldata transferoverhead. buckets,eachofwhichcontainssequenceswithsimilarlengths. Then,wecanfillthebatch withsequencesinthesamebucket,sothatwecanminimizewastedcomputationalresources, asillustratedinFigure11.8(c). AnotherapproachtoimplementingbatchinginLLMsistodisaggregatetheprefillingand decodingprocesses[Wuetal.,2023a;Pateletal.,2024;Zhongetal.,2024]. Forexample,we canperformprefillingononeGPU,andperformdecodingonanotherGPU.Oneadvantageof disaggregationisthatwecanrearrangetheinputsequencesinthebatchtobetterfillit,because there is no interference between prefilling and decoding. For example, we can concatenate multiple short sequences into a longer one, thus ensuring that the lengths of sequences in thebatchareasconsistentaspossible,asillustratedinFigure11.8(d). Inthisway,wecan 11.2 Efficient Inference Techniques 611 maximizethethroughputoftheprefillingphase. However,asatrade-off,weneedtotransfer the KV cache to the devices performing decoding, which also incurs extra communication overhead. Typically,thismethodrequiresahigh-bandwidth,low-latencynetworktoachieve optimalperformance. Inthissection,wewilldiscussseveralimprovementstotheabovebasicbatchingstrategies. Mostofthemarebasedonanaggregatedarchitecture,thatis,decodingandprefillingcanbe consideredasdifferentstagesofamodelexecutedonthesamedevice. 1. Scheduling ApracticalLLMinferencesystemtypicallyconsistsoftwocomponents: • Scheduler. Its primary role is to efficiently queue and dispatch tasks (i.e., input se- quences)totheinferenceenginebasedonthecurrentsystemloadandtaskpriorities. Thisofteninvolvesavarietyofbatchingstrategiesthatgroupcertainrequeststogether tomaximizeprocessingefficiencyinsomeway. • InferenceEngine. ItisresponsiblefortheactualexecutionoftheLLMs,processing thequeuedrequestsastheycomein. Asdiscussedpreviously,thisengineinvolvesboth prefillinganddecodingprocesses. This architecture is illustrated in Figure 11.9. Incorporating scheduling into batch pro- cessingprovidesaflexiblewaytooptimizeboththesystem’sthroughputandlatency,thereby achievingabetterbalancebetweenthem. Forexample,thebatchingmethodsshowninFigure 11.8(a)and(b)canbeconsideredoneofthesimplestschedulingstrategies,calledrequest- levelscheduling. Inthisstrategy,onceabatchisfilledandsenttotheengine,theprocessing oftheentirebatchcannotbeinterrupted. Theschedulerwaitsforthisbatchtobeprocessed beforehandlingthenextbatch[Timoninetal.,2022]. A more sophisticated scheduling strategy, called iteration-based scheduling, interacts withtheinferenceengineateachtokenpredictionstepratherthanatthesequencelevel. This approach allows dynamic batch adjustment during inference, as illustrated in Figure 11.10. Suchfine-grainedcontrolletsthesystemprioritizecriticaltokensorsequencesinreal-time. For instance,ifanurgentrequestarrivesatsomedecodingstep,theschedulercanaddthisrequest intothebatchsothatitcanbeprocessedasearlyaspossible. Inthefollowingsubsections,we willdiscussbatchingmethodsbasedoniteration-basedscheduling. 2. Continuous Batching Continuousbatchingisaniteration-basedschedulingmethodusedintheOrcasystem[Yu et al., 2022]. In this method, an iteration refers to either the entire prefilling procedure or a single decoding step. For example, given an input sequence x=x ...x and an output 0 m sequencey=y ...y ,therearen+1iterationsintotal: oneforprefilling,andnforgenerating 1 n the output tokens (one per token). During scheduling, the batch can be adjusted between iterations. For example, we can either add a new input sequence to the batch, or remove a complete sequence from the batch at some iteration, even if the batch processing is not yet 612 Chapter 11. Inference RequestPool x 1,x 2,x 3,... batch Inference Scheduler Engine batch(afterprocessing) Predictions y 2,y 1,y 3,... Figure 11.9: Illustration of the LLM inference architecture involving a scheduler and an inferenceengine. Eachtime,theschedulerselectsanumberofuserrequeststoformabatch andsendsittotheinferenceengine. Theschedulercaninteractwiththeinferenceengineand adjustthebatchatcertainpointsduringinference,suchasatthebeginningofbatchprocessing andatthestartofeachtokenprediction. finished. Thegeneralprocessofcontinuousbatchingincludesthefollowingsteps: • Initially,abatchiscreatedwithoneormoreinputsequences,basedonboththeinference engine’sprocessingcapacityandthecurrentuserrequests. Thebatchisthenfedintothe inferenceengine. • Theinferenceengineprocessesthebatchiterationbyiteration. Aftereachiteration,the schedulermayadjustthebatchinoneofthefollowingways: – Ifasequenceinthebatchcompletesgeneration(i.e.,generatestheend-of-sequence symbol),thatsequenceisremovedfromthebatch. – Ifanewuserrequestarrivesandtheinferenceenginehasadditionalprocessing capacity,itisaddedtothebatch. – If no sequences are added to or removed from the batch, the batch remains un- changed. • Theprocessingterminatesonlywhenallsequenceshavebeencompletedandnonew userrequestsarrive. SeeFigure11.11foranexampleofcontinuousbatching. Inthisexample,westartwithtwo userrequests,x andx . Thesetwosequencesarepackedintoabatchandsenttotheinference 1 2 engine for processing. After the engine completes two iterations, a new",
    "inferenceengine. • Theinferenceengineprocessesthebatchiterationbyiteration. Aftereachiteration,the schedulermayadjustthebatchinoneofthefollowingways: – Ifasequenceinthebatchcompletesgeneration(i.e.,generatestheend-of-sequence symbol),thatsequenceisremovedfromthebatch. – Ifanewuserrequestarrivesandtheinferenceenginehasadditionalprocessing capacity,itisaddedtothebatch. – If no sequences are added to or removed from the batch, the batch remains un- changed. • Theprocessingterminatesonlywhenallsequenceshavebeencompletedandnonew userrequestsarrive. SeeFigure11.11foranexampleofcontinuousbatching. Inthisexample,westartwithtwo userrequests,x andx . Thesetwosequencesarepackedintoabatchandsenttotheinference 1 2 engine for processing. After the engine completes two iterations, a new user request, x , 3 arrives. Atthispoint,thescheduleradjuststhebatchbyaddingx toit. Theinferenceengine 3 11.2 Efficient Inference Techniques 613 Requests Requests x1,x2,x3arrived x1,x2,x3arrived Begin Begin Iteration1 1 1 Iteration1 1 1 (prefilling) 2 (prefilling) 2 3 3 3 3 3 3 1 1 1 1 1 1 Iteration2 2 2 Iteration2 2 2 3 3 3 3 3 3 3 3 Request Request x4arrived x4arrived 1 1 1 1 1 1 onedecodingstep Iteration3 2 2 2 2 2 2 forx1,x2,x3 Iteration3 3 3 3 3 3 3 3 3 3 3 4 4 4 prefillingforx4 1 1 1 1 1 1 Iteration4 2 2 2 2 2 2 Iteration4 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 End x4isaddedto Moreiterations thenextbatch End (a)Request-levelScheduling (b)Iteration-levelScheduling Figure 11.10: Illustrations of request-level scheduling and iteration-based scheduling. In request-levelscheduling,onceabatchiscreatedandsenttotheinferenceengine,wecannot adjustthebatch. Inotherwords,schedulingonlyoccursaftertheprocessingofabatchfinishes. Initeration-levelscheduling,wecanperformschedulingduringbatchprocessing. Forexample, ifanewrequestarrivesatsomepointduringinference,wecanaddittothebatchandcontinue processing. thencontinuesprocessingtheupdatedbatch. Notethattheinferenceenginenowprocesses different sequences in different ways: x and x proceed with the decoding process (i.e., 1 2 predictingthenexttokens),whilex undergoestheprefillingprocess. Aftersometime,the 3 generationforx completes. Asithappens,twomoreuserrequests,x andx ,arrive. The 2 4 5 schedulerremovesthecompletedsequencex fromthebatchand,consideringthecurrentload 2 oftheinferenceengine,addsx tothebatch. However,x mustwaituntilanothersequencein 4 5 thebatchfinishesbeforeitcanbeadded. The idea behind continuous batching is to keep the inference engine fully utilized by 614 Chapter 11. Inference input input x1,x2 arrived batch batch 1 1 1 1 1 1 1 Scheduler 2 2 Scheduler 2 2 2 output prefillingforx1andx2. output o x n 1 e a d n e d co x d 2 i . ngstepfor (a)Iteration1 (b)Iteration2 input input x3arrived batch batch 1 1 1 1 1 1 1 1 1 1 1 Scheduler 2 2 2 2 Scheduler 2 2 2 2 2 3 3 onedecodingstepfor onedecodingstepfor output x1andx2. output x1andx2,andprefilling forx3. (c)Iteration3 (d)Iteration4 input input x4,x5 arrived continuethesecondsequence complete inthebatchwithx4 batch batch 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Scheduler 2 2 2 2 2 2 Scheduler 4 4 4 3 3 3 3 3 3 3 onedecodingstepfor onedecodingstepfor output x1,x2andx3. output x1andx3,andprefilling y2 x2’spredictioncompletes. forx4. (e)Iteration5 (f)Iteration6 Figure 11.11: Illustration of batch adjustment in continuous batching. Instead of fixing a batch of input sequences and processing them to completion (as in request-level batching), continuousbatchingdynamicallyupdatesthebatchduringinference. Thesystemcontinuously accepts and adds new requests (e.g., x and x ) into the current batch as long as there is 3 4 availablecomputecapacity. processingasmanysequencesaspossible,therebymaximizingcomputationalresourceusage. Akeydifferencebetweencontinuousbatchingandstandardbatching(seeFigure11.8)lies in the fact that, in continuous batching, prefilling and decoding can occur simultaneously across different sequences, whereas in standard batching, these two phases are performed sequentially for the entire batch. As discussed in Section 11.1.2, prefilling is considered a 11.2 Efficient Inference Techniques 615 compute-boundprocess,whiledecodingisconsideredamemory-boundprocess. Theintuition behindoverlappingprefillinganddecodingistoreduceidletimesforbothcomputationand datatransfer. Considertwomini-batches: oneforprefillingandonefordecoding. Whilethe prefillingmini-batchkeepstheGPUsoccupied,thedecodingmini-batchcanperformmemory transfersconcurrently. Anotherdifferencebetweencontinuousbatchingandstandardbatchingisthatcontinuous batching is prefilling-prioritized, while standard batching is decoding-prioritized [Agrawal et al., 2024]. In continuous batching, once the inference engine",
    "are performed sequentially for the entire batch. As discussed in Section 11.1.2, prefilling is considered a 11.2 Efficient Inference Techniques 615 compute-boundprocess,whiledecodingisconsideredamemory-boundprocess. Theintuition behindoverlappingprefillinganddecodingistoreduceidletimesforbothcomputationand datatransfer. Considertwomini-batches: oneforprefillingandonefordecoding. Whilethe prefillingmini-batchkeepstheGPUsoccupied,thedecodingmini-batchcanperformmemory transfersconcurrently. Anotherdifferencebetweencontinuousbatchingandstandardbatchingisthatcontinuous batching is prefilling-prioritized, while standard batching is decoding-prioritized [Agrawal et al., 2024]. In continuous batching, once the inference engine has spare computational resources, the scheduler will add new requests to the batch. In other words, these newly addedrequestswillbeprocessedforprefillingasearlyaspossible. Thisapproachimproves systemthroughput,butatthecostofincreasedlatency,asthenewlyaddedrequestsextendthe processingtimeofearlierones. Incontrast,instandardbatching,oncethebatchiscreated,we mustwaitforthelastsequenceinthebatchtocompletebeforeprocessingnewrequests. This ensuresrelativelylowlatency,butresultsinlowerdeviceutilizationandsystemthroughput. Itisimportanttonotethatthecostofcontinuousbatchingisthatweneedtocontinuously reorganize the batches, which involves rearranging the data in memory. Each time a new requestisadded,theschedulerneedstoreassessandoptimizethecurrentbatchstructure. This dynamicadjustmentcanincuradditionalmemoryandcomputationaloverhead,especiallywhen thebatchesarefrequentlyadjusted. Therefore,whilethismethodcanimprovethroughput,it mayalsoleadtoincreasedmemoryfragmentationand, insomecases, introduceadditional latency. 3. PagedAttention PagedAttention(orpagedKVcaching)isatechniqueusedinthevLLMsystem[Kwonetal., 2023]. Inspiredbyoperatingsystempaging,itoptimizesmemoryusageduringLLMinference —particularlyfortheKVcache—byaddressingfragmentedmemoryallocationindynamic batching scenarios with variable-length sequences. The idea behind PagedAttention is to break down large memory requirements for KV caching into more manageable \"pages\" or chunksofmemory. Inthisway,wedonotneedtostoretheKVcacheofthefullsequencein acontinuousmemory. Instead,theKVcacheisdividedintofixed-sizeblocks(analogousto memorypagesinanoperatingsystem),whichcanbenon-contiguouslyallocatedinphysical memory. OneadvantageofPagedAttentionisthatitenablesflexiblememorymanagement, supportingdynamicsequencegrowthwithoutrequiringexpensivereallocationorcopyingof largecontiguousmemoryregions. NotethatPagedAttentionisnotspecificallydesignedfor batching. Butitindeedhelpsimprovememoryefficiencyinbatchedinferencescenarios,where memorymanagementismoredemandingandcomplicated. ConsiderasimpleexampleofmemoryallocationinFigure11.12inwhichself-attention isperformedforabatchconsistingoftwosequences. Foreachsequence,weneedtoattend the current token to the key-value pairs in the KV cache of this sequence, as required by self-attention. In the standard implementation of self-attention, the KV cache is stored in a contiguous block of memory, allowing us to efficiently access this continuous memory. However, in a paged KV caching system, the KV cache is divided into smaller, fixed-size 616 Chapter 11. Inference attend KVCache Sequence1 ⟨SOS⟩ I think this moive is better than Sequence2 ⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩ I really like reading (a)Twosequencesinabatch PhysicalMemoryBlocks used than reading ⟨SOS⟩ I think this movie is better ⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩ I really like yromemdetnemgarf (b)MemoryallocationforKVcachinginstandardself-attention PhysicalMemoryBlocks used than reading ⟨SOS⟩ I think this ⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩ movie is better I really like (c)MemoryallocationforKVcachinginPagedAttention Figure11.12: IllustrationofmemoryallocationinPagedAttention. Therearetwosequencesin thebatch,asillustratedinsub-figure(a). Sincethememoryisfragmented,theKVcacheis storedinalargeunusedblockofmemoryinstandardself-attention(seesub-figure(b)),butthe fragmentedmemoryisnotused. Bycontrast,inPagedAttention(seesub-figure(c)),theKV cacheisdividedintosmallerblocksandthusfitsintofragmentedmemory. 11.2 Efficient Inference Techniques 617 memory blocks which are not necessarily contiguous. These smaller KV cache blocks can be more effectively allocated to fragmented memory regions, thereby improving memory utilization. AnotherbenefitofdistributingchunksoftheKVcacheacrossdifferentmemory blocks is that it enables parallelization of the caching process. For example, if the input sequenceislongandthememorybandwidthissufficient,itwouldbebeneficialtowriteand readthekeyandvaluevectorsofdifferentsegmentsofthesequenceinparallelacrossmultiple memoryblocks. Ingeneral,storingcontiguousdatainnon-contiguousregionscancauseissues,forexam- ple, accessing fragmented data requires additional seek time, which reduces I/O efficiency. However,whenhandlinglarge-scaledata(e.g.,performingmultiplicationonextremelylarge matrices), we typically do not process all the data at once but instead divide it into smaller blocksforblock-levelcomputation. Fromthisperspective, itisalsoreasonabletopartition the attention computation. If the paging strategy is well designed, the additional overhead in memory access can be minimal, while the improvement in memory utilization can be significant. 4. Chunked Prefilling Wehaveseenthat,initeration-levelscheduling,prefillinganddecodingfordifferentsequences can occur simultaneously. This can be seen as a prefilling-prioritized strategy which can maximize the throughput. However, one such iteration can take a long time if the input sequence is very long and the prefilling process dominates the computation. In this case, decodingforothersequenceshastowaituntiltheprefillingcompletes,leadingtoincreased latency for generating output tokens. Therefore, while prefilling-prioritized strategies are effective for maximizing hardware utilization, they may introduce significant variability in token generation latency, particularly when the system is handling a mix of long and short inputsequences. Asimplewaytoreducedecodinglatencyistomakecomputationsfordifferentsequences inthebatchcomparable. Onesuchmethodistodividesequencesintochunksandperform prefillingchunkbychunk. Thisapproach,oftenreferredtoaschunkedprefilling,processes smallerportionsofeachsequenceatatime,allowingthesystemtobetterbalancethecomputa- tionalloadacrosssequences[Agrawaletal.,2023]. Bychoosinganappropriatechunksize, wecanensurethatwhenprefillinganddecodingoverlapfortwosequences,theirprocessing withinthesameiterationtendstotakeasimilaramountoftime. Asaresult,decodingidletime isreducedandoverallthroughputisimproved. Figure11.13showsanillustrationofchunkedprefillinginafewiterations. Inthisexample, thebatchcontainstwosequences. Thewholeprefillingprocessofthefirstsequenceisdivided into three prefilling steps, giving rise to the chunks denoted P , P and P . Each chunk 11 12 13 correspondstooneiterationandcanthusoverlapwithonedecodingstep. Inthisway,during theprefillingofthefirstsequence,",
    "system is handling a mix of long and short inputsequences. Asimplewaytoreducedecodinglatencyistomakecomputationsfordifferentsequences inthebatchcomparable. Onesuchmethodistodividesequencesintochunksandperform prefillingchunkbychunk. Thisapproach,oftenreferredtoaschunkedprefilling,processes smallerportionsofeachsequenceatatime,allowingthesystemtobetterbalancethecomputa- tionalloadacrosssequences[Agrawaletal.,2023]. Bychoosinganappropriatechunksize, wecanensurethatwhenprefillinganddecodingoverlapfortwosequences,theirprocessing withinthesameiterationtendstotakeasimilaramountoftime. Asaresult,decodingidletime isreducedandoverallthroughputisimproved. Figure11.13showsanillustrationofchunkedprefillinginafewiterations. Inthisexample, thebatchcontainstwosequences. Thewholeprefillingprocessofthefirstsequenceisdivided into three prefilling steps, giving rise to the chunks denoted P , P and P . Each chunk 11 12 13 correspondstooneiterationandcanthusoverlapwithonedecodingstep. Inthisway,during theprefillingofthefirstsequence, wecan perform threedecodingsteps, ratherthanonlya singledecodingstep,asisthecaseinstandarditeration-levelscheduling. Asaresult,theidle timeofthedecodingprocessisreduced,andtheoutputtokenscanbegeneratedearlier. ChunkedPrefillingimprovesdecodingefficiencybyoverlappingprefillinganddecoding, 618 Chapter 11. Inference PrefillinginOneGo Iter.1 Iter.2 Iter.3 Iter.4 Iter.5 Sequence1 P D D D ··· 11 11 12 13 Sequence2 P D D D D ··· 21 21 22 23 24 IdleTime Thepredictionofthe secondoutputtoken isdelayed. (a)SimpleIteration-levelScheduling Chunk1 Chunk2 Chunk3 Iter.1 Iter.2 Iter.3 Iter.4 Iter.5 Iter.6 Sequence1 P P P D D ··· 11 12 13 11 12 Sequence2 P D D D D D ··· 21 21 22 23 24 25 Thesecondoutputtoken canbepredictedduring prefillingforsequence1. (b)ChunkedPrefilling Figure11.13: Comparisonofsimpleiteration-basedschedulingandchunkedprefilling. P xy denotes the y-th prefilling step for sequence x, and D denotes the y-th decoding step for xy sequencex. Insimpleiteration-basedscheduling(orprefilling-prioritizedscheduling),since prefillingistreatedasasingleiteration,D hastowaitforthecompletionoftheprefilling 22 ofsequence1. Inchunkedprefilling,theprefillingprocesscanbedividedintomultiplesteps. Thus,D canexecuteduringprefillingforsequence1(i.e.,duringP ). 22 12 butatthecostofadditionalmemoryoverheadandschedulingcomplexity. Instandardprefilling, weprocessthewholeinputsequenceonce,buildingtheKVcacheinonego. Bycontrast,in chunkedprefilling,eachchunkneedsaseparateforwardpasstocomputeitsattentionoutputs andupdatetheKVcache. Asaresult,weneedtomaintaintheKVcacheofearlychunkswhile processinglaterchunks. Thisalsocompromisestheparallelismofcompletingtheprefillingfor theentiresequenceinasinglepass. Inpractice,itisusuallypossibletobalancethroughput andlatencybychoosinganappropriatechunksize. Itisworthnotingthatthemethodsdiscussedinthissubsectioncanbroadlybecategorized aspriority-basedschedulingmethods. Inthesemethods,wecangiveprioritytocertainrequests, ortocertainprefillingordecodingsteps,sothatsystemresourcesareallocatedinawaythat better aligns with specific performance goals. As presented above, for example, we may prioritizedecodingoverprefillingtominimizetokengenerationlatency,orprioritizeprefilling over decoding to maximize overall throughput in batch-processing scenarios. Practitioners candesigncustomprioritypoliciesforspecificneedsandoperationalconstraintsinreal-world applications,suchasrequestdeadlinesandimportancelevelsdefinedbyusers. 11.2 Efficient Inference Techniques 619 11.2.3 Parallelization ParallelizationisawidelyusedapproachtoscaleupLLMinference,especiallyforlarge-scale deployments. In Chapter 7, we have discussed several common parallelization strategies toparallelizeLLMpre-training,suchasmodelparallelism,tensorparallelism,andpipeline parallelism. Wehavealsodiscussedefficientarchitecturesthatareeasytodeployindistributed computingsystems. Forexample,inMoEmodels,weassignsdifferentexpertstodifferent devices3. Onlytheactiveexpertsforagiveninputareexecuted,whichsignificantlyimproves computational efficiency while maintaining model quality. Many of these methods can be directlyappliedtoLLMinferencewithminimalmodifications. However, applying these parallelization techniques to inference poses new challenges compared to pre-training. These issues become especially pronounced in real-time or low- latencyinferencescenarios,whereloadimbalanceacrossdevicesandcommunicationoverhead cansignificantlyimpactperformance. Forexample, unlikepre-training, wherebatchescan bepreparedinadvance,inferencemusthandlevariable-lengthsequencesinrealtime. This makes it harder to maintain optimal device utilization and complicates scheduling across heterogeneous computational resources. A related issue is load balancing. When a large number of requests arrive in a short period of time, the system must efficiently distribute workloadsacrossavailabledevices. Forexample,real-worldrequeststypicallyexhibithighly variable computational demands due to differences in task types and prompt lengths. Such variabilityrenderssimplestaticloadbalancingapproachesineffective,andsoweneedtouse finer-grainedstrategiesthatcanadapttoruntimeconditions. Theproblembecomesevenmore complicatedwhenwedeploythesystemonheterogeneoushardwareandtherearestrictlatency constraints. InthedevelopmentofLLMs,parallelizationiscloselyrelatedtoLLMserving. Generally, building a high-quality LLM serving system is not a simple task — it typically requires thecombinationofmultipletechniques,suchasarchitecturaldesign,workloaddistribution, and LLM-specific hardware/software optimizations. As such, LLM serving constitutes an exceptionallybroadsubjectthatoftendemandssubstantialengineeringexpertise. Here,we willnotgointothedetailsofLLMserving. Forrelatedconceptsandtechniques,readersmay refertorelevantopen-sourcesystems(suchasvLLM4,TensorRT-LLM5 andTGI6)andpapers [Popeetal.,2023;Lietal.,2024a]. 11.2.4 Remarks WehaveconsideredmanymethodsforimprovingtheefficiencyofLLMsinthisandprevious chapters. Althoughtheseapproachesaddressdifferentissues,mostofthemessentiallyexplore trade-offsbetweenvariousperformancefactors. Oneimportanttrade-offisbetweeninference speed and accuracy. For example, techniques like quantization, pruning, and knowledge 3InLLMs,theexpertsaretypicallymodularFFNs. SoeachexpertisapartoftheFFNcomponentinthe Transformerarchitecture. 4https://github.com/vllm-project/vllm 5https://github.com/NVIDIA/TensorRT-LLM 6https://github.com/huggingface/text-generation-inference 620 Chapter 11. Inference distillation can significantly reduce computational overhead and latency but may introduce minordegradationsinmodelperformance. Conversely,preservingfullprecisionorusinglarger modelsenhancesaccuracybutatthecostofslowerinferenceandhigherresourcedemands. AnotherimportantconsiderationinLLMinferenceisthememory-computetrade-off. As in computer system design, we need to consider the balance between memory usage and computationrequiredtogeneratetheoutput. Inparticular,storingintermediateresultssuchas KVcachesduringinferencecansignificantlyreduceredundantcomputation,butatthecostof increasedmemoryusage. InKVcaching,storingpastattentionstatesavoidsrecomputation of self-attention over previous tokens, thereby reducing compute time per token. However, as the number of tokens grows, so does the memory footprint of the KV cache, especially whenprocessingverylongsequencesormultiplesequencesinparallel. Inresponse,various techniques have been developed to reduce memory consumption by partially recomputing intermediatestates. Forinstance,chunkedorwindowedattentionlimitstheattentionspantoa recentsubsetoftokens,reducingKVcachesizeatthecostofreducedcontextoradditional computeifpastinformationmustbereprocessed. Note that considering the memory-compute trade-off is a very general principle. It can beextendedbeyondattentionmechanismsandTransformerstoothercomponentsinsystem design. Anexample isthe choiceofdata precision. Usinglower-precisionformats suchas FP16orINT8canreducebothmemoryusageandmemorybandwidthrequirements,effectively alleviatingpressureonthememorysubsystem. However,lowerprecisionmayleadtonumerical instabilityorslightaccuracydegradation,requiringcarefulcalibrationorretraining. Thus,this trade-offcanalsobeseenasamemory-compute-accuracytriangle,whereimprovementsin onedimensionmaycomeattheexpenseofanother. Beyond speed, accuracy, and memory,",
    "whenprocessingverylongsequencesormultiplesequencesinparallel. Inresponse,various techniques have been developed to reduce memory consumption by partially recomputing intermediatestates. Forinstance,chunkedorwindowedattentionlimitstheattentionspantoa recentsubsetoftokens,reducingKVcachesizeatthecostofreducedcontextoradditional computeifpastinformationmustbereprocessed. Note that considering the memory-compute trade-off is a very general principle. It can beextendedbeyondattentionmechanismsandTransformerstoothercomponentsinsystem design. Anexample isthe choiceofdata precision. Usinglower-precisionformats suchas FP16orINT8canreducebothmemoryusageandmemorybandwidthrequirements,effectively alleviatingpressureonthememorysubsystem. However,lowerprecisionmayleadtonumerical instabilityorslightaccuracydegradation,requiringcarefulcalibrationorretraining. Thus,this trade-offcanalsobeseenasamemory-compute-accuracytriangle,whereimprovementsin onedimensionmaycomeattheexpenseofanother. Beyond speed, accuracy, and memory, several other dimensions also influence LLM inference efficiency. Some of these dimensions have been discussed in this chapter, while othershavenot. Hereweoutlinethemasfollows. • Throughputvs. Latency: Inlarge-scalemulti-userLLMservingscenarios,weoften aim to maximize system throughput. For example, as discussed in this section, we can batch multiple requests together to increase the number of tokens processed at thesametime. However,batchingincreaseswaitingtimeandmayleadtohigherper- requestlatency,especiallyforshortorinteractiverequests. Bycontrast,optimizingfor low latency often requires serving requests individually or in smaller batches, which underutilizes hardware resources and reduces throughput. Achieving a good balance dependsonthequality-of-servicerequirementsanduserinteractionpatterns. • Generalization vs. Specialization: General-purpose LLMs are trained to perform a wide range of tasks with a single set of parameters. While flexible, they may be less efficientoraccurateforspecifictasks. Specializedmodelscanyieldbetterperformance and lower inference costs for targeted applications. However, maintaining multiple specializedmodelsincreasessystemcomplexityandstoragerequirements. Thetrade-off betweenmaintainingasinglegeneralmodelversusmultiplespecializedmodelsisan importantsystem-leveldesignchoice. 11.3 Inference-time Scaling 621 ecnamrofreP Inference-time Scaling Fine-tuning Scaling Pre-training Scaling Compute Figure11.14: Scalingforpre-training,fine-tuningandinferencestages[Briski,2025]. • EnergyEfficiencyvs. Performance: High-performanceinferenceoftenrequiresrun- ninglargemodelsathighthroughputonpowerfulaccelerators,whichconsumescon- siderableenergy. Thismaybeproblematicforedgedeploymentsorenergy-sensitive environments. Techniqueslikemodelcompressioncanimproveenergyefficiency,but usuallywithsomedegradationinoutputqualityorincreaseinlatency. Energyconstraints thusintroduceanotherimportantdimensioninoptimizingLLMinference. 11.3 Inference-time Scaling Scalinglawscanbeconsideredoneofthefundamentalprinciplesguidingthedevelopmentof LLMs. Inpreviouschapters,wediscussedseveraltimesthatscalinguptrainingdata,model size,andcomputecaneffectivelyimprovetheperformanceofpretraining. Infact,scalinglaws alsoapplytodownstreamstagessuchasfine-tuningandinference(seeFigure11.14). Herewe considerinference-timescaling,whichhasbeenwidelyemployedbyrecentLLMstosolve complexproblems,suchascomplexmathproblems[Snelletal.,2025]. Unlikepre-trainingand fine-tuningscaling,whichfocusesonimprovingLLMsviaparameterupdates,inference-time scalingimprovesthesemodelsduringinferencewithoutfurthertraining. Thisincludesalarge varietyofmethodswhichscaleLLMsindifferentdimensions,suchasensemblingmultiple modeloutputs,increasingcontextlength,adoptingmoreaggressivedecodingalgorithms,and usingexternaltoolstoextendmodelcapabilities. Whileinference-timescalingiswide-ranging,inthissectionweconsiderthosemethods thatincorporatemorecomputeintoinference(calledinference-timecomputescaling). Hereis 622 Chapter 11. Inference alistofinference-time(test-time)computescalingmethods,organizedbycategory: • Context Scaling. It involves scaling the input or context to improve generation (or potentiallyscaletheoutput). • SearchScaling. Itinvolvesincreasingcomputationaleffortduringdecoding. • OutputEnsembling. Itinvolvescombiningmultiplemodeloutputs. • GeneratingandVerifyingThinkingPaths. ItinvolvesguidingLLMstogenerateand verifythinkingpathsforsolvingcomplexreasoningproblems. Wewilldescribethesemethodsinthefollowingsubsections. 11.3.1 Context Scaling ContextscalingimprovesLLMperformancebyextendingtheinputtothemodel. Astraight- forwardapproachistoincorporatemorehelpfulcontextduringinference,allowingthemodel toconditionitspredictionsonmorepriorinformation. Oneexampleisfew-shotprompting. It augments the context with multiple input-output examples, and so the model can learn taskbehaviorimplicitlyfromtheseexampleswithoutparameterupdates. Ontopoffew-shot prompting,wecanusechain-of-thoughtpromptingtoencouragethemodeltoproduceinter- mediate reasoning steps before final answers. Note that chain-of-thought prompting is one of the most important methods in addressing reasoning problems. By explicitly providing intermediate steps in problem-solving, we can prompt the model to break down complex tasksintosimplersub-tasks,whichisfoundtobeverybeneficialforgeneratingaccurateand interpretableoutputs. Beyond extending the prompt with examples or reasoning steps, another approach to contextscalinginvolvesdynamicallyincorporatingexternalknowledge. Thisisoftenachieved throughRAG.RAGsystemsfirstretrieverelevantdocumentsnippetsfromalargecollectionof documentsoradatabasebasedonthecurrentinput. Theseretrievedpiecesofinformationare thenaddedtothecontextprovidedtotheLLM.Thisessentiallyexpandsthecontexttoinclude timely or specialized external knowledge. By doing so, the model grounds its responses in specificknowledgefoundintheexternalsource. TheLLMthuscangenerateresponsesthat arenotonlyrelevanttotheinputbutalsofactuallyaccurateandup-to-date. However,asthecontextgrows,thesemethodsoftensufferfromtheconstraintsoffinite context window length. While model architectures and techniques (like efficient attention models)arecontinuallyevolvingtosupportlongercontexts,processingextremelylonginputs stillposeschallenges. Increasedcomputationalcostisonefactor. Morecritically,whenthe context window becomes very large, the model might struggle to attend effectively to the mostrelevantinformation(e.g.,the“lostinthemiddle”phenomenon). Therefore,effective contextscalingisnotjustaboutaddingmoreinformation,butalsoaboutstrategicallyselecting, structuring, and presenting the most pertinent information within the model’s processing capabilities. Hereweomitthedetaileddiscussionofthesemethods,astheyhavealreadybeencovered inpreviouschapters. SeeChapters8and9formoredetails,includingprompting,RAG,and 11.3 Inference-time Scaling 623 long-sequencemodelingmethods. 11.3.2 Search Scaling InLLMs,decodingisasearchprocessthataimstoefficientlyfindthebestoutputsequence giventheinputsequence. Searchscaling(ordecodingscaling)typicallyinvolvestwoaspects: scalingtheoutputlengthandscalingthesearchspace. Scaling the output length refers to increasing the number of tokens generated during inference. This is especially important in tasks that require long-form generation, such as storywriting. Morerecently,generatingoutputswithlongthinkingpathshasshownstrong performance in math problem solving and code generation. For example, encouraging the modeltogeneratelongthinkingpathsbeforeproducingthefinalanswershasbeenfoundtobe verybeneficialinperformingcomplexreasoning. Thisideahasbeenwidelyusedindeveloping recentLLMsforreasoning,suchasOpenAI[2024]’so1andDeepseek[2025]’sR1. Wewill discussmoreaboutoutputlengthscalinginSection11.3.4. Scalingthesearchspace,ontheotherhand,referstoexpandingthesetofcandidateoutput sequencesconsideredduringsearch,sothathigher-qualityoutputscanbefound. Asdiscussed inSection11.1.3,asimpleexampleisthatinbeamsearchweincreasethebeamwidthtoallow more candidate sequences to be explored in parallel at each decoding step. This increases thechanceofdiscoveringbetteroutputs,especiallyintaskswheretheoptimalsolutionisnot immediatelyapparentfromlocaldecisions. In addition to decoding algorithm adjustments, it is also possible to explore compact structurestoencodealargenumberofoutputs. Forexample,wecanconstructandnavigatea treeorgraphofreasoningsteps[Yaoetal.,2024]. Inthisparadigm,eachnoderepresentsa partialsolutionorintermediatestep,andedgesrepresenttransitionsbetweenreasoningstates. Suchstructuredsearchenablesthemodeltoconsidermultiplepathssimultaneously. Another relateddirectionisMonteCarlotreesearch-inspireddecoding,wherethemodelstochastically exploresandscoresdifferentpathsbasedonlearnedheuristicsorexternalrewardmodels. Searchscalingisaverygeneralidea,anditisoftenimplicitlyinvolvedinthedesignof searchproceduresthatexploitsearchstructure,heuristics,andmodeluncertainty. Manyofthe abovemethodshavebeendiscussedpreviously,thoughtheywerenotoriginallydevelopedwith scalingastheirprimarygoal. However,searchscalinginherentlycomeswithcomputational costs. Increasing beam width, for instance, directly translates to",
    "to be explored in parallel at each decoding step. This increases thechanceofdiscoveringbetteroutputs,especiallyintaskswheretheoptimalsolutionisnot immediatelyapparentfromlocaldecisions. In addition to decoding algorithm adjustments, it is also possible to explore compact structurestoencodealargenumberofoutputs. Forexample,wecanconstructandnavigatea treeorgraphofreasoningsteps[Yaoetal.,2024]. Inthisparadigm,eachnoderepresentsa partialsolutionorintermediatestep,andedgesrepresenttransitionsbetweenreasoningstates. Suchstructuredsearchenablesthemodeltoconsidermultiplepathssimultaneously. Another relateddirectionisMonteCarlotreesearch-inspireddecoding,wherethemodelstochastically exploresandscoresdifferentpathsbasedonlearnedheuristicsorexternalrewardmodels. Searchscalingisaverygeneralidea,anditisoftenimplicitlyinvolvedinthedesignof searchproceduresthatexploitsearchstructure,heuristics,andmodeluncertainty. Manyofthe abovemethodshavebeendiscussedpreviously,thoughtheywerenotoriginallydevelopedwith scalingastheirprimarygoal. However,searchscalinginherentlycomeswithcomputational costs. Increasing beam width, for instance, directly translates to higher memory usage and longerinferencetimes. Inpractice,thereisoftenapointofdiminishingreturns,wherefurther expansionofthesearchspaceyieldsmarginalimprovementsinoutputqualityatasignificant computational expense. Therefore, an effective strategy often involves finding an optimal balancebetweenscalingandcomputationalfeasibility. 11.3.3 Output Ensembling Ifwehavemultiplemodeloutputs,itisoftenbeneficialtocombinethemtomitigatetheimpact ofindividualmodelerrorsandsynthesizeasuperiorfinaloutput. Eachmodelmightcapture differentaspectsoftheunderlyingdatadistributionorpossessuniquestrengthsandweaknesses. Byensembling,wecanaverageoutthenoiseorrandomerrorspresentinindividualpredictions, 624 Chapter 11. Inference leading to a more stable and reliable outcome. In LLM ensembling, one of the simplest approachesistoaveragetheprobabilitydistributionsoverthenexttokenfromeachmodel,and selectthebesttokenusingthisaverageddistribution. Or,ifweregardtheproblemasadiscrete decision-makingtask,majorityvotingcanbeemployed. Moresophisticatedmethodsmight involvere-rankingcandidateoutputsgeneratedbydifferentmodelsbasedonaseparatescoring functionorevenusingameta-learnertointelligentlycombinethepredictions. The“scaling”fromoutputensemblingcomesatthecostofrunningmultiplemodelsor sampling multiple outputs. This not only increases the latency of inference but also leads to the additional complexity of managing multiple models. But the quality of outputs does notcontinuetoimproveindefinitelyasmoremodelsareadded. Insomecases,thebenefits of output ensembling may diminish as the number of component models in the ensemble exceedsacertainthreshold. Instead,thebenefitsofensemblingaregenerallygreaterwhenthe individualmodelsarediverse(i.e.,theymakedifferenterrors),eveniftherearearelatively smallnumberofcomponentmodels. Therefore,itiscommonpracticetouseasetofdiverse LLMswhichdifferintheirtrainingdata,modelarchitectures,orfine-tuningobjectives. InLLMs,“scaling”oftenimpliesmakingthings“bigger”forqualitywithmoreresources. However, in addition to scaling up the quality, scaling can mean more. It can also signify scaling up the robustness (making the system less prone to errors and more reliable) and exploration (covering a wider range of potential solutions). In output ensembling, these dimensionsarenaturallyintegrated. Forinstance,theveryactofaveragingorvotingacross different model outputs is a direct strategy to scale up robustness against individual model failures. Furthermore, by intentionally including varied models, ensembling increases the chances of discovering novel or superior solutions. In this sense, scaling is not limited to makingmodelslargerorrunningthemlonger—italsomeansstrategiesformakinginference morerobust,exploratory,andadaptive. 11.3.4 Generating and Verifying Thinking Paths Sofar,wehaveviewedinference-timescalingasageneralclassofmethodsforscalingvarious aspectsofinference,suchassequencelength,modelsize,and/orsearchstrategies. Infact,one successfulapplicationistheuseofinference-timescalingtoenhancethereasoningcapabilities ofLLMs. Aswehaveseen,thereasoningperformanceofLLMscanbeimprovedbyusing chain-of-thought methods. We can therefore make use of the chain-of-thought prompts to generateintermediatereasoningstepsandreachacorrectanswer. However,reasoningproblems areoftensocomplicatedthatwecannotobtainhigh-qualitysolutionsbyprovidingsimplechain- of-thoughtprompts. Forexample,whensolvingamathproblem,wetypicallyneedtoreason overasequenceofsteps. Ateachstep,weneedtoworkoutsomeintermediateresult,verifyit, andthendeterminewhattodonext. Thereasoningpathisnotafixedpatternbutadynamically generatedthinkingprocessthatofteninvolvestrial-and-error,backtracking,andself-correction. Thisrequiresmoresophisticatedpromptingstrategiesorsearchalgorithmstonavigatesuch complexreasoning. Inthissubsection,wefocusoninference-scalingmethodsthatgobeyond simplechain-of-thoughttoaddresscomplexreasoningproblemsmoreeffectively. Atahighlevel,methodsforscalingthereasoningofLLMscanbecategorizedintotwo 11.3 Inference-time Scaling 625 classes: • Training-freeMethods. Thesemethodsaimtoimprovereasoningcapabilitieswithout requiring any modification or retraining of the pre-trained parameters. Instead, they focusontechniquesappliedduringinference,suchassophisticatedpromptingstrategies (e.g.,chain-of-thought)andalgorithmiccontroloverthereasoningprocess(e.g.,search). • Training-based Methods. These methods involve further training or fine-tuning the model parameters to explicitly improve reasoning abilities, such as supervised fine- tuning on datasets with reasoning examples (e.g., math problems with step-by-step solutions). Inthefollowing,wefirstdiscusstraining-freemethods,andthentraining-basedmethods. 1. Solution-level Search with Verifiers Given an input sequence (e.g., a math problem), there are many possible output sequences (e.g.,solutionstotheproblem). Ifwehaveamodeltoevaluateorverifyeachsolution,wecan selectthebestone. Thisisthefundamentalprinciplebehindmethodslikebest-of-N sampling, wheremultipleoutputsaregenerated,andtheoptimalresultispickedbasedonsomeselection mechanism. Suchaselectionprocesscanbeviewedasasearchproblem,whichinvolvestwo components: • Search Algorithm. This defines the strategy used to explore the space of possible outputsequences(solutions)andgenerateasetofcandidates. Itcanrangefromsimple independentsamplingtomoresophisticatedsearchtechniquesasdiscussedinSection 11.1.3. • Verifier. Thisisamodelorfunctionresponsibleforevaluatingthequality,correctness, or utility of each candidate solution generated by the search algorithm. It provides a score, a probability, or a judgment that allows the system to select the best among thecandidates. TheverifiercanbeanotherLLM,orevenasetofpredefinedrulesor heuristics. Given an input problem x, we define that an output solution y can be represented as a sequenceofreasoningsteps: y = (a ,a ,...,a ) (11.37) 1 2 nr wherea isthei-threasoningstep,anda isthelaststepwhichshouldcontaintheanswerto i nr theproblem. SeeFigure11.15foranexampleofamulti-stepreasoningpath. Thesearchalgorithmcanefficientlygenerateasetofcandidatesolutions D = {y ,...,y } (11.38) c 1 K Then,wecanuseaverifier,whichevaluateseachsolutionbythefunctionV(y),toscore 626 Chapter 11. Inference Express(5−4i)−2(3+6i)asa x Problem(x) complexnumber.Thinkstepbystep. Nowwedistributethe2totheterms Step1(a 1) intheparenthesis.2(3+6i)=6+12i. So(5−4i)−2(3+6i)isequivalentto Step2(a 2) (5−4i)−(6+12i). Nowwesubtracttheterms. x Solutionwith Step3(a 3) 5−4i−6−12i=−1−16i. AReasoningPath That’stheanswer. x Step4(a 4) Youcanalsowriteitas−1−16i. x Step5(a 5) Answer −1−16i. Figure11.15: Illustrationofmulti-stepreasoning. ThisexampleisfromthePRM800Kdataset [Lightmanetal.,2024]. Givenamathproblem,theLLMispromptedtogenerateathinking path (or reasoning path) consisting of several reasoning steps. Each step",
    "Then,wecanuseaverifier,whichevaluateseachsolutionbythefunctionV(y),toscore 626 Chapter 11. Inference Express(5−4i)−2(3+6i)asa x Problem(x) complexnumber.Thinkstepbystep. Nowwedistributethe2totheterms Step1(a 1) intheparenthesis.2(3+6i)=6+12i. So(5−4i)−2(3+6i)isequivalentto Step2(a 2) (5−4i)−(6+12i). Nowwesubtracttheterms. x Solutionwith Step3(a 3) 5−4i−6−12i=−1−16i. AReasoningPath That’stheanswer. x Step4(a 4) Youcanalsowriteitas−1−16i. x Step5(a 5) Answer −1−16i. Figure11.15: Illustrationofmulti-stepreasoning. ThisexampleisfromthePRM800Kdataset [Lightmanetal.,2024]. Givenamathproblem,theLLMispromptedtogenerateathinking path (or reasoning path) consisting of several reasoning steps. Each step addresses a sub- problem based on the results of the previous steps. The answer to the original problem is containedinthelaststep. thecandidatesinD . Thefinaloutputisthebestcandidateselectedbytheverifier c yˆ = argmaxV(y) (11.39) y∈Dc Althoughverifyingtheentirereasoningpathispossible,asimpleralternativeistoverify onlythefinalreasoningstep. InthiswaytheverifierfunctionV(y)issimplifiedtodepend solely on the final answer contained within a . This can be achieved in various ways, nr dependingonthenatureoftheproblemandtheexpectedanswerformat. • Forsomemathandcodingproblems,wecanuseoff-the-shelftoolsasverifiers. Examples includeproofcheckersformathematicaltheorems,interpretersorcompilersforcode execution,andunittestsystemsforverifyingprogramcorrectnessagainstpredefined testcases. • Ifthereislabeleddataforevaluatingtheanswer,suchashumanpreferencedata,wecan trainarewardmodelonsuchdata. Thelearnedrewardmodelisthenusedastheverifier 11.3 Inference-time Scaling 627 whichassignsascalarscoretoeachcandidateanswer. • Iftherearenoexistingsystemsorsuitablerewardmodels,wecanuseanotherLLMto actastheverifier. ThisLLMispromptedtoassessthequalityofthecandidateanswer. It could potentially be a more capable model, or the same LLM used with a specific “evaluator”prompt. • Alternatively, simpler heuristic-based verifiers can be designed. A commonly used approach is to employ majority voting, where the most frequently occurring answer amongasetofcandidatesisselected. Basedontheseverifiers,wecansearchtoobtainasetofcandidatesolutionsforselection. Onesimplestrategy,whichisoftenreferredtoasparallelscaling[Brownetal.,2024;Snell etal.,2024], involvesgeneratingK candidatesolutionsbyrunningthebaseLLMK times independently. In this process, we can adjust the temperature in sampling to control the diversityintheoutputs. TheverifierthenassesseseachoftheseK completesolutions,and theonewiththehighestscoreisselectedasthefinaloutput. Thisisconceptuallyverysimilar to best-of-N sampling, which in previous chapters we primarily described as a method of selectingthebestonefromasetofsampledoutputsusingarewardmodel. Anotherapproachissequentialscaling,whichbuildsasequenceofsolutionsincrementally [Gouetal.,2024;Zhangetal.,2024]. ItstartswithaninitialsolutiongeneratedbytheLLM withprompting. Then,weuseaverifier(oftenthesameLLM)toevaluatethesolution. This can be seen as a critique stage. The output of this stage is some form of feedback, such as textualcritiquespinpointingerrorsorsuggestingimprovements,numericalscoresreflecting solutionquality,orevenarevisedplanorintermediatesteptoguidethenextgeneration. This feedback, along with the original problem and the current solution, is then used to prompt theLLMtogenerateapotentiallyimprovedsolution. Thiscanbeseenasarefinestage. This critique-refinecyclecanberepeated,forminganiterativeloop: y = Refine(x,y ,Feedback(y )) (11.40) k+1 k k whereFeedback(y )representsthefeedbackfromtheverifier. TheRefine(·)functiongen- k erates the improved solution y by prompting the LLM with the original problem x, the k+1 previous solution y , and this feedback. The process can be iterated for K times, or until k the solution quality, as assessed by the verifier, converges to a satisfactory level. This iter- ative framework, where a solution is progressively improved through cycles of generation, evaluation(critique),andrevision,ispreciselywhatconstitutesself-refinement[Shinnetal., 2023;Madaanetal.,2024]. Insuchscenarios,theroleoftheverifierisnotjusttopickthebest completesolutionfromastaticset,buttoactivelyguidethegenerationprocessitself. See Figure 11.16 for illustrations of parallel scaling and sequential scaling. Note that thereareotherwaystoperformsearchandobtaindifferentsetsofcandidatesolutions. One alternativemethodistoorganizesearchasatreestructure. Thisapproach,oftenreferredtoas treesearch,providesamorestructuredwaytoexplorethespaceofpossiblereasoningpaths. In solution-levelsearch,eachnodeofthetreerepresentsacompletesolution. Duringsearch,we needtoexpandanodetoasetofchildnodes,representingnewsolutionsthatcanbeconsidered 628 Chapter 11. Inference Solution1 Sampling y 1 Problem Solution2 x y 2 Solution3 y 3 (a)ParallelScaling Self-refinement Problem Solution1 Solution2 Solution3 x y y y 1 2 3 (b)SequentialScaling Figure11.16: Illustrationsofparallelscalingandsequentialscaling. Inparallelscaling,we obtain multiple solutions by running the LLM several times independently. In sequential scaling,theLLMgeneratesaninitialsolution. Then,weusetheLLMtorefineititeratively, witheachrefinementyieldinganew,possiblybettersolution. inverification. Theexpansionprocesstypicallyinvolvestakinganexistingsolution(theparent node)andusingtheLLMtogeneratevariationsoralternativesolutions. 2. Step-level Search with Verifiers Whilethemethodsdiscussedaboveprimarilyfocusongeneratingcompletesolutionsbefore final selection, the search process can also be integrated more deeply into the step-by-step generationofthereasoningpathitself. Thisleadstoapproachesthatperformstep-levelsearch withverifiers,whereguidanceorpruningoccursatintermediatereasoningsteps{a ,...,a } 1 n k ratherthanonlyafterafullsolutionyisformed. Suchfine-grainedcontrolisparticularlybeneficialforcomplexreasoningproblemswhere asingleincorrectintermediatestepcanrendertheentiresubsequentreasoningchaininvalid. Byevaluatingorguidingthegenerationateachintermediatestep,theLLMcanexplorethe reasoningspacemoreeffectively,potentiallypruningunpromisingpathsearlyorallocating moreresourcestoexploremoreplausibleones. Step-level search with verifiers can also be modeled as a tree search problem. In this paradigm, each node (or state) corresponds to a partial reasoning path, a = (a ,...,a ), ≤i 1 i representingthesequenceofireasoningstepstakensofar(i.e.,apathfromtherootnodeto thecurrentnode). Theobjectiveofthesearchprocessistoexploretheunderlyingstatespace, startingfromaninitialemptypath,tofindacompletepaththatconstitutesacorrectsolution. Note that we use a here to represent a partial reasoning path instead of y . While this ≤i ≤i makesnotationabitinconsistentwiththatusedforrepresentingcompletesolutions(y)orfull 11.3 Inference-time Scaling 629 pathsinsolution-levelsearch,itservestohighlightthefocusonindividualactionsorsteps. Thecorecomponentsofstep-levelsearchwithverifiersare: • NodeRepresentation. Anodeisapartialreasoningpatha =(a ,...,a ). Theroot ≤i 1 i nodeisanemptypath,andterminalnodesarecompletereasoningpaths. • Node Expansion. Given",
    "), ≤i 1 i representingthesequenceofireasoningstepstakensofar(i.e.,apathfromtherootnodeto thecurrentnode). Theobjectiveofthesearchprocessistoexploretheunderlyingstatespace, startingfromaninitialemptypath,tofindacompletepaththatconstitutesacorrectsolution. Note that we use a here to represent a partial reasoning path instead of y . While this ≤i ≤i makesnotationabitinconsistentwiththatusedforrepresentingcompletesolutions(y)orfull 11.3 Inference-time Scaling 629 pathsinsolution-levelsearch,itservestohighlightthefocusonindividualactionsorsteps. Thecorecomponentsofstep-levelsearchwithverifiersare: • NodeRepresentation. Anodeisapartialreasoningpatha =(a ,...,a ). Theroot ≤i 1 i nodeisanemptypath,andterminalnodesarecompletereasoningpaths. • Node Expansion. Given a current partial path a , the LLM is used to generate one ≤i (1) (M) or more candidate next reasoning steps {a ,...,a }. Each candidate step, when i+1 i+1 (j) appendedtoa ,formsanewpotentialpartialpatha =(a ,...,a ,a ). ≤i ≤i+1 1 i i+1 • Verification. The verifier V(·) evaluates the quality of a newly generated step in the context of the current partial path a =(a ,...,a ) and the original problem x. As ≤i 1 i with solution-level verification, step-level verifiers might output a numerical score, a categoricallabel,andtextualfeedback. • Search. Thisgovernshowthesearchspaceisexplored. Basedontheevaluationsfrom theverifier,thesearchstrategydecideswhichpartialpathstoextendfurther,whichto prune,andtheorderofexploration. Thisstep-by-stepverificationallowsfordynamicadjustmentstothereasoningprocess. Ifa stepa isdeemedincorrectorunpromisingbyV(·),thesearchalgorithmcanbacktrackand i+1 explorealternativestepsfroma ,orevenfromanearliernodea (wherei′<i). Conversely, ≤i ≤i′ ifastepishighlyrated,resourcescanbefocusedonextendingthatpath. SeeFigure11.17for anillustrationofstep-levelsearchwithverifiers. Clearly,thissearchframeworkisverysimilartothatusedindecodingmethodsforLLMs, asdiscussedinSection11.1.3. Forexample,beamsearchmaintainsasetofK mostpromising partialsequencesateachgenerationstep. Thisisaformofstep-levelsearchwherethe“verifier” isimplicitlytheLLM’sownprobabilitymodel,andthe“search”isthepruningmechanismto maintainthebeamsize. However,step-levelsearchwithexplicitverifiers,asdescribedhere,presentsdifferences fromstandarddecoding. Oneofthemisthatthe verifiercanbea muchmoresophisticated componentthanjusttherawoutputprobabilitiesofthegenerativeLLM.Thedesignofstep- levelverifiersbasicallyfollowsthatofsolution-levelverification. Astep-levelverifiermight bealanguagemodelthatassessesthequalityofanindividualreasoningstepwithinthecontext oftheprecedingpath. ThisLLMcanevenbefine-tunedtoenhanceitsverificationcapability. Alternatively,fordomainswithwell-definedrules,itcouldbeasymbolicengineorasetof programmaticchecks. Furthermore,verifierscanbedesignedtopredictthefutureutilityor likelihoodofsuccessgiventhecurrentpartialpath,drawinginspirationfromvaluefunctions inreinforcementlearning. Humanexpertisecanalsobeincorporatedtoprovidejudgmentson criticalsteps,especiallyinhigh-stakesscenarios. Oneexampleofsuchastep-levelverifier,particularlywhenusinghumanfeedbacktoassess intermediateprogress,istheprocessrewardmodel(PRM).APRMistypicallyaseparate languagemodeltrainedtooutputascalarrewardforeachreasoningstepa withinapartial i′ patha . Itprovidesamoredirectandfine-grainedsupervisorysignalcomparedtooutcome ≤i rewardmodels(ORMs)whichonlyevaluatethefinalsolution. However,thedevelopment 630 Chapter 11. Inference InputProblem(Root) x PrunedbyLikelihood (1) (2) (3) (4) (5) a a a a a 1 ⊠ 1 ⊠ 1 ⊠ 1 1 ⊠ PrunedbyVerification (1) (2) (3) (4) (5) a a a a a 2 ⊠ 2 ⊠ 2 2 ⊠ 2 ⊠ SelectedReasoningStep (1) (2) (3) (4) (5) a a a a a 3 3 ⊠ 3 ⊠ 3 ⊠ 3 ⊠ (1) (2) (3) (4) (5) a a a a a 4 ⊠ 4 ⊠ 4 4 ⊠ 4 ⊠ (j) Figure11.17: Illustrationofstep-levelsearchwithverifiers. a =thej-thcandidateforthe i i-threasoningstep,⊠=candidateprunedbytheLLM’soutputprobability,and⊠=candidate pruned by the verifier. Given the input problem as the root node, we expand the tree by generatingmultiplereasoningstepsateachexpansion. Eachcandidatecanbeprunedbyeither likelihood(asinstandarddecoding)orstep-levelverification. Theunprunedcandidatesare then expanded to generate further reasoning steps. The process is iterated until a complete reasoning chain leading to a final answer is generated, or until a predefined search limit is reached. ofPRMsreliesonstep-levelhumanannotations,suchaspreferencesondifferentnextsteps. Collecting supervision for each intermediate step is considerably more labor-intensive and requiresgreatercognitiveeffortfromhumanannotatorsthansimplylabelingfinaloutcomes. Onealternativeapproachtodevelopingtrainingdataforstep-levelverificationistouse LLMstogeneratesuchannotationsautomatically. Forexample,wecantakeastrongLLM, referredtoasateachermodel,andpromptittofirstgenerateacompletereasoningpathfor agivenproblem. Then,ateachintermediatestepwithinthispath,wecanpromptthesame teacherLLM(oranothercapableLLM)togenerateseveralalternativecandidatenextsteps in addition to the one it originally chose. The teacher LLM can then be prompted again to evaluatethesealternatives. Theseevaluationresults(e.g.,correctvs. incorrect)canthenserve asdataannotations. Alternatively,thegeneralizationcapabilitiesofPRMscanbeleveraged. WecantrainaPRMontaskswherestep-levelverificationiseasierandthengeneralizethis PRMtoothertaskswithlittleornoadditionaltraining. 11.3 Inference-time Scaling 631 Notethatstep-levelverificationalsocomeswithitsownproblems. Frequentverification, especially if using an LLM as the verifier, can substantially increase computational costs and latency. The design of effective step-level verifiers is non-trivial itself. An inaccurate verifiermightprematurelydiscardgoodreasoningpathsorfailtoidentifyflawedones,thereby misleading the search. This makes the development of such systems more complex and difficult. 3. Encouraging Long Thinking Sofarinthissubsection,mostofthemethodsareimplicitlybasedonasimpleidea: generating longer reasoning paths can help. In addition to CoT and search with verifications, we can consideralternativemethodstoachievethis. Forexample,wecanprompttheLLMbyexplicitly askingforextendeddeliberation. Beyonddirectprompting,wecanalsomakemodifications tothedecodingprocessitself,suchasadjustingtokenlimitsorapplyingpenaltiesforshort outputs. Another approach is to employ multi-stage generation schemes where the model incrementallybuildsuponitsreasoning. 4. Training-based Scaling As well as considering inference-time scaling methods without training, we also wish to consider methods that can improve intrinsic reasoning capabilities of LLMs by modifying theirparametersthroughfurthertraining. Whilesuchtraining-basedscalingmethodstypically requireadditionaltrainingcostandcomputationalresources,theyinstillstrongerreasoning skillsdirectlyintothemodelparameters,whichinturncanleadtomoreeffectiveandefficient reasoning performance. We can even combine them with training-free methods",
    "employ multi-stage generation schemes where the model incrementallybuildsuponitsreasoning. 4. Training-based Scaling As well as considering inference-time scaling methods without training, we also wish to consider methods that can improve intrinsic reasoning capabilities of LLMs by modifying theirparametersthroughfurthertraining. Whilesuchtraining-basedscalingmethodstypically requireadditionaltrainingcostandcomputationalresources,theyinstillstrongerreasoning skillsdirectlyintothemodelparameters,whichinturncanleadtomoreeffectiveandefficient reasoning performance. We can even combine them with training-free methods for better inference-timescalingresults. Although our discussion here is restricted to reasoning problems, methods for training- basedscalingarecommon. MostofthemhavebeendiscussedinChapter10. Here,wewill briefly describe how these methods can be applied to improving inference-time scaling for reasoningproblems. • Fine-tuningonReasoningData. Oneofthemostdirectwaystoenhancereasoning isbyfine-tuningpre-trainedLLMsondatasetsspecificallycuratedforreasoningtasks. Thesedatasetscanrangefromsimpleinput-outputpairstomorestructuredformatsthat includestep-by-stepreasoningprocesses. Typicalexamplesincludedatasetsofmath wordproblems,logicaldeductionexercises,orcodegenerationwithexplanations. By trainingonsuchdata,themodellearnsfromcommonreasoningpatterns,andthuscan generatedetailedandcoherentreasoningpathsattesttime. • ReinforcementLearningforReasoning. Ifweregardaverifierasarewardmodel,we canseethatthemethodsdiscussedintheprevioussubsectionareadirectapplication oftherewardmodeltoreasoningproblems, thoughtheyaretraining-free. Ofcourse, wecanapplythisrewardmodeltoLLMfine-tuning. Thisfollowsastandardparadigm ofreinforcementlearning. Givenarewardmodel,theLLM,actingasapolicy,isfine- tuned using reinforcement learning algorithms. The LLM generates reasoning steps 632 Chapter 11. Inference orfullsolutions,receivesfeedback(rewards)fromtherewardmodel,andupdatesits parameters to produce outputs that maximize these rewards. This process aligns the LLMoutputwithnotionsofhigh-qualityreasoning,therebyencouragingtheLLMto generatemorereliablereasoningpaths. Anotherkeyissueisthetrainingofthereward model. Generally,thisrewardmodelcouldbeanoutcomerewardmodelthatevaluates thecorrectnessorqualityofthefinalanswer,oraprocessrewardmodelthatassesses thequalityofeachintermediatereasoningstep,asdiscussedinthecontextofstep-level verifiers. In some cases, we can even develop a reward model based on simple rules, suchasgivingbonusestolongeroutputs. • Knowledge Distillation for Reasoning. In this approach, a smaller, more efficient studentLLMistrainedtomimicthereasoningoutputsorinternalrepresentationsofa larger,morecapableteacherLLM.Theteachermodelmightgeneratedetailedreasoning stepsforavarietyofproblems. Thestudentmodelthenlearnstoreproducethesehigh- qualityreasoningdemonstrations. Thisstrategymakesstrongerreasoningcapabilities more accessible by deploying them in smaller models that are less computationally expensiveatinferencetime. • IterativeRefinement. Training-basedscalingcanalsoinvolveiterativerefinement. For example,anLLMcangeneratesolutionstoasetofproblems. Thesesolutionsandtheir reasoningpathsarethenverified,eitherbyhumansorautomaticverifiers. Thecorrect reasoning paths are subsequently added to the training data, and the LLM is further fine-tunedonthisaugmenteddataset. ThiscreatesacyclewheretheLLMprogressively improvesitsreasoningcapabilitiesthroughrepeatedgeneration,critique,andlearning. Theprimaryadvantageofthesetraining-basedscalingmethodsisthattheyendowtheLLM withstrongerinherentreasoningskills. Thisdirectlycontributestoimprovedinference-time scalinginseveralways: itcanleadtomoreefficientinference,astheLLMmightrequireless extensive search or fewer generation samples to arrive at a correct solution. Moreover, the basequalityofgeneratedstepsorsolutionsishigher. Therefore,awell-trainedLLMmight generalize its learned reasoning abilities to novel problems more effectively than an LLM relyingsolelyonin-contextlearningortraining-freeinferenceschemes. On the other hand, training-based approaches also present challenges, compared to the training-free counterparts. The creation of high-quality, large-scale training datasets for reasoningcanbeexpensiveandlabor-intensive. Thefine-tuningprocessitself,particularlyfor thelargestLLMsorwhenusingRL,canbecomputationallyintensiveandrequiresubstantial engineering effort. There is also the risk of the model overfitting to the specific types of problemsorreasoningstylespresentinthetrainingdata,potentiallylimitingitsperformance onout-of-distributiontasks. 11.4 Summary In this chapter, we have discussed the inference issue for LLMs. We have presented the prefilling-decodingframeworkandrelateddecodingalgorithmsforLLMinference. Then,we 11.4 Summary 633 havedescribedseveraltechniquesforefficientinference. Wehavealsodiscussedinference- time scaling, which has been considered one of the most important methods for improving LLMreasoning. Inference over sequential data has long been a concern in AI [Wozengraft and Reiffen, 1961;Viterbi,1967;Forney,1972]. InthecontextofNLP,thislineofworkdatesbacktothe veryearlydaysofspeechrecognitionandstatisticalmachinetranslation[Koehn,2010],where researchersfacedthechallengeofefficientlysearchingvasthypothesisspacestofindthemost probableoutputsequence. Techniqueslikebeamsearchandvariouspruningstrategieswere developedthentomakethiscomputationallytractable. Atthattime,modelswererelatively weak,andmuchoftheresearchfocusedondevelopingpowerfulsearchalgorithmstoreduce searcherrors. Thesefoundationalideascontinuetoinfluencemodernapproaches. Asweentertheeradominatedbydeeplearningmethods,modelsbasedondeepneural networkshavebecomeextremelypowerful. Evenwithverysimplesearchalgorithms,these modelscanachieveexcellentresults. Inthiscontext,inferencenolongerseemsas“important” asitoncewas,andresearchattentionhasgraduallyshiftedtowardmodelarchitectures,training methods,andscalingupmodels. However,historytendstorepeatitself. WiththeriseofLLMs,inferencehasonceagain attractedsignificantattention. Thisrenewedfocusisprimarilymanifestedintwoaspects: • The inference cost for LLMs is very high. For example, efficiently deploying LLMs in high-concurrency, low-latency scenarios remains a challenging problem, making inferenceefficiencycriticallyimportant. Inthiscontext,efficientarchitecturedesigns, optimizedsearchalgorithms,andvariousinferenceoptimizationstrategiesholdsubstan- tialpracticalsignificance. • Input and output sequence lengths have significantly increased in complex tasks. Es- peciallyintaskslikemathematicalreasoning,thegrowthofsequencelengthsfurther highlightstheimportanceofinferenceefficiency. Moreover,scalingtheinferencepro- cesshasrecentlyproventobeaneffectivewaytoimprovethereasoningcapabilitiesof models. Therefore,achievingefficientinferencescalingisemergingasaparticularly promisingresearchdirection. Inferenceisnowawide-rangingtopicthatencompassesmanytechniques. Itinvolvesnot only the development of model architectures and decoding algorithms, but is increasingly shapedbytheintricateengineeringandsophisticatedsystems-leveloptimizationsrequiredto deployLLMseffectivelyandefficiently. ManyofthesetechniquesarebeyondthescopeofNLP oraspecificAIarea. Instead,thefrontierofLLMinferenceoptimizationnowextendsdeeply intodomainstraditionallyconsideredcorecomputerscienceandengineering. Thissystemic perspectivehasbroughtmanynewideastothestudyofinferenceproblems. Unfortunately, thischaptercannotcoverallrelevanttechniques—indeed,thatwouldbeanalmostimpossible taskinitself. Ultimately,thebestwaytobetterunderstandandmasterthesetechniquesmay stilllieinhands-onpractice. Bibliography [Ackley et al., 1985] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithmforboltzmannmachines. Cognitivescience,9(1):147–169,1985. [AcsandKornai,2016] JuditAcsandAndrásKornai. Evaluatingembeddingsondictionary-based similarity. InProceedingsofthe1stWorkshoponEvaluatingVector-SpaceRepresentationsforNLP, pages78–82,2016. [Adi et al., 2016] Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grainedanalysisofsentenceembeddingsusingauxiliarypredictiontasks. InProceedingsof InternationalConferenceonLearningRepresentations,2016. [Agirre et al., 2009] Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalová, Marius Pasca, and Aitor Soroa. A study on similarity and relatedness using distributional and wordnet-based approaches. InProceedingsofHumanLanguageTechnologies: The2009AnnualConferenceofthe NorthAmericanChapteroftheAssociationforComputationalLinguistics,pages19–27,2009. [Agrawaletal.,2023]",
    "[Adi et al., 2016] Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grainedanalysisofsentenceembeddingsusingauxiliarypredictiontasks. InProceedingsof InternationalConferenceonLearningRepresentations,2016. [Agirre et al., 2009] Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalová, Marius Pasca, and Aitor Soroa. A study on similarity and relatedness using distributional and wordnet-based approaches. InProceedingsofHumanLanguageTechnologies: The2009AnnualConferenceofthe NorthAmericanChapteroftheAssociationforComputationalLinguistics,pages19–27,2009. [Agrawaletal.,2023] AmeyAgrawal,AshishPanwar,JayashreeMohan,NipunKwatra,BhargavS Gulavani,andRamachandranRamjee. Sarathi: Efficientllminferencebypiggybackingdecodes withchunkedprefills. arXivpreprintarXiv:2308.16369,2023. [Agrawaletal.,2024] AmeyAgrawal,NitinKedia,AshishPanwar,JayashreeMohan,NipunKwatra, BhargavGulavani,AlexeyTumanov,andRamachandranRamjee. Taming{Throughput-Latency} tradeoff in {LLM} inference with {Sarathi-Serve}. In 18th USENIX Symposium on Operating SystemsDesignandImplementation(OSDI24),pages117–134,2024. [Ainslieetal.,2020] JoshuaAinslie,SantiagoOntanon,ChrisAlberti,VaclavCvicek,ZacharyFisher, PhilipPham,AnirudhRavula,SumitSanghai,QifanWang,andLiYang. Etc: Encodinglongand structuredinputsintransformers. InProceedingsofthe2020ConferenceonEmpiricalMethodsin NaturalLanguageProcessing(EMNLP),pages268–284,2020. [Ainslieetal.,2023] JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-headcheckpoints. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages4895–4901,2023. [Akhbardehetal.,2021] FarhadAkhbardeh,ArkadyArkhangorodsky,MagdalenaBiesialska,Ondˇrej Bojar,RajenChatterjee,VishravChaudhary,MartaR.Costa-jussa,CristinaEspaña-Bonet,Angela Fan,ChristianFedermann,MarkusFreitag,YvetteGraham,RomanGrundkiewicz,BarryHaddow, LeonieHarter,KennethHeafield,ChristopherHoman,MatthiasHuck,KwabenaAmponsah-Kaakyire, JungoKasai,DanielKhashabi,KevinKnight,TomKocmi,PhilippKoehn,NicholasLourie,Christof Monz,MakotoMorishita,MasaakiNagata,AjayNagesh,ToshiakiNakazawa,MatteoNegri,Santanu Pal,AllahseraAugusteTapo,MarcoTurchi,ValentinVydrin,andMarcosZampieri. Findingsof 636 BIBLIOGRAPHY the2021conferenceonmachinetranslation(WMT21). InProceedingsoftheSixthConferenceon MachineTranslation,pages1–88,2021. [Akyüreketal.,2023] EkinAkyürek,DaleSchuurmans,JacobAndreas,TengyuMa,andDennyZhou. Whatlearningalgorithmisin-contextlearning? investigationswithlinearmodels. InProceedingsof TheEleventhInternationalConferenceonLearningRepresentations,2023. [Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisitingneuralscalinglawsinlanguageandvision. AdvancesinNeuralInformationProcessing Systems,35:22300–22312,2022. [Alayracetal.,2022] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr, YanaHasson,KarelLenc,ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,Eliza Rutherford,SerkanCabi,TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,RicardoBarreira,OriolVinyals,AndrewZisserman,andKarenSimonyan. Flamingo: a visuallanguagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessingSystems, 35:23716–23736,2022. [Allaletal.,2024] LoubnaBenAllal,AntonLozhkov,andDanielvanStrien.cosmopedia:howtocreate large-scalesyntheticdataforpre-training. https://huggingface.co/blog/cosmopedia, 2024. [Allauzenetal.,2014] CyrilAllauzen,BillByrne,AdriàdeGispert,GonzaloIglesias,andMichael Riley. Pushdown automata in statistical machine translation. Computational Linguistics, 40(3): 687–723,2014. [AllenandHospedales,2019] CarlAllenandTimothyHospedales. Analogiesexplained: Towards understandingwordembeddings. InInternationalConferenceonMachineLearning,pages223–231. PMLR,2019. [Almazroueietal.,2023] EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,Alessandro Cappelli,RuxandraCojocaru,MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,JulienLaunay, QuentinMalartic,DanieleMazzotta,BadreddineNoune,BaptistePannier,andGuilhermePenedo. Thefalconseriesofopenlanguagemodels. arXivpreprintarXiv:2311.16867,2023. [Alzantot et al., 2018] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava,andKai-WeiChang. Generatingnaturallanguageadversarialexamples. InProceedings ofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2890–2896, 2018. [Ammaretal.,2020] WaleedAmmar,GeorgeMulcaire,YuliaTsvetkov,GuillaumeLample,Chris Dyer, and Noah A Smith. Massively multilingual word embeddings. In Proceedings of the 8th InternationalConferenceonLearningRepresentations(ICLR),2020. [Andersonetal.,2017] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould. Guided open vocabulary image captioning with constrained beam search. In Proceedings of the 2017 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages936–945,2017. [Andreasetal.,2016] JacobAndreas,MarcusRohrbach,TrevorDarrell,andDanKlein. Neuralmodule networks. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages 39–48,2016. [Antoletal.,2015] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra, CLawrenceZitnick,andDeviParikh. Vqa: Visualquestionanswering. InProceedingsoftheIEEE BIBLIOGRAPHY 637 internationalconferenceoncomputervision,pages2425–2433,2015. [Arjovskyetal.,2016] MartinArjovsky,AmarShah,andYoshuaBengio. Unitaryevolutionrecurrent neuralnetworks. InInternationalconferenceonmachinelearning,pages1120–1128,2016. [AronoffandFudeman,2011] MarkAronoffandKirstenFudeman. Whatismorphology?,volume8. JohnWiley&Sons,2011. [Aroraetal.,2017] SanjeevArora,YingyuLiang,andTengyuMa. Asimplebuttough-to-beatbaseline forsentenceembeddings. InInternationalconferenceonlearningrepresentations,2017. [Artetxe et al., 2017] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word embeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the AssociationforComputationalLinguistics(Volume1: LongPapers),pages451–462,2017. [Aschenbrenner,2024] LeopoldAschenbrenner. Situationalawareness: Thedecadeahead,2024. URL https://situational-awareness.ai/. [AshandDoléans-Dade,1999] RobertB.AshandCatherineA.Doléans-Dade. Probability&Measure Theory. AcademicPress,1999. [Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,AndyJones,NicholasJoseph,BenjaminMann,NovaDasSarma,NelsonElhage,Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei,TomB.Brown,JackClark,SamMcCandlish,ChrisOlah,andJaredKaplan. Ageneral languageassistantasalaboratoryforalignment. arXivpreprintarXiv:2112.00861,2021. [ÅströmandWittenmark,2013] KarlJÅströmandBjörnWittenmark. Computer-controlledsystems: theoryanddesign. CourierCorporation,2013. [Atkinson and Shiffrin, 1968] Richard C Atkinson and Richard M Shiffrin. Human memory: A proposedsystemanditscontrolprocesses. InPsychologyoflearningandmotivation,volume2, pages89–195.Elsevier,1968. [Augusteetal.,2017] JeremyAuguste,ArnaudRey,andBenoitFavre. Evaluationofwordembed- dingsagainstcognitiveprocesses: primedreactiontimesinlexicaldecisionandnamingtasks. In Proceedingsofthe2ndworkshoponevaluatingvectorspacerepresentationsforNLP,pages21–26, 2017. [Baetal.,2016] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXiv preprintarXiv:1607.06450,2016. [Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, NihalV.Nayak,AbheeshtSharma,TaewoonKim,M.SaifulBari,ThibaultFévry,ZaidAlyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, HanWang,JasonAlanFries,MagedSaeedAlShaibani,ShanyaSharma,UrmishThakker,Khalid Almubarak, Xiangru Tang, Dragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: Anintegrateddevelopmentenvironmentandrepositoryfornaturallanguageprompts. InProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics: System Demonstrations,pages93–104,2022. [Bachlechneretal.,2021] ThomasBachlechner,BodhisattwaPrasadMajumder,HenryMao,Gary Cottrell,andJulianMcAuley.Rezeroisallyouneed:Fastconvergenceatlargedepth.InProceedings ofUncertaintyinArtificialIntelligence,pages1352–1361.PMLR,2021. [Baevski et al., 2020] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self- 638 BIBLIOGRAPHY supervisedlearningofdiscretespeechrepresentations. InProceedingsofICLR2020,2020. [Bahdanauetal.,2014] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachine translationbyjointlylearningtoalignandtranslate. arXivpreprintarXiv:1409.0473,2014. [Bahdanauetal.,2016] DzmitryBahdanau,JanChorowski,DmitriySerdyuk,PhilemonBrakel,and YoshuaBengio. End-to-endattention-basedlargevocabularyspeechrecognition. In2016IEEE internationalconferenceonacoustics,speechandsignalprocessing(ICASSP),pages4945–4949. IEEE,2016. [Bahl and Mercer, 1976] L. R. Bahl and R. L. Mercer. Part of speech assignment by a statistical decisionalgorithm. InProceedingsofIEEEInternationalSymposiumonInformationTheory,pages 88–89,1976. [Baietal.,2021] JiangangBai,YujingWang,YirenChen,YamingYang,JingBai,JingYu,andYunhai Tong. Syntax-bert: Improvingpre-trainedtransformerswithsyntaxtrees. InProceedingsofthe16th ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics: MainVolume, pages3011–3020,2021. [Bakarov,2018] AmirBakarov. Asurveyofwordembeddingsevaluationmethods. arXivpreprint arXiv:1801.09536,2018. [Banarescuetal.,2013] LauraBanarescu,ClaireBonial,ShuCai,MadalinaGeorgescu,KiraGriffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract MeaningRepresentationforsembanking. InProceedingsofthe7thLinguisticAnnotationWorkshop andInteroperabilitywithDiscourse,pages178–186,2013. [Baoetal.,2021] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: Bertpre-trainingofimage transformers. InProceedingsofInternationalConferenceonLearningRepresentations,2021. [Bapnaetal.,2018] AnkurBapna,MiaXuChen,OrhanFirat,YuanCao,andYonghuiWu.",
    "Mercer. Part of speech assignment by a statistical decisionalgorithm. InProceedingsofIEEEInternationalSymposiumonInformationTheory,pages 88–89,1976. [Baietal.,2021] JiangangBai,YujingWang,YirenChen,YamingYang,JingBai,JingYu,andYunhai Tong. Syntax-bert: Improvingpre-trainedtransformerswithsyntaxtrees. InProceedingsofthe16th ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics: MainVolume, pages3011–3020,2021. [Bakarov,2018] AmirBakarov. Asurveyofwordembeddingsevaluationmethods. arXivpreprint arXiv:1801.09536,2018. [Banarescuetal.,2013] LauraBanarescu,ClaireBonial,ShuCai,MadalinaGeorgescu,KiraGriffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract MeaningRepresentationforsembanking. InProceedingsofthe7thLinguisticAnnotationWorkshop andInteroperabilitywithDiscourse,pages178–186,2013. [Baoetal.,2021] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: Bertpre-trainingofimage transformers. InProceedingsofInternationalConferenceonLearningRepresentations,2021. [Bapnaetal.,2018] AnkurBapna,MiaXuChen,OrhanFirat,YuanCao,andYonghuiWu. Training deeperneuralmachinetranslationmodelswithtransparentattention. InProceedingsofthe2018 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3028–3033,2018. [Barber, 2012] DavidBarber. BayesianReasoningandMachineLearning. CambridgeUniversity Press,2012. [Barham et al., 2022] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand,DanHurt,MichaelIsard,HyeontaekLim,RuomingPang,SudipRoy,BrennanSaeta,Parker Schuh,RyanSepassi,LaurentElShafey,ChandramohanA.Thekkath,andYonghuiWu. Pathways: Asynchronous distributed dataflow for ml. In Proceedings of Machine Learning and Systems, volume4,pages430–449,2022. [BaroniandLenci,2010] MarcoBaroniandAlessandroLenci. Distributionalmemory: Ageneral frameworkforcorpus-basedsemantics. ComputationalLinguistics,36(4):673–721,2010. [Baronietal.,2014] MarcoBaroni,GeorgianaDinu,andGermánKruszewski. Don’tcount,predict! a systematiccomparisonofcontext-countingvs.context-predictingsemanticvectors. InProceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),pages238–247,2014. [Barrault et al., 2020] Loïc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-jussà, ChristianFedermann,YvetteGraham,RomanGrundkiewicz,BarryHaddow,MatthiasHuck,Eric Joanis,TomKocmi,PhilippKoehn,Chi-kiuLo,NikolaLjubešic´,ChristofMonz,MakotoMorishita, MasaakiNagata,ToshiakiNakazawa,SantanuPal,MattPost,andMarcosZampieri. Findingsof BIBLIOGRAPHY 639 the2020conferenceonmachinetranslation(WMT20). InProceedingsoftheFifthConferenceon MachineTranslation,pages1–55,2020. [Baum and Petrie, 1966] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic functionsoffinitestatemarkovchains. Theannalsofmathematicalstatistics,37(6):1554–1563, 1966. [Baumetal.,1970] LeonardEBaum,TedPetrie,GeorgeSoules,andNormanWeiss. Amaximization techniqueoccurringinthestatisticalanalysisofprobabilisticfunctionsofmarkovchains. Theannals ofmathematicalstatistics,41(1):164–171,1970. [Belinkov, 2022] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances. ComputationalLinguistics,48(1):207–219,2022. [BelinkovandBisk,2018] YonatanBelinkovandYonatanBisk. Syntheticandnaturalnoisebothbreak neuralmachinetranslation. InInternationalConferenceonLearningRepresentations,2018. [Bello,2020] IrwanBello. Lambdanetworks: Modelinglong-rangeinteractionswithoutattention. In ProceedingsofInternationalConferenceonLearningRepresentations,2020. [Beltagy et al., 2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long- documenttransformer. arXiv:2004.05150,2020. [Bengioetal.,2015] EmmanuelBengio,Pierre-LucBacon,JoellePineau,andDoinaPrecup. Condi- tionalcomputationinneuralnetworksforfastermodels. arXivpreprintarXiv:1511.06297,2015. [Bengio,1991] YoshuaBengio.Artificialneuralnetworksandtheirapplicationtosequencerecognition. PhDthesis,McGillUniversity,1991. [Bengio et al., 1994] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencieswithgradientdescentisdifficult. IEEEtransactionsonneuralnetworks,5(2):157–166, 1994. [Bengioetal.,2000] YoshuaBengio,RéjeanDucharme,andPascalVincent. Aneuralprobabilistic languagemodel. AdvancesinNeuralInformationProcessingSystems,13,2000. [Bengioetal.,2003] YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJauvin. Aneural probabilisticlanguagemodel. JournalofMachineLearningResearch,3:1137–1155,2003a. [Bengioetal.,2003] YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJauvin. Aneural probabilisticlanguagemodel. JournalofMachineLearningResearch,3(Feb):1137–1155,2003b. [Bengioetal.,2006] YoshuaBengio,PascalLamblin,DanPopovici,andHugoLarochelle. Greedy layer-wisetrainingofdeepnetworks. Advancesinneuralinformationprocessingsystems,19,2006. [Bengioetal.,2013] YoshuaBengio,NicholasLéonard,andAaronCourville.Estimatingorpropagating gradientsthroughstochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432, 2013. [Bengioetal.,2024] YoshuaBengio,GeoffreyHinton,AndrewYao,DawnSong,PieterAbbeel,Trevor Darrell,YuvalNoahHarari,Ya-QinZhang,LanXue,ShaiShalev-Shwartz,GillianK.Hadfield,Jeff Clune,TeganMaharaj,FrankHutter,AtilimGunesBaydin,SheilaA.McIlraith,QiqiGao,Ashwin Acharya,DavidKrueger,AncaDragan,PhilipTorr,StuartRussell,DanielKahneman,JanMarkus Brauner,andSörenMindermann. Managingextremeairisksamidrapidprogress. Science,384 (6698):842–845,2024. [BentivogliandGiampiccolo,2011] LuisaBentivogliandDaniloGiampiccolo. Pascalrecognizing 640 BIBLIOGRAPHY textualentailmentchallenge(rte-7)attac2011. https://tac.nist.gov/2011/RTE/,2011. [Berg-Kirkpatricketal.,2012] TaylorBerg-Kirkpatrick,DavidBurkett,andDanKlein. Anempirical investigation of statistical significance inNLP. In Proceedings of the2012 Joint Conference on EmpiricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning, pages995–1005,2012. [Bestaetal.,2024] MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,MichalPodstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and TorstenHoefler. Graphofthoughts: Solvingelaborateproblemswithlargelanguagemodels. In ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages17682–17690,2024. [Bhattamishraetal.,2020] SatwikBhattamishra,KabirAhuja,andNavinGoyal. Ontheabilityand limitationsoftransformerstorecognizeformallanguages. InProceedingsofthe2020Conferenceon EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages7096–7116,2020. [Bhattasalietal.,2020] ShohiniBhattasali,JonathanBrennan,Wen-MingLuh,BertaFranzluebbers, andJohnHale. Thealicedatasets: fMRI&EEGobservationsofnaturallanguagecomprehension. InProceedingsofthe12thLanguageResourcesandEvaluationConference,pages120–125,2020. [BickelandDoksum,2015] PeterJBickelandKjellADoksum. Mathematicalstatistics: basicideas andselectedtopics,volumesI-IIpackage. ChapmanandHall/CRC,2015. [Bidermanetal.,2021] StellaBiderman,SidBlack,CharlesFoster,LeoGao,EricHallahan,Horace He, Ben Wang, and Phil Wang. Rotary embeddings: A relative revolution. https://blog. eleuther.ai/rotary-embeddings/,2021. [Birchetal.,2018] AlexandraBirch,AndrewFinch,Minh-ThangLuong,GrahamNeubig,andYusuke Oda. Findingsofthesecondworkshoponneuralmachinetranslationandgeneration. InProceedings ofthe2ndWorkshoponNeuralMachineTranslationandGeneration,pages1–10,2018. [Bishop,1995] ChristopherBishop. Regularizationandcomplexitycontrolinfeed-forwardnetworks. InProceedingsInternationalConferenceonArtificialNeuralNetworksICANN’95,pages141–148, 1995a. [Bishop,1995] ChristopherM.Bishop. Trainingwithnoiseisequivalenttotikhonovregularization. Neuralcomputation,7(1):108–116,1995b. [Bishop,2006] ChristopherM.Bishop. PatternRecognitionandMachineLearning. Springer,2006. [Blacoe and Lapata, 2012] William Blacoe and Mirella Lapata. A comparison of vector-based representationsforsemanticcomposition. InProceedingsofthe2012jointconferenceonempirical methodsinnaturallanguageprocessingandcomputationalnaturallanguagelearning,pages546– 556,2012. [Blei,2012] DavidMBlei. Probabilistictopicmodels. CommunicationsoftheACM,55(4):77–84, 2012. [Bleietal., 2003] DavidMBlei, AndrewYNg, andMichaelIJordan. Latentdirichletallocation. JournalofmachineLearningresearch,3(Jan):993–1022,2003. [BlumandMitchell,1998] AvrimBlumandTomMitchell. Combininglabeledandunlabeleddatawith co-training. InProceedingsoftheeleventhannualconferenceonComputationallearningtheory, pages92–100,1998. [Bojanowski et al., 2017] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enrichingwordvectorswithsubwordinformation. Transactionsoftheassociationforcomputational BIBLIOGRAPHY 641 linguistics,5:135–146,2017. [Bommasanietal.,2021] RishiBommasani,DrewA.Hudson,EhsanAdeli,RussAltman,Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun- skill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen,KathleenA.Creel,JaredDavis,DoraDemszky,ChrisDonahue,MoussaDoumbouya,Esin Durmus,StefanoErmon,JohnEtchemendy,KawinEthayarajh,LiFei-Fei,ChelseaFinn,Trevor Gale,LaurenE.Gillespie,KaranGoel,NoahD.Goodman,ShelbyGrossman,NeelGuha,Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, ThomasF.Icard,SaahilJain,DanJurafsky,PratyushaKalluri,SiddharthKaramcheti,GeoffKeeling, Fereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, AnanyaKumar,FaisalLadhak,MinaLee,TonyLee,JureLeskovec,IsabelleLevent,XiangLisaLi, XuechenLi,TengyuMa,AliMalik,ChristopherD.Manning,SuvirP.Mirchandani,EricMitchell, ZaneleMunyikwa,SurajNair,AvanikaNarayan,DeepakNarayanan,BenjaminNewman,AllenNie, JuanCarlosNiebles,HamedNilforoshan,J.F.Nyarko,GirayOgut,LaurelOrr,IsabelPapadimitriou, JoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,AditiRaghunathan,RobertReich, HongyuRen, FriedaRong, YusufH.Roohani, CamiloRuiz, JackRyan, ChristopherR’e, Dorsa Sadigh,ShioriSagawa,KeshavSanthanam,AndyShih,KrishnaParasuramSrinivasan,AlexTamkin, RohanTaori,ArminW.Thomas,FlorianTramèr,RoseE.Wang,WilliamWang,BohanWu,Jiajun Wu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou,MateiA.Zaharia,Michael Zhang,TianyiZhang,XikunZhang,YuhuiZhang,LuciaZheng,KaitlynZhou,andPercyLiang. On theopportunitiesandrisksoffoundationmodels. ArXiv,2021. [Bondarenkoetal.,2021] YelyseiBondarenko,MarkusNagel,andTijmenBlankevoort. Understanding andovercomingthechallengesofefficienttransformerquantization. InProceedingsofthe2021 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages7947–7969,2021. [BorjiandItti,2012] AliBorjiandLaurentItti. State-of-the-artinvisualattentionmodeling. IEEE transactionsonpatternanalysisandmachineintelligence,35(1):185–207,2012. [Boulanger-Lewandowskietal., 2013] NicolasBoulanger-Lewandowski, YoshuaBengio, andPas- cal Vincent. Audio chord recognition with recurrent neural networks. In Proceedings",
    "Ranjay Krishna, Rohith Kuditipudi, AnanyaKumar,FaisalLadhak,MinaLee,TonyLee,JureLeskovec,IsabelleLevent,XiangLisaLi, XuechenLi,TengyuMa,AliMalik,ChristopherD.Manning,SuvirP.Mirchandani,EricMitchell, ZaneleMunyikwa,SurajNair,AvanikaNarayan,DeepakNarayanan,BenjaminNewman,AllenNie, JuanCarlosNiebles,HamedNilforoshan,J.F.Nyarko,GirayOgut,LaurelOrr,IsabelPapadimitriou, JoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,AditiRaghunathan,RobertReich, HongyuRen, FriedaRong, YusufH.Roohani, CamiloRuiz, JackRyan, ChristopherR’e, Dorsa Sadigh,ShioriSagawa,KeshavSanthanam,AndyShih,KrishnaParasuramSrinivasan,AlexTamkin, RohanTaori,ArminW.Thomas,FlorianTramèr,RoseE.Wang,WilliamWang,BohanWu,Jiajun Wu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou,MateiA.Zaharia,Michael Zhang,TianyiZhang,XikunZhang,YuhuiZhang,LuciaZheng,KaitlynZhou,andPercyLiang. On theopportunitiesandrisksoffoundationmodels. ArXiv,2021. [Bondarenkoetal.,2021] YelyseiBondarenko,MarkusNagel,andTijmenBlankevoort. Understanding andovercomingthechallengesofefficienttransformerquantization. InProceedingsofthe2021 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages7947–7969,2021. [BorjiandItti,2012] AliBorjiandLaurentItti. State-of-the-artinvisualattentionmodeling. IEEE transactionsonpatternanalysisandmachineintelligence,35(1):185–207,2012. [Boulanger-Lewandowskietal., 2013] NicolasBoulanger-Lewandowski, YoshuaBengio, andPas- cal Vincent. Audio chord recognition with recurrent neural networks. In Proceedings of 14th InternationalSocietyforMusicInformationRetrievalConference,2013. [BourlardandWellekens,1990] HerveBourlardandChristianJWellekens. Linksbetweenmarkov modelsandmultilayerperceptrons. IEEETransactionsonpatternanalysisandmachineintelligence, 12(12):1167–1178,1990. [BourlardandMorgan,1993] HerveA.BourlardandNelsonMorgan. ConnectionistSpeechRecogni- tion: AHybridApproach. KluwerAcademicPublishers,USA,1993. [Boxetal.,2015] GeorgeEPBox,GwilymMJenkins,GregoryCReinsel,andGretaMLjung. Time seriesanalysis: forecastingandcontrol(4thed.). JohnWiley&Sons,2015. [BradleyandTerry,1952] RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblock designs: I.themethodofpairedcomparisons. Biometrika,39(3/4):324–345,1952. [Brandonetal.,2024] WilliamBrandon,MayankMishra,AniruddhaNrusimha,RameswarPanda,and JonathanRaganKelly. Reducingtransformerkey-valuecachesizewithcross-layerattention. arXiv preprintarXiv:2405.12981,2024. [Breiman,1996] LeoBreiman. Baggingpredictors. MachineLearning,24(2):123–140,1996. [Brill,1992] EricBrill. Asimplerule-basedpartofspeechtagger. InSpeechandNaturalLanguage: 642 BIBLIOGRAPHY ProceedingsofaWorkshopHeldatHarriman,NewYork,February23-26,1992,1992. [Briski,2025] KariBriski. Howscalinglawsdrivesmarter,morepowerfulai,2025. [Brown et al., 2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, ChristopherRé,andAzaliaMirhoseini. Largelanguagemonkeys: Scalinginferencecomputewith repeatedsampling. arXivpreprintarXiv:2407.21787,2024. [Brownetal.,1993] PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,andRobertL. Mercer. Themathematicsofstatisticalmachinetranslation: Parameterestimation. Computational Linguistics,19(2):263–311,1993. [Brownetal.,2020] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners. Advancesinneural informationprocessingsystems,33:1877–1901,2020. [Bubeck et al., 2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, HamidPalangi,MarcoTúlioRibeiro,andYiZhang. Sparksofartificialgeneralintelligence: Early experimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023. [Buckman et al., 2016] Jacob Buckman, Miguel Ballesteros, and Chris Dyer. Transition-based dependencyparsingwithheuristicbacktracking. InProceedingsofthe2016Conferenceonempirical methodsinnaturallanguageprocessing,pages2313–2318,2016. [Bulatovetal.,2022] AydarBulatov,YuryKuratov,andMikhailBurtsev. Recurrentmemorytrans- former. AdvancesinNeuralInformationProcessingSystems,35:11079–11091,2022. [BurchiandVielzeuf,2021] MaximeBurchiandValentinVielzeuf. Efficientconformer: Progressive downsamplingandgroupedattentionforautomaticspeechrecognition. InProceedingsof2021IEEE AutomaticSpeechRecognitionandUnderstandingWorkshop(ASRU),pages8–15.IEEE,2021. [Burgesetal.,2005] ChrisBurges,TalShaked,ErinRenshaw,AriLazier,MattDeeds,NicoleHamilton, andGregHullender.Learningtorankusinggradientdescent.InProceedingsofthe22ndinternational conferenceonMachinelearning,pages89–96,2005. [BurnhamandAnderson,2002] KennethP.BurnhamandDavidR.Anderson. Modelselectionand multimodelinference: apracticalinformation-theoreticapproach. Spinger,2002. [Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, LeopoldAschenbrenner,YiningChen,AdrienEcoffet,ManasJoglekar,JanLeike,IlyaSutskever, andJeffWu. Weak-to-stronggeneralization: Elicitingstrongcapabilitieswithweaksupervision. arXivpreprintarXiv:2312.09390,2023a. [Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschenbrenner, Jeffrey Wu, Pavel Izmailov, LeoGao,BowenBaker,andJanHendrikKirchner. Weak-to-stronggeneralization,2023b. URL https://https://openai.com/index/weak-to-strong-generalization. [Buttcher et al., 2016] Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. Information retrieval: Implementingandevaluatingsearchengines. MITPress,2016. [Caballeroetal.,2023] EthanCaballero,KshitijGupta,IrinaRish,andDavidKrueger. Brokenneural BIBLIOGRAPHY 643 scalinglaws. InICLR2023WorkshoponMathematicalandEmpiricalUnderstandingofFoundation Models,2023. [Campbell,1997] JosephPCampbell. Speakerrecognition: Atutorial. ProceedingsoftheIEEE,85(9): 1437–1462,1997. [Caoetal.,2007] ZheCao,TaoQin,Tie-YanLiu,Ming-FengTsai,andHangLi. Learningtorank: frompairwiseapproachtolistwiseapproach. InProceedingsofthe24thinternationalconferenceon Machinelearning,pages129–136,2007. [Caronetal.,2021] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,Piotr Bojanowski,andArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. In ProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages9650–9660,2021. [Casacubertaetal.,2009] FranciscoCasacuberta,JorgeCivera,ElsaCubel,AntonioLLagarda,Guy Lapalme, Elliott Macklovitch, and Enrique Vidal. Human interaction for high-quality machine translation. CommunicationsoftheACM,52(10):135–138,2009. [Ceretal.,2018] DanielCer,YinfeiYang,ShengyiKong,NanHua,NicoleLimtiaco,RhomniSt. John,NoahConstant,MarioGuajardo-Cespedes,SteveYuan,ChrisTar,Yun-HsuanSung,Brian Strope,andRayKurzweil. Universalsentenceencoder. arXivpreprintarXiv:1803.11175,2018. [Chanetal.,2016] WilliamChan,NavdeepJaitly,QuocLe,andOriolVinyals. Listen,attendandspell: Aneuralnetworkforlargevocabularyconversationalspeechrecognition. In2016IEEEinternational conferenceonacoustics,speechandsignalprocessing(ICASSP),pages4960–4964.IEEE,2016. [Changetal.,2024] KaiyanChang,SongchengXu,ChenglongWang,YingfengLuo,TongXiao,and Jingbo Zhu. Efficient prompting methods for large language models: A survey. arXiv preprint arXiv:2404.01077,2024. [Chang,1967] Wing-TsitChang. Reflectionsonthingsathand. ColumbiaUniversityPress,1967. [ChangandCollins,2011] Yin-WenChangandMichaelCollins. Exactdecodingofphrase-basedtrans- lationmodelsthroughlagrangianrelaxation. InProceedingsofthe2011ConferenceonEmpirical MethodsinNaturalLanguageProcessing,pages26–37,2011. [Charniak,1997] EugeneCharniak. Statisticalparsingwithacontext-freegrammarandwordstatistics. AAAI/IAAI,2005(598-603):18,1997. [Chatfield,2003] ChrisChatfield. Theanalysisoftimeseries: anintroduction. Chapmanandhall/CRC, 2003. [Chaudhari et al., 2021] Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. Anattentivesurveyofattentionmodels. ACMTransactionsonIntelligentSystemsandTechnology (TIST),12(5):1–32,2021. [Chenetal.,2023] BanghaoChen,ZhaofengZhang,NicolasLangrené,andShengxinZhu. Unleashing the potential of prompt engineering in large language models: a comprehensive review. arXiv preprintarXiv:2310.14735,2023a. [Chenetal.,2018] KehaiChen,RuiWang,MasaoUtiyama,EiichiroSumita,andTiejunZhao. Syntax- directedattentionforneuralmachinetranslation. InProceedingsoftheAAAIconferenceonartificial intelligence,2018a. [Chenetal.,2023] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,VikasYadav, ZhengTang,VijaySrinivasan,TianyiZhou,HengHuang,andHongxiaJin. Alpagasus: Traininga betteralpacawithfewerdata. arXivpreprintarXiv:2307.08701,2023b. 644 BIBLIOGRAPHY [Chenetal.,2024] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,VikasYadav, ZhengTang,VijaySrinivasan,TianyiZhou,HengHuang,andHongxiaJin. Alpagasus: Traininga betteralpacawithfewerdata. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024a. [Chenetal.,2020] MarkChen,AlecRadford,RewonChild,JeffreyWu,HeewooJun,DavidLuan,and IlyaSutskever. Generativepretrainingfrompixels. InInternationalconferenceonmachinelearning, pages1691–1703.PMLR,2020a. [Chenetal.,2018] MiaXuChen,OrhanFirat,AnkurBapna,MelvinJohnson,WolfgangMacherey, GeorgeFoster,LlionJones,MikeSchuster,NoamShazeer,NikiParmar,AshishVaswani,Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both worlds: Combiningrecentadvancesinneuralmachinetranslation. InProceedingsofthe56thAnnual MeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages76–86, 2018b. [Chenetal.,2018] RickyTQChen,YuliaRubanova,JesseBettencourt,andDavidKDuvenaud. Neural ordinarydifferentialequations. Advancesinneuralinformationprocessingsystems,31,2018c. [Chenetal.,2022] SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,ZhuoChen, JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,LongZhou,ShuoRen,YanminQian,Yao Qian,JianWu,MichaelZeng,andFuruWei.Wavlm:Large-scaleself-supervisedpre-trainingforfull stackspeechprocessing. IEEEJournalofSelectedTopicsinSignalProcessing,16(6):1505–1518, 2022. [Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Ex- tending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595,2023c. [ChenandGoodman,1999] StanleyF.ChenandJoshuaGoodman. Anempiricalstudyofsmoothing techniquesforlanguagemodeling. ComputerSpeechandLanguage,13:359–394,1999. [Chenetal.,2020] TianlongChen,JonathanFrankle,ShiyuChang,SijiaLiu,YangZhang,Zhangyang Wang,andMichaelCarbin. Thelotterytickethypothesisforpre-trainedbertnetworks. Advancesin neuralinformationprocessingsystems,33:15834–15846,2020b. [Chenetal.,2015] TianqiChen,IanGoodfellow,andJonathonShlens. Net2net: Acceleratinglearning viaknowledgetransfer. arXivpreprintarXiv:1511.05641,2015. [ChenandHe,2021] XinleiChenandKaimingHe.",
    "Qian,JianWu,MichaelZeng,andFuruWei.Wavlm:Large-scaleself-supervisedpre-trainingforfull stackspeechprocessing. IEEEJournalofSelectedTopicsinSignalProcessing,16(6):1505–1518, 2022. [Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Ex- tending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595,2023c. [ChenandGoodman,1999] StanleyF.ChenandJoshuaGoodman. Anempiricalstudyofsmoothing techniquesforlanguagemodeling. ComputerSpeechandLanguage,13:359–394,1999. [Chenetal.,2020] TianlongChen,JonathanFrankle,ShiyuChang,SijiaLiu,YangZhang,Zhangyang Wang,andMichaelCarbin. Thelotterytickethypothesisforpre-trainedbertnetworks. Advancesin neuralinformationprocessingsystems,33:15834–15846,2020b. [Chenetal.,2015] TianqiChen,IanGoodfellow,andJonathonShlens. Net2net: Acceleratinglearning viaknowledgetransfer. arXivpreprintarXiv:1511.05641,2015. [ChenandHe,2021] XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages 15750–15758,2021. [Chenetal.,2020] Yen-ChunChen,LinjieLi,LichengYu,AhmedElKholy,FaisalAhmed,ZheGan, YuCheng,andJingjingLiu. Uniter: Universalimage-textrepresentationlearning. InProceedingsof Europeanconferenceoncomputervision,pages104–120,2020c. [Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-playfine-tuningconvertsweaklanguagemodelstostronglanguagemodels. arXivpreprint arXiv:2401.01335,2024b. [Chevalieretal.,2023] AlexisChevalier,AlexanderWettig,AnirudhAjith,andDanqiChen. Adapting languagemodelstocompresscontexts.InProceedingsofthe2023ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages3829–3846,2023. BIBLIOGRAPHY 645 [Chietal.,2022] Ta-ChungChi,Ting-HanFan,PeterJRamadge,andAlexanderRudnicky. Kerple: Kernelizedrelativepositionalembeddingforlengthextrapolation. AdvancesinNeuralInformation ProcessingSystems,35:8386–8399,2022. [Chietal.,2023] Ta-ChungChi,Ting-HanFan,AlexanderRudnicky,andPeterRamadge. Dissecting transformerlengthextrapolationviathelensofreceptivefieldanalysis. InProceedingsofthe61st AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages 13522–13537,2023. [Chiang,2005] DavidChiang. Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation. In Proceedingsofthe43rdannualmeetingoftheassociationforcomputationallinguistics(acl’05), pages263–270,2005. [Chiang,2007] DavidChiang. Hierarchicalphrase-basedtranslation. computationallinguistics,33(2): 201–228,2007. [ChiangandCholak,2022] DavidChiangandPeterCholak. Overcomingatheoreticallimitationof self-attention. arXivpreprintarXiv:2202.12172,2022. [Chiangetal.,2023] DavidChiang,PeterCholak,andAnandPillay. Tighterboundsontheexpressivity oftransformerencoders. arXivpreprintarXiv:2301.10743,2023a. [Chiangetal.,2023] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang, LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023b. URL https://lmsys.org/blog/2023-03-30-vicuna/. [Child et al., 2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequenceswithsparsetransformers. arXivpreprintarXiv:1904.10509,2019. [ChiuandRaffel,2018] Chung-ChengChiuandColinRaffel. Monotonicchunkwiseattention. In Proceedingsofthe8thInternationalConferenceonLearningRepresentationsICLR,2018. [Choetal.,2021] JaeminCho,JieLei,HaoTan,andMohitBansal. Unifyingvision-and-languagetasks viatextgeneration. InInternationalConferenceonMachineLearning,pages1931–1942.PMLR, 2021. [ChoandEsipova,2016] KyunghyunChoandMashaEsipova. Canneuralmachinetranslationdo simultaneoustranslation? arXivpreprintarXiv:1606.02012,2016. [Cho et al., 2014] Kyunghyun Cho, Bart van Merriënboer, Çag˘lar Guu˙lçehre, Dzmitry Bahdanau, FethiBougares,HolgerSchwenk,andYoshuaBengio. Learningphraserepresentationsusingrnn encoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on EmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1724–1734,2014. [ChoeandCharniak,2016] DoKookChoeandEugeneCharniak. Parsingaslanguagemodeling. In Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages 2331–2336,2016. [Chollet,2021] FrançoisChollet. DeepLearningwithPython(2nded.). ManningPublications,2021. [Choromanski et al., 2020] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, XingyouSong,AndreeaGane,TamásSarlós,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin, LukaszKaiser,DavidBenjaminBelanger,LucyJ.Colwell,andAdrianWeller. Rethinkingattention withperformers. InProceedingsofInternationalConferenceonLearningRepresentations,2020. [ChorowskiandJaitly,2017] JanChorowskiandNavdeepJaitly. Towardsbetterdecodingandlanguage 646 BIBLIOGRAPHY modelintegrationinsequencetosequencemodels. Proc.Interspeech2017,pages523–527,2017. [Chorowski et al., 2019] Jan Chorowski, Ron J Weiss, Samy Bengio, and Aäron Van Den Oord. Unsupervisedspeechrepresentationlearningusingwavenetautoencoders. IEEE/ACMtransactions onaudio,speech,andlanguageprocessing,27(12):2041–2053,2019. [Chorowskietal.,2015] JanKChorowski,DzmitryBahdanau,DmitriySerdyuk,KyunghyunCho,and YoshuaBengio. Attention-basedmodelsforspeechrecognition. Advancesinneuralinformation processingsystems,28,2015. [Chowdheryetal.,2022] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,Gau- ravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann, ParkerSchuh,KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes, YiTay,NoamShazeer,VinodkumarPrabhakaran,EmilyReif,NanDu,BenHutchinson,ReinerPope, JamesBradbury,JacobAustin,MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,Anselm Levskaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,Kevin Robinson,LiamFedus,DennyZhou,DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph, AlexanderSpiridonov,RyanSepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM. Dai,ThanumalayanSankaranarayanaPillai,MariePellat,AitorLewkowycz,EricaMoreira,Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, OrhanFirat, MicheleCatasta, JasonWei, KathyMeier-Hellstern, DouglasEck, JeffDean, SlavPetrov, andNoahFiedel. Palm: Scalinglanguagemodelingwithpathways. arXivpreprint arXiv:2204.02311,2022. [Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural informationprocessingsystems,30,2017. [Chuetal.,2023] ZhengChu,JingchangChen,QianglongChen,WeijiangYu,TaoHe,HaotianWang, WeihuaPeng,MingLiu,BingQin,andTingLiu. Asurveyofchainofthoughtreasoning: Advances, frontiersandfuture. arXivpreprintarXiv:2309.15402,2023. [Chuangetal.,2020] Yung-SungChuang,Chi-LiangLiu,Hung-yiLee,andLin-shanLee. Speechbert: Anaudio-and-textjointlylearnedlanguagemodelforend-to-endspokenquestionanswering. In ProceedingsofInterspeech2020,pages4168–4172,2020. [Chungetal.,2022] HyungWonChung,LeHou,S.Longpre,BarretZoph,YiTay,WilliamFedus, EricLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,AlbertWebson,ShixiangShaneGu, ZhuyunDai,MiracSuzgun,XinyunChen,AakankshaChowdhery,DashaValter,SharanNarang, GauravMishra,AdamsWeiYu,VincentZhao,YanpingHuang,AndrewM.Dai,HongkunYu,Slav Petrov,EdHuaihsinChi,JeffDean,JacobDevlin,AdamRoberts,DennyZhou,QuocV.Le,and JasonWei. Scalinginstruction-finetunedlanguagemodels. arXivpreprintarXiv:2210.11416,2022. [Chungetal.,2014] JunyoungChung,CaglarGulcehre,KyunghyunCho,andYoshuaBengio. Empiri- calevaluationofgatedrecurrentneuralnetworksonsequencemodeling. InProceedingsofNIPS 2014WorkshoponDeepLearning,December2014,2014. [Church,2011] KennethChurch.Apendulumswungtoofar.LinguisticIssuesinLanguageTechnology, 6,2011. [ChurchandHanks,1990] KennethWardChurchandPatrickHanks. Wordassociationnorms,mutual information, and lexicography. Computational Linguistics, 16(1):22–29, 1990. URL https: //aclanthology.org/J90-1003. BIBLIOGRAPHY 647 [Clark et al., 2019] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. Whatdoesbertlookat? ananalysisofbert’sattention. InProceedingsofthe2019ACLWorkshop BlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages276–286,2019a. [Clarketal.,2019] KevinClark,Minh-ThangLuong,QuocVLe,andChristopherDManning. Electra: Pre-trainingtextencodersasdiscriminatorsratherthangenerators. InProceedingsofInternational ConferenceonLearningRepresentations,2019b. [Clarketal.,2008] StephenClark,BobCoecke,andMehrnooshSadrzadeh. Acompositionaldistribu- tionalmodelofmeaning. InProceedingsoftheSecondQuantumInteractionSymposium(QI-2008), pages133–140.Oxford,2008. [ClineandDhillon,2014] AlanKaylorClineandInderjitS.Dhillon. Computationofthesingularvalue decomposition. InLeslieHogben,editor,HandbookofLinearAlgebra(2dned.).CRCPress,2014. [Cobbeetal.,2021] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,2021. [Cohnetal.,2016] TrevorCohn,CongDuyVuHoang,EkaterinaVymolova,KaishengYao,Chris Dyer,andGholamrezaHaffari. Incorporatingstructuralalignmentbiasesintoanattentionalneural translationmodel. InProceedingsofthe2016ConferenceoftheNorthAmericanChapterofthe AssociationforComputationalLinguistics: HumanLanguageTechnologies,pages876–885,2016. [CollobertandWeston,2008] RonanCollobertandJasonWeston. Aunifiedarchitecturefornatural language processing: deep neural networks with multitask learning. In Proceedings of the 25th internationalconferenceonMachinelearning(ICML08),pages160–167,2008. [Conneauetal.,2017] AlexisConneau,DouweKiela,HolgerSchwenk,LoïcBarrault,andAntoine Bordes. Supervisedlearningofuniversalsentencerepresentationsfromnaturallanguageinference data. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing, pages670–680,2017a. [Conneauetal.,2017] AlexisConneau,DouweKiela,HolgerSchwenk,LoïcBarrault,andAntoine Bordes. Supervisedlearningofuniversalsentencerepresentationsfromnaturallanguageinference data. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing, pages670–680,2017b. [Conneauetal.,2017] AlexisConneau,HolgerSchwenk,LoïcBarrault,andYannLecun. Verydeep convolutionalnetworksfortextclassification. InProceedingsofthe15thConferenceoftheEuropean ChapteroftheAssociationforComputationalLinguistics: Volume1,LongPapers,pages1107–1116, 2017c. [Conneauetal.,2018] AlexisConneau,GermánKruszewski,GuillaumeLample,LoïcBarrault,and MarcoBaroni. Whatyoucancramintoasinglevector: Probingsentenceembeddingsforlinguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pages2126–2136,2018. [Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, GuillaumeWenzek,FranciscoGuzmán,ÉdouardGrave,MyleOtt,LukeZettlemoyer,andVeselin Stoyanov. Unsupervisedcross-lingualrepresentationlearningatscale. InProceedingsofthe58th AnnualMeetingoftheAssociationforComputationalLinguistics,pages8440–8451,2020. [Cortes and Vapnik, 1995] Corinna Cortes and Vladimir Vapnik. Support-vector networks.",
    "Volume1,LongPapers,pages1107–1116, 2017c. [Conneauetal.,2018] AlexisConneau,GermánKruszewski,GuillaumeLample,LoïcBarrault,and MarcoBaroni. Whatyoucancramintoasinglevector: Probingsentenceembeddingsforlinguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pages2126–2136,2018. [Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, GuillaumeWenzek,FranciscoGuzmán,ÉdouardGrave,MyleOtt,LukeZettlemoyer,andVeselin Stoyanov. Unsupervisedcross-lingualrepresentationlearningatscale. InProceedingsofthe58th AnnualMeetingoftheAssociationforComputationalLinguistics,pages8440–8451,2020. [Cortes and Vapnik, 1995] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Mind, 648 BIBLIOGRAPHY MachineLearning:273–297,1995. [Costeetal.,2024] ThomasCoste,UsmanAnwar,RobertKirk,andDavidKrueger. Rewardmodel ensembleshelpmitigateoveroptimization. InTheTwelfthInternationalConferenceonLearning Representations,2024. [CotterellandSchütze,2015] RyanCotterellandHinrichSchütze. Morphologicalword-embeddings. In Proceedings of the 2015 Conference of the North American Chapter of the Association for ComputationalLinguistics: HumanLanguageTechnologies,pages1287–1292,2015. [Cuietal.,2024] GanquCui,LifanYuan,NingDing,GuanmingYao,BingxiangHe,WeiZhu,Yuan Ni,GuotongXie,RuobingXie,YankaiLin,ZhiyuanLiu,andMaosongSun. ULTRAFEEDBACK: Boosting language models with scaled AI feedback. In Proceedings of the 41st International ConferenceonMachineLearning,volume235,pages9722–9744,2024. [CurreyandHeafield,2018] AnnaCurreyandKennethHeafield. Multi-sourcesyntacticneuralmachine translation. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage Processing,pages2961–2966,2018. [Cybenko,1989] GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathe- maticsofcontrol,signalsandsystems,2(4):303–314,1989. [Daietal.,2023] DamaiDai,YutaoSun,LiDong,YaruHao,ShumingMa,ZhifangSui,andFuruWei. Whycangptlearnin-context? languagemodelssecretlyperformgradientdescentasmeta-optimizers. InFindingsoftheAssociationforComputationalLinguistics: ACL2023,pages4005–4019,2023. [Daietal.,2019] ZihangDai,ZhilinYang,YimingYang,JaimeGCarbonell,QuocLe,andRuslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages 2978–2988,2019. [Daoetal.,2022] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information ProcessingSystems,35:16344–16359,2022. [Daoetal.,2023] TriDao,DanielHaziza,FranciscoMassa,andGrigorySizov.Flash-decodingforlong- context inference. https://pytorch.org/blog/flash-decoding/, 2023. Retrieved 2023-10-23. [DavisandMermelstein,1980] StevenDavisandPaulMermelstein. Comparisonofparametricrepre- sentationsformonosyllabicwordrecognitionincontinuouslyspokensentences. IEEEtransactions onacoustics,speech,andsignalprocessing,28(4):357–366,1980. [Dayanetal.,1995] PeterDayan,GeoffreyEHinton,RadfordMNeal,andRichardSZemel. The helmholtzmachine. Neuralcomputation,7(5):889–904,1995. [deGispertetal.,2010] AdriàdeGispert,GonzaloIglesias,GraemeBlackwood,EduardoR.Banga, andWilliamByrne. Hierarchicalphrase-basedtranslationwithweightedfinite-statetransducersand shallow-ngrammars. Computationallinguistics,36(3):505–533,2010. [Deepseek,2025] Deepseek. Deepseek-r1: Incentivizingreasoningcapabilityinllmsviareinforcement learning. arXivpreprintarXiv:2501.12948,2025. [Deerwesteretal.,1990] ScottDeerwester,SusanTDumais,GeorgeWFurnas,ThomasKLandauer, andRichardHarshman. Indexingbylatentsemanticanalysis. JournaloftheAmericansocietyfor informationscience,41(6):391–407,1990. BIBLIOGRAPHY 649 [Dehghanietal.,2018] MostafaDehghani,StephanGouws,OriolVinyals,JakobUszkoreit,andŁukasz Kaiser. Universaltransformers. arXivpreprintarXiv:1807.03819,2018. [Del Corro et al., 2023] Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed Awadallah,andSubhabrataMukherjee. Skipdecode: Autoregressiveskipdecodingwithbatching andcachingforefficientllminference. arXivpreprintarXiv:2307.02628,2023. [Deletangetal.,2024] GregoireDeletang,AnianRuoss,Paul-AmbroiseDuquenne,ElliotCatt,Tim Genewein,ChristopherMattern,JordiGrau-Moya,LiKevinWenliang,MatthewAitchison,Laurent Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. In The Twelfth InternationalConferenceonLearningRepresentations,2024. [Dempsteretal.,1977] ArthurPDempster,NanMLaird,andDonaldBRubin. Maximumlikelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological),39(1):1–22,1977. [Dengetal.,2022] MingkaiDeng,JianyuWang,Cheng-PingHsieh,YihanWang,HanGuo,Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcementlearning. InProceedingsofthe2022ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages3369–3391,2022. [Deorasetal.,2011] AnoopDeoras,TomášMikolov,andKennethChurch. Afastre-scoringstrategyto capturelong-distancedependencies. InProceedingsofthe2011ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages1116–1127,2011. [Devereuxetal.,2010] BarryDevereux,ColinKelly,andAnnaKorhonen. Usingfmriactivationto conceptualstimulitoevaluatemethodsforextractingconceptualrepresentationsfromcorpora. In ProceedingsoftheNAACLHLT2010FirstWorkshoponComputationalNeurolinguistics,pages 70–78,2010. [Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. InProceedingsofthe 2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,2019. [DiGangietal.,2019] MattiaAntoninoDiGangi,MatteoNegri,RoldanoCattoni,RobertoDessi,and MarcoTurchi. Enhancingtransformerforend-to-endspeech-to-texttranslation. InProceedingsof MachineTranslationSummitXVII:ResearchTrack,pages21–31,2019. [Dingetal.,2021] MingDing,ZhuoyiYang,WenyiHong,WendiZheng,ChangZhou,DaYin,Junyang Lin,XuZou,ZhouShao,HongxiaYang,andJieTang. Cogview: Masteringtext-to-imagegeneration viatransformers. AdvancesinNeuralInformationProcessingSystems,34:19822–19835,2021. [Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, JiahangXu,FanYang,andMaoYang. Longrope: Extendingllmcontextwindowbeyond2million tokens. arXivpreprintarXiv:2402.13753,2024. [Doersch,2016] CarlDoersch. Tutorialonvariationalautoencoders. stat,1050:13,2016. [DolanandBrockett,2005] BillDolanandChrisBrockett. Automaticallyconstructingacorpusof sententialparaphrases. InProceedingsofThirdInternationalWorkshoponParaphrasing(IWP2005), 2005. [Dongetal.,2019] LiDong,NanYang,WenhuiWang,FuruWei,XiaodongLiu,YuWang,Jianfeng Gao,MingZhou,andHsiao-WuenHon. Unifiedlanguagemodelpre-trainingfornaturallanguage 650 BIBLIOGRAPHY understandingandgeneration. Advancesinneuralinformationprocessingsystems,32,2019. [Dongetal.,2022] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun, JingjingXu,andZhifangSui. Asurveyonin-contextlearning. arXivpreprintarXiv:2301.00234, 2022. [Dongetal.,2021] YiheDong,Jean-BaptisteCordonnier,andAndreasLoukas. Attentionisnotall youneed: Pureattentionlosesrankdoublyexponentiallywithdepth. InInternationalConferenceon MachineLearning,pages2793–2803.PMLR,2021. [Dosovitskiyetal.,2021] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn, XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,Sylvain Gelly,JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage recognitionatscale. InProceedingsofICLR2021,2021. [Downey,2021] AllenB.Downey. ThinkBayes: BayesianStatisticsinPython(2nded.). O’Reilly Media,2021. [Droretal.,2020] RotemDror,LotemPeled-Cohen,andSegevShlomov. NeuralNetworkMethodsfor NaturalLanguageProcessing. Morgan&ClaypoolPublishers,2020. [Drozdovetal.,2022] AndrewDrozdov,NathanaelSchärli,EkinAkyürek,NathanScales,Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. In Proceedings of The Eleventh International Conference on Learning Representations,2022. [Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive promptingfordecomposingcomplexquestions. InProceedingsofthe2022ConferenceonEmpirical MethodsinNaturalLanguageProcessing,pages1251–1265,2022. [Dubeyetal.,2024] AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,Ahmad Al-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3 herdofmodels. arXivpreprintarXiv:2407.21783,2024. [Duboisetal.,2024] YannDubois,ChenXuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani, JimmyBa,CarlosGuestrin,PercySLiang,andTatsunoriBHashimoto. Alpacafarm: Asimulation frameworkformethodsthatlearnfromhumanfeedback. AdvancesinNeuralInformationProcessing Systems,36,2024. [Duchietal.,2011] JohnDuchi,EladHazan,andYoramSinger. Adaptivesubgradientmethodsfor onlinelearningandstochasticoptimization. Journalofmachinelearningresearch,12(7),2011. [Dufteretal., 2022] PhilippDufter, MartinSchmitt, andHinrichSchütze. Positioninformationin transformers: Anoverview. ComputationalLinguistics,48(3):733–763,2022. [Dyeretal.,2013] ChrisDyer,VictorChahuneau,andNoahASmith. Asimple,fast,andeffective reparameterizationofibmmodel2. InProceedingsofthe2013ConferenceoftheNorthAmerican ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages 644–648,2013. [Ebrahimietal.,2018] JavidEbrahimi,AnyiRao,DanielLowd,andDejingDou. HotFlip: White-box adversarial examples for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31–36, Melbourne, Australia,2018. [Edunovetal.,2018] SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understanding BIBLIOGRAPHY 651 back-translationatscale. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages489–500,2018. [Ee,2017] WeinanEe. Aproposalonmachinelearningviadynamicalsystems. Communicationsin MathematicsandStatistics,5:1–11,022017. [EikemaandAziz,2020] BryanEikemaandWilkerAziz.Ismapdecodingallyouneed?theinadequacy ofthemodeinneuralmachinetranslation. InProceedingsofthe28thInternationalConferenceon ComputationalLinguistics,pages4506–4520,2020. [Eisenstein et al., 2023] Jacob Eisenstein,",
    "adversarial examples for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31–36, Melbourne, Australia,2018. [Edunovetal.,2018] SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understanding BIBLIOGRAPHY 651 back-translationatscale. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages489–500,2018. [Ee,2017] WeinanEe. Aproposalonmachinelearningviadynamicalsystems. Communicationsin MathematicsandStatistics,5:1–11,022017. [EikemaandAziz,2020] BryanEikemaandWilkerAziz.Ismapdecodingallyouneed?theinadequacy ofthemodeinneuralmachinetranslation. InProceedingsofthe28thInternationalConferenceon ComputationalLinguistics,pages4506–4520,2020. [Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,DJDvijotham,AdamFisch,KatherineHeller,StephenPfohl,DeepakRamachandran, andPeterShaw. Helpingorherding? rewardmodelensemblesmitigatebutdonoteliminatereward hacking. arXivpreprintarXiv:2312.09244,2023. [Elbayadetal.,2020] MahaElbayad,JiataoGu,EdouardGrave,andMichaelAuli. Depth-adaptive transformer. InProceedingsofInternationalConferenceonLearningRepresentations,2020. [Elman,1990] JeffreyLElman. Findingstructureintime. Cognitivescience,14(2):179–211,1990. [Elskenetal.,2019] ThomasElsken,JanHendrikMetzen,andFrankHutter. Neuralarchitecturesearch: Asurvey. JournalofMachineLearningResearch,20(55):1–21,2019a. [Elskenetal.,2019] ThomasElsken,JanHendrikMetzen,andFrankHutter. Neuralarchitecturesearch: Asurvey. TheJournalofMachineLearningResearch,20(1):1997–2017,2019b. [Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why doesunsupervisedpre-traininghelpdeeplearning? InProceedingsofthethirteenthinternational conferenceonartificialintelligenceandstatistics,pages201–208,2010. [Fanetal.,2018] AngelaFan,MikeLewis,andYannDauphin. Hierarchicalneuralstorygeneration. In Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages889–898,2018. [Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with structured dropout. In Proceedings of International Conference on Learning Representations,2019. [Fanetal.,2021] HaoqiFan,BoXiong,KarttikeyaMangalam,YanghaoLi,ZhichengYan,Jitendra Malik,andChristophFeichtenhofer.Multiscalevisiontransformers.InProceedingsoftheIEEE/CVF InternationalConferenceonComputerVision,pages6824–6835,2021. [Fanetal.,2020] YangFan,ShufangXie,YingceXia,LijunWu,TaoQin,Xiang-YangLi,andTie-Yan Liu. Multi-branchattentivetransformer. arXivpreprintarXiv:2006.10270,2020. [Faruquietal.,2016] ManaalFaruqui,YuliaTsvetkov,PushpendreRastogi,andChrisDyer. Problems withevaluationofwordembeddingsusingwordsimilaritytasks. InProceedingsofthe1stWorkshop onEvaluatingVector-SpaceRepresentationsforNLP,pages30–35,2016. [Fedusetal.,2022] WilliamFedus,JeffDean,andBarretZoph. Areviewofsparseexpertmodelsin deeplearning. arXivpreprintarXiv:2209.01667,2022a. [Fedusetal.,2022] WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scaling totrillionparametermodelswithsimpleandefficientsparsity. TheJournalofMachineLearning Research,23(1):5232–5270,2022b. [Fellbaum,2005] ChristianeFellbaum. Wordnetandwordnets. InKeithBrown,editor,Encyclopedia 652 BIBLIOGRAPHY ofLanguageandLinguistics(2nded.).Elsevier,2005. [Fengetal.,2016] ShiFeng,ShujieLiu,NanYang,MuLi,MingZhou,andKennyZhu. Improving attentionmodelingwithimplicitdistortionandfertilityformachinetranslation. InProceedingsof COLING2016,the26thInternationalConferenceonComputationalLinguistics: TechnicalPapers, pages3082–3092,2016. [Feng et al., 2021] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi, TerukoMitamura,andEduardHovy.AsurveyofdataaugmentationapproachesforNLP.InFindings oftheAssociationforComputationalLinguistics: ACL-IJCNLP2021,pages968–988,2021. [Fernandesetal.,2023] PatrickFernandes,AmanMadaan,EmmyLiu,AntónioFarinhas,PedroHen- rique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig,andAndréF.T.Martins. Bridgingthegap: Asurveyonintegrating(human)feedbackfor natural languagegeneration. Transactions ofthe Association forComputational Linguistics, 11: 1643–1668,2023. [Firth,1957] JohnRFirth. Asynopsisoflinguistictheory,1930-1955. Studiesinlinguisticanalysis, 1957. [Forney,1972] GDJRForney. Maximum-likelihoodsequenceestimationofdigitalsequencesinthe presenceofintersymbolinterference. IEEETransactionsonInformationtheory,18(3):363–378, 1972. [FranklinandGraesser,1996] StanFranklinandArtGraesser. Isitanagent,orjustaprogram?: A taxonomyforautonomousagents. InInternationalworkshoponagenttheories,architectures,and languages,pages21–35.Springer,1996. [Freedmanetal.,2007] DavidFreedman,RobertPisani,andRogerPurves. Statistics(4thed.). W.W. Norton&Company,2007. [Freedman,2009] DavidA.Freedman. StatisticalModels: TheoryandPractice(2nded.). Cambridge UniversityPress,2009. [FreitagandAl-Onaizan,2017] MarkusFreitagandYaserAl-Onaizan. Beamsearchstrategiesfor neuralmachinetranslation. InProceedingsoftheFirstWorkshoponNeuralMachineTranslation, pages56–60,2017. [Freitagetal., 2022] MarkusFreitag, DavidGrangier, QijunTan, andBowenLiang. Highquality ratherthanhighmodelprobability: Minimumbayesriskdecodingwithneuralmetrics. Transactions oftheAssociationforComputationalLinguistics,10:811–825,2022. [Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The Europeanperspective. PsychologyPress,2014. [Friedl,2006] JeffreyFriedl. MasteringRegularExpressions(3rded.). O’ReillyMedia,2006. [Fu et al., 2022] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and ChristopherRe. Hungryhungryhippos: Towardslanguagemodelingwithstatespacemodels. In ProceedingsofTheEleventhInternationalConferenceonLearningRepresentations,2022. [Fuller,2009] WayneAFuller. Introductiontostatisticaltimeseries. JohnWiley&Sons,2009. [Gage, 1994] PhilipGage. Anewalgorithmfordatacompression. CUsersJournal, 12(2):23–38, 1994. [Galeetal.,2019] TrevorGale,ErichElsen,andSaraHooker. Thestateofsparsityindeepneural BIBLIOGRAPHY 653 networks. arXivpreprintarXiv:1902.09574,2019. [Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile Lukosiute,AnnaChen,AnnaGoldie,AzaliaMirhoseini,CatherineOlsson,DannyHernandez,Dawn Drain,DustinLi,EliTran-Johnson,EthanPerez,JacksonKernion,JamieKerr,JaredMueller,Joshua Landau,KamalNdousse,KarinaNguyen,LianeLovitt,MichaelSellitto,NelsonElhage,Noemí Mercado,NovaDasSarma,OliverRausch,RobertLasenby,RobinLarson,SamRinger,Sandipan Kundu,SauravKadavath,ScottJohnston,ShaunaKravec,SheerElShowk,TameraLanham,Timothy Telleen-Lawton,TomHenighan,TristanHume,YuntaoBai,ZacHatfield-Dodds,BenMann,Dario Amodei,NicholasJoseph,SamMcCandlish,TomBrown,ChristopherOlah,JackClark,SamuelR. Bowman,andJaredKaplan. Thecapacityformoralself-correctioninlargelanguagemodels. arXiv preprintarXiv:2302.07459,2023. [Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. InInternationalConferenceonMachineLearning,pages10835–10866.PMLR, 2023a. [Gaoetal.,2023] LuyuGao,AmanMadaan,ShuyanZhou,UriAlon,PengfeiLiu,YimingYang,Jamie Callan,andGrahamNeubig. Pal: Program-aidedlanguagemodels. InInternationalConferenceon MachineLearning,pages10764–10799.PMLR,2023b. [Gaoetal.,2023] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai, JiaweiSun,andHaofenWang. Retrieval-augmentedgenerationforlargelanguagemodels: Asurvey. arXivpreprintarXiv:2312.10997,2023c. [Gargetal.,2019] SarthakGarg,StephanPeitz,UdhyakumarNallasamy,andMatthiasPaulik. Jointly learningtoalignandtranslatewithtransformermodels. InProceedingsofthe2019Conferenceon EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon NaturalLanguageProcessing(EMNLP-IJCNLP),pages4453–4462,2019. [Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural InformationProcessingSystems,35:30583–30598,2022. [Geetal.,2024] YuanGe,YilunLiu,ChiHu,WeibinMeng,ShiminTao,XiaofengZhao,Hongxia Ma,LiZhang,BoxingChen,HaoYang,BeiLi,TongXiao,andJingboZhu. Clusteringandranking: Diversity-preservedinstructionselectionthroughexpert-alignedqualityestimation. arXivpreprint arXiv:2402.18191,2024. [Gehringetal.,2017] JonasGehring,MichaelAuli,DavidGrangier,andYannDauphin.Aconvolutional encodermodelforneuralmachinetranslation. InProceedingsofthe55thAnnualMeetingofthe AssociationforComputationalLinguistics(Volume1: LongPapers),pages123–135,2017a. [Gehringetal.,2017] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannNDauphin. Convolutionalsequencetosequencelearning. InInternationalconferenceonmachinelearning, pages1243–1252.PMLR,2017b. [Gelmanetal.,2020] AndrewGelman,JohnB.Carlin,HalS.Stern,DavidB.Dunson,AkiVehtari, andDonaldB.Rubin. BayesianDataAnalysis(2nded.). ChapmanandHall/CRC,2020. [GemmaTeam,2024] GoogleDeepMindGemmaTeam. Gemma: OpenModelsBasedonGemini ResearchandTechnology,2024. [Germannetal.,2004] UlrichGermann,MichaelJahr,KevinKnight,DanielMarcu,andKenjiYamada. Fastandoptimaldecodingformachinetranslation. ArtificialIntelligence,154(1-2):127–143,2004. 654 BIBLIOGRAPHY [Géron,2019] AurélienGéron.Hands-OnMachineLearningwithScikit-Learn,Keras,andTensorFlow: Concepts,Tools,andTechniquestoBuildIntelligentSystems(2nded.). O’ReillyMedia,2019. [Gersetal.,2000] FelixAGers,JürgenSchmidhuber,andFredCummins.Learningtoforget:Continual predictionwithlstm. Neuralcomputation,12(10):2451–2471,2000. [Ghazvininejadetal.,2019] MarjanGhazvininejad,OmerLevy,YinhanLiu,andLukeZettlemoyer. Mask-predict: Paralleldecodingofconditionalmaskedlanguagemodels. InProceedingsofthe2019 ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages6112–6121,2019. [Gholamietal.,2022] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In Low-PowerComputerVision,pages291–326.ChapmanandHall/CRC,2022. [GildeaandJurafsky,2002] DanielGildeaandDanielJurafsky. Automaticlabelingofsemanticroles. ComputationalLinguistics,28(3):245–288,2002. [Gladkovaetal., 2016] AnnaGladkova, AleksandrDrozd, andSatoshiMatsuoka. Analogy-based detectionofmorphologicalandsemanticrelationswithwordembeddings: whatworksandwhat doesn’t. InProceedingsoftheNAACLStudentResearchWorkshop,pages8–15,2016. [GlorotandBengio,2010] XavierGlorotandYoshuaBengio. Understandingthedifficultyoftraining deepfeedforwardneuralnetworks. InProceedingsofthethirteenthinternationalconferenceon artificialintelligenceandstatistics,pages249–256.JMLRWorkshopandConferenceProceedings, 2010. [GoelandByrne,2000] VaibhavaGoelandWilliamJByrne. Minimumbayes-riskautomaticspeech recognition.",
    "Neuralcomputation,12(10):2451–2471,2000. [Ghazvininejadetal.,2019] MarjanGhazvininejad,OmerLevy,YinhanLiu,andLukeZettlemoyer. Mask-predict: Paralleldecodingofconditionalmaskedlanguagemodels. InProceedingsofthe2019 ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages6112–6121,2019. [Gholamietal.,2022] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney, and Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In Low-PowerComputerVision,pages291–326.ChapmanandHall/CRC,2022. [GildeaandJurafsky,2002] DanielGildeaandDanielJurafsky. Automaticlabelingofsemanticroles. ComputationalLinguistics,28(3):245–288,2002. [Gladkovaetal., 2016] AnnaGladkova, AleksandrDrozd, andSatoshiMatsuoka. Analogy-based detectionofmorphologicalandsemanticrelationswithwordembeddings: whatworksandwhat doesn’t. InProceedingsoftheNAACLStudentResearchWorkshop,pages8–15,2016. [GlorotandBengio,2010] XavierGlorotandYoshuaBengio. Understandingthedifficultyoftraining deepfeedforwardneuralnetworks. InProceedingsofthethirteenthinternationalconferenceon artificialintelligenceandstatistics,pages249–256.JMLRWorkshopandConferenceProceedings, 2010. [GoelandByrne,2000] VaibhavaGoelandWilliamJByrne. Minimumbayes-riskautomaticspeech recognition. ComputerSpeech&Language,14(2):115–135,2000. [Gomez et al., 2017] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations. Advances in neural informationprocessingsystems,30,2017. [Goodfellowetal.,2015] IanGoodfellow,JonathonShlens,andChristianSzegedy. Explainingand harnessingadversarialexamples. InInternationalConferenceonLearningRepresentations,2015. [Goodfellowetal.,2016] IanGoodfellow,YoshuaBengio,andAaronCourville. DeepLearning. MIT Press,2016. [Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience. Springer,1984. [Goodman,1996] JoshuaGoodman. Parsingalgorithmsandmetrics. In34thAnnualMeetingofthe AssociationforComputationalLinguistics,pages177–183,1996a. [Goodman,1996] JoshuaGoodman. Parsingalgorithmsandmetrics. In34thAnnualMeetingofthe AssociationforComputationalLinguistics,pages177–183,1996b. [Gordonetal.,2021] MitchellAGordon,KevinDuh,andJaredKaplan. Dataandparameterscaling lawsforneuralmachinetranslation. InProceedingsofthe2021ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages5915–5922,2021. [Gouetal.,2021] JianpingGou,BaoshengYu,StephenJMaybank,andDachengTao. Knowledge distillation: Asurvey. InternationalJournalofComputerVision,129:1789–1819,2021. [Gouetal.,2024] ZhibinGou,ZhihongShao,YeyunGong,YujiuYang,NanDuan,WeizhuChen, BIBLIOGRAPHY 655 etal. Critic: Largelanguagemodelscanself-correctwithtool-interactivecritiquing. InTheTwelfth InternationalConferenceonLearningRepresentations,2024. [GravesandJaitly,2014] AlexGravesandNavdeepJaitly. Towardsend-to-endspeechrecognition withrecurrentneuralnetworks. InProceedingsofInternationalconferenceonmachinelearning, pages1764–1772,2014. [Gravesetal.,2006] AlexGraves,SantiagoFernández,FaustinoGomez,andJürgenSchmidhuber. Connectionisttemporalclassification: labellingunsegmentedsequencedatawithrecurrentneural networks. InProceedingsofthe23rdinternationalconferenceonMachinelearning,pages369–376, 2006. [Graves et al., 2013] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognitionwithdeepbidirectionallstm. InIEEEworkshoponautomaticspeechrecognitionand understanding,pages273–278.IEEE,2013a. [Gravesetal.,2013] AlexGraves,Abdel-rahmanMohamed,andGeoffreyHinton. Speechrecognition withdeeprecurrentneuralnetworks. In2013IEEEinternationalconferenceonacoustics,speech andsignalprocessing,pages6645–6649.IEEE,2013b. [Gravesetal.,2014] AlexGraves,GregWayne,andIvoDanihelka. Neuralturingmachines. arXiv preprintarXiv:1410.5401,2014. [Graves et al., 2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, AgnieszkaGrabska-Barwinska,SergioGomezColmenarejo,EdwardGrefenstette,TiagoRamalho, JohnAgapiou,AdriáPuigdoménechBadia,KarlMoritzHermann,YoriZwols,GeorgOstrovski, AdamCain,HelenKing,ChristopherSummerfield,PhilBlunsom,KorayKavukcuoglu,andDemis Hassabis. Hybridcomputingusinganeuralnetworkwithdynamicexternalmemory. Nature,538 (7626):471–476,2016. [Gray,1998] RobertM.Gray.Quantization.IEEEtransactionsoninformationtheory,44(6):2325–2383, 1998. [Grissom II et al., 2014] Alvin Grissom II, He He, Jordan Boyd-Graber, John Morgan, and Hal Daumé III. Don’t until the final verb wait: Reinforcement learning for simultaneous machine translation. InProceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage Processing(EMNLP),pages1342–1352,2014. [GroverandLeskovec,2016] AdityaGroverandJureLeskovec. node2vec: Scalablefeaturelearning fornetworks. InProceedingsofthe22ndACMSIGKDDinternationalconferenceonKnowledge discoveryanddatamining,pages855–864,2016. [GuandDao,2023] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselective statespaces. arXivpreprintarXiv:2312.00752,2023. [Guetal.,2021] AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswith structuredstatespaces. InProceedingsofInternationalConferenceonLearningRepresentations, 2021. [Guetal.,2022] AlbertGu,KaranGoel,AnkitGupta,andChristopherRé.Ontheparameterizationand initializationofdiagonalstatespacemodels. AdvancesinNeuralInformationProcessingSystems, 35:35971–35983,2022a. [Guetal.,2022] AlbertGu,KaranGoel,KhaledSaab,andChrisRé.Structuredstatespaces:Combining continuous-time,recurrent,andconvolutionalmodels. https://hazyresearch.stanford. 656 BIBLIOGRAPHY edu/blog/2022-01-14-s4-3,2022b. Retrieved2022-01-14. [Gu et al., 2017] Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. Learning to translateinreal-timewithneuralmachinetranslation. InProceedingsoftheEuropeanChapterof theAssociationforComputationalLinguistics(EACL)Conference,2017,2017. [Guetal., 2018] JiataoGu, JamesBradbury, CaimingXiong, VictorO.K.Li, andRichardSocher. Non-autoregressive neural machine translation. In Proceedings of International Conference on LearningRepresentations,2018. [Gulatietal.,2020] AnmolGulati,JamesQin,Chung-ChengChiu,NikiParmar,YuZhang,Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmentedtransformerforspeechrecognition. ProceedingsofInterspeech2020,pages 5036–5040,2020. [Gulcehreetal.,2016] CaglarGulcehre,MarcinMoczulski,MishaDenil,andYoshuaBengio. Noisy activationfunctions. InMariaFlorinaBalcanandKilianQ.Weinberger,editors,Proceedingsof The33rdInternationalConferenceonMachineLearning,volume48ofProceedingsofMachine LearningResearch,pages3059–3068.PMLR,2016. [Gulcehreetal.,2017] CaglarGulcehre,OrhanFirat,KelvinXu,KyunghyunCho,andYoshuaBengio. Onintegratingalanguagemodelintoneuralmachinetranslation. ComputerSpeech&Language,45: 137–148,2017. [Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,AdilSalim,ShitalShah,HarkiratSinghBehl,XinWang,SébastienBubeck,RonenEldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv preprint arXiv:2306.11644,2023. [Guo et al., 2019] Maosheng Guo, Yu Zhang, and Ting Liu. Gaussian transformer: a lightweight approach for natural language inference. In Proceedings of the AAAI Conference on Artificial Intelligence,pages6489–6496,2019. [Guoetal.,2024] QingyanGuo,RuiWang,JunliangGuo,BeiLi,KaitaoSong,XuTan,GuoqingLiu, JiangBian,andYujiuYang. Connectinglargelanguagemodelswithevolutionaryalgorithmsyields powerfulpromptoptimizers. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024. [Guoetal.,2020] QipengGuo,XipengQiu,PengfeiLiu,XiangyangXue,andZhengZhang. Multi- scale self-attention for text classification. In Proceedings of the AAAI Conference on Artificial Intelligence,volume34,pages7847–7854,2020. [GuptaandBerant,2020] AnkitGuptaandJonathanBerant. Gmat: Globalmemoryaugmentationfor transformers. arXivpreprintarXiv:2006.03274,2020. [Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficienttransformersviatop-kattention. InProceedingsoftheSecondWorkshoponSimple andEfficientNaturalLanguageProcessing,pages39–52,2021. [Gupta et al., 2004] Madan Gupta, Liang Jin, and Noriyasu Homma. Static and dynamic neural networks: fromfundamentalstoadvancedtheory. JohnWiley&Sons,2004. [Guu et al., 2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrievalaugmentedlanguagemodelpre-training. InProceedingsofInternationalconferenceon BIBLIOGRAPHY 657 machinelearning,pages3929–3938.PMLR,2020. [Guyon and Elisseeff, 2003] Isabelle Guyon and André Elisseeff. An introduction to variable and featureselection. Journalofmachinelearningresearch,3(Mar):1157–1182,2003. [Haber and Ruthotto, 2017] Eldad Haber and Lars Ruthotto.",
    "dynamic neural networks: fromfundamentalstoadvancedtheory. JohnWiley&Sons,2004. [Guu et al., 2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrievalaugmentedlanguagemodelpre-training. InProceedingsofInternationalconferenceon BIBLIOGRAPHY 657 machinelearning,pages3929–3938.PMLR,2020. [Guyon and Elisseeff, 2003] Isabelle Guyon and André Elisseeff. An introduction to variable and featureselection. Journalofmachinelearningresearch,3(Mar):1157–1182,2003. [Haber and Ruthotto, 2017] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverseproblems,34(1):014004,2017. [Hahn, 2020] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. TransactionsoftheAssociationforComputationalLinguistics,8:156–171,2020. [Hamilton,1994] JamesDouglasHamilton. TimeSeriesAnalysis. PrincetonUniversityPress,1994. [Hanetal.,2022] KaiHan,YunheWang,HantingChen,XinghaoChen,JianyuanGuo,ZhenhuaLiu, YehuiTang,AnXiao,ChunjingXu,YixingXu,ZhaohuiYang,YimanZhang,andDachengTao. A surveyonvisiontransformer. IEEEtransactionsonpatternanalysisandmachineintelligence,45(1): 87–110,2022. [Han et al., 2020] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin,AnmolGulati,RuomingPang,andYonghuiWu. Contextnet: Improvingconvolutionalneural networksforautomaticspeechrecognitionwithglobalcontext. InProceedingsofInterspeech2020, pages3610–3614,2020. [Hanetal.,2021] XuHan,ZhengyanZhang,NingDing,YuxianGu,XiaoLiu,YuqiHuo,Jiezhong Qiu,LiangZhang,WentaoHan,MinlieHuang,QinJin,YanyanLan,YangLiu,ZhiyuanLiu,Zhiwu Lu,XipengQiu,RuihuaSong,JieTang,Ji-RongWen,JinhuiYuan,WayneXinZhao,andJunZhu. Pre-trainedmodels: Past,presentandfuture. AIOpen,2:225–250,2021a. [Hanetal.,2021] YizengHan,GaoHuang,ShijiSong,LeYang,HonghuiWang,andYulinWang. Dy- namicneuralnetworks: Asurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence, 44(11):7436–7456,2021b. [Hanetal.,2024] ZeyuHan,ChaoGao,JinyangLiu,JeffZhang,andSaiQianZhang. Parameter- efficientfine-tuningforlargemodels: Acomprehensivesurvey. arXivpreprintarXiv:2403.14608, 2024. [Hannunetal.,2014] AwniHannun,CarlCase,JaredCasper,BryanCatanzaro,GregDiamos,Erich Elsen,RyanPrenger,SanjeevSatheesh,ShubhoSengupta,andAdamCoates. Deepspeech: Scaling upend-to-endspeechrecognition. arXivpreprintarXiv:1412.5567,2014. [Hao et al., 2019] Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and Zhaopeng Tu. Multi- granularityself-attentionforneuralmachinetranslation. InProceedingsofthe2019Conferenceon EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon NaturalLanguageProcessing(EMNLP-IJCNLP),pages887–897,2019. [Haoetal.,2022] YidingHao,DanaAngluin,andRobertFrank. Formallanguagerecognitionbyhard attentiontransformers: Perspectivesfromcircuitcomplexity. TransactionsoftheAssociationfor ComputationalLinguistics,10:800–810,2022. [Harlapetal.,2018] AaronHarlap,DeepakNarayanan,AmarPhanishayee,VivekSeshadri,Nikhil Devanur,GregGanger,andPhilGibbons. Pipedream:Fastandefficientpipelineparalleldnntraining. arXivpreprintarXiv:1806.03377,2018. [Harris,1954] ZelligSHarris. Distributionalstructure. Word,10(2-3):146–162,1954. [Hasleretal.,2018] EvaHasler,AdriàdeGispert,GonzaloIglesias,andBillByrne. Neuralmachine translationdecodingwithterminologyconstraints. InProceedingsofthe2018Conferenceofthe 658 BIBLIOGRAPHY North American Chapter of the Association for Computational Linguistics: Human Language Technologies,Volume2(ShortPapers),pages506–512,2018. [Hastieetal.,2009] TrevorHastie,RobertTibshirani,andJeromeFriedman. TheElementsofStatistical Learning. Spinger,2009. [Heetal.,2017] DiHe,HanqingLu,YingceXia,TaoQin,LiweiWang,andTie-YanLiu. Decoding withvaluenetworksforneuralmachinetranslation. AdvancesinNeuralInformationProcessing Systems,30,2017. [He et al., 2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassinghuman-levelperformanceonimagenetclassification. InProceedingsofthe IEEEinternationalconferenceoncomputervision,pages1026–1034,2015. [Heetal.,2016] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages770–778,2016a. [Heetal.,2016] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Identitymappingsindeep residualnetworks. InProceedingsofECCV2016,pages630–645,2016b. [Heetal.,2019] KaimingHe,RossGirshick,andPiotrDollár. Rethinkingimagenetpre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918–4927, 2019. [Heetal.,2022] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Maskedautoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFConferenceon ComputerVisionandPatternRecognition,pages16000–16009,2022. [Heetal.,2021] PengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen. Deberta: Decoding- enhancedbertwithdisentangledattention. InProceedingsofInternationalConferenceonLearning Representations,2021. [Heetal.,2016] WeiHe,ZhongjunHe,HuaWu,andHaifengWang. Improvedneuralmachinetransla- tionwithsmtfeatures. InProceedingsoftheThirtiethAAAIconferenceonartificialintelligence, 2016c. [Heetal.,2018] XuanliHe,GholamrezaHaffari,andMohammadNorouzi. Sequencetosequencemix- turemodelfordiversemachinetranslation. InProceedingsofthe22ndConferenceonComputational NaturalLanguageLearning,pages583–592,2018. [Heafieldetal.,2021] KennethHeafield,QianqianZhu,andRomanGrundkiewicz. Findingsofthe WMT2021sharedtaskonefficienttranslation. InProceedingsoftheSixthConferenceonMachine Translation,pages639–651,2021. [HendrycksandGimpel,2016] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus). arXivpreprintarXiv:1606.08415,2016. [Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages 2744–2751,2020. [Hendrycksetal.,2021] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In ProceedingsofInternationalConferenceonLearningRepresentations,2021. BIBLIOGRAPHY 659 [Hestnessetal.,2017] JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun, HassanKianinejad,MdMostofaAliPatwary,YangYang,andYanqiZhou. Deeplearningscalingis predictable,empirically. arXivpreprintarXiv:1712.00409,2017. [Hewitt,2024] JohnHewitt. Instructionfollowingwithoutinstructiontuning,2024. URLhttps: //nlp.stanford.edu/~johnhew/instruction-following.html. [HewittandLiang,2019] JohnHewittandPercyLiang. Designingandinterpretingprobeswithcontrol tasks. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing andthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP), pages2733–2743,2019. [Hewittetal.,2024] JohnHewitt,NelsonFLiu,PercyLiang,andChristopherDManning. Instruction followingwithoutinstructiontuning. arXivpreprintarXiv:2409.14254,2024. [HildebrandandVogel,2008] AlmutSiljaHildebrandandStephanVogel. Combinationofmachine translationsystemsviahypothesisselectionfromcombinedn-bestlists. InProceedingsofthe8th ConferenceoftheAssociationforMachineTranslationintheAmericas: StudentResearchWorkshop, pages254–261,2008. [Hilletal.,2016] FelixHill,KyunghyunCho,andAnnaKorhonen.Learningdistributedrepresentations ofsentencesfromunlabelleddata. InProceedingsofthe2016ConferenceoftheNorthAmerican ChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages 1367–1377,2016. [Hinton, 2018] Geoff Hinton. Coursera neural networks for machine learning lecture 6, 2018. URL http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_ slides_lec6.pdf. [Hintonetal.,2015] GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeina neuralnetwork. arXivpreprintarXiv:1503.02531,2015. [Hinton,2007] GeoffreyEHinton. Learningmultiplelayersofrepresentation. Trendsincognitive sciences,11(10):428–434,2007. [Hinton and Roweis, 2002] Geoffrey E Hinton and Sam Roweis. Stochastic neighbor embedding. Advancesinneuralinformationprocessingsystems,15,2002. [Hinton and Salakhutdinov, 2006] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionalityofdatawithneuralnetworks. science,313(5786):504–507,2006. [Hintonetal.,2006] GeoffreyEHinton,SimonOsindero,andYee-WhyeTeh.Afastlearningalgorithm fordeepbeliefnets. Neuralcomputation,18(7):1527–1554,2006. [Hinton et al., 2012] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and RuslanRSalakhutdinov. Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors. arXivpreprintarXiv:1207.0580,2012. [Hoangetal.,2017] CongDuyVuHoang,GholamrezaHaffari,andTrevorCohn. Towardsdecoding ascontinuousoptimisationinneuralmachinetranslation. InProceedingsofthe2017Conferenceon EmpiricalMethodsinNaturalLanguageProcessing,pages146–156,2017. [Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neuralcomputation,9(8):1735–1780,1997. [Hoffmannetal.,2022] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,Aidan 660 BIBLIOGRAPHY Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and LaurentSifre. Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556, 2022. [HokampandLiu,2017] ChrisHokampandQunLiu. Lexicallyconstraineddecodingforsequence generationusinggridbeamsearch. InProceedingsofthe55thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages1535–1546,2017. [Holmström and Koistinen, 1992] Lasse Holmström",
    "Neuralcomputation,9(8):1735–1780,1997. [Hoffmannetal.,2022] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya, TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,Aidan 660 BIBLIOGRAPHY Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and LaurentSifre. Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556, 2022. [HokampandLiu,2017] ChrisHokampandQunLiu. Lexicallyconstraineddecodingforsequence generationusinggridbeamsearch. InProceedingsofthe55thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages1535–1546,2017. [Holmström and Koistinen, 1992] Lasse Holmström and Petri Koistinen. Using additive noise in back-propagationtraining. IEEETransactionsonNeuralNetworks,3(1):24–38,1992. [Holtzmanetal.,2020] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecurious caseofneuraltextdegeneration. InProceedingsofthe6thInternationalConferenceonLearning RepresentationsICLR,2020a. [Holtzmanetal.,2020] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecurious caseofneuraltextdegeneration. InInternationalConferenceonLearningRepresentations,2020b. [Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuninglanguagemodelswith(almost)nohumanlabor. InProceedingsofthe61st AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages 14409–14428,2023. [Hopfield,1982] JohnJHopfield. Neuralnetworksandphysicalsystemswithemergentcollective computationalabilities. Proceedingsofthenationalacademyofsciences,79(8):2554–2558,1982. [Hopfield, 1984] John J Hopfield. Neurons with graded response have collective computational propertieslikethoseoftwo-stateneurons. Proceedingsofthenationalacademyofsciences,81(10): 3088–3092,1984. [Houetal.,2020] LuHou,ZhiqiHuang,LifengShang,XinJiang,XiaoChen,andQunLiu. Dynabert: Dynamicbertwithadaptivewidthanddepth. AdvancesinNeuralInformationProcessingSystems, 33:9782–9793,2020. [Houlsbyetal.,2019] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,Quentin DeLaroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransfer learningforNLP. InProceedingsofthe36thInternationalConferenceonMachineLearning,pages 2790–2799.PMLR,2019. [Howardetal.,2019] AndrewHoward,RuomingPang,HartwigAdam,QuocV.Le,MarkSandler, BoChen,WeijunWang,Liang-ChiehChen,MingxingTan,GraceChu,VijayVasudevan,andYukun Zhu. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computervision,pages1314–1324,2019. [Hsuetal.,2021] Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai,KushalLakhotia,Ruslan Salakhutdinov,andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearning bymaskedpredictionofhiddenunits. IEEE/ACMTransactionsonAudio,Speech,andLanguage Processing,29:3451–3460,2021. [Huetal.,2021] ChiHu,ChenglongWang,XiangnanMa,XiaMeng,YinqiaoLi,TongXiao,Jingbo Zhu, and Changliang Li. Ranknas: Efficient neural architecture search by pairwise ranking. In Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages 2469–2480,2021. BIBLIOGRAPHY 661 [Huetal., 2022] EdwardJHu, yelongshen, PhillipWallis, ZeyuanAllen-Zhu, YuanzhiLi, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In InternationalConferenceonLearningRepresentations,2022. [Huangetal.,2018] Cheng-ZhiAnnaHuang,AshishVaswani,JakobUszkoreit,IanSimon,Curtis Hawthorne,NoamShazeer,AndrewMDai,MatthewDHoffman,MonicaDinculescu,andDouglas Eck. Musictransformer: Generatingmusicwithlong-termstructure. InProceedingsofInternational ConferenceonLearningRepresentations,2018. [Huang et al., 2012] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improvingwordrepresentationsviaglobalcontextandmultiplewordprototypes. InProceedingsof the50thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers), pages873–882,2012. [Huangetal.,2016] GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQWeinberger. Deep networkswithstochasticdepth. InProceedingsofthe14thEuropeanConference,pages646–661. Springer,2016. [Huangetal.,2017] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger. Denselyconnectedconvolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputer visionandpatternrecognition,pages4700–4708,2017a. [Huang,2009] LiangHuang. Dynamicprogramming-basedsearchalgorithmsinNLP. InProceedings ofHumanLanguageTechnologies: The2009AnnualConferenceoftheNorthAmericanChapterof theAssociationforComputationalLinguistics,CompanionVolume: TutorialAbstracts,2009. [Huangetal.,2017] LiangHuang,KaiZhao,andMingboMa. Whentofinish? optimalbeamsearch forneuraltextgeneration(modulobeamsize). InProceedingsofthe2017ConferenceonEmpirical MethodsinNaturalLanguageProcessing,pages2134–2139,2017b. [Huangetal., 2020] XiaoShiHuang, FelipePerez, JimmyBa, andMaksimsVolkovs. Improving transformeroptimizationthroughbetterinitialization. InProceedingsofInternationalConferenceon MachineLearning,pages4475–4483.PMLR,2020a. [Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, DehaoChen,HyoukJoongLee,JiquanNgiam,QuocVLe,YonghuiWu,andZhifengChen. Gpipe: Efficienttrainingofgiantneuralnetworksusingpipelineparallelism. Advancesinneuralinformation processingsystems,32,2019. [Huangetal.,2015] ZhihengHuang,WeiXu,andKaiYu. Bidirectionallstm-crfmodelsforsequence tagging. arXivpreprintarXiv:1508.01991,2015. [Huangetal.,2020] ZhihengHuang,DavisLiang,PengXu,andBingXiang. Improvetransformer modelswithbetterrelativepositionembeddings. InFindingsoftheAssociationforComputational Linguistics: EMNLP2020,pages3327–3335,2020b. [HubelandWiesel,1959] DavidHHubelandTorstenNWiesel. Receptivefieldsofsingleneuronesin thecat’sstriatecortex. TheJournalofphysiology,148(3):574,1959. [Hurley,2011] PatrickHurley. AConciseIntroductiontoLogic(11thed.). WadsworthPublishing, 2011. [Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrenttransformers. Advancesinneuralinformationprocessingsystems,35: 33248–33261,2022. 662 BIBLIOGRAPHY [Hutchison et al., 2013] Keith A Hutchison, David A Balota, James H Neely, Michael J Cortese, EmilyRCohen-Shikora,Chi-ShingTse,MelvinJYap,JesseJBengson,DaleNiemeyer,andErin Buchanan. Thesemanticprimingproject. Behaviorresearchmethods,45(4):1099–1114,2013. [IoffeandSzegedy, 2015] SergeyIoffeandChristianSzegedy. Batchnormalization: Accelerating deepnetworktrainingbyreducinginternalcovariateshift. InInternationalconferenceonmachine learning,pages448–456.PMLR,2015. [Ivanovetal.,2021] AndreiIvanov,NikoliDryden,TalBen-Nun,ShigangLi,andTorstenHoefler. Datamovementisallyouneed:Acasestudyonoptimizingtransformers. InProceedingsofMachine LearningandSystems,volume3,pages711–732,2021. [Jackendoff,1992] RaySJackendoff. Semanticstructures,volume18. MITpress,1992. [Jacobetal.,2018] BenoitJacob,SkirmantasKligys,BoChen,MenglongZhu,MatthewTang,Andrew Howard,HartwigAdam,andDmitryKalenichenko. Quantizationandtrainingofneuralnetworks forefficientinteger-arithmetic-onlyinference. InProceedingsoftheIEEEconferenceoncomputer visionandpatternrecognition,pages2704–2713,2018. [Jaderbergetal.,2015] MaxJaderberg,KarenSimonyan,AndrewZisserman,andkoraykavukcuoglu. Spatialtransformernetworks. Advancesinneuralinformationprocessingsystems,28,2015. [JaegerandHaas,2004] HerbertJaegerandHaraldHaas. Harnessingnonlinearity: Predictingchaotic systemsandsavingenergyinwirelesscommunication. science,304(5667):78–80,2004. [Jaegleetal.,2021] AndrewJaegle,SebastianBorgeaud,Jean-BaptisteAlayrac,CarlDoersch,Catalin Ionescu,DavidDing,SkandaKoppula,DanielZoran,AndrewBrock,EvanShelhamer,OlivierJ. Hénaff,MatthewM.Botvinick,AndrewZisserman,OriolVinyals,andJoãoCarreira. Perceiverio: Ageneralarchitectureforstructuredinputs&outputs. InProceedingsofInternationalConference onLearningRepresentations,2021. [Janssen,2012] TheoM.V.Janssen. Compositionality: itshistoriccontext. InM.Werning,W.Hinzen, andE.Machery,editors,TheOxfordhandbookofcompositionality.OxfordUniversityPress,2012. [Jean et al., 2015] Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. Montreal neural machine translation systems for wmt’15. In Proceedings of the tenth workshoponstatisticalmachinetranslation,pages134–140,2015. [Jelinek,1998] FrederickJelinek. Statisticalmethodsforspeechrecognition. MITPress,1998. [Jia and Liang, 2017] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehensionsystems. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages2021–2031,2017. [Jiangetal.,2023] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,Deven- dra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, LucileSaulnier,LélioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825,2023a. [Jiangetal.,2023] HuiqiangJiang,QianhuiWu,Chin-YewLin,YuqingYang,andLiliQiu. Llmlingua: Compressingpromptsforacceleratedinferenceoflargelanguagemodels. InProceedingsofthe2023 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages13358–13376,2023b. [Jiangetal.,2020] ZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig. Howcanweknow whatlanguagemodelsknow? TransactionsoftheAssociationforComputationalLinguistics, 8: BIBLIOGRAPHY 663 423–438,2020. [Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,andQunLiu. Tinybert: Distillingbertfornaturallanguageunderstanding. InFindingsofthe AssociationforComputationalLinguistics: EMNLP2020,pages4163–4174,2020. [Jolliffe,2002] IanTJolliffe. Principalcomponentanalysisforspecialtypesofdata. Springer,2002. [Joshietal.,2017] MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer. Triviaqa: A largescaledistantlysupervisedchallengedatasetforreadingcomprehension. InProceedingsofthe 55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:",
    "arXiv:2310.06825,2023a. [Jiangetal.,2023] HuiqiangJiang,QianhuiWu,Chin-YewLin,YuqingYang,andLiliQiu. Llmlingua: Compressingpromptsforacceleratedinferenceoflargelanguagemodels. InProceedingsofthe2023 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages13358–13376,2023b. [Jiangetal.,2020] ZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig. Howcanweknow whatlanguagemodelsknow? TransactionsoftheAssociationforComputationalLinguistics, 8: BIBLIOGRAPHY 663 423–438,2020. [Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang,andQunLiu. Tinybert: Distillingbertfornaturallanguageunderstanding. InFindingsofthe AssociationforComputationalLinguistics: EMNLP2020,pages4163–4174,2020. [Jolliffe,2002] IanTJolliffe. Principalcomponentanalysisforspecialtypesofdata. Springer,2002. [Joshietal.,2017] MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer. Triviaqa: A largescaledistantlysupervisedchallengedatasetforreadingcomprehension. InProceedingsofthe 55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers), pages1601–1611,2017. [Joshietal.,2020] MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,LukeZettlemoyer,and OmerLevy. Spanbert: Improvingpre-trainingbyrepresentingandpredictingspans. Transactionsof theassociationforcomputationallinguistics,8:64–77,2020. [Joulinetal.,2017] ArmandJoulin,ÉdouardGrave,PiotrBojanowski,andTomášMikolov. Bagof tricksforefficienttextclassification. InProceedingsofthe15thConferenceoftheEuropeanChapter oftheAssociationforComputationalLinguistics: Volume2,ShortPapers,pages427–431,2017. [JurafskyandMartin,2008] DanJurafskyandJamesH.Martin. SpeechandLanguageProcessing (2nded.). PrenticeHall,2008. [Kahneman,2011] DanielKahneman. Thinking,fastandslow. macmillan,2011. [Kalchbrenneretal.,2014] NalKalchbrenner,EdwardGrefenstette,andPhilBlunsom. Aconvolu- tionalneuralnetworkformodellingsentences. InProceedingsofthe52ndAnnualMeetingofthe AssociationforComputationalLinguistics(Volume1: LongPapers),pages655–665,2014. [Kaplanetal.,2020] JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess, RewonChild,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneural languagemodels. arXivpreprintarXiv:2001.08361,2020. [Katharopoulosetal.,2020] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançois Fleuret.Transformersarernns:Fastautoregressivetransformerswithlinearattention.InInternational conferenceonmachinelearning,pages5156–5165.PMLR,2020. [KellyandStone,1975] EdwardF.KellyandPhilipJ.Stone. ComputerrecognitionofEnglishword senses. AmericanElsevierPub,1975. [Kernes, 2021] Jonathan Kernes. Master positional encoding: Part i, 05 2021. URL https://towardsdatascience.com/ master-positional-encoding-part-i-63c05d90a0c3. [Khanetal.,2020] AsifullahKhan,AnabiaSohail,UmmeZahoora,andAqsaSaeedQureshi. Asurvey oftherecentarchitecturesofdeepconvolutionalneuralnetworks. Artificialintelligencereview,53 (8):5455–5516,2020. [Khandelwaletal.,2019] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMike Lewis. Generalizationthroughmemorization: Nearestneighborlanguagemodels. InProceedingsof InternationalConferenceonLearningRepresentations(ICLR),2019. [Khandelwaletal.,2020] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMike Lewis. Generalizationthroughmemorization: Nearestneighborlanguagemodels. InInternational ConferenceonLearningRepresentations,2020. 664 BIBLIOGRAPHY [Khayrallahetal.,2017] HudaKhayrallah,GauravKumar,KevinDuh,MattPost,andPhilippKoehn. Neurallatticesearchfordomainadaptationinmachinetranslation. InProceedingsoftheEighth InternationalJointConferenceonNaturalLanguageProcessing(Volume2: ShortPapers),pages 20–25,2017. [Khotetal.,2023] TusharKhot,HarshTrivedi,MatthewFinlayson,YaoFu,KyleRichardson,Peter Clark,andAshishSabharwal. Decomposedprompting: Amodularapproachforsolvingcomplex tasks. InProceedingsofTheEleventhInternationalConferenceonLearningRepresentations,2023. [Kidger,2022] PatrickKidger. Onneuraldifferentialequations. arXivpreprintarXiv:2202.02435, 2022. [Kikuchietal.,2016] YutaKikuchi,GrahamNeubig,RyoheiSasano,HiroyaTakamura,andManabu Okumura. Controlling output length in neural encoder-decoders. In Proceedings of the 2016 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1328–1338,2016. [KimandCho,2021] GyuwanKimandKyunghyunCho.Length-adaptivetransformer:Trainoncewith lengthdrop,useanytimewithsearch. InProceedingsofthe59thAnnualMeetingoftheAssociation forComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguage Processing(Volume1: LongPapers),pages6501–6511,2021. [Kim et al., 2019] Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R. Thomas McCoy,IanTenney,AlexisRoss,TalLinzen,BenjaminVanDurme,SamuelR.Bowman,andEllie Pavlick. Probingwhatdifferentnlptasksteachmachinesaboutfunctionwordcomprehension. In ProceedingsoftheEighthJointConferenceonLexicalandComputationalSemantics(*SEM2019), pages235–249,2019. [Kimetal.,2023] SehoonKim,ColemanHooper,ThanakulWattanawong,MinwooKang,Ruohan Yan,HasanGenc,GraceDinh,QijingHuang,KurtKeutzer,MichaelW.Mahoney,YakunSophia Shao,andAmirGholami. Fullstackoptimizationoftransformerinference: asurvey. arXivpreprint arXiv:2302.14017,2023. [Kimetal.,2021] WonjaeKim,BokyungSon,andIldooKim. Vilt: Vision-and-languagetransformer withoutconvolutionorregionsupervision. InProceedingsofInternationalConferenceonMachine Learning,pages5583–5594.PMLR,2021. [Kim,2014] YoonKim. Convolutionalneuralnetworksforsentenceclassification. InProceedings ofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages 1746–1751,October2014. [KimandRush,2016] YoonKimandAlexanderMRush. Sequence-levelknowledgedistillation. In Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages 1317–1327,2016. [Kimetal.,2016] YoonKim,YacineJernite,DavidSontag,andAlexanderMRush. Character-aware neurallanguagemodels. InProceedingsoftheThirtiethAAAIconferenceonartificialintelligence, 2016. [KimandAwadalla,2020] YoungJinKimandHanyHassanAwadalla. Fastformers: Highlyefficient transformermodelsfornaturallanguageunderstanding. InProceedingsofSustaiNLP:Workshopon SimpleandEfficientNaturalLanguageProcessing,pages149–158,2020. [KingmaandBa,2014] DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,2014. BIBLIOGRAPHY 665 [KingmaandWelling,2014] DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes. InProceedingsof2ndInternationalConferenceonLearningRepresentations,ICLR2014,2014. [KingmaandWelling,2019] DiederikP.KingmaandMaxWelling. Anintroductiontovariational autoencoders. FoundationsandTrends®inMachineLearning,2019. [Kirkpatricketal., 2017] JamesKirkpatrick, RazvanPascanu, NeilRabinowitz, JoelVeness, Guil- laumeDesjardins,AndreiA.Rusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska- Barwinska,DemisHassabis,ClaudiaClopath,DharshanKumaran,andRaiaHadsell. Overcoming catastrophicforgettinginneuralnetworks. Proceedingsofthenationalacademyofsciences,114 (13):3521–3526,2017. [Kirosetal.,2015] RyanKiros,YukunZhu,RussRSalakhutdinov,RichardZemel,RaquelUrtasun, AntonioTorralba,andSanjaFidler. Skip-thoughtvectors. Advancesinneuralinformationprocessing systems,28,2015. [Kitaevetal.,2020] NikitaKitaev,LukaszKaiser,andAnselmLevskaya. Reformer: Theefficient transformer. InProceedingsofInternationalConferenceonLearningRepresentations,2020. [Kleinetal.,2017] GuillaumeKlein,YoonKim,YuntianDeng,JeanSenellart,andAlexanderMRush. Opennmt: Open-sourcetoolkitforneuralmachinetranslation. InProceedingsofACL2017,System Demonstrations,pages67–72,2017. [Klementievetal.,2012] AlexandreKlementiev,IvanTitov,andBinodBhattarai. Inducingcrosslingual distributedrepresentationsofwords. InProceedingsofCOLING2012,pages1459–1474,2012. [Klerkeetal.,2015] SigridKlerke,HéctorMartínezAlonso,andAndersSøgaard. Lookinghard: Eye trackingfordetectinggrammaticalityofautomaticallycompressedsentences. InProceedingsofthe 20thNordicConferenceofComputationalLinguistics(NODALIDA2015),pages97–105,2015. [Knight,1999] KevinKnight. Decodingcomplexityinword-replacementtranslationmodels. Compu- tationallinguistics,25(4):607–615,1999. [Knight,2009] KevinKnight. Bayesianinferencewithtears,2009. [Knight,2018] LindaKnight. Thesparrowtweets,2018. [Kochenderfer and Wheeler, 2019] Mykel J. Kochenderfer and Tim A. Wheeler. Algorithms for Optimization. TheMITPress,2019. [Koehn,2004] PhilippKoehn. Pharaoh: abeamsearchdecoderforphrase-basedstatisticalmachine translationmodels. InConferenceoftheAssociationforMachineTranslationintheAmericas,pages 115–124.Springer,2004. [Koehn,2010] PhilippKoehn. StatisticalMachineTranslation. CambridgeUniversityPress,2010. [KoehnandKnowles,2017] PhilippKoehnandRebeccaKnowles. Sixchallengesforneuralmachine translation. InProceedingsoftheFirstWorkshoponNeuralMachineTranslation, pages28–39, 2017. [Koehn et al., 2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North AmericanChapteroftheAssociationforComputationalLinguistics,pages127–133,2003. [Koehnetal.,2007] PhilippKoehn,HieuHoang,AlexandraBirch,ChrisCallison-Burch,Marcello Federico,NicolaBertoldi,BrookeCowan,WadeShen,ChristineMoran,RichardZens,ChrisDyer, OndˇrejBojar,AlexandraConstantin,andEvanHerbst. Moses: Opensourcetoolkitforstatistical 666 BIBLIOGRAPHY machinetranslation.InProceedingsofthe45thAnnualMeetingoftheAssociationforComputational LinguisticsCompanionVolumeProceedingsoftheDemoandPosterSessions,pages177–180,2007. [Kojimaetal.,2022] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusuke Iwasawa. Largelanguagemodelsarezero-shotreasoners. Advancesinneuralinformationprocessing systems,35:22199–22213,2022. [KonishiandKitagawa,2007] SadanoriKonishiandGenshiroKitagawa. InformationCriteriaand StatisticalModeling. Spinger,2007. [Korthikantietal.,2023] VijayAnandKorthikanti,JaredCasper,SangkugLym,LawrenceMcAfee, MichaelAndersch,MohammadShoeybi,andBryanCatanzaro. Reducingactivationrecomputation inlargetransformermodels. ProceedingsofMachineLearningandSystems,5,2023. [Krakovna et al., 2020] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specification gam- ing: the flip side of ai ingenuity. https://deepmind.google/discover/blog/ specification-gaming-the-flip-side-of-ai-ingenuity,2020. [Krebs et al., 2018] Alicia Krebs, Alessandro Lenci, and Denis Paperno. SemEval-2018 task 10: Capturingdiscriminativeattributes. InProceedingsofThe12thInternationalWorkshoponSemantic Evaluation,pages732–740,2018. [Krizhevskyetal., 2017] AlexKrizhevsky, IlyaSutskever, andGeoffreyEHinton. Imagenetclas- sification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017. [Kudo,2018] TakuKudo. Subwordregularization: Improvingneuralnetworktranslationmodelswith multiplesubwordcandidates. InProceedingsofthe56thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages66–75,2018. [KudoandRichardson,2018] TakuKudoandJohnRichardson. Sentencepiece: Asimpleandlanguage independentsubwordtokenizeranddetokenizerforneuraltextprocessing. InProceedingsofthe 2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations, pages66–71,2018. [Kulikovetal.,2019] IliaKulikov,AlexanderMiller,KyunghyunCho,andJasonWeston.Importanceof searchandevaluationstrategiesinneuraldialoguemodeling.InProceedingsofthe12thInternational ConferenceonNaturalLanguageGeneration,pages76–87,2019. [Kulis,2013] BrianKulis. Metriclearning: Asurvey. FoundationsandTrends®inMachineLearning, 5(4):287–364,2013. [Kumaretal.,2016] AnkitKumar,OzanIrsoy,PeterOndruska,MohitIyyer,JamesBradbury,Ishaan Gulrajani,VictorZhong,RomainPaulus,andRichardSocher. Askmeanything: Dynamicmemory networksfornaturallanguageprocessing. InInternationalconferenceonmachinelearning,pages 1378–1387,2016. [Kumaretal.,2021] SachinKumar,EricMalmi,AliakseiSeveryn,andYuliaTsvetkov. Controlledtext generationascontinuousoptimizationwithmultipleconstraints. AdvancesinNeuralInformation ProcessingSystems,34:14542–14554,2021. [KumarandByrne,2004] ShankarKumarandWilliamByrne. MinimumBayes-riskdecodingfor statisticalmachinetranslation. InProceedingsoftheHumanLanguageTechnologyConferenceof theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HLT-NAACL2004, BIBLIOGRAPHY 667 pages169–176,2004a.",
    "TakuKudo. Subwordregularization: Improvingneuralnetworktranslationmodelswith multiplesubwordcandidates. InProceedingsofthe56thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages66–75,2018. [KudoandRichardson,2018] TakuKudoandJohnRichardson. Sentencepiece: Asimpleandlanguage independentsubwordtokenizeranddetokenizerforneuraltextprocessing. InProceedingsofthe 2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations, pages66–71,2018. [Kulikovetal.,2019] IliaKulikov,AlexanderMiller,KyunghyunCho,andJasonWeston.Importanceof searchandevaluationstrategiesinneuraldialoguemodeling.InProceedingsofthe12thInternational ConferenceonNaturalLanguageGeneration,pages76–87,2019. [Kulis,2013] BrianKulis. Metriclearning: Asurvey. FoundationsandTrends®inMachineLearning, 5(4):287–364,2013. [Kumaretal.,2016] AnkitKumar,OzanIrsoy,PeterOndruska,MohitIyyer,JamesBradbury,Ishaan Gulrajani,VictorZhong,RomainPaulus,andRichardSocher. Askmeanything: Dynamicmemory networksfornaturallanguageprocessing. InInternationalconferenceonmachinelearning,pages 1378–1387,2016. [Kumaretal.,2021] SachinKumar,EricMalmi,AliakseiSeveryn,andYuliaTsvetkov. Controlledtext generationascontinuousoptimizationwithmultipleconstraints. AdvancesinNeuralInformation ProcessingSystems,34:14542–14554,2021. [KumarandByrne,2004] ShankarKumarandWilliamByrne. MinimumBayes-riskdecodingfor statisticalmachinetranslation. InProceedingsoftheHumanLanguageTechnologyConferenceof theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HLT-NAACL2004, BIBLIOGRAPHY 667 pages169–176,2004a. [Kumar and Byrne, 2004] Shankar Kumar and William Byrne. Minimum bayes-risk decoding for statisticalmachinetranslation. InProceedingsoftheHumanLanguageTechnologyConferenceof theNorthAmericanChapteroftheAssociationforComputationalLinguistics: HLT-NAACL2004, pages169–176,2004b. [KungandPeng,2023] Po-NienKungandNanyunPeng. Domodelsreallylearntofollowinstructions? anempiricalstudyofinstructiontuning. arXivpreprintarXiv:2305.11383,2023. [Kupiec,1992] JulianKupiec. Robustpart-of-speechtaggingusingahiddenmarkovmodel. Computer Speech&Language,6:225–242,1992. [Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, CodyHaoYu,JosephEGonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementfor largelanguagemodelservingwithpagedattention. arXivpreprintarXiv:2309.06180,2023. [Laffertyetal.,2001] JohnLafferty,AndrewMcCallum,andFernandoPereira. Conditionalrandom fields: Probabilisticmodelsforsegmentingandlabelingsequencedata. InProceedingsofthe18th InternationalConferenceonMachineLearning2001,pages282–289,2001. [Lagunasetal.,2021] FrançoisLagunas,EllaCharlaix,VictorSanh,andAlexanderMRush. Block pruningforfastertransformers. InProceedingsofthe2021ConferenceonEmpiricalMethodsin NaturalLanguageProcessing,pages10619–10629,2021. [Lai et al., 2015] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent convolutional neural networksfortextclassification. InTwenty-ninthAAAIconferenceonartificialintelligence,2015. [LakeandBaroni,2018] BrendenLakeandMarcoBaroni. Generalizationwithoutsystematicity: On thecompositionalskillsofsequence-to-sequencerecurrentnetworks. InInternationalconferenceon machinelearning,pages2873–2882.PMLR,2018. [Lambertetal.,2024] NathanLambert,ValentinaPyatkin,JacobMorrison,LJMiranda,BillYuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. arXiv preprintarXiv:2403.13787,2024. [LampleandConneau,2019] GuillaumeLampleandAlexisConneau. Cross-linguallanguagemodel pretraining. arXivpreprintarXiv:1901.07291,2019. [Lample et al., 2016] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami,andChrisDyer. Neuralarchitecturesfornamedentityrecognition. InProceedingsofthe 2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages260–270,2016. [Lampleetal.,2019] GuillaumeLample,AlexandreSablayrolles,Marc’AurelioRanzato,Ludovic Denoyer,andHervéJégou. Largememorylayerswithproductkeys. AdvancesinNeuralInformation ProcessingSystems,32,2019. [Lanetal.,2020] ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma, andRaduSoricut. Albert: Alitebertforself-supervisedlearningoflanguagerepresentations. In ProceedingsofInternationalConferenceonLearningRepresentations,2020. [Landaueretal.,1998] ThomasKLandauer,PeterWFoltz,andDarrellLaham. Anintroductionto latentsemanticanalysis. Discourseprocesses,25(2-3):259–284,1998. [Langetal.,1990] KevinJLang,AlexHWaibel,andGeoffreyEHinton. Atime-delayneuralnetwork 668 BIBLIOGRAPHY architectureforisolatedwordrecognition. Neuralnetworks,3(1):23–43,1990. [LapesaandEvert,2014] GabriellaLapesaandStefanEvert. Alargescaleevaluationofdistributional semanticmodels: Parameters,interactionsandmodelselection. TransactionsoftheAssociationfor ComputationalLinguistics,2:531–546,2014. [Lawson,2003] MarkV.Lawson. FiniteAutomata(1sted.). ChapmanandHall/CRC,2003. [LeandMikolov,2014] QuocLeandTomasMikolov. Distributedrepresentationsofsentencesand documents. InInternationalconferenceonmachinelearning,pages1188–1196.PMLR,2014. [Leblondetal.,2021] RémiLeblond,Jean-BaptisteAlayrac,LaurentSifre,MirunaPislar,LespiauJean- Baptiste,IoannisAntonoglou,KarenSimonyan,andOriolVinyals. Machinetranslationdecoding beyond beam search. In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing,pages8410–8434,2021. [LeCun and Bengio, 1995] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech,andtimeseries. Thehandbookofbraintheoryandneuralnetworks,3361(10):1995,1995. [LeCunetal., 1989] YannLeCun, BernhardBoser, JohnSDenker, DonnieHenderson, RichardE Howard,WayneHubbard,andLawrenceDJackel. Backpropagationappliedtohandwrittenzipcode recognition. Neuralcomputation,1(4):541–551,1989. [LeCunetal.,2012] YannALeCun,LéonBottou,GenevieveBOrr,andKlaus-RobertMüller. Efficient backprop. InNeuralnetworks: Tricksofthetrade,pages9–48.Springer,2012. [Leeetal.,2023] HarrisonLee,SamratPhatale,HassanMansoor,KellieRenLu,ThomasMesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcementlearningfromhumanfeedbackwithaifeedback. arXivpreprintarXiv:2309.00267, 2023. [Leeetal.,2017] JasonLee,KyunghyunCho,andThomasHofmann. Fullycharacter-levelneural machinetranslationwithoutexplicitsegmentation.TransactionsoftheAssociationforComputational Linguistics,5:365–378,2017. [Leeetal.,2020] JasonLee,ElmanMansimov,andKyunghyunCho. Deterministicnon-autoregressive neuralsequencemodelingbyiterativerefinement. In2018ConferenceonEmpiricalMethodsin NaturalLanguageProcessing,EMNLP2018,pages1173–1182,2020. [Lee et al., 2019] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee Koh. Attentionmodelsingraphs: Asurvey. ACMTransactionsonKnowledgeDiscoveryfromData (TKDD),13(6):1–25,2019. [Lenci,2018] AlessandroLenci. Distributionalmodelsofwordmeaning. AnnualreviewofLinguistics, 4:151–171,2018. [Lepikhinetal.,2021] DmitryLepikhin,HyoukJoongLee,YuanzhongXu,DehaoChen,OrhanFirat, YanpingHuang,MaximKrikun,NoamShazeer,andZhifengChen. Gshard: Scalinggiantmodels withconditionalcomputationandautomaticsharding. InProceedingsofInternationalConference onLearningRepresentations,2021. [Lesteretal.,2021] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter- efficientprompttuning. InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages3045–3059,2021. [Leviathan et al., 2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In Proceedings of International Conference on Machine BIBLIOGRAPHY 669 Learning,pages19274–19286.PMLR,2023. [LevyandGoldberg,2014] OmerLevyandYoavGoldberg. Dependency-basedwordembeddings. In Proceedingsofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume 2: ShortPapers),pages302–308,2014a. [Levy and Goldberg, 2014] Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and explicitwordrepresentations. InProceedingsoftheeighteenthconferenceoncomputationalnatural languagelearning,pages171–180,2014b. [LevyandGoldberg,2014] OmerLevyandYoavGoldberg. Neuralwordembeddingasimplicitmatrix factorization. Advancesinneuralinformationprocessingsystems,27,2014c. [Levyetal.,2015] OmerLevy,YoavGoldberg,andIdoDagan. Improvingdistributionalsimilarity with lessons learned from word embeddings. Transactions of the association for computational linguistics,3:211–225,2015. [Lewisetal.,2020] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMo- hamed,OmerLevy,VeselinStoyanov,andLukeZettlemoyer. Bart: Denoisingsequence-to-sequence pre-trainingfornaturallanguagegeneration,translation,andcomprehension. InProceedingsofthe 58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7871–7880,2020a. [Lewisetal.,2020] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, andDouweKiela. Retrieval-augmentedgenerationforknowledge-intensivenlptasks. Advancesin NeuralInformationProcessingSystems,33:9459–9474,2020b. [Lietal.,2024] BaolinLi,YankaiJiang,VijayGadepally,andDeveshTiwari. Llminferenceserving: Surveyofrecentadvancesandopportunities. arXivpreprintarXiv:2407.12391,2024a. [Lietal.,2020] BeiLi,HuiLiu,ZiyangWang,YufanJiang,TongXiao,JingboZhu,TongranLiu,and ChangliangLi. Doesmulti-encoderhelp? acasestudyoncontext-awareneuralmachinetranslation. InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages 3512–3518,2020a. [Lietal.,2020] BeiLi,ZiyangWang,HuiLiu,YufanJiang,QuanDu,TongXiao,HuizhenWang,and JingboZhu. Shallow-to-deeptrainingforneuralmachinetranslation. InProceedingsofthe2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 995–1005, 2020b. [Lietal.,2021] BeiLi,ZiyangWang,HuiLiu,QuanDu,TongXiao,ChunliangZhang,andJingbo Zhu. Learninglight-weighttranslationmodelsfromdeeptransformer. InProceedingsoftheAAAI ConferenceonArtificialIntelligence,volume35,pages13217–13225,2021a. [Lietal., 2022] BeiLi, QuanDu, TaoZhou, YiJing, ShuhanZhou, XinZeng, TongXiao, Jingbo Zhu, Xuebo Liu, and Min Zhang. Ode transformer: An ordinary differential equation-inspired modelforsequencegeneration. InProceedingsofthe60thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages8335–8351,2022a. [Lietal.,2022] BeiLi,TongZheng,YiJing,ChengboJiao,TongXiao,andJingboZhu. Learning multiscaletransformermodelsforsequencegeneration. InInternationalConferenceonMachine Learning,pages13225–13241.PMLR,2022b.",
    "Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 995–1005, 2020b. [Lietal.,2021] BeiLi,ZiyangWang,HuiLiu,QuanDu,TongXiao,ChunliangZhang,andJingbo Zhu. Learninglight-weighttranslationmodelsfromdeeptransformer. InProceedingsoftheAAAI ConferenceonArtificialIntelligence,volume35,pages13217–13225,2021a. [Lietal., 2022] BeiLi, QuanDu, TaoZhou, YiJing, ShuhanZhou, XinZeng, TongXiao, Jingbo Zhu, Xuebo Liu, and Min Zhang. Ode transformer: An ordinary differential equation-inspired modelforsequencegeneration. InProceedingsofthe60thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages8335–8351,2022a. [Lietal.,2022] BeiLi,TongZheng,YiJing,ChengboJiao,TongXiao,andJingboZhu. Learning multiscaletransformermodelsforsequencegeneration. InInternationalConferenceonMachine Learning,pages13225–13241.PMLR,2022b. [Lietal.,2023] BeiLi,RuiWang,JunliangGuo,KaitaoSong,XuTan,HanyHassan,ArulMenezes, TongXiao,JiangBian,andJingBoZhu. Deliberatethengenerate: Enhancedpromptingframework 670 BIBLIOGRAPHY fortextgeneration. arXivpreprintarXiv:2305.19835,2023a. [Li,2011] HangLi. LearningtoRankforInformationRetrievalandNaturalLanguageProcessing. Onlineaccess: Morgan&ClaypoolSynthesisCollectionFive.Morgan&ClaypoolPublishers,2011. ISBN9781608457076. [Lietal.,2022] HongkangLi,MengWang,SijiaLiu,andPin-YuChen. Atheoreticalunderstanding ofshallowvisiontransformers: Learning,generalization,andsamplecomplexity. InTheEleventh InternationalConferenceonLearningRepresentations,2022c. [Lietal.,2022] HuayangLi,YixuanSu,DengCai,YanWang,andLemaoLiu. Asurveyonretrieval- augmentedtextgeneration. arXivpreprintarXiv:2202.01110,2022d. [Li et al., 2021] Jicheng Li, Pengzhi Gao, Xuanfu Wu, Yang Feng, Zhongjun He, Hua Wu, and HaifengWang. Mixupdecodingfordiversemachinetranslation. InFindingsoftheAssociationfor ComputationalLinguistics: EMNLP2021,pages312–320,2021b. [Lietal.,2020] JingLi,AixinSun,JiangleiHan,andChenliangLi. Asurveyondeeplearningfor namedentityrecognition. IEEETransactionsonKnowledgeandDataEngineering,34(1):50–70, 2020c. [LiandJurafsky,2016] JiweiLiandDanJurafsky. Mutualinformationanddiversedecodingimprove neuralmachinetranslation. arXivpreprintarXiv:1601.00372,2016. [Li et al., 2016] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. A diversity-promotingobjectivefunctionforneuralconversationmodels. InProceedingsofthe2016 ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human LanguageTechnologies,pages110–119,2016. [Lietal.,2017] JiweiLi,WillMonroe,andDanJurafsky. Learningtodecodeforfuturesuccess. arXiv preprintarXiv:1701.06549,2017a. [Lietal.,2017] JunhuiLi,DeyiXiong,ZhaopengTu,MuhuaZhu,MinZhang,andGuodongZhou. Modelingsourcesyntaxforneuralmachinetranslation. InProceedingsofthe55thAnnualMeeting oftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages688–697,2017b. [Lietal.,2021] JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,ShafiqJoty,CaimingXiong, andStevenChuHongHoi. Alignbeforefuse: Visionandlanguagerepresentationlearningwith momentumdistillation. Advancesinneuralinformationprocessingsystems,34:9694–9705,2021c. [Lietal.,2022] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage- image pre-training for unified vision-language understanding and generation. In International ConferenceonMachineLearning,pages12888–12900.PMLR,2022e. [Lietal.,2024] ShandaLi,ChongYou,GuruGuruganesh,JoshuaAinslie,SantiagoOntanon,Manzil Zaheer,SumitSanghai,YimingYang,SanjivKumar,andSrinadhBhojanapalli. Functionalinter- polation for relative positions improves long context transformers. In The Twelfth International ConferenceonLearningRepresentations,2024b. [Lietal.,2023] ShengguiLi,FuzhaoXue,ChaitanyaBaranwal,YongbinLi,andYangYou. Sequence parallelism: Longsequencetrainingfromsystemperspective. InProceedingsofthe61stAnnual MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages2391–2404, 2023b. [LiandLiang,2021] XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuousprompts forgeneration. InProceedingsofthe59thAnnualMeetingoftheAssociationforComputational BIBLIOGRAPHY 671 Linguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages4582–4597,2021. [Li et al., 2019] Xintong Li, Guanlin Li, Lemao Liu, Max Meng, and Shuming Shi. On the word alignment from neural machine translation. In Proceedings of the 57th Annual Meeting of the AssociationforComputationalLinguistics,pages1293–1303,2019. [Lietal.,2022] YanghaoLi,Chao-YuanWu,HaoqiFan,KarttikeyaMangalam,BoXiong,Jitendra Malik,andChristophFeichtenhofer. Mvitv2: Improvedmultiscalevisiontransformersforclassifica- tionanddetection. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern Recognition,pages4804–4814,2022f. [Lietal.,2018] YanyangLi,TongXiao,YinqiaoLi,QiangWang,ChangmingXu,andJingboZhu. A simpleandeffectiveapproachtocoverage-awareneuralmachinetranslation. InProceedingsofthe 56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2: ShortPapers), pages292–297,2018. [Li,2023] YinhengLi. Apracticalsurveyonzero-shotpromptdesignforin-contextlearning. InPro- ceedingsofthe14thInternationalConferenceonRecentAdvancesinNaturalLanguageProcessing, pages641–647,2023. [Lietal.,2023] YuchengLi,BoDong,FrankGuerin,andChenghuaLin. Compressingcontextto enhanceinferenceefficiencyoflargelanguagemodels. InProceedingsofthe2023Conferenceon EmpiricalMethodsinNaturalLanguageProcessing,pages6342–6353,2023c. [Li et al., 2021] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of convolutionalneuralnetworks: analysis,applications,andprospects. IEEEtransactionsonneural networksandlearningsystems,2021d. [Lialinetal.,2023] VladislavLialin,VijetaDeshpande,andAnnaRumshisky. Scalingdowntoscale up: Aguidetoparameter-efficientfine-tuning. arXivpreprintarXiv:2303.15647,2023. [Liaoetal.,2021] KaiyuanLiao,YiZhang,XuanchengRen,QiSu,XuSun,andBinHe. Aglobalpast- futureearlyexitmethodforacceleratinginferenceofpre-trainedlanguagemodels. InProceedingsof the2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages2013–2023,2021. [Lightmanetal.,2024] HunterLightman,VineetKosaraju,YuriBurda,HarrisonEdwards,Bowen Baker,TeddyLee,JanLeike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let’sverifystepby step. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. [Likhomanenkoetal.,2021] TatianaLikhomanenko,QiantongXu,GabrielSynnaeve,RonanCollobert, andAlexRogozhnikov. Cape: Encodingrelativepositionswithcontinuousaugmentedpositional embeddings. AdvancesinNeuralInformationProcessingSystems,34:16079–16092,2021. [Linetal.,2022] TianyangLin,YuxinWang,XiangyangLiu,andXipengQiu.Asurveyoftransformers. AIOpen,2022a. [Linetal.,2022] YeLin,ShuhanZhou,YanyangLi,AnxiangMa,TongXiao,andJingboZhu. Multi- pathtransformerisbetter: Acasestudyonneuralmachinetranslation. InFindingsoftheAssociation forComputationalLinguistics: EMNLP2022,pages5646–5656,2022b. [Linetal.,2017] ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,Bowen Zhou,andYoshuaBengio. Astructuredself-attentivesentenceembedding. InProceedingsofthe5th InternationalConferenceonLearningRepresentations(ICLR),2017. 672 BIBLIOGRAPHY [Lingetal.,2015] WangLing,ChrisDyer,AlanWBlack,IsabelTrancoso,RamónFermandez,Silvio Amir,LuisMarujo,andTiagoLuís. Findingfunctioninform: Compositionalcharactermodelsfor openvocabularywordrepresentation. InProceedingsofthe2015ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages1520–1530,2015. [Linzen,2016] TalLinzen. Issuesinevaluatingsemanticspacesusingwordanalogies. InProceedings ofthe1stWorkshoponEvaluatingVector-SpaceRepresentationsforNLP,pages13–18,2016. [Lippmann,1989] RichardPLippmann. Reviewofneuralnetworksforspeechrecognition. Neural computation,1(1):1–38,1989. [Lipton et al., 2015] Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrentneuralnetworksforsequencelearning. arXivpreprintarXiv:1506.00019,2015. [Liuetal.,2024] AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprintarXiv:2412.19437,2024a. [Liuetal.,2020] FenglinLiu,XuanchengRen,ZhiyuanZhang,XuSun,andYuexianZou. Rethinking skipconnectionwithlayernormalization. InProceedingsofthe28thinternationalconferenceon computationallinguistics,pages3586–3598,2020a. [Liuetal., 2023] HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstruction tuning. arXivpreprintarXiv:2304.08485,2023a. [LiuandMotoda,2012] HuanLiuandHiroshiMotoda. Featureselectionforknowledgediscoveryand datamining,volume454. SpringerScience&BusinessMedia,2012. [Liuetal.,2022] JiachangLiu,DinghanShen,YizheZhang,WilliamBDolan,LawrenceCarin,and WeizhuChen. Whatmakesgoodin-contextexamplesforgpt-3? InProceedingsofDeepLearning InsideOut(DeeLIO2022): The3rdWorkshoponKnowledgeExtractionandIntegrationforDeep LearningArchitectures,pages100–114,2022. [Liuetal.,2016] LemaoLiu,MasaoUtiyama,AndrewFinch,andEiichiroSumita. Agreementon target-bidirectionalneuralmachinetranslation. InProceedingsofthe2016ConferenceoftheNorth AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies, pages411–416,2016a. [Liuetal.,2016] LemaoLiu,MasaoUtiyama,AndrewFinch,andEiichiroSumita. Neuralmachine translation with supervised attention. In Proceedings of COLING 2016, the 26th International ConferenceonComputationalLinguistics: TechnicalPapers,pages3093–3102,2016b. [Liuetal.,2020] LiyuanLiu,XiaodongLiu,JianfengGao,WeizhuChen,andJiaweiHan. Under- standingthedifficultyoftrainingtransformers. InProceedingsofthe2020ConferenceonEmpirical MethodsinNaturalLanguageProcessing(EMNLP),pages5747–5763,2020b. [Liuetal.,2020] LiyuanLiu,XiaodongLiu,JianfengGao,WeizhuChen,andJiaweiHan. Under- standingthedifficultyoftrainingtransformers. InProceedingsofthe2020ConferenceonEmpirical MethodsinNaturalLanguageProcessing(EMNLP),pages5747–5763,November2020c. [Liuetal.,2023] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGraham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural languageprocessing. ACMComputingSurveys,55(9):1–35,2023b. [Liuetal.,2018] PeterJLiu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,Lukasz Kaiser,andNoamShazeer. Generatingwikipediabysummarizinglongsequences. InProceedings BIBLIOGRAPHY 673 ofInternationalConferenceonLearningRepresentations,2018. [Liuetal.,2017] ShusenLiu,Peer-TimoBremer,JayaramanJThiagarajan,VivekSrikumar,BeiWang, YardenLivnat,andValerioPascucci. Visualexplorationofsemanticrelationshipsinneuralword embeddings. IEEEtransactionsonvisualizationandcomputergraphics,24(1):553–562,2017. [Liuetal.,2024] TianqiLiu,YaoZhao,RishabhJoshi,MishaKhalman,MohammadSaleh,PeterJ Liu,andJialuLiu. Statisticalrejectionsamplingimprovespreferenceoptimization. InTheTwelfth InternationalConferenceonLearningRepresentations,2024b. [Liu, 2009] Tie-YanLiu. Learningtorankforinformationretrieval. FoundationsandTrends®in InformationRetrieval,3(3):225–331,2009. [Liuetal.,2023] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJie Tang. Gptunderstands,too. AIOpen,2023c. [Liuetal.,2023] XiaoxiaLiu,JingyiWang,JunSun,XiaohanYuan,GuoliangDong,PengDi,Wenhai Wang, and Dongxia Wang. Prompting frameworks for large language models: A survey. arXiv preprintarXiv:2311.12785,2023d. [Liu et al., 2024]",
    "PeterJLiu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,Lukasz Kaiser,andNoamShazeer. Generatingwikipediabysummarizinglongsequences. InProceedings BIBLIOGRAPHY 673 ofInternationalConferenceonLearningRepresentations,2018. [Liuetal.,2017] ShusenLiu,Peer-TimoBremer,JayaramanJThiagarajan,VivekSrikumar,BeiWang, YardenLivnat,andValerioPascucci. Visualexplorationofsemanticrelationshipsinneuralword embeddings. IEEEtransactionsonvisualizationandcomputergraphics,24(1):553–562,2017. [Liuetal.,2024] TianqiLiu,YaoZhao,RishabhJoshi,MishaKhalman,MohammadSaleh,PeterJ Liu,andJialuLiu. Statisticalrejectionsamplingimprovespreferenceoptimization. InTheTwelfth InternationalConferenceonLearningRepresentations,2024b. [Liu, 2009] Tie-YanLiu. Learningtorankforinformationretrieval. FoundationsandTrends®in InformationRetrieval,3(3):225–331,2009. [Liuetal.,2023] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJie Tang. Gptunderstands,too. AIOpen,2023c. [Liuetal.,2023] XiaoxiaLiu,JingyiWang,JunSun,XiaohanYuan,GuoliangDong,PengDi,Wenhai Wang, and Dongxia Wang. Prompting frameworks for large language models: A survey. arXiv preprintarXiv:2311.12785,2023d. [Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang Wang,TongXiao,andJingboZhu. Forgettingcurve: Areliablemethodforevaluatingmemorization capabilityforlong-contextmodels. InProceedingsofthe2024ConferenceonEmpiricalMethodsin NaturalLanguageProcessing,pages4667–4682,2024c. [Liuetal.,2023] YangLiu,YaoZhang,YixinWang,FengHou,JinYuan,JiangTian,YangZhang, ZhongchaoShi,JianpingFan,andZhiqiangHe. Asurveyofvisualtransformers. IEEETransactions onNeuralNetworksandLearningSystems,2023e. [Liuetal.,2019] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbert pretrainingapproach. arXivpreprintarXiv:1907.11692,2019. [Liuetal.,2020] YuchenLiu,JunnanZhu,JiajunZhang,andChengqingZong. Bridgingthemodality gapforspeech-to-texttranslation. arXivpreprintarXiv:2010.14920,2020d. [Longpreetal.,2023] ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay, DennyZhou,QuocV.Le,BarretZoph,JasonWei,andAdamRoberts.Theflancollection:Designing dataandmethodsforeffectiveinstructiontuning. InInternationalConferenceonMachineLearning, pages22631–22648.PMLR,2023. [Lopez,2008] AdamLopez. Statisticalmachinetranslation. ACMComputingSurveys(CSUR),40(3): 1–49,2008. [Luetal.,2016] JiasenLu,JianweiYang,DhruvBatra,andDeviParikh. Hierarchicalquestion-image co-attentionforvisualquestionanswering. Advancesinneuralinformationprocessingsystems,29, 2016. [Lund,1995] KevinLund. Semanticandassociativepriminginhigh-dimensionalsemanticspace. In Proc.ofthe17thAnnualconferencesoftheCognitiveScienceSociety,1995,1995. [LundandBurgess,1996] KevinLundandCurtBurgess. Producinghigh-dimensionalsemanticspaces fromlexicalco-occurrence. Behaviorresearchmethods,instruments,&computers,28(2):203–208, 1996. [Luongetal.,2015] Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproaches toattention-basedneuralmachinetranslation. InProceedingsofthe2015ConferenceonEmpirical 674 BIBLIOGRAPHY MethodsinNaturalLanguageProcessing,pages1412–1421,2015. [Maetal.,2019] MingboMa,LiangHuang,HaoXiong,RenjieZheng,KaiboLiu,BaigongZheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. Stacl: Simultaneoustranslationwithimplicitanticipationandcontrollablelatencyusingprefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,pages3025–3036,2019. [MaandHovy,2016] XuezheMaandEduardHovy. End-to-endsequencelabelingviabi-directional lstm-cnns-crf. InProceedingsofthe54thAnnualMeetingoftheAssociationforComputational Linguistics(Volume1: LongPapers),pages1064–1074,2016. [Maetal.,2023] XuezheMa,ChuntingZhou,XiangKong,JunxianHe,LiangkeGui,GrahamNeubig, JonathanMay, andLukeZettlemoyer. Mega: Movingaverageequippedgatedattention. InThe EleventhInternationalConferenceonLearningRepresentations,2023. [Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretrainingandinferencewithunlimitedcontextlength. arXivpreprintarXiv:2404.08801,2024. [Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, SarahWiegreffe,UriAlon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,ShashankGupta,Bod- hisattwaPrasadMajumder,KatherineHermann,SeanWelleck,AmirYazdanbakhsh,andPeterClark. Self-refine: Iterativerefinementwithself-feedback. AdvancesinNeuralInformationProcessing Systems,36,2024. [Malaviya et al., 2018] Chaitanya Malaviya, Pedro Ferreira, and André FT Martins. Sparse and constrainedattentionforneuralmachinetranslation. InProceedingsofthe56thAnnualMeetingof theAssociationforComputationalLinguistics(Volume2: ShortPapers),pages370–376,2018. [ManningandSchütze,1999] ChrisManningandHinrichSchütze. FoundationsofStatisticalNatural LanguageProcessing. TheMITPress,1999. [Manning,2022] ChristopherDManning. Humanlanguageunderstanding&reasoning. Daedalus, 151(2):127–138,2022. [Manningetal.,2008] ChristopherD.Manning,PrabhakarRaghavan,andHinrichSchütze.Introduction toInformationRetrieval. CambridgeUniversityPress,2008. [Manningetal.,2020] ChristopherDManning,KevinClark,JohnHewitt,UrvashiKhandelwal,and OmerLevy. Emergentlinguisticstructureinartificialneuralnetworkstrainedbyself-supervision. ProceedingsoftheNationalAcademyofSciences,117(48):30046–30054,2020. [Marcus,1993] GaryFMarcus. Negativeevidenceinlanguageacquisition. Cognition,46(1):53–85, 1993. [Markman,2013] ArthurBMarkman. Knowledgerepresentation. PsychologyPress,2013. [Markov,1913] AAMarkov. Essaid’unerecherchestatistiquesurletexteduroman. EugeneOnegin” illustrantlaliaisondesepreuveenchain(‘Exampleofastatisticalinvestigationofthetextof“Eugene Onegin”illustratingthedependencebetweensamplesinchain”).In:IzvistiaImperatorskoiAkademii Nauk(Bulletindel’AcadémieImpérialedesSciencesdeSt.-Pétersbourg).6thser,7:153–162,1913. [Martins et al., 2022] Pedro Henrique Martins, Zita Marinho, and André FT Martins. ∞-former: Infinite memory transformer-former: Infinite memory transformer. In Proceedings of the 60th AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages BIBLIOGRAPHY 675 5468–5485,2022. [Marufetal.,2019] SameenMaruf,AndréFTMartins,andGholamrezaHaffari. Selectiveattention forcontext-awareneuralmachinetranslation. InProceedingsofthe2019ConferenceoftheNorth AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies, Volume1(LongandShortPapers),pages3092–3102,2019. [MasoudniaandEbrahimpour,2014] SaeedMasoudniaandRezaEbrahimpour. Mixtureofexperts: a literaturesurvey. TheArtificialIntelligenceReview,42(2):275,2014. [Matusovetal.,2006] EvgenyMatusov,NicolaUeffing,andHermannNey. Computingconsensus translationformultiplemachinetranslationsystemsusingenhancedhypothesisalignment. In11th ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages33–40, 2006. [Mavietal.,2024] VaibhavMavi,AnubhavJangra,andAdamJatowt. Multi-hopquestionanswering. FoundationsandTrends®inInformationRetrieval,17(5):457–586,2024. [McCallumetal.,2000] AndrewMcCallum,DayneFreitag,andFernandoC.N.Pereira. Maximumen- tropymarkovmodelsforinformationextractionandsegmentation. InProceedingsoftheSeventeenth InternationalConferenceonMachineLearning,pages591–598,2000. [McCannetal.,2017] BryanMcCann,JamesBradbury,CaimingXiong,andRichardSocher. Learned intranslation: Contextualizedwordvectors. Advancesinneuralinformationprocessingsystems,30, 2017. [McCarleyetal.,2019] JSMcCarley,RishavChakravarti,andAvirupSil. Structuredpruningofa bert-basedquestionansweringmodel. arXivpreprintarXiv:1910.06360,2019. [McClaveandSincich,2006] JamesT.McClaveandTerrySincich. Statistics(10thed.). PrenticeHall, 2006. [McCullochandPitts,1943] WarrenS.McCullochandWalterPitts. Alogicalcalculusoftheideas immanentinnervousactivity. Thebulletinofmathematicalbiophysics,5(4):115–133,1943. [McElreath,2020] RichardMcElreath. StatisticalRethinking: ABayesianCoursewithExamplesinR andSTAN(2nded.). ChapmanandHall/CRC,2020. [McNamara,2005] TimothyPMcNamara. Semanticpriming: Perspectivesfrommemoryandword recognition. PsychologyPress,2005. [Meisteretal.,2020] ClaraMeister,TimVieira,andRyanCotterell. Best-firstbeamsearch. Transac- tionsoftheAssociationforComputationalLinguistics,8:795–809,2020. [Merrilletal.,2022] WilliamMerrill,AshishSabharwal,andNoahASmith. Saturatedtransformers areconstant-depththresholdcircuits. TransactionsoftheAssociationforComputationalLinguistics, 10:843–856,2022. [MeyerandSchvaneveldt,1971] DavidEMeyerandRogerWSchvaneveldt.Facilitationinrecognizing pairsofwords: evidenceofadependencebetweenretrievaloperations. Journalofexperimental psychology,90(2):227,1971. [Mi et al., 2016] Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. Coverage embedding models for neural machine translation. In Proceedings of the 2016 Conference on EmpiricalMethodsinNaturalLanguageProcessing,pages955–960,2016a. [Mi et al., 2016] Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. Supervised attentions for neural 676 BIBLIOGRAPHY machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural LanguageProcessing,pages2283–2288,2016b. [Micheletal.,2019] PaulMichel,OmerLevy,andGrahamNeubig. Aresixteenheadsreallybetter thanone? Advancesinneuralinformationprocessingsystems,32,2019. [Micikevicius et al., 2018] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, ErichElsen,DavidGarcia,BorisGinsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh, andHaoWu. Mixedprecisiontraining. InProceedingsofInternationalConferenceonLearning Representations,2018. [Mielkeetal.,2021] SabrinaJ.Mielke,ZaidAlyafeai,ElizabethSalesky,ColinRaffel,MananDey, MatthiasGallé,ArunRaja,ChengleiSi,WilsonY.Lee,BenoîtSagot,andSamsonTan. Between wordsandcharacters: Abriefhistoryofopen-vocabularymodelingandtokenizationinnlp. arXiv preprintarXiv:2112.10508,2021. [Miettinen,1999] KaisaMiettinen.Nonlinearmultiobjectiveoptimization,volume12.SpringerScience &BusinessMedia,1999. [Mikolovetal., 2010] TomasMikolov, MartinKarafiát, LukasBurget, JanCernocky`, andSanjeev Khudanpur. Recurrentneuralnetworkbasedlanguagemodel. InProceedingsofInterspeech,pages 1045–1048,2010. [Mikolovetal.,2013] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimation ofwordrepresentationsinvectorspace. InProceedingsoftheInternationalConferenceonLearning Representations(ICLR2013),2013a. [Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlyaSutskever. Exploitingsimilaritiesamong languagesformachinetranslation. arXivpreprintarXiv:1309.4168,2013b. [Mikolovetal., 2013] TomasMikolov, IlyaSutskever, KaiChen, GregCorrado, andJeffreyDean. Distributed representations of words and phrases and their compositionality. In",
    "[Mielkeetal.,2021] SabrinaJ.Mielke,ZaidAlyafeai,ElizabethSalesky,ColinRaffel,MananDey, MatthiasGallé,ArunRaja,ChengleiSi,WilsonY.Lee,BenoîtSagot,andSamsonTan. Between wordsandcharacters: Abriefhistoryofopen-vocabularymodelingandtokenizationinnlp. arXiv preprintarXiv:2112.10508,2021. [Miettinen,1999] KaisaMiettinen.Nonlinearmultiobjectiveoptimization,volume12.SpringerScience &BusinessMedia,1999. [Mikolovetal., 2010] TomasMikolov, MartinKarafiát, LukasBurget, JanCernocky`, andSanjeev Khudanpur. Recurrentneuralnetworkbasedlanguagemodel. InProceedingsofInterspeech,pages 1045–1048,2010. [Mikolovetal.,2013] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimation ofwordrepresentationsinvectorspace. InProceedingsoftheInternationalConferenceonLearning Representations(ICLR2013),2013a. [Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlyaSutskever. Exploitingsimilaritiesamong languagesformachinetranslation. arXivpreprintarXiv:1309.4168,2013b. [Mikolovetal., 2013] TomasMikolov, IlyaSutskever, KaiChen, GregCorrado, andJeffreyDean. Distributed representations of words and phrases and their compositionality. In Proceedings of the26thInternationalConferenceonNeuralInformationProcessingSystems-Volume2, pages 3111–3119,2013c. [Mikolovetal.,2013] TomasMikolov,Wen-tauYih,andGeoffreyZweig. Linguisticregularitiesin continuousspacewordrepresentations. InProceedingsofthe2013conferenceofthenorthamerican chapter of the association for computational linguistics: Human language technologies, pages 746–751,2013d. [Miller et al., 2016] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. In Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages 1400–1409,2016. [Minetal.,2019] SewonMin,VictorZhong,LukeZettlemoyer,andHannanehHajishirzi. Multi-hop readingcomprehensionthroughquestiondecompositionandrescoring. InProceedingsofthe57th AnnualMeetingoftheAssociationforComputationalLinguistics,pages6097–6109,2019. [Minaeeetal.,2024] ShervinMinaee,TomasMikolov,NarjesNikzad,MeysamChenaghlu,Richard Socher, XavierAmatriain, andJianfengGao. Largelanguagemodels: Asurvey. arXivpreprint arXiv:2402.06196,2024. [MinskyandPapert,1969] MarvinMinskyandSeymourPapert. Perceptrons. MITpress,1969. [Mishraetal.,2022] SwaroopMishra,DanielKhashabi,ChittaBaral,andHannanehHajishirzi. Cross- BIBLIOGRAPHY 677 task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages 3470–3487,2022. [MitchellandLapata,2010] JeffMitchellandMirellaLapata. Compositionindistributionalmodelsof semantics. Cognitivescience,34(8):1388–1429,2010. [Mitchell,1997] TomM.Mitchell. MachineLearning. McGraw-HillEducation,1997. [MnihandKavukcuoglu,2013] AndriyMnihandKorayKavukcuoglu. Learningwordembeddings efficientlywithnoise-contrastiveestimation. Advancesinneuralinformationprocessingsystems,26, 2013. [Mnihetal.,2016] VolodymyrMnih,AdriàPuigdomènechBadia,MehdiMirza,AlexGraves,Tim Harley, TimothyPLillicrap, DavidSilver, andKorayKavukcuoglu. Asynchronousmethodsfor deepreinforcementlearning. InProceedingsofthe33rdInternationalConferenceonInternational ConferenceonMachineLearning,pages1928–1937,2016. [Mohrietal., 2018] MehryarMohri, AfshinRostamizadeh, andAmeetTalwalkar. Foundationsof MachineLearning(2nded.). MITPress,2018. [MohtashamiandJaggi,2024] AmirkeivanMohtashamiandMartinJaggi. Random-accessinfinite contextlengthfortransformers. AdvancesinNeuralInformationProcessingSystems,36,2024. [Montague,1974] RichardMontague. Universalgrammar. InR.Thomason,editor,FormalPhilosophy: SelectedPapersofRichardMontague.YaleUniversityPress,1974. [Muetal.,2024] JesseMu,XiangLi,andNoahGoodman. Learningtocompresspromptswithgist tokens. AdvancesinNeuralInformationProcessingSystems,36,2024. [Müller and Sennrich, 2021] Mathias Müller and Rico Sennrich. Understanding the properties of minimumbayesriskdecodinginneuralmachinetranslation. InProceedingsofthe59thAnnual MeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConference onNaturalLanguageProcessing(Volume1: LongPapers),pages259–272,2021. [Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint arXiv:2404.07143,2024. [Murphy,2012] KevinP.Murphy. MachineLearning: AProbabilisticPerspective. MITPress,2012. [MurrayandChiang,2018] KentonMurrayandDavidChiang.Correctinglengthbiasinneuralmachine translation. InProceedingsoftheThirdConferenceonMachineTranslation: ResearchPapers,pages 212–223,2018. [Nageletal.,2021] MarkusNagel,MariosFournarakis,RanaAliAmjad,YelyseiBondarenko,Mart VanBaalen,andTijmenBlankevoort. Awhitepaperonneuralnetworkquantization. arXivpreprint arXiv:2106.08295,2021. [NairandHinton,2009] VinodNairandGeoffreyEHinton. 3dobjectrecognitionwithdeepbelief nets. Advancesinneuralinformationprocessingsystems,22,2009. [Nakanoetal.,2021] ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,Christina Kim,ChristopherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assistedquestion-answeringwithhumanfeedback. arXivpreprint arXiv:2112.09332,2021. 678 BIBLIOGRAPHY [Narayananetal.,2021] DeepakNarayanan,MohammadShoeybi,JaredCasper,PatrickLeGresley, MostofaPatwary,VijayKorthikanti,DmitriVainbrand,PrethviKashinkunti,JulieBernauer,Bryan Catanzaro,AmarPhanishayee,andMateiZaharia. Efficientlarge-scalelanguagemodeltraining on gpu clusters using megatron-lm. In Proceedings of the International Conference for High PerformanceComputing,Networking,StorageandAnalysis,pages1–15,2021. [Neelakantanetal.,2015] ArvindNeelakantan,LukeVilnis,QuocVLe,IlyaSutskever,LukaszKaiser, KarolKurach,andJamesMartens. Addinggradientnoiseimproveslearningforverydeepnetworks. arXivpreprintarXiv:1511.06807,2015. [Neisser,2014] UlricNeisser. CognitivePsychology: ClassicEdition. PsychologyPress,2014. [Nesterov, 1983] Yurii E Nesterov. A method for solving the convex programming problem with convergencerateo(1/kˆ2). InDokl.akad.naukSssr,volume269,pages543–547,1983. [Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under rewardtransformations: Theoryandapplicationtorewardshaping. InProceedingsoftheSixteenth InternationalConferenceonMachineLearning,pages278–287,1999. [Nguyenetal.,2020] Xuan-PhiNguyen,ShafiqJoty,StevenHoi,andRichardSocher. Tree-structured attentionwithhierarchicalaccumulation. InProceedingsofthe8thInternationalConferenceon LearningRepresentationsICLR,2020. [Nvidia, 2025] Nvidia. Nvidia nim llms benchmarking. https://docs.nvidia.com/nim/ benchmarking/llm/latest/metrics.html,2025. Retrieved2025-03-17. [Och, 2003] Franz Josef Och. Minimum error rate training in statistical machine translation. In Proceedingsofthe41stannualmeetingoftheAssociationforComputationalLinguistics, pages 160–167,2003. [Och and Ney, 2002] Franz Josef Och and Hermann Ney. Discriminative training and maximum entropymodelsforstatisticalmachinetranslation. InProceedingsofthe40thAnnualMeetingofthe AssociationforComputationalLinguistics,pages295–302,2002. [OchandNey,2003] FranzJosefOchandHermannNey. Asystematiccomparisonofvariousstatistical alignmentmodels. ComputationalLinguistics,29(1):19–51,2003. [Olive,2022] DavidOlive.Robuststatistics,2022.URLhttp://parker.ad.siu.edu/Olive/ ol-bookp.htm. [OlshausenandField,1997] BrunoAOlshausenandDavidJField.Sparsecodingwithanovercomplete basisset: Astrategyemployedbyv1? Visionresearch,37(23):3311–3325,1997. [Oord et al., 2017] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representationlearning. Advancesinneuralinformationprocessingsystems,30,2017. [Oordetal.,2018] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwith contrastivepredictivecoding. arXivpreprintarXiv:1807.03748,2018. [OpenAI,2024] OpenAI. Learningtoreasonwithllms,September2024. URLhttps://openai. com/index/learning-to-reason-with-llms/. [OpitzandMaclin,1999] DavidOpitzandRichardMaclin. Popularensemblemethods: Anempirical study. Journalofartificialintelligenceresearch,11:169–198,1999. [OppenheimandSchafer,1975] AlanVOppenheimandRonaldWSchafer. Digitalsignalprocess- ing(book). Prentice-Hall,1975. BIBLIOGRAPHY 679 [Orvieto et al., 2023] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,RazvanPascanu,andSohamDe.Resurrectingrecurrentneuralnetworksforlongsequences. arXivpreprintarXiv:2303.06349,2023. [Osgood,1952] CharlesEOsgood. Thenatureandmeasurementofmeaning. Psychologicalbulletin, 49(3):197,1952. [Ottetal., 2018] MyleOtt, MichaelAuli, DavidGrangier, andMarc’AurelioRanzato. Analyzing uncertaintyinneuralmachinetranslation. InInternationalConferenceonMachineLearning,pages 3956–3965.PMLR,2018a. [Ottetal.,2018] MyleOtt,SergeyEdunov,DavidGrangier,andMichaelAuli. Scalingneuralmachine translation. InProceedingsoftheThirdConferenceonMachineTranslation: ResearchPapers,pages 1–9,October2018b. [Ottetal.,2019] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,David Grangier,andMichaelAuli.fairseq:Afast,extensibletoolkitforsequencemodeling.InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics(Demonstrations),pages48–53,2019. [Ouyangetal.,2022] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, JacobHilton,FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulF. Christiano,JanLeike,andRyanLowe.",
    "2018] MyleOtt, MichaelAuli, DavidGrangier, andMarc’AurelioRanzato. Analyzing uncertaintyinneuralmachinetranslation. InInternationalConferenceonMachineLearning,pages 3956–3965.PMLR,2018a. [Ottetal.,2018] MyleOtt,SergeyEdunov,DavidGrangier,andMichaelAuli. Scalingneuralmachine translation. InProceedingsoftheThirdConferenceonMachineTranslation: ResearchPapers,pages 1–9,October2018b. [Ottetal.,2019] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,David Grangier,andMichaelAuli.fairseq:Afast,extensibletoolkitforsequencemodeling.InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics(Demonstrations),pages48–53,2019. [Ouyangetal.,2022] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, JacobHilton,FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulF. Christiano,JanLeike,andRyanLowe. Traininglanguagemodelstofollowinstructionswithhuman feedback. AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022. [Padó and Lapata, 2007] Sebastian Padó and Mirella Lapata. Dependency-based construction of semanticspacemodels. ComputationalLinguistics,33(2):161–199,2007. [Paletal.,2023] KoyenaPal,JiudingSun,AndrewYuan,ByronCWallace,andDavidBau.Futurelens: Anticipatingsubsequenttokensfromasinglehiddenstate. InProceedingsofthe27thConferenceon ComputationalNaturalLanguageLearning(CoNLL),pages548–560,2023. [Panetal.,2022] AlexanderPan,KushBhatia,andJacobSteinhardt. Theeffectsofrewardmisspec- ification: Mappingandmitigatingmisalignedmodels. InInternationalConferenceonLearning Representations,2022. [Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and WilliamYangWang. Automaticallycorrectinglargelanguagemodels: Surveyingthelandscape of diverse automated correction strategies. Transactions of the Association for Computational Linguistics,12:484–506,2024. [Pang et al., 2002] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? sentiment classificationusingmachinelearningtechniques.InProceedingsofthe2002ConferenceonEmpirical MethodsinNaturalLanguageProcessing(EMNLP2002),pages79–86,2002. [Papinenietal.,2002] KishorePapineni,SalimRoukos,ToddWard,andWeijingZhu. Bleu: amethod forautomaticevaluationofmachinetranslation. InProceedingsofthe40thAnnualmeetingofthe AssociationforComputationalLinguistics,pages311–318,2002. [Parisietal.,2022] AaronParisi,YaoZhao,andNoahFiedel. Talm: Toolaugmentedlanguagemodels. arXivpreprintarXiv:2205.12255,2022. [Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continuallifelonglearningwithneuralnetworks: Areview. Neuralnetworks,113:54–71, 680 BIBLIOGRAPHY 2019. [Park et al., 2019] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge distillation. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition, pages3967–3976,2019. [Parmaretal.,2018] NikiParmar,AshishVaswani,JakobUszkoreit,LukaszKaiser,NoamShazeer, AlexanderKu,andDustinTran. Imagetransformer. InInternationalconferenceonmachinelearning, pages4055–4064.PMLR,2018. [Pascanuetal.,2013] RazvanPascanu,TomasMikolov,andYoshuaBengio.Onthedifficultyoftraining recurrent neural networks. In International conference on machine learning, pages 1310–1318. PMLR,2013. [Pateletal.,2024] PratyushPatel,EshaChoukse,ChaojieZhang,AashakaShah,ÍñigoGoiri,Saeed Maleki,andRicardoBianchini. Splitwise: Efficientgenerativellminferenceusingphasesplitting. In2024ACM/IEEE51stAnnualInternationalSymposiumonComputerArchitecture(ISCA),pages 118–132.IEEE,2024. [Pearson,1901] KarlPearson. Onlinesandplanesofclosestfittosystemsofpointsinspace. The London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559–572, 1901. [Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, AlessandroCappelli,HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedwebdatasetforfalconllm: outperformingcuratedcorporawithwebdata,andwebdata only. arXivpreprintarXiv:2306.01116,2023. [Pengetal.,2019] BaoyunPeng,XiaoJin,JiahengLiu,DongshengLi,YichaoWu,YuLiu,Shunfeng Zhou,andZhaoningZhang. Correlationcongruenceforknowledgedistillation. InProceedingsof theIEEE/CVFInternationalConferenceonComputerVision,pages5007–5016,2019. [Pengetal.,2023] BoPeng,EricAlcaide,QuentinAnthony,AlonAlbalak,SamuelArcadinho,Stella Biderman,HuanqiCao,XinCheng,MichaelChung,LeonDerczynski,XingjianDu,MatteoGrella, KranthiGv,XuzhengHe,HaowenHou,PrzemyslawKazienko,JanKocon,JiamingKong,Bartlomiej Koptyra,HaydenLau,JiajuLin,KrishnaSriIpsitMantri,FerdinandMom,AtsushiSaito,Guangyu Song,XiangruTang,JohanS.Wind,StanislawWozniak,ZhenyuanZhang,QinghuaZhou,JianZhu, andRui-JieZhu. Rwkv: Reinventingrnnsforthetransformerera. arXivpreprintarXiv:2305.13048, 2023. [Pengetal.,2024] BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole. YaRN:Efficient contextwindowextensionoflargelanguagemodels. InTheTwelfthInternationalConferenceon LearningRepresentations,2024. [Pengetal.,2021] HPeng,NPappas,DYogatama,RSchwartz,NSmith,andLKong. Randomfeature attention. InProceedingsofInternationalConferenceonLearningRepresentations(ICLR2021), 2021. [Penningtonetal.,2014] JeffreyPennington,RichardSocher,andChristopherD.Manning. Glove: Globalvectorsforwordrepresentation. InProceedingsofEmpiricalMethodsinNaturalLanguage Processing(EMNLP),pages1532–1543,2014. [Pérezetal.,2018] JorgePérez,JavierMarinkovic´,andPabloBarceló. Ontheturingcompleteness ofmodernneuralnetworkarchitectures. InProceedingsofInternationalConferenceonLearning Representations,2018. BIBLIOGRAPHY 681 [Perozzietal.,2014] BryanPerozzi,RamiAl-Rfou,andStevenSkiena. Deepwalk: Onlinelearning ofsocialrepresentations. InProceedingsofthe20thACMSIGKDDinternationalconferenceon Knowledgediscoveryanddatamining,pages701–710,2014. [Petersetal.,2018] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark, KentonLee,andLukeZettlemoyer. Deepcontextualizedwordrepresentations. InProceedingsofthe 2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongPapers),2018. [Petronietal.,2019] FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin, YuxiangWu,andAlexanderMiller. Languagemodelsasknowledgebases? InProceedingsofthe 2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational JointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2463–2473,2019. [Pham et al., 2019] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Müller, Sebastian Stüker,andAlexanderWaibel. Verydeepself-attentionnetworksforend-to-endspeechrecognition. arXivpreprintarXiv:1904.13377,2019. [Picone,1993] JosephWPicone. Signalmodelingtechniquesinspeechrecognition. Proceedingsof theIEEE,81(9):1215–1247,1993. [Piresetal.,2023] TelmoPessoaPires,AntónioVLopes,YannickAssogba,andHendraSetiawan. Onewidefeedforwardisallyouneed. arXivpreprintarXiv:2309.01826,2023. [Plackett, 1975] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical SocietySeriesC:AppliedStatistics,24(2):193–202,1975. [Plautetal.,1986] DavidCPlaut,StevenJNowlan,andGeoffreyEHinton. Experimentsonlearning bybackpropagation. Technicalreport,Carnegie-MellonUniversity,1986. [Polyak,1964] BorisTPolyak. Somemethodsofspeedinguptheconvergenceofiterationmethods. Ussrcomputationalmathematicsandmathematicalphysics,4(5):1–17,1964. [Popeetal.,2023] ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin,JamesBradbury, JonathanHeek,KefanXiao,ShivaniAgrawal,andJeffDean.Efficientlyscalingtransformerinference. InProceedingsofMachineLearningandSystems,2023. [Porter,1980] MartinFPorter. Analgorithmforsuffixstripping. Program,1980. [PostandVilar,2018] MattPostandDavidVilar. Fastlexicallyconstraineddecodingwithdynamic beamallocationforneuralmachinetranslation. InProceedingsofthe2018ConferenceoftheNorth AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies, Volume1(LongPapers),pages1314–1324,2018. [Prasadetal.,2023] ArchikiPrasad,PeterHase,XiangZhou,andMohitBansal. Grips: Gradient- free, edit-based instruction search for prompting large language models. In Proceedings of the 17thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages 3845–3864,2023. [Prechelt,1998] LutzPrechelt. Earlystopping-butwhen? InNeuralNetworks: Tricksofthetrade, pages55–69.Springer,1998. [Pressetal.,2021] OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwith linear biases enables input length extrapolation. In Proceedings of International Conference on LearningRepresentations,2021. [Pressetal.,2022] OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwith 682 BIBLIOGRAPHY linear biases enables input length extrapolation. In Proceedings of International Conference on LearningRepresentations,2022. [Pressetal.,2023] OfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahASmith,andMike Lewis. Measuringandnarrowingthecompositionalitygapinlanguagemodels. InFindingsofthe AssociationforComputationalLinguistics: EMNLP2023,pages5687–5711,2023. [Provilkovetal.,2020] IvanProvilkov,DmitriiEmelianenko,andElenaVoita. Bpe-dropout: Simple andeffectivesubwordregularization. InProceedingsofthe58thAnnualMeetingoftheAssociation forComputationalLinguistics,pages1882–1892,2020. [Pryzantetal.,2023] ReidPryzant,DanIter,JerryLi,YinTatLee,ChenguangZhu,andMichaelZeng. Automaticpromptoptimizationwith\"gradientdescent\"andbeamsearch. InThe2023Conference onEmpiricalMethodsinNaturalLanguageProcessing,2023. [Qiu et al., 2020] Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for ComputationalLinguistics: EMNLP2020,pages2555–2565,2020a. [Qiuetal.,2020] XipengQiu,TianxiangSun,YigeXu,YunfanShao,NingDai,andXuanjingHuang. Pre-trainedmodelsfornaturallanguageprocessing:Asurvey. ScienceChinaTechnologicalSciences, 63(10):1872–1897,2020b. [Rabiner and Juang, 1993] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of speech recognition. Prentice-Hall,Inc.,1993. [RabinerandGold,1975] LawrenceRRabinerandBernardGold. Theoryandapplicationofdigital signalprocessing. Prentice-Hall,1975. [Radfordetal.,2018] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improving languageunderstandingbygenerativepre-training. OpenAI,2018. [Radfordetal.,2019] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlya Sutskever. Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8),2019. [Radfordetal.,2021]",
    "Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for ComputationalLinguistics: EMNLP2020,pages2555–2565,2020a. [Qiuetal.,2020] XipengQiu,TianxiangSun,YigeXu,YunfanShao,NingDai,andXuanjingHuang. Pre-trainedmodelsfornaturallanguageprocessing:Asurvey. ScienceChinaTechnologicalSciences, 63(10):1872–1897,2020b. [Rabiner and Juang, 1993] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of speech recognition. Prentice-Hall,Inc.,1993. [RabinerandGold,1975] LawrenceRRabinerandBernardGold. Theoryandapplicationofdigital signalprocessing. Prentice-Hall,1975. [Radfordetal.,2018] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improving languageunderstandingbygenerativepre-training. OpenAI,2018. [Radfordetal.,2019] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlya Sutskever. Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8),2019. [Radfordetal.,2021] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh, SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Internationalconferenceonmachinelearning,pages8748–8763.PMLR,2021. [Raeetal., 2019] JackWRae, AnnaPotapenko, SiddhantMJayakumar, ChloeHillier, andTimo- thyPLillicrap. Compressivetransformersforlong-rangesequencemodelling. InProceedingsof InternationalConferenceonLearningRepresentations,2019a. [Raeetal.,2019] JackWRae,AnnaPotapenko,SiddhantMJayakumar,ChloeHillier,andTimothyP Lillicrap. Compressivetransformersforlong-rangesequencemodelling. InInternationalConference onLearningRepresentations,2019b. [Rafailovetal.,2024] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,Stefano Ermon,andChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyareward model. AdvancesinNeuralInformationProcessingSystems,36,2024. [Raffeletal.,2017] ColinRaffel,Minh-ThangLuong,PeterJLiu,RonJWeiss,andDouglasEck. Onlineandlinear-timeattentionbyenforcingmonotonicalignments. InProceedingsofthe34th InternationalConferenceonMachineLearning-Volume70,pages2837–2846,2017. [Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, BIBLIOGRAPHY 683 MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwith aunifiedtext-to-texttransformer. JournalofMachineLearningResearch,21(140):1–67,2020. [Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activationfunctions. arXivpreprintarXiv:1710.05941,2017. [RamshawandMarcus,1995] LanceRamshawandMitchMarcus. Textchunkingusingtransformation- basedlearning. InThirdWorkshoponVeryLargeCorpora,1995. [Reddy,1976] DRajReddy. Speechrecognitionbymachine: Areview. ProceedingsoftheIEEE,64 (4):501–531,1976. [ReimersandGurevych,2019] NilsReimersandIrynaGurevych. Sentence-bert:Sentenceembeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP),pages3982–3992,2019. [ReisingerandMooney,2010] JosephReisingerandRaymondMooney. Multi-prototypevector-space modelsofwordmeaning. InHumanLanguageTechnologies: The2010AnnualConferenceofthe NorthAmericanChapteroftheAssociationforComputationalLinguistics,pages109–117,2010. [Renetal.,2017] ZhouRen,XiaoyuWang,NingZhang,XutaoLv,andLi-JiaLi. Deepreinforcement learning-basedimagecaptioningwithembeddingreward. InProceedingsoftheIEEEconferenceon computervisionandpatternrecognition,pages290–298,2017. [Rifaietal.,2011] SalahRifai,PascalVincent,XavierMuller,XavierGlorot,andYoshuaBengio. Con- tractiveauto-encoders: Explicitinvarianceduringfeatureextraction. InProceedingsofInternational ConferenceonMachineLearning,2011. [Rogersetal.,2018] AnnaRogers,ShashwathHosurAnanthakrishna,andAnnaRumshisky. What’s inyourembedding,andhowitpredictstaskperformance. InProceedingsofthe27thInternational ConferenceonComputationalLinguistics,pages2690–2703,2018. [Rolnicketal.,2019] DavidRolnick,ArunAhuja,JonathanSchwarz,TimothyLillicrap,andGregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Systems,32,2019. [Romeroetal.,2014] AdrianaRomero,NicolasBallas,SamiraEbrahimiKahou,AntoineChassang, CarloGatta,andYoshuaBengio. Fitnets: Hintsforthindeepnets. arXivpreprintarXiv:1412.6550, 2014. [Rosenblatt,1957] FrankRosenblatt. Theperceptron,aperceivingandrecognizingautomatonProject Para. CornellAeronauticalLaboratory,1957. [Rosenfeldetal.,2020] JonathanSRosenfeld,AmirRosenfeld,YonatanBelinkov,andNirShavit. A constructivepredictionofthegeneralizationerroracrossscales. InProceedingsofInternational ConferenceonLearningRepresentations,2020. [Ross,1924] WilliamDavidRoss. Aristotle’smetaphysics. ClarendonPress,1924. [Rostietal.,2007] Antti-VeikkoRosti,SpyrosMatsoukas,andRichardSchwartz. Improvedword-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the AssociationofComputationalLinguistics,pages312–319,2007. [Roy et al., 2021] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for ComputationalLinguistics,9:53–68,2021. 684 BIBLIOGRAPHY [Ruanetal.,2024] JunhaoRuan,LongMeng,WeiqiaoShan,TongXiao,andJingboZhu. Asurveyof llmsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs,2024. [Rubenstein and Goodenough, 1965] Herbert Rubenstein and John B Goodenough. Contextual correlatesofsynonymy. CommunicationsoftheACM,8(10):627–633,1965. [Rubinetal.,2022] OhadRubin,JonathanHerzig,andJonathanBerant. Learningtoretrieveprompts forin-contextlearning. InProceedingsofthe2022ConferenceoftheNorthAmericanChapterofthe AssociationforComputationalLinguistics: HumanLanguageTechnologies,pages2655–2671,2022. [Ruder, 2017] Sebastian Ruder. Deep learning for nlp best practices. https://ruder.io/ deep-learning-nlp-best-practices/index.html,2017. [Rumelhartetal.,1986] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams. Learning representationsbyback-propagatingerrors. nature,323(6088):533–536,1986. [RushandCollins,2012] AlexanderMRushandMJCollins. Atutorialondualdecompositionand lagrangianrelaxationforinferenceinnaturallanguageprocessing. JournalofArtificialIntelligence Research,45:305–362,2012. [Rushetal.,2015] AlexanderMRush,SumitChopra,andJasonWeston. Aneuralattentionmodelfor abstractivesentencesummarization. InProceedingsofthe2015ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages379–389,2015. [Russell,2019] StuartRussell. HumanCompatible: ArtificialIntelligenceandtheProblemofControls. Viking,2019. [Russell and Norvig, 2010] Stuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern Approach(3nded.). PrenticeHall,2010. [Sanhetal.,2020] VictorSanh,ThomasWolf,andAlexanderRush. Movementpruning: Adaptive sparsitybyfine-tuning. AdvancesinNeuralInformationProcessingSystems,33:20378–20389,2020. [Sanhetal.,2022] VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,Zaid Alyafeai, AntoineChaffin, ArnaudStiegler, ArunRaja, MananDey, MSaifulBari, CanwenXu, UrmishThakker,ShanyaSharmaSharma,ElizaSzczechla,TaewoonKim,GunjanChhablani,Nihal Nayak,DebajyotiDatta,JonathanChang,MikeTian-JianJiang,HanWang,MatteoManica,Sheng Shen,ZhengXinYong,HarshitPandey,RachelBawden,ThomasWang,TrishalaNeeraj,JosRozen, AbheeshtSharma,AndreaSantilli,ThibaultFevry,JasonAlanFries,RyanTeehan,TevenLeScao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In Proceedings of International Conference on Learning Representations,2022. [Sankaranetal.,2016] BaskaranSankaran,HaitaoMi,YaserAl-Onaizan,andAbeIttycheriah. Tempo- ralattentionmodelforneuralmachinetranslation. arXivpreprintarXiv:1608.02927,2016. [Santacroceetal.,2023] MichaelSantacroce,ZixinWen,YelongShen,andYuanzhiLi. Whatmatters inthestructuredpruningofgenerativelanguagemodels? arXivpreprintarXiv:2302.03773,2023. [SantosandGatti,2014] CícerodosSantosandMaíraGatti. Deepconvolutionalneuralnetworksfor sentimentanalysisofshorttexts.InProceedingsofCOLING2014,the25thInternationalConference onComputationalLinguistics: TechnicalPapers,pages69–78,2014. [SchacterandBuckner,1998] DanielLSchacterandRandyLBuckner. Primingandthebrain. Neuron, 20(2):185–195,1998. [Schapire, 1990] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2): BIBLIOGRAPHY 685 197–227,1990. [Schicketal.,2023] TimoSchick,JaneA.Yu,ZhengbaoJiang,FabioPetroni,PatrickLewis,Gautier Izacard,QingfeiYou,ChristoforosNalmpantis,EdouardGrave,andSebastianRiedel. PEER:A collaborativelanguagemodel. InProceedingsofTheEleventhInternationalConferenceonLearning Representations,2023. [Schicketal.,2024] TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli, EricHambro,LukeZettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Language modelscanteachthemselvestousetools. AdvancesinNeuralInformationProcessingSystems,36, 2024. [Schlagetal.,2021] ImanolSchlag,KazukiIrie,andJürgenSchmidhuber. Lineartransformersare secretlyfastweightprogrammers. InProceedingsofInternationalConferenceonMachineLearning, pages9355–9366.PMLR,2021. [Schmidhuber,2015] JürgenSchmidhuber. Deeplearninginneuralnetworks: Anoverview. Neural networks,61:85–117,2015. [Schnabeletal.,2015] TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval- uation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empiricalmethodsinnaturallanguageprocessing,pages298–307,2015a. [Schnabeletal.,2015] TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval- uation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empiricalmethodsinnaturallanguageprocessing,pages298–307,2015b. [Schneider et al., 2019] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervisedpre-trainingforspeechrecognition. InINTERSPEECH,2019. [Schulmanetal.,2015]",
    "TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval- uation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empiricalmethodsinnaturallanguageprocessing,pages298–307,2015a. [Schnabeletal.,2015] TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval- uation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empiricalmethodsinnaturallanguageprocessing,pages298–307,2015b. [Schneider et al., 2019] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervisedpre-trainingforspeechrecognition. InINTERSPEECH,2019. [Schulmanetal.,2015] JohnSchulman,SergeyLevine,PhilippMoritz,MichaelJordan,andPieter Abbeel. Trustregionpolicyoptimization. InProceedingsofthe32ndInternationalConferenceon InternationalConferenceonMachineLearning-Volume37,pages1889–1897,2015. [Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017. [SchusterandNakajima, 2012] MikeSchusterandKaisukeNakajima. Japaneseandkoreanvoice search. InProceedingsofInternationalConferenceonAcoustics,SpeechandSignalProcessing, pages5149–5152,2012. [Schusteretal.,2022] TalSchuster,AdamFisch,JaiGupta,MostafaDehghani,DaraBahri,VinhTran, YiTay,andDonaldMetzler. Confidentadaptivelanguagemodeling. AdvancesinNeuralInformation ProcessingSystems,35:17456–17472,2022. [Schwartzetal.,2020] RoySchwartz,GabrielStanovsky,SwabhaSwayamdipta,JesseDodge,and NoahASmith. Therighttoolforthejob:Matchingmodelandinstancecomplexities. InProceedings ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics, pages6640–6651, 2020. [See,2018] AbigailSee. Deeplearning,structureandinnatepriors: Adiscussionbetweenyannlecun and christopher manning, 02 2018. URL http://www.abigailsee.com/2018/02/21/ deep-learning-structure-and-innate-priors.html. [Seeetal.,2017] AbigailSee,PeterJLiu,andChristopherDManning.Gettothepoint:Summarization withpointer-generatornetworks. InProceedingsofthe55thAnnualMeetingoftheAssociationfor 686 BIBLIOGRAPHY ComputationalLinguistics(Volume1: LongPapers),pages1073–1083,2017. [Seni et al., 2010] Giovanni Seni, John Elder, and Robert Grossman. Ensemble Methods in Data Mining: ImprovingAccuracyThroughCombiningPredictions. MorganandClaypoolPublishers, 2010. [Sennrichetal.,2016] RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneuralmachine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the AssociationforComputationalLinguistics(Volume1: LongPapers),pages86–96,2016a. [Sennrichetal.,2016] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslation ofrarewordswithsubwordunits. InProceedingsofthe54thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages1715–1725,2016b. [Seoetal.,2017] MinjoonSeo,AniruddhaKembhavi,AliFarhadi,andHannanehHajishirzi. Bidirec- tionalattentionflowformachinecomprehension. InProceedingsofInternationalConferenceon LearningRepresentations,2017. [Shannon,1948] C.E.Shannon. Amathematicaltheoryofcommunication. TheBellSystemTechnical Journal,27(3):379–423,1948a. [Shannon,1948] ClaudeE.Shannon. Amathematicaltheoryofcommunication. Report,BellLabs, 1948b. [Shannon,1951] ClaudeEShannon. Predictionandentropyofprintedenglish. Bellsystemtechnical journal,30(1):50–64,1951. [Shawetal., 2018] PeterShaw, JakobUszkoreit, andAshishVaswani. Self-attentionwithrelative positionrepresentations. InProceedingsofthe2018ConferenceoftheNorthAmericanChapterof theAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume2(Short Papers),pages464–468,2018. [Shazeer,2019] NoamShazeer. Fasttransformerdecoding: Onewrite-headisallyouneed. arXiv preprintarXiv:1911.02150,2019. [Shazeer,2020] NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202, 2020. [Shazeer et al., 2017] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-expertslayer. InProceedingsofInternationalConferenceonLearningRepresentations, 2017. [Shenetal.,2020] DinghanShen,MingzhiZheng,YelongShen,YanruQu,andWeizhuChen.Asimple buttough-to-beatdataaugmentationapproachfornaturallanguageunderstandingandgeneration. arXivpreprintarXiv:2009.13818,2020a. [Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, MichaelWMahoney,andKurtKeutzer. Q-bert: Hessianbasedultralowprecisionquantizationof bert. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages8815–8821, 2020b. [Shen et al., 2016] Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In Proceedings of the 54th AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages 1683–1692,2016. BIBLIOGRAPHY 687 [Shenetal.,2019] TianxiaoShen,MyleOtt,MichaelAuli,andMarc’AurelioRanzato. Mixturemodels fordiversemachinetranslation: Tricksofthetrade. InInternationalconferenceonmachinelearning, pages5719–5728,2019. [Shietal.,2016] XingShi,InkitPadhi,andKevinKnight. Doesstring-basedneuralmtlearnsource syntax? InProceedingsofthe2016conferenceonempiricalmethodsinnaturallanguageprocessing, pages1526–1534,2016. [Shinn et al., 2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and ShunyuYao. Reflexion: Languageagentswithverbalreinforcementlearning. AdvancesinNeural InformationProcessingSystems,36:8634–8652,2023. [Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,andBryanCatanzaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusing modelparallelism. arXivpreprintarXiv:1909.08053,2019. [ShortenandKhoshgoftaar,2019] ConnorShortenandTaghiMKhoshgoftaar. Asurveyonimagedata augmentationfordeeplearning. Journalofbigdata,6(1):1–48,2019. [Silver et al., 2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, TimothyLillicrap,FanHui,LaurentSifre,GeorgevandenDriessche,ThoreGraepel,andDemis Hassabis. Masteringthegameofgowithouthumanknowledge. nature,550(7676):354–359,2017. [Singhal,2005] AmitSinghal. Introducingtheknowledgegraph: things,notstrings,2005. [Skalseetal.,2022] JoarSkalse,NikolausHowe,DmitriiKrasheninnikov,andDavidKrueger.Defining andcharacterizingrewardgaming. AdvancesinNeuralInformationProcessingSystems,35:9460– 9471,2022. [Skorskietal.,2021] MaciejSkorski,AlessandroTemperoni,andMartinTheobald. Revisitingweight initializationofdeepneuralnetworks. InAsianConferenceonMachineLearning,pages1192–1207. PMLR,2021. [Smithetal.,2017] SamuelLSmith,DavidHPTurban,StevenHamblin,andNilsYHammerla.Offline bilingualwordvectors,orthogonaltransformationsandtheinvertedsoftmax. InProceedingsofthe 5thInternationalConferenceonLearningRepresentations(ICLR),2017. [Smithetal.,2018] SamuelL.Smith,Pieter-JanKindermans,ChrisYing,andQuocV.Le. Don’tdecay thelearningrate,increasethebatchsize. InProceedingsofthe6thInternationalConferenceon LearningRepresentationsICLR,2018. [Snelletal.,2022] CharlieSnell,DanKlein,andRuiqiZhong. Learningbydistillingcontext. arXiv preprintarXiv:2209.15189,2022. [Snell et al., 2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test- time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314,2024. [Snelletal.,2025] CharlieVictorSnell,JaehoonLee,KelvinXu,andAviralKumar. ScalingLLM test-timecomputeoptimallycanbemoreeffectivethanscalingparametersforreasoning. InThe ThirteenthInternationalConferenceonLearningRepresentations,2025. [Soetal.,2019] DavidSo,QuocLe,andChenLiang. Theevolvedtransformer. InProceedingsof Internationalconferenceonmachinelearning,pages5877–5886.PMLR,2019. [Socheretal.,2011] RichardSocher,CliffCLin,ChrisManning,andAndrewYNg. Parsingnatural 688 BIBLIOGRAPHY scenesandnaturallanguagewithrecursiveneuralnetworks. InProceedingsofthe28thinternational conferenceonmachinelearning(ICML-11),pages129–136,2011. [Socheretal.,2013] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning, AndrewYNg,andChristopherPotts. Recursivedeepmodelsforsemanticcompositionalityovera sentimenttreebank. InProceedingsofthe2013conferenceonempiricalmethodsinnaturallanguage processing,pages1631–1642,2013. [Søgaard, 2016] Anders Søgaard. Evaluating word embeddings with fmri and eye-tracking. In Proceedingsofthe1stworkshoponevaluatingvector-spacerepresentationsforNLP,pages116–121, 2016. [Solorio-Fernández et al., 2020] Saúl Solorio-Fernández, J Ariel Carrasco-Ochoa, and José Fco Martínez-Trinidad. A review of unsupervised feature selection methods. Artificial Intelligence Review,53(2):907–948,2020. [Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and",
    "[Søgaard, 2016] Anders Søgaard. Evaluating word embeddings with fmri and eye-tracking. In Proceedingsofthe1stworkshoponevaluatingvector-spacerepresentationsforNLP,pages116–121, 2016. [Solorio-Fernández et al., 2020] Saúl Solorio-Fernández, J Ariel Carrasco-Ochoa, and José Fco Martínez-Trinidad. A review of unsupervised feature selection methods. Artificial Intelligence Review,53(2):907–948,2020. [Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequencetosequencepre-trainingforlanguagegeneration. InInternationalConferenceonMachine Learning,pages5926–5936.PMLR,2019. [Sperberetal., 2018] MatthiasSperber, JanNiehues, GrahamNeubig, SebastianStüker, andAlex Waibel. Self-attentionalacousticmodels. InProceedingsofInterspeech2018,pages3723–3727, 2018. [Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and RuslanSalakhutdinov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. The journalofmachinelearningresearch,15(1):1929–1958,2014. [Srivastavaetal.,2015] RupeshKumarSrivastava,KlausGreff,andJürgenSchmidhuber. Highway networks. arXivpreprintarXiv:1505.00387,2015. [StahlbergandByrne,2019] FelixStahlbergandBillByrne. Onnmtsearcherrorsandmodelerrors: Catgotyourtongue? InProceedingsofthe2019ConferenceonEmpiricalMethodsinNatural LanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing (EMNLP-IJCNLP),pages3356–3362,2019. [Stahlbergetal.,2016] FelixStahlberg,EvaHasler,AurelienWaite,andBillByrne. Syntactically guidedneuralmachinetranslation. InProceedingsofthe54thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume2: ShortPapers),pages299–305,2016. [Sternberg,1996] RobertJSternberg. Cognitivepsychology. HarcourtBraceCollegePublishers,1996. [Stewart,1993] GilbertWStewart. Ontheearlyhistoryofthesingularvaluedecomposition. SIAM review,35(4):551–566,1993. [Stiennonetal.,2020] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. AdvancesinNeuralInformationProcessingSystems,33:3008–3021,2020. [Stocketal.,2021] PierreStock,AngelaFan,BenjaminGraham,EdouardGrave,RémiGribonval, HerveJegou,andArmandJoulin. Trainingwithquantizationnoiseforextrememodelcompression. InProceedingsofInternationalConferenceonLearningRepresentations,2021. [Strubelletal.,2018] EmmaStrubell,PatrickVerga,DanielAndor,DavidWeiss,andAndrewMcCal- lum. Linguistically-informedself-attentionforsemanticrolelabeling. InProceedingsofthe2018 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages5027–5038,2018. BIBLIOGRAPHY 689 [Suetal.,2021] JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhanced transformerwithrotarypositionembedding. arXivpreprintarXiv:2104.09864,2021. [Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhancedtransformerwithrotarypositionembedding. Neurocomputing,568:127063, 2024. [Suetal.,2022] YixuanSu,TianLan,YanWang,DaniYogatama,LingpengKong,andNigelCollier. Acontrastiveframeworkforneuraltextgeneration. AdvancesinNeuralInformationProcessing Systems,35:21548–21561,2022. [Sukhbaataretal.,2015] SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus. End-to- endmemorynetworks. Advancesinneuralinformationprocessingsystems,28,2015. [Sukhbaataretal.,2019] SainbayarSukhbaatar,ÉdouardGrave,PiotrBojanowski,andArmandJoulin. Adaptiveattentionspanintransformers.InProceedingsofthe57thAnnualMeetingoftheAssociation forComputationalLinguistics,pages331–335,2019. [Sunetal.,2023] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,Jianyong Wang, andFuruWei. Retentivenetwork: Asuccessortotransformerforlargelanguagemodels. arXivpreprintarXiv:2307.08621,2023. [Sunetal.,2020] ZeweiSun,ShujianHuang,Hao-RanWei,Xin-yuDai,andJiajunChen. Generating diversetranslationbymanipulatingmulti-headattention. InProceedingsoftheAAAIConferenceon ArtificialIntelligence,volume34,pages8976–8983,2020a. [Sunetal.,2020] ZhiqingSun,HongkunYu,XiaodanSong,RenjieLiu,YimingYang,andDenny Zhou. Mobilebert: acompacttask-agnosticbertforresource-limiteddevices. InProceedingsofthe 58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages2158–2170,2020b. [Sundermeyeretal.,2012] MartinSundermeyer,RalfSchlüter,andHermannNey.Lstmneuralnetworks forlanguagemodeling. InProceedingsoftheThirteenthannualconferenceoftheinternational speechcommunicationassociation,2012. [Sutskever,2013] IlyaSutskever. Trainingrecurrentneuralnetworks. UniversityofTorontoToronto, 2013. [Sutskeveretal.,2013] IlyaSutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton. Onthe importanceofinitializationandmomentumindeeplearning. InInternationalconferenceonmachine learning,pages1139–1147.PMLR,2013. [Sutskeveretal.,2014] IlyaSutskever,OriolVinyals,andQuocVLe. Sequencetosequencelearning withneuralnetworks. Advancesinneuralinformationprocessingsystems,27,2014. [SuttonandMcCallum,2012] CharlesSuttonandAndrewMcCallum. Anintroductiontoconditional randomfields. FoundationsandTrends®inMachineLearning,4(4):267–373,2012. [Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction(2nded.). TheMITPress,2018. [Szabó, 2020] ZoltánGendlerSzabó. Compositionality. InEdwardN.Zalta, editor, TheStanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2020 edition, 2020. [Szegedyetal.,2014] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,Dumitru Erhan,IanGoodfellow,andRobFergus. Intriguingpropertiesofneuralnetworks. InProceedingsof the2ndInternationalConferenceonLearningRepresentations,2014a. 690 BIBLIOGRAPHY [Szegedyetal.,2014] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,Dumitru Erhan,IanGoodfellow,andRobFergus. Intriguingpropertiesofneuralnetworks. InProceedingsof 2ndInternationalConferenceonLearningRepresentations(ICLR2014),2014b. [Szegedyetal.,2016] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conferenceoncomputervisionandpatternrecognition,pages2818–2826,2016. [Szepesvári,2010] CsabaSzepesvári. Algorithmsforreinforcementlearning. SynthesisLectureson ArtificialIntelligenceandMachineLearning,4(1):1–103,2010. [Taietal.,2015] KaiShengTai,RichardSocher,andChristopherDManning. Improvedsemantic representationsfromtree-structuredlongshort-termmemorynetworks. InProceedingsofthe53rd AnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJoint ConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages1556–1566,2015. [Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. The web as a knowledge-base for answeringcomplexquestions. arXivpreprintarXiv:1803.06643,2018. [TanandLe,2019] MingxingTanandQuocLe. Efficientnet: Rethinkingmodelscalingforconvolu- tionalneuralnetworks. InInternationalconferenceonmachinelearning,pages6105–6114.PMLR, 2019. [Tangetal., 2015] DuyuTang, BingQin, andTingLiu. Documentmodelingwithgatedrecurrent neuralnetworkforsentimentclassification. InProceedingsofthe2015conferenceonempirical methodsinnaturallanguageprocessing,pages1422–1432,2015. [Tank and Hopfield, 1987] David W Tank and JJ Hopfield. Neural computation by concentrating informationintime. ProceedingsoftheNationalAcademyofSciences,84(7):1896–1900,1987. [Taorietal.,2023] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos Guestrin,PercyLiang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllama model. https://github.com/tatsu-lab/stanford_alpaca,2023. [Taskaretal.,2005] BenTaskar,SimonLacoste-Julien,andDanKlein. Adiscriminativematching approach to word alignment. In Proceedings of Human Language Technology Conference and ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages73–80,2005. [Tayetal.,2020] YiTay,DaraBahri,LiuYang,DonaldMetzler,andDa-ChengJuan. Sparsesinkhorn attention. In Proceedings of International Conference on Machine Learning, pages 9438–9447. PMLR,2020a. [Tayetal.,2020] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler. Efficienttransformers: Asurvey. CoRR,abs/2009.06732,2020b. [Teametal., 2024] GemmaTeam, MorganeRiviere, ShreyaPathak, PierGiuseppeSessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118,2024. [Teknium,2023] Teknium. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassis- tants,2023. URLhttps://huggingface.co/datasets/teknium/OpenHermes-2.5. [Telgarsky,2016] MatusTelgarsky. Benefitsofdepthinneuralnetworks. InConferenceonlearning theory,pages1517–1539.PMLR,2016. [Tenney et al., 2019] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical BIBLIOGRAPHY 691 nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,pages4593–4601,2019a. [Tenney et al., 2019] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy,NajoungKim,BenjaminVanDurme,SamBowman,DipanjanDas,andElliePavlick. What doyoulearnfromcontext? probingforsentencestructureincontextualizedwordrepresentations. In ProceedingsofInternationalConferenceonLearningRepresentations,2019b. [Timonin et al., 2022] Denis Timonin, BoYang Hsueh, and Vinh Nguyen. Accelerated",
    "691 nlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,pages4593–4601,2019a. [Tenney et al., 2019] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy,NajoungKim,BenjaminVanDurme,SamBowman,DipanjanDas,andElliePavlick. What doyoulearnfromcontext? probingforsentencestructureincontextualizedwordrepresentations. In ProceedingsofInternationalConferenceonLearningRepresentations,2019b. [Timonin et al., 2022] Denis Timonin, BoYang Hsueh, and Vinh Nguyen. Accelerated inference for large transformer models using nvidia tri- ton inference server. https://developer.nvidia.com/blog/ accelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/, 2022. [Tissieretal.,2017] JulienTissier,ChristopheGravier,andAmauryHabrard. Dict2vec: Learningword embeddingsusinglexicaldictionaries. InProceedingsofthe2017ConferenceonEmpiricalMethods inNaturalLanguageProcessing,pages254–263,2017. [Tjong Kim Sang, 2002] Erik F. Tjong Kim Sang. Introduction to the CoNLL-2002 shared task: Language-independentnamedentityrecognition.InProceedingsofCOLING-02:The6thConference onNaturalLanguageLearning2002(CoNLL-2002),2002. [TjongKimSangandBuchholz,2000] ErikF.TjongKimSangandSabineBuchholz. Introduction totheCoNLL-2000sharedtaskchunking. InProceedingsofFourthConferenceonComputational NaturalLanguageLearningandtheSecondLearningLanguageinLogicWorkshop,2000. [Touvronetal.,2023] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a. [Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel, LukasBlecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa, IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,Jenya Lee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,PushkarMishra, IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,Alan Schelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang, RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang, AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,2023b. [TrentinandGori,2001] EdmondoTrentinandMarcoGori. Asurveyofhybridann/hmmmodelsfor automaticspeechrecognition. Neurocomputing,37(1-4):91–126,2001. [Tsvetkovetal.,2015] YuliaTsvetkov,ManaalFaruqui,WangLing,GuillaumeLample,andChris Dyer. Evaluationofwordvectorrepresentationsbysubspacealignment. InProceedingsofthe2015 ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2049–2054,2015. [Tuetal.,2016] ZhaopengTu,ZhengdongLu,YangLiu,XiaohuaLiu,andHangLi.Modelingcoverage forneuralmachinetranslation. InProceedingsofthe54thAnnualMeetingoftheAssociationfor 692 BIBLIOGRAPHY ComputationalLinguistics(Volume1: LongPapers),pages76–85,2016. [Tulving and Schacter, 1990] Endel Tulving and Daniel L Schacter. Priming and human memory systems. Science,247(4940):301–306,1990. [Uesatoetal.,2022] JonathanUesato,NateKushman,RamanaKumar,FrancisSong,NoahSiegel, LisaWang,AntoniaCreswell,GeoffreyIrving,andIrinaHiggins. Solvingmathwordproblemswith process-andoutcome-basedfeedback. arXivpreprintarXiv:2211.14275,2022. [Uffink,2017] JosUffink. Boltzmann’sWorkinStatisticalPhysics. InEdwardN.Zalta,editor,The StanfordEncyclopediaofPhilosophy.MetaphysicsResearchLab,StanfordUniversity,Spring2017 edition,2017. [Ulyanovetal.,2016] DmitryUlyanov,AndreaVedaldi,andVictorLempitsky. Instancenormalization: Themissingingredientforfaststylization. arXivpreprintarXiv:1607.08022,2016. [VanderMaatenandHinton,2008] LaurensVanderMaatenandGeoffreyHinton. Visualizingdata usingt-sne. Journalofmachinelearningresearch,9(11),2008. [Vapnik and Chervonenkis, 1971] Vladimir Vapnik and Alexey Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications,16(2):264–279,1971. [Vaswanietal.,2017] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones, AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InProceedingsof AdvancesinNeuralInformationProcessingSystems,volume30,2017. [Velicˇkovic´ et al., 2018] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero, PietroLiò,andYoshuaBengio. Graphattentionnetworks. InInternationalConferenceonLearning Representations,2018. [Vijayakumaretal.,2018] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun, StefanLee, DavidCrandall, andDhruvBatra. Diversebeamsearchforimproveddescriptionof complexscenes. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,2018. [Vincentetal.,2010] PascalVincent,HugoLarochelle,IsabelleLajoie,YoshuaBengio,Pierre-Antoine Manzagol,andLéonBottou. Stackeddenoisingautoencoders: Learningusefulrepresentationsina deepnetworkwithalocaldenoisingcriterion. Journalofmachinelearningresearch,11(12),2010. [Vinyalsetal.,2015] OriolVinyals,ŁukaszKaiser,TerryKoo,SlavPetrov,IlyaSutskever,andGeoffrey Hinton. Grammarasaforeignlanguage. Advancesinneuralinformationprocessingsystems,28, 2015. [Viterbi,1967] AndrewJViterbi. Errorboundsforconvolutionalcodesandanasymptoticallyoptimum decodingalgorithm. IEEETransactionsonInformationTheory,1967. [Vogel et al., 1996] Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word alignmentinstatisticaltranslation. InCOLING1996Volume2: The16thInternationalConference onComputationalLinguistics,1996. [Voitaetal.,2018] ElenaVoita,PavelSerdyukov,RicoSennrich,andIvanTitov. Context-awareneural machinetranslationlearnsanaphoraresolution. InProceedingsofthe56thAnnualMeetingofthe AssociationforComputationalLinguistics(Volume1: LongPapers),pages1264–1274,2018. [Voitaetal.,2019] ElenaVoita,DavidTalbot,FedorMoiseev,RicoSennrich,andIvanTitov. Ana- lyzingmulti-headself-attention: Specializedheadsdotheheavylifting,therestcanbepruned. In Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages BIBLIOGRAPHY 693 5797–5808,2019. [VonOswaldetal.,2023] JohannesVonOswald,EyvindNiklasson,EttoreRandazzo,JoãoSacramento, AlexanderMordvintsev,AndreyZhmoginov,andMaxVladymyrov. Transformerslearnin-context by gradient descent. In Proceedings of International Conference on Machine Learning, pages 35151–35174.PMLR,2023. [Waibel et al., 1989] Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin J Lang. Phoneme recognition using time-delay neural networks. IEEE transactions on acoustics,speech,andsignalprocessing,37(3):328–339,1989. [Wallaceetal.,2019] EricWallace,YizhongWang,SujianLi,SameerSingh,andMattGardner. Donlp modelsknownumbers? probingnumeracyinembeddings. InProceedingsofthe2019Conference onEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConference onNaturalLanguageProcessing(EMNLP-IJCNLP),pages5307–5315,2019. [Wangetal.,2024] ChenglongWang,HangZhou,YiminHu,YifuHuo,BeiLi,TongranLiu,Tong Xiao,andJingboZhu. Esrl:Efficientsampling-basedreinforcementlearningforsequencegeneration. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages19107–19115,2024. [Wangetal., 2020] HanruiWang, ZhanghaoWu, ZhijianLiu, HanCai, LigengZhu, ChuangGan, and Song Han. Hat: Hardware-aware transformers for efficient natural language processing. In Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages 7675–7688,2020a. [Wangetal.,2022] HongyuWang,ShumingMa,LiDong,ShaohanHuang,DongdongZhang,and FuruWei. Deepnet: Scalingtransformersto1,000layers. arXivpreprintarXiv:2203.00555,2022a. [Wangetal.,2022] HongyuWang,ShumingMa,ShaohanHuang,LiDong,WenhuiWang,Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary,XiaSong,andFuruWei. Foundationtransformers. arXivpreprintarXiv:2210.06423, 2022b. [Wang et al., 2022] Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. Skipbert: Efficientinferencewithshallowlayerskipping. InProceedingsofthe60thAnnualMeetingofthe AssociationforComputationalLinguistics(Volume1: LongPapers),pages7287–7301,2022c. [Wang and Yoon, 2021] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learningforvisualintelligence: Areviewandnewoutlooks. IEEEtransactionsonpatternanalysis andmachineintelligence,44(6):3048–3068,2021. [Wangetal.,2023] LiyuanWang,XingxingZhang,HangSu,andJunZhu. Acomprehensivesurveyof continuallearning: Theory,methodandapplication. arXivpreprintarXiv:2302.00487,2023a. [Wangetal.,2023] PeihaoWang,RameswarPanda,LucasTorrobaHennigen,PhilipGreengard,Leonid Karlinsky,RogerioFeris,DavidDanielCox,ZhangyangWang,andYoonKim. Learningtogrow pretrainedmodelsforefficienttransformertraining. InProceedingsofTheEleventhInternational ConferenceonLearningRepresentations,2023b. [Wangetal.,2018] QiangWang,FuxueLi,TongXiao,YanyangLi,YinqiaoLi,andJingboZhu. Multi- layerrepresentationfusionforneuralmachinetranslation. InProceedingsofthe27thInternational ConferenceonComputationalLinguistics,pages3015–3026,2018a. [Wangetal.,2019] QiangWang,BeiLi,TongXiao,JingboZhu,ChangliangLi,DerekFWong,and LidiaSChao. Learningdeeptransformermodelsformachinetranslation. InProceedingsofthe57th 694 BIBLIOGRAPHY AnnualMeetingoftheAssociationforComputationalLinguistics,pages1810–1822,2019a. [Wangetal.,2020] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attentionwithlinearcomplexity. arXivpreprintarXiv:2006.04768,2020b. [Wang et al., 2019] Wei Wang, Vincent Wenchen Zheng, Han Yu, and",
    "and student-teacher learningforvisualintelligence: Areviewandnewoutlooks. IEEEtransactionsonpatternanalysis andmachineintelligence,44(6):3048–3068,2021. [Wangetal.,2023] LiyuanWang,XingxingZhang,HangSu,andJunZhu. Acomprehensivesurveyof continuallearning: Theory,methodandapplication. arXivpreprintarXiv:2302.00487,2023a. [Wangetal.,2023] PeihaoWang,RameswarPanda,LucasTorrobaHennigen,PhilipGreengard,Leonid Karlinsky,RogerioFeris,DavidDanielCox,ZhangyangWang,andYoonKim. Learningtogrow pretrainedmodelsforefficienttransformertraining. InProceedingsofTheEleventhInternational ConferenceonLearningRepresentations,2023b. [Wangetal.,2018] QiangWang,FuxueLi,TongXiao,YanyangLi,YinqiaoLi,andJingboZhu. Multi- layerrepresentationfusionforneuralmachinetranslation. InProceedingsofthe27thInternational ConferenceonComputationalLinguistics,pages3015–3026,2018a. [Wangetal.,2019] QiangWang,BeiLi,TongXiao,JingboZhu,ChangliangLi,DerekFWong,and LidiaSChao. Learningdeeptransformermodelsformachinetranslation. InProceedingsofthe57th 694 BIBLIOGRAPHY AnnualMeetingoftheAssociationforComputationalLinguistics,pages1810–1822,2019a. [Wangetal.,2020] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa. Linformer: Self-attentionwithlinearcomplexity. arXivpreprintarXiv:2006.04768,2020b. [Wang et al., 2019] Wei Wang, Vincent Wenchen Zheng, Han Yu, and Chunyan Miao. A survey ofzero-shotlearning. ACMTransactionsonIntelligentSystemsandTechnology(TIST),10:1–37, 2019b. [Wangetal.,2018] XinWang,FisherYu,Zi-YiDou,TrevorDarrell,andJosephEGonzalez. Skipnet: Learningdynamicroutinginconvolutionalnetworks. InProceedingsoftheEuropeanConferenceon ComputerVision(ECCV),pages409–424,2018b. [Wangetal.,2022] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,andDennyZhou. Rationale-augmentedensemblesinlanguagemodels. arXivpreprintarXiv:2207.00747,2022d. [Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoninginlanguagemodels. InProceedingsofTheEleventhInternationalConferenceonLearning Representations,2023c. [Wangetal.,2020] YaqingWang,QuanmingYao,JamesT.Kwok,andLionelM.Ni. Generalizing fromafewexamples: Asurveyonfew-shotlearning. ACMComputingSurveys,53(3):1–34,2020c. [Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, DavidStap,EshaanPathak,GiannisKaramanolakis,HaizhiGaryLai,IshanPurohit,IshaniMondal, JacobAnderson,KirbyKuznia,KrimaDoshi,KuntalKumarPal,MaitreyaPatel,MehradMoradshahi, MihirParmar,MiraliPurohit,NeerajVarshney,PhaniRohithaKaza,PulkitVerma,RavsehajSingh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-naturalinstructions: Generalization via declarativeinstructionson1600+nlptasks. InProceedingsofthe2022ConferenceonEmpirical MethodsinNaturalLanguageProcessing,pages5085–5109,2022e. [Wangetal.,2023] YizhongWang,HamishIvison,PradeepDasigi,JackHessel,TusharKhot,Khy- athiRaghaviChandu,DavidWadden,KelseyMacMillan,NoahA.Smith,IzBeltagy,andHannaneh Hajishirzi. How far can camels go? exploring the state of instruction tuning on open resources. AdvancesinNeuralInformationProcessingSystems,36:74764–74786,2023d. [Wangetal.,2023] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel Khashabi,andHannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers),pages13484–13508,2023e. [Wangetal.,2023] ZhenyiWang,EnnengYang,LiShen,andHengHuang. Acomprehensivesurvey offorgettingindeeplearningbeyondcontinuallearning. arXivpreprintarXiv:2307.09218,2023f. [Wang et al., 2020] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large languagemodels.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage Processing(EMNLP),pages6151–6162,2020d. [Warstadtetal.,2019] AlexWarstadt,AmanpreetSingh,andSamuelRBowman. Neuralnetwork acceptabilityjudgments. TransactionsoftheAssociationforComputationalLinguistics,7:625–641, 2019. BIBLIOGRAPHY 695 [WebsterandKit,1992] JonathanJWebsterandChunyuKit. Tokenizationastheinitialphaseinnlp. InProceedingsofCOLING1992volume4: The14thinternationalconferenceoncomputational linguistics,1992. [Weietal.,2022] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester, NanDu,AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. In ProceedingsofInternationalConferenceonLearningRepresentations,2022a. [Weietal.,2022] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud, DaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,EdH.Chi,TatsunoriHashimoto, OriolVinyals, PercyLiang, JeffDean, andWilliamFedus. Emergentabilitiesoflargelanguage models. arXivpreprintarXiv:2206.07682,2022b. [Weietal.,2022] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia, EdH.Chi,QuocV.Le,andDennyZhou. Chain-of-thoughtpromptingelicitsreasoninginlarge languagemodels. AdvancesinNeuralInformationProcessingSystems,35:24824–24837,2022c. [Weiss et al., 2021] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In ProceedingsofInternationalConferenceonMachineLearning,pages11080–11090.PMLR,2021. [Wellecketal.,2023] SeanWelleck,XimingLu,PeterWest,FaezeBrahman,TianxiaoShen,Daniel Khashabi,andYejinChoi. Generatingsequencesbylearningtoself-correct. InProceedingsofThe EleventhInternationalConferenceonLearningRepresentations,2023. [Weng, 2021] Lilian Weng. How to train really large models on many gpus? lil- ianweng.github.io, Sep 2021. URL https://lilianweng.github.io/posts/ 2021-09-25-train-large/. [Werbos, 1990] Paul J Werbos. Backpropagation through time: what it does and how to do it. ProceedingsoftheIEEE,78(10):1550–1560,1990. [Weston et al., 2015] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Proceedingsofthe3rdInternationalConferenceonLearningRepresentations,ICLR2015,2015. [Wiener,1960] NorbertWiener. Somemoralandtechnicalconsequencesofautomation: Asmachines learntheymaydevelopunforeseenstrategiesatratesthatbaffletheirprogrammers. Science,131 (3410):1355–1358,1960. [WiggsandMartin,1998] CheriLWiggsandAlexMartin. Propertiesandmechanismsofperceptual priming. Currentopinioninneurobiology,8(2):227–233,1998. [Wiheretal.,2022] GianWiher,ClaraMeister,andRyanCotterell. Ondecodingstrategiesforneural textgenerators. TransactionsoftheAssociationforComputationalLinguistics,10:997–1012,2022. [Williamsetal.,2018] AdinaWilliams,NikitaNangia,andSamuelBowman. Abroad-coveragechal- lengecorpusforsentenceunderstandingthroughinference. InProceedingsofthe2018Conference oftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage Technologies,Volume1(LongPapers),pages1112–1122,2018. [Williams,1992] RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionist reinforcementlearning. Machinelearning,8:229–256,1992. [WilliamsandPeng,1990] RonaldJWilliamsandJingPeng. Anefficientgradient-basedalgorithmfor on-linetrainingofrecurrentnetworktrajectories. Neuralcomputation,2(4):490–501,1990. [WilliamsandZipser,1989] RonaldJWilliamsandDavidZipser. Alearningalgorithmforcontinually runningfullyrecurrentneuralnetworks. Neuralcomputation,1(2):270–280,1989. 696 BIBLIOGRAPHY [Wingateetal.,2022] DavidWingate,MohammadShoeybi,andTaylorSorensen. Promptcompression andcontrastiveconditioningforcontrollabilityandtoxicityreductioninlanguagemodels.InFindings oftheAssociationforComputationalLinguistics: EMNLP2022,pages5621–5634,2022. [Wittgenstein,1953] LudwigWittgenstein. Philosophicalinvestigations.PhilosophischeUntersuchun- gen. Macmillan,1953. [Wold et al., 1987] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometricsandintelligentlaboratorysystems,2(1-3):37–52,1987. [Wolpert, 1996] David H. Wolpert. The lack of a priori distinctions between learning algorithms. NeuralComputatoin,8(7):1341–1390,1996. [WolpertandMacready,1997] DavidH.WolpertandWilliamG.Macready. Nofreelunchtheorems foroptimization. IEEETransactionsonEvolutionaryComputation,1(1):67–82,1997. [WozengraftandReiffen,1961] JohnM.WozengraftandBarneyReiffen. SequentialDecoding. The MITPress,1961. [Wright and Ma, 2022] John Wright and Yi Ma. High-Dimensional Data Analysis with Low- Dimensional Models: Principles, Computation, and Applications. Cambridge University Press, 2022. [Wuetal.,2023] BingyangWu,YinminZhong,ZiliZhang,ShengyuLiu,FangyueLiu,YuanhangSun, GangHuang,XuanzheLiu,andXinJin. Fastdistributedinferenceservingforlargelanguagemodels. arXivpreprintarXiv:2305.05920,2023a. [Wuetal.,2018] FelixWu,AngelaFan,AlexeiBaevski,YannDauphin,andMichaelAuli. Payless attentionwithlightweightanddynamicconvolutions. InProceedingsofInternationalConferenceon LearningRepresentations,2018a. [Wuetal.,2019] FelixWu,AngelaFan,AlexeiBaevski,YannDauphin,andMichaelAuli. Payless attentionwithlightweightanddynamicconvolutions. InProceedingsofInternationalConferenceon LearningRepresentations,2019. [Wuetal.,2024] WilsonWu,JohnXMorris,andLionelLevine. Dolanguagemodelsplanforfuture tokens? arXivpreprintarXiv:2404.00859,2024. [Wuetal.,2020] XuanfuWu,YangFeng,andChenzeShao. Generatingdiversetranslationfrommodel distributionwithdropout. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural LanguageProcessing(EMNLP),pages1088–1097,2020a. [Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,Apurva Shah,MelvinJohnson,XiaobingLiu,ŁukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff",
    "WilsonWu,JohnXMorris,andLionelLevine. Dolanguagemodelsplanforfuture tokens? arXivpreprintarXiv:2404.00859,2024. [Wuetal.,2020] XuanfuWu,YangFeng,andChenzeShao. Generatingdiversetranslationfrommodel distributionwithdropout. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural LanguageProcessing(EMNLP),pages1088–1097,2020a. [Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, WolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,Apurva Shah,MelvinJohnson,XiaobingLiu,ŁukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google’sneuralmachinetranslationsystem: Bridgingthegapbetweenhumanandmachine translation. arXivpreprintarXiv:1609.08144,2016. [Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizingtransformers. InProceedingsofInternationalConferenceonLearningRepresentations, 2021. [WuandHe,2018] YuxinWuandKaimingHe. Groupnormalization. InProceedingsoftheEuropean conferenceoncomputervision(ECCV),pages3–19,2018. BIBLIOGRAPHY 697 [Wuetal.,2023] ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,AlaneSuhr,PrithvirajAmmanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives betterrewardsforlanguagemodeltraining. InThirty-seventhConferenceonNeuralInformation ProcessingSystems,2023b. [Wuetal.,2020] ZhanghaoWu,ZhijianLiu,JiLin,YujunLin,andSongHan. Litetransformerwith long-shortrangeattention. InProceedingsofInternationalConferenceonLearningRepresentations (ICLR),2020b. [Wuetal., 2018] ZuxuanWu, TusharNagarajan, AbhishekKumar, StevenRennie, LarrySDavis, KristenGrauman,andRogerioFeris. Blockdrop: Dynamicinferencepathsinresidualnetworks. In ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages8817–8826, 2018b. [Xiaetal.,2024] MengzhouXia,SadhikaMalladi,SuchinGururangan,SanjeevArora,andDanqiChen. Less: Selectinginfluentialdatafortargetedinstructiontuning. arXivpreprintarXiv:2402.04333, 2024. [Xiaoetal.,2024] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficient streaming language models with attention sinks. In Proceedings of The Twelfth International ConferenceonLearningRepresentations,2024. [Xiaoetal.,2013] TongXiao,JingboZhu,andTongranLiu. Baggingandboostingstatisticalmachine translationsystems. ArtificialIntelligence,195:496–527,2013. [Xiaoetal.,2019] TongXiao,YinqiaoLi,JingboZhu,ZhengtaoYu,andTongranLiu.Sharingattention weightsforfasttransformer. InProceedingsoftheTwenty-EighthInternationalJointConferenceon ArtificialIntelligence(IJCAI-19),pages5292–5298,2019. [Xieetal.,2017] SainingXie,RossGirshick,PiotrDollár,ZhuowenTu,andKaimingHe. Aggregated residual transformations for deep neural networks. In Proceedings of the IEEE conference on computervisionandpatternrecognition,pages1492–1500,2017. [Xieetal.,2022] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa. Anexplanation ofin-contextlearningasimplicitbayesianinference. InProceedingsofInternationalConferenceon LearningRepresentations,2022. [Xinetal.,2020] JiXin,RaphaelTang,JaejunLee,YaoliangYu,andJimmyLin. Deebert: Dynamic early exiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the AssociationforComputationalLinguistics,pages2246–2251,2020. [Xiongetal.,2020] RuibinXiong,YunchangYang,DiHe,KaiZheng,ShuxinZheng,ChenXing, HuishuaiZhang,YanyanLan,LiweiWang,andTieyanLiu.Onlayernormalizationinthetransformer architecture. InInternationalConferenceonMachineLearning,pages10524–10533,2020. [Xuetal.,2024] CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,Chongyang Tao,QingweiLin,andDaxinJiang. Wizardlm: Empoweringlargepre-trainedlanguagemodelsto followcomplexinstructions. InTheTwelfthInternationalConferenceonLearningRepresentations, 2024. [XuandMcauley,2023] CanwenXuandJulianMcauley. Asurveyondynamicneuralnetworksfor naturallanguageprocessing. InFindingsoftheAssociationforComputationalLinguistics: EACL 2023,pages2325–2336,2023. [Xuetal.,2021] ChenXu,BojieHu,YanyangLi,YuhaoZhang,ShenHuang,QiJu,TongXiao,and 698 BIBLIOGRAPHY JingboZhu. Stackedacoustic-and-textualencoding: Integratingthepre-trainedmodelsintospeech translationencoders.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputational Linguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages2619–2630,2021a. [Xuetal.,2023] ChenXu,RongYe,QianqianDong,ChengqiZhao,TomKo,MingxuanWang,Tong Xiao,andJingboZhu. Recentadvancesindirectspeech-to-texttranslation. InProceedingsofthe Thirty-SecondInternationalJointConferenceonArtificialIntelligence(IJCAI-23): SurveyTrack, pages6796–6804,2023a. [Xuetal.,2023] ChenXu,YuhaoZhang,ChengboJiao,XiaoqianLiu,ChiHu,XinZeng,TongXiao, AnxiangMa,HuizhenWang,andJingboZhu. Bridgingthegranularitygapforacousticmodeling. In FindingsoftheAssociationforComputationalLinguistics: ACL2023,pages10816–10833,2023b. [Xuetal.,2020] HongfeiXu,QiuhuiLiu,JosefvanGenabith,DeyiXiong,andJingyiZhang. Lipschitz constrainedparameterinitializationfordeeptransformers.InProceedingsofthe58thAnnualMeeting oftheAssociationforComputationalLinguistics,pages397–402,July2020. [Xu et al., 2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generationwithvisualattention. InInternationalconferenceonmachinelearning,pages2048–2057. PMLR,2015. [Xuetal.,2023] PengXu,XiatianZhu,andDavidAClifton. Multimodallearningwithtransformers: Asurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023c. [Xuetal.,2021] ZenanXu,DayaGuo,DuyuTang,QinliangSu,LinjunShou,MingGong,Wanjun Zhong, Xiaojun Quan, Daxin Jiang, and Nan Duan. Syntax-enhanced pre-trained model. In Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages5412–5422,2021b. [Yangetal., 2024] AnYang, BaosongYang, BeichenZhang, BinyuanHui, BoZheng, BowenYu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprintarXiv:2412.15115,2024. [Yangetal.,2018] BaosongYang,ZhaopengTu,DerekFWong,FandongMeng,LidiaSChao,and TongZhang. Modelinglocalnessforself-attentionnetworks. InProceedingsofthe2018Conference onEmpiricalMethodsinNaturalLanguageProcessing,pages4449–4458,2018a. [Yangetal.,2018] YilinYang,LiangHuang,andMingboMa. Breakingthebeamsearchcurse: A studyof(re-)scoringmethodsandstoppingcriteriaforneuralmachinetranslation. InProceedings ofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3054–3059, 2018b. [Yangetal.,2023] ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,Zicheng Liu, andLijuan Wang. The dawnof lmms: Preliminary explorationswith gpt-4v(ision). arXiv preprintarXiv:2309.17421,2023a. [Yangetal.,2019] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov, andQuocVLe. Xlnet:Generalizedautoregressivepretrainingforlanguageunderstanding. Advances inneuralinformationprocessingsystems,32,2019. [Yangetal.,2023] ZiYang,SamridhiChoudhary,SiegfriedKunzmann,andZhengZhang.Quantization- awareandtensor-compressedtrainingoftransformersfornaturallanguageunderstanding. arXiv BIBLIOGRAPHY 699 preprintarXiv:2306.01076,2023b. [Yangetal.,2016] ZichaoYang,DiyiYang,ChrisDyer,XiaodongHe,AlexSmola,andEduardHovy. Hierarchicalattentionnetworksfordocumentclassification. InProceedingsofthe2016conference oftheNorthAmericanchapteroftheassociationforcomputationallinguistics: humanlanguage technologies,pages1480–1489,2016. [Yaoetal.,2024] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,and KarthikNarasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. AdvancesinNeuralInformationProcessingSystems,36,2024. [Yaoetal.,2007] YuanYao,LorenzoRosasco,andAndreaCaponnetto. Onearlystoppingingradient descentlearning. ConstructiveApproximation,26:289–315,2007. [Yarowsky, 1994] David Yarowsky. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the AssociationforComputationalLinguistics,pages88–95,1994. [Yarowsky, 1995] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. InProceedingsofthe33rdannualmeetingoftheassociationforcomputationallinguistics, pages189–196,1995. [Yeetal.,2021] RongYe,MingxuanWang,andLeiLi. End-to-endspeechtranslationviacross-modal progressivetraining. arXivpreprintarXiv:2104.10380,2021. [Yinetal.,2023] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen. Asurveyonmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023. [Youetal.,2020] WeiqiuYou,SimengSun,andMohitIyyer. Hard-codedgaussianattentionforneural machinetranslation.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputational Linguistics,pages7689–7700,2020. [Yuetal.,2022] Gyeong-InYu,JooSeongJeong,Geon-WooKim,SoojeongKim,andByung-Gon Chun. Orca: Adistributedservingsystemfor{Transformer-Based}generativemodels. In16th USENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI22),pages521–538, 2022. [Yuetal.,2023] YaodongYu,SamBuchanan,DruvPai,TianzheChu,ZiyangWu,ShengbangTong, BenjaminDHaeffele,andYiMa. White-boxtransformersviasparseratereduction. arXivpreprint arXiv:2306.01129,2023a. [Yuetal.,2023] ZihanYu,LiangHe,ZhenWu,XinyuDai,andJiajunChen. Towardsbetterchain-of- thoughtpromptingstrategies: Asurvey. arXivpreprintarXiv:2310.04959,2023b. [Yukseletal.,2012] SenihaEsenYuksel,JosephNWilson,andPaulDGader. Twentyyearsofmixture ofexperts. IEEEtransactionsonneuralnetworksandlearningsystems,23(8):1177–1193,2012. [Yun et al., 2019] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and SanjivKumar. Aretransformersuniversalapproximatorsofsequence-to-sequencefunctions? In ProceedingsofInternationalConferenceonLearningRepresentations,2019. [Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C.Alberti,S.Ontañón,PhilipPham,AnirudhRavula,QifanWang,L.Yang,andA.Ahmed. Big bird: Transformersforlongersequences. Advancesinneuralinformationprocessingsystems,33: 17283–17297,2020. [Zaslavskiyetal.,2009] MikhailZaslavskiy,MarcDymetman,andNicolaCancedda. Phrase-based 700 BIBLIOGRAPHY statisticalmachinetranslationasatravelingsalesmanproblem.InProceedingsoftheJointConference",
    "Asurvey. arXivpreprintarXiv:2310.04959,2023b. [Yukseletal.,2012] SenihaEsenYuksel,JosephNWilson,andPaulDGader. Twentyyearsofmixture ofexperts. IEEEtransactionsonneuralnetworksandlearningsystems,23(8):1177–1193,2012. [Yun et al., 2019] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and SanjivKumar. Aretransformersuniversalapproximatorsofsequence-to-sequencefunctions? In ProceedingsofInternationalConferenceonLearningRepresentations,2019. [Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, C.Alberti,S.Ontañón,PhilipPham,AnirudhRavula,QifanWang,L.Yang,andA.Ahmed. Big bird: Transformersforlongersequences. Advancesinneuralinformationprocessingsystems,33: 17283–17297,2020. [Zaslavskiyetal.,2009] MikhailZaslavskiy,MarcDymetman,andNicolaCancedda. Phrase-based 700 BIBLIOGRAPHY statisticalmachinetranslationasatravelingsalesmanproblem.InProceedingsoftheJointConference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural LanguageProcessingoftheAFNLP,pages333–341,2009. [Zeiler, 2012] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,2012. [Zellersetal.,2018] RowanZellers,YonatanBisk,RoySchwartz,andYejinChoi. Swag: Alarge-scale adversarialdatasetforgroundedcommonsenseinference. InProceedingsofthe2018Conferenceon EmpiricalMethodsinNaturalLanguageProcessing,pages93–104,2018. [Zhangetal.,2021] AstonZhang,ZacharyC.Lipton,MuLi,andAlexanderJ.Smola. Diveintodeep learning. arXivpreprintarXiv:2106.11342,2021. [ZhangandSennrich,2019] BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization. AdvancesinNeuralInformationProcessingSystems,32,2019. [Zhangetal.,2018] BiaoZhang,DeyiXiong,andJinsongSu. Acceleratingneuraltransformervia anaverageattentionnetwork. InProceedingsofthe56thAnnualMeetingoftheAssociationfor ComputationalLinguistics(Volume1: LongPapers),pages1789–1798,2018a. [Zhangetal.,2019] BiaoZhang,IvanTitov,andRicoSennrich. Improvingdeeptransformerwith depth-scaledinitializationandmergedattention.InProceedingsofthe2019ConferenceonEmpirical MethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural LanguageProcessing(EMNLP-IJCNLP),pages898–909,2019a. [Zhang et al., 2020] Jiajun Zhang, Long Zhou, Yang Zhao, and Chengqing Zong. Synchronous bidirectionalinferenceforneuralsequencegeneration. ArtificialIntelligence,281:103234,2020a. [Zhang et al., 2019] Juexiao Zhang, Yubei Chen, Brian Cheung, and Bruno A Olshausen. Word embeddingvisualizationviadictionarylearning. arXivpreprintarXiv:1910.03833,2019b. [Zhangetal.,2020] WeiEmmaZhang,QuanZSheng,AhoudAlhazmi,andChenliangLi. Adversarial attacksondeep-learningmodelsinnaturallanguageprocessing: Asurvey. ACMTransactionson IntelligentSystemsandTechnology(TIST),11(3):1–41,2020b. [Zhang et al., 2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networksfortextclassification. Advancesinneuralinformationprocessingsystems,28,2015. [Zhangetal.,2018] XiangwenZhang,JinsongSu,YueQin,YangLiu,RongrongJi,andHongjiWang. Asynchronousbidirectionaldecodingforneuralmachinetranslation. InProceedingsoftheAAAI conferenceonartificialintelligence,volume32,2018b. [ZhangandYang,2021] YuZhangandQiangYang.Asurveyonmulti-tasklearning.IEEETransactions onKnowledgeandDataEngineering,pages1–1,2021. [Zhangetal.,2024] YunxiangZhang,MuhammadKhalifa,LajanugenLogeswaran,JaekyeomKim, MoontaeLee,HonglakLee,andLuWang.Smalllanguagemodelsneedstrongverifierstoself-correct reasoning. InACL(Findings),2024. [Zhangetal.,2020] ZhuoshengZhang,YuweiWu,JunruZhou,SufengDuan,HaiZhao,andRuiWang. Sg-net: Syntax-guidedmachinereadingcomprehension. InProceedingsoftheAAAIConferenceon ArtificialIntelligence,pages9636–9643,2020c. [Zhangetal., 2023] ZhuoshengZhang, YaoYao, AstonZhang, XiangruTang, XinbeiMa, Zhiwei He, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao. Igniting language intelligence: The hitchhiker’s guide from chain-of-thought reasoning to language agents. arXiv BIBLIOGRAPHY 701 preprintarXiv:2311.11797,2023a. [Zhangetal.,2023] ZhuoshengZhang,AstonZhang,MuLi,andAlexSmola. Automaticchainof thoughtpromptinginlargelanguagemodels. InTheEleventhInternationalConferenceonLearning Representations,2023b. [Zhaoetal.,2006] HaiZhao,Chang-NingHuang,MuLi,andBao-LiangLu. Effectivetagsetselection inChinesewordsegmentationviaconditionalrandomfieldmodeling. InProceedingsofthe20th PacificAsiaConferenceonLanguage,InformationandComputation,pages87–94,2006. [Zhaoetal.,2024] HaoZhao,MaksymAndriushchenko,FrancescoCroce,andNicolasFlammarion. Longismoreforalignment: Asimplebuttough-to-beatbaselineforinstructionfine-tuning. arXiv preprintarXiv:2402.04833,2024. [Zhaoetal.,2023] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou, YingqianMin, BeichenZhang, JunjieZhang, ZicanDong, YifanDu, ChenYang, YushuoChen, Z.Chen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,JianyunNie,and JirongWen. Asurveyoflargelanguagemodels. arXivpreprintarXiv:2303.18223,2023. [Zhengetal.,2019] BaigongZheng,RenjieZheng,MingboMa,andLiangHuang. Simplerandfaster learningofadaptivepoliciesforsimultaneoustranslation. InProceedingsofthe2019Conferenceon EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon NaturalLanguageProcessing(EMNLP-IJCNLP),pages1349–1354,2019. [Zhengetal.,2018] ZaixiangZheng,HaoZhou,ShujianHuang,LiliMou,XinyuDai,JiajunChen, andZhaopengTu. Modelingpastandfutureforneuralmachinetranslation. Transactionsofthe AssociationforComputationalLinguistics,6:145–157,2018. [Zhongetal.,2024] YinminZhong,ShengyuLiu,JundaChen,JianboHu,YiboZhu,XuanzheLiu, XinJin,andHaoZhang. {DistServe}: Disaggregatingprefillanddecodingforgoodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation(OSDI24),pages193–210,2024. [Zhouetal.,2023] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,Xuezhe Ma,AviaEfrat,PingYu,LiliYu,SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,and OmerLevy. Lima: Lessismoreforalignment. arXivpreprintarXiv:2305.11206,2023a. [Zhouetal.,2023] DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang, DaleSchuurmans,ClaireCui,OlivierBousquet,QuocV.Le,andEdH.Chi.Least-to-mostprompting enablescomplexreasoninginlargelanguagemodels. InProceedingsofTheEleventhInternational ConferenceonLearningRepresentations,2023b. [Zhouetal.,2021] HaoyiZhou,ShanghangZhang,JieqiPeng,ShuaiZhang,JianxinLi,HuiXiong, andWancaiZhang. Informer: Beyondefficienttransformerforlongsequencetime-seriesforecasting. InProceedingsoftheAAAIconferenceonartificialintelligence,volume35,pages11106–11115, 2021. [Zhouetal., 2017] LongZhou, WenpengHu, JiajunZhang, andChengqingZong. Neuralsystem combinationformachinetranslation. InProceedingsofthe55thAnnualMeetingoftheAssociation forComputationalLinguistics(Volume2: ShortPapers),pages378–384,2017. [Zhouetal.,2020] WangchunshuZhou,CanwenXu,TaoGe,JulianMcAuley,KeXu,andFuruWei. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information ProcessingSystems,33:18330–18341,2020. 702 BIBLIOGRAPHY [Zhouetal.,2023] YongchaoZhou,AndreiIoanMuresanu,ZiwenHan,KeiranPaster,SilviuPitis, HarrisChan, andJimmyBa. Largelanguagemodelsarehuman-levelpromptengineers. InThe EleventhInternationalConferenceonLearningRepresentations,2023c. [Zhou, 2012] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms. Chapman and Hall/CRC,2012a. [Zhou,2012] Zhi-HuaZhou. Ensemblemethods: foundationsandalgorithms. CRCpress,2012b. [ZophandLe,2016] BarretZophandQuocLe. Neuralarchitecturesearchwithreinforcementlearning. InProceedingsofInternationalConferenceonLearningRepresentations,2016. [Zophetal.,2020] BarretZoph,GolnazGhiasi,Tsung-YiLin,YinCui,HanxiaoLiu,EkinDogus Cubuk,andQuocLe. Rethinkingpre-trainingandself-training. Advancesinneuralinformation processingsystems,33:3833–3845,2020. Index k-NN,445 AGI,361 k-NNLM,447 AIC,53 k-NNlanguagemodeling,447 ALiBi,456 k-nearestneighbors,42,445 alignment,415 l regularization,100 alignmentlink,66 2 l norm,21 alignmentscores,220 p n-gramlanguagemodeling,80 alternativehypothesis,58 p-norm,21 ARprocesses,180 p-normdistance,22 artificialgeneralintelligence,361 p-value,58 Artificialneuralnetworks,71 (Higher-order)Runge-KuttaMethods,306 Associativity,20 0-1Loss,44 attacks,107 0-1Masking,286 attentionfield,313 attentionhead,229 long-termmemory,182 attentionweight,219 attentionwithlinearbiases,456 A*search,262 auto-encoders,108 A2C,560 auto-encoding,46,62 absolutepositionalencoding,197,273 auto-regressive,61 action-valuefunction,555 automatedmachinelearning,336,515 AdaDelta,93 automaticpromptdesign,515 AdaGrad,92 AutoML,336,515 Adam,93 autonomousagents,513 AdaptiveGradientDescent,92 autoregressiveprocesses,180 AdaptiveMomentEstimation,93 averagingpooling,87 add-αsmoothing,36 AdditiveAttention,221 backtranslation,263 additivesmoothing,36 back-propagationthroughtime,177 addressing,237 back-translation,107 advantage,559 backgroundtask,153 advantageactor-critic,560 backwardpass,78 adversarialmachinelearning,107 BART,385 adversarialsamples,107 batch,95 affinetransformation,72 batchgradientdescent,95 Agent,417 batching,95 704 INDEX Beamsearch,247 coding,182 beamsize,247 columnvector,19 beamwidth,247 combinatorialoptimizationproblems,254 BernoullinaiveBayes,33 Commutativity,20 BERT,365 completion,370 Best-of-N sampling,583 componentmodels,49 BGD,95 componentsystems,260 bi-directionalmodels,180 compositionalgeneralization,499 BIC,53 compositionalmodels,209 bilineartransform,333 Compositionality,140 binaryclassification,27 Computationgraphs,75 binaryvariable,22 conditionalcomputation,337 Boltzmanndistribution,104 ConditionalProbability,23 BoNsampling,583 conditionalrandomfields,130,204 BPE,131 connectionisttemporalclassification,202 BPTT,177 ConstantInitialization,96 Bradley-Terrymodel,562 constituentsystems,260 branches,288 constrainedoptimization,258 BytePairEncoding,131 constrainedsearch,266 contextwindow,143 CAE,112 Contextuality,141 calculationannotation,488 Continuousbatching,611 canonicalization,125 continuousmemory,321 capacity,182 continuousoptimization,256 catastrophicforgetting,401 contractionmapping,113 categories,26 contractiveauto-encoder,112 causallanguagemodeling,374 contrastivelearning,45 chainofthought,489 ContrastiveLoss,45 chainrule,23 convolution,189 chainruleofdifferentiation,77 convolutionkernels,85 chain-of-thoughtprompting,422 convolutionoperation,188 checkpointensembling,261 convolutionproduct,189 chunking,61 convolutionallayer,85 Classification,26 Convolutionalneuralnetworks,85 classificationmodel,27 correlationcoefficients,144 classificationsystem,26",
    "binaryclassification,27 Computationgraphs,75 binaryvariable,22 conditionalcomputation,337 Boltzmanndistribution,104 ConditionalProbability,23 BoNsampling,583 conditionalrandomfields,130,204 BPE,131 connectionisttemporalclassification,202 BPTT,177 ConstantInitialization,96 Bradley-Terrymodel,562 constituentsystems,260 branches,288 constrainedoptimization,258 BytePairEncoding,131 constrainedsearch,266 contextwindow,143 CAE,112 Contextuality,141 calculationannotation,488 Continuousbatching,611 canonicalization,125 continuousmemory,321 capacity,182 continuousoptimization,256 catastrophicforgetting,401 contractionmapping,113 categories,26 contractiveauto-encoder,112 causallanguagemodeling,374 contrastivelearning,45 chainofthought,489 ContrastiveLoss,45 chainrule,23 convolution,189 chainruleofdifferentiation,77 convolutionkernels,85 chain-of-thoughtprompting,422 convolutionoperation,188 checkpointensembling,261 convolutionproduct,189 chunking,61 convolutionallayer,85 Classification,26 Convolutionalneuralnetworks,85 classificationmodel,27 correlationcoefficients,144 classificationsystem,26 corruptedinput,114 classifier,26 corruption,114 Cloze,294 cosine,196 CNN,85 CosineAttention,220 co-adaptation,101 costfunctions,43 co-attention,237 CoT,489 code,109 COTprompting,422 INDEX 705 coverage,240 distantreward,38 coveragevector,241 distributedrepresentation,139 CRFs,130,204 distributedrepresentations,80 cross-entropy,25 distribution,24 cross-linguallanguagemodels,395 distributionalhypothesis,142 cross-validation,56 distributionalrepresentation,142 CTC,202 distributionalsemantics,142,169 cumulativereward,556 distributionalwordrepresentation,142 curvefitting,43 Distributivity,20 Divergence-basedLoss,43 DAEs,113 DocumentRotation,385 dataaugmentation,107,263,342 dotproduct,20 decayfactor,93 Dot-productAttention,220 decisionboundary,31 DPO,575 decisionsurface,31 dropout,101 decoder,109 dropoutrate,101 decoder-onlyarchitecture,273 DTG,504 Decoding,212,593 duration,182 decoding,203 DynamicNeuralNetworks,337 decodingblocks,272 decodinglayers,272 earlyexitclassifier,340 decodingsystem,214 earlystop,48 deduction,39 earlystopping,94,337 deeplearning,71 early-stop,249 deepneuralnetwork,41 edgeprobing,293 deepneuralnetworks,71 effectivenumberofparameters,53 deliberate-then-generate,504 embeddingmatrix,82 deltarule,91 emergentabilities,292,433 demonstrations,371 emissionprobability,204 denoising,113 Emission-likeFeatures,205 Denoisingauto-encoders,113 encoder,109,171 dependentvariable,49 encoder-decoder,63 depth,74 encoder-decoderarchitecture,212 depthgrowth,303 encoder-decoderattention,225 depth-firstsearch,255 encoder-onlyarchitecture,273 diagonalstate-spacemodels,335 encodingblocks,269 diagonalization,335 encodinglayers,269 dilatedcontextwindow,315 encodingsystem,214 directpreferenceoptimization,575 end-to-endmemorynetworks,183 discriminantfunction,30 ensemblelearning,55 discriminativemodels,32,33 ensembling,260 Distance-basedLoss,43 errorgradient,177 706 INDEX Error-basedLoss,46 gate,87 error-drivenlearning,34 gatedlinearunit,428 error-propagation,78 gatedrecurrentunits,185 Euclideannorm,22 gaussianerrorlinearunit,428 Eulermethod,306 Gaussianmixturemodels,209 eventextraction,67 GaussiannaiveBayes,33 evidencelowerbound(ELBO),117 Gaussiannoise,106 expectation,24 GeLU,428 expectedembedding,257 generalization,47 expectedrepresentation,221 generalizationerror,50 expectedvalue,24 generativemodels,32 exploding and vanishing gradient problems, Globalvectors,157 177 GloVe,157 externalmemories,183,445 GLU,429 Extrapolation,453 GMMs,209 ExtrinsicEvaluation,163 GPT,365 GQA,451 factoranalysis,153 gradientdescent,91 fastText,162 GradientDescentwithMomentum,92 feature,28 graphicalmodels,203 featurelearning,108 greedystrategy,246 featuremap,189 Groupedqueryattention,451 Featuremapping,40 GRUs,185 Featureselection,152 hardprompts,519 featuresub-spaces,226 head,229 featurevector,72 heads,275 feed-forwardneuralnetworkbasedlanguage held-outdata,48 model,80 hiddenlayer,82 feed-forwardneuralnetworks,74 hiddenMarkovmodel,130 few-shotCOTprompting,423 hiddenMarkovmodels,203 few-shotlearning,57 hingeloss,45 FFNNLM,80 histogrampruning,248 FFNNs,74 HMM,130 filters,85 HMMs,203 Fisher’slineardiscriminant,152 homonymy,141 FixedLearningRates,97 humanpreferencealignment,533 forgetgate,184 hypothesisselection,261 forwardpass,76 fractionalcount,136 ICA,153 frames,200 ICL,422 Frobeniusnorm,113 ICT,371 fullyconnected,73 identitymatrix,18 INDEX 707 IDF,146 labelsmoothing,105 imagecaptioning,213 labeledsamples,26 image-to-textgeneration,213 labels,26 importancesampling,564 languagemodeling,61 impulsenoise,114 large-margintraining,44 in-contextlearning,371,422,467 Lassoregularization,100 independentcomponentanalysis,153 latentDirichletallocation,152 independentvariable,49 latentsemanticanalysis,147 indicatorfunction,34 latentsemanticindexing,147 induction,39 layer,72 inductivebias,39 layerdropout,305 inductiveinference,38 Layer-sensitiveInitialization,96 inductivereasoning,38 LDA,152 InferenceEngine,611 LearningfromHumanFeedback,415 inference-timescaling,621 learningrate,91 informationextraction,67 LearningRateDecay,97 informationgain,134 learningratescheduling,97 InitializationwithPredefinedDistributions,96 least-to-mostprompting,495 inputgate,184 LeCuninitialization,96 inputinversion,546 left-singularvectors,148 instructionalignment,533 lemma,125 instructionfine-tuning,411,536 lemmatization,125 interactivemachinetranslation,252 lengthnormalization,240 interference,396 lengthreward,239 internalmemories,445 lexicalsemantics,140 Interpolation,453 linearattentionmodel,326 IntrinsicEvaluation,163 Linearclassifier,29 inversedocumentfrequency,146 lineardiscriminantanalysis,152 irreducibleerror,435 lineardiscriminantfunction,30 iteration-basedscheduling,611 LinearMulti-stepMethods,306 lineartransformation,72 Jacobianmatrix,112 linear-chainCRF,205 JointProbability,23 linearizedtrees,210 linearlyseparable,40 kernelfusion,347 linguisticregularity,165 kernelmethods,40,324 Lipschitzconstant,302 key-valuecache,438,591 localattention,234 keywordextraction,67 localtruncationerror,306 Knowledgedistillation,341 log-linear,33 Kullback-Leibler(KL)divergence,25 logisticregression,33 KVcache,438,591 Longshort-termmemory,184 labelmapping,479 long-contextLLMs,436 708 INDEX lossfunctions,43 mixture-of-experts,309 low-rankapproximation,149 MLE,32 LSA,147 MML,53 LSI,147 mode,264 LSTM,184 modelaveraging,55 modelcapacity,52 MAE,44 modelcomplexity,52 MAP,263 modeldepth,74 margin,44 modelerrors,50 Margin-basedLoss,44 Modelevaluation,50 MarginalProbability,23 modelfunction,336 Markovassumption,130 modelgrowth,303 maskedlanguagemodel,294 Modelselection,50 maskedlanguagemodeling,366,374 modelwidth,74 maskingnoise,114 MoE,309 matrix,18 momentum,92 matrixaddition,19 monotonicity,254 Matrixproduct,21 morphologicalanalysis,124 matrix-matrixproduct,21 movingaverage,332 maxpooling,86 MQA,451 maximumaposteriori,263 MSE,44 maximumlikelihoodestimation,32 multi-branchneuralnetworks,229 maximumnorm,22 Multi-classclassification,27 mBERT,394 Multi-headattention,226 MBR,264 multi-headattention,275 MDL,53 Multi-labelclassification,28 mean,24 multi-layerattention,232 meanabsoluteerror,44 multi-layerneuralnetwork,73 meansquareerror,44 multi-lingualBERT,394 memory,182 multi-queryattention,328,451 memorycell,184 multinomialnaiveBayes,33 memory-basedmethods,445 multiplelinearregressionmodel,180 MERT,47 multiplicativeattention,220 metriclearning,65 multivariateregression,43 mini-batchgradientdescent,95 minibatch,282 namedentityrecognition,61 minimumBayesrisk,264 NAS,336,515 minimumdescriptionlength,53 NER,61 minimumerror-ratetraining,47 NeuralArchitectureSearch,336 minimum-spanningtree,292 neuralarchitecturesearch,515 mining,49 neurallanguagemodels,80 mixturemodel,55 neuralmachinetranslation,215 INDEX 709 neuralnets,71 parametricmethods,41 neuralnetworks,71 parametrictest,58 neuralTuringmachines,183 parsetree,63 neurons,71 part-of-speechtagging,60 nextsentenceprediction,377 patches,354 NMT,215 paths,288 non-autoregressivedecoding,266 PCA,108,149 non-autoregressivegeneration,266 perceptrons,71 Non-linearactivationfunctions,41 PerformanceEstimation,516 non-parametric,58 performancefunction,557 Non-parametricmethods,41 performancegaprecovered,551 norm,21 permutedlanguagemodeling,375 normalization,88,125 PGR,551 NSP,377 Plackett-Lucemodel,569 nucleussampling,601 PMI,144 nullhypothesis,58 pointwisemutualinformation,134,144 polysemy,141 objectivefunction,42 poolinglayer,85 Occam’sRazor,42 POStagging,60 ODEs,305 positionalencoding,194 offlinereinforcementlearning,578 post-norm,270 one-hot,138 post-trainingquantization,346 one-hotrepresentations,80 PPO,419,566 one-shotCOTprompting,423 pre-norm,277 onlinesequence-to-sequencesystem,252 pre-training,38 OOV,35 Prefilling,593 open-vocabulary,35 prefixfine-tuning,523 optimization,42 prefixlanguagemodeling,381 ordinarydifferentialequations,305 primeword,167 ORMs,629 priming,166 orthogonalvectors,148 principalcomponent,151 out-of-vocabulary,35 principalcomponentanalysis,149 outcomerewardmodels,629 principalcomponentcoefficients,149 Outcome-basedApproaches,580 principalcomponentloadings,149 outputequation,332 principalcomponentsanalysis,108 over-translation,240 PRM,629 overfitting,39 Probability,22 overoptimizationproblem,573 probabilitydensity,23 padding,85 probabilitydistribution,24 pairwisemethod,45 probabilityfunction,22 parallelscaling,627 probabilitymeasure,22 parametersharing,85 probes,291 710 INDEX probingclassifier,292 reorderingproblem,234 probingpredictor,292 reparameterizationtrick,117 problemdecomposition,492 request-levelscheduling,611 processrewardmodel,629 resetgate,185 Process-basedApproaches,580 residualconnections,89,179 productrule,23 Residualneuralnetworks,89 progressivedownsampling,344 retrieval-augmentedapproach,322 promptembeddings,528 retrieval-augmentedgeneration,448 promptengineering,467 return,556 promptoptimization,515 reversibleresidualnetworks,328 PromptSearchSpace,515 rewardgaming,573",
    "ORMs,629 priming,166 orthogonalvectors,148 principalcomponent,151 out-of-vocabulary,35 principalcomponentanalysis,149 outcomerewardmodels,629 principalcomponentcoefficients,149 Outcome-basedApproaches,580 principalcomponentloadings,149 outputequation,332 principalcomponentsanalysis,108 over-translation,240 PRM,629 overfitting,39 Probability,22 overoptimizationproblem,573 probabilitydensity,23 padding,85 probabilitydistribution,24 pairwisemethod,45 probabilityfunction,22 parallelscaling,627 probabilitymeasure,22 parametersharing,85 probes,291 710 INDEX probingclassifier,292 reorderingproblem,234 probingpredictor,292 reparameterizationtrick,117 problemdecomposition,492 request-levelscheduling,611 processrewardmodel,629 resetgate,185 Process-basedApproaches,580 residualconnections,89,179 productrule,23 Residualneuralnetworks,89 progressivedownsampling,344 retrieval-augmentedapproach,322 promptembeddings,528 retrieval-augmentedgeneration,448 promptengineering,467 return,556 promptoptimization,515 reversibleresidualnetworks,328 PromptSearchSpace,515 rewardgaming,573 promptingengineering,420 rewardhacking,573 proximalpolicyoptimization,419,566 RewardModel,417 Ridgeregularization,100 Q-valuefunction,555 right-singularvectors,148 QKVattention,223 risk,46 query-key-valueattention,223 RLHF,416,534 RAG,448 RMSProp,93 randomprocesses,173 RNNs,83 randomvariable,22 RoBERTa,393 Ranking,49 robuststatistics,105 Ranking-basedLoss,45 routingmodel,309 ratiofunction,564 rowvector,19 receiver,221 RPR,297 receptivefield,85,187 reconstructionloss,109 salt-and-peppernoise,114 rectifiedlinearunit,428 sampleefficient,57,547 recurrentcell,83,184 samples,26 Recurrentneuralnetworks,83 saturatingactivationfunctions,178 recurrentunit,83 Scalar,18 regression,43 scalarproduct,19 regularexpressions,125 scaleddot-productattention,220 reinforcementlearning,38 scalinglaws,345,433 reinforcementlearningfromhumanfeedback, Scheduler,611 416,534 searcherrors,50,248 rejectionsampling,584 searchproblem,50 relationextraction,67,482 self-attention,226 relativeentropy,25 self-consistency,507 relativepositionalencoding,197 self-instruct,543 relativepositionalrepresentation,297 self-pacedreading,167 ReLU,428 self-supervisedlearning,38,367 remove-one,136 self-training,367 INDEX 711 semi-orthogonal,148 sourcesequence,212 semi-supervisedlearning,37 source-sidesequence,212 semi-unitary,148 SpanMasking,385 sender,221 spanprediction,67 sensorymemory,182 sparseattentionmodels,296 sentenceembedding,210 sparseauto-encoders,111 sentencelengthprediction,294 sparsecoding,111 SentenceReordering,385 sparseexpertmodels,338 sentence-leveldepth-adaptivemodel,338 sparsitypenalty,111 seq2seq,211 sparsityratio,313 SequenceEncodingModels,368 Speculativedecoding,604 SequenceGenerationModels,368 speculativeexecution,604 Sequencelabeling,60 speechencoder,352 sequencelabeling,129 speed-accuracytrade-off,250 sequence-to-sequence,62,211 SSMs,332 sequentialscaling,627 standarddeviation,24 SFT,415,534 standardization,89 SGD,94 stateequation,332 shallow-to-deeptraining,303 statevariables,332 sharedencoder,352 state-spacemodels,332 short-termmemory,182 state-valuefunction,555 shortcutconnections,89 Statisticallanguagemodeling,61 shrinkageestimator,104 statisticalmachinetranslation,240 significancelevel,58 statisticalparsing,63 significancetests,58 steepestdescent,91 similarityfunction,65 stem,126 similaritylearning,65 stemming,126 simplex,257 stepfunction,72 simultaneoustranslation,252 stochasticgradientdescent,94 sine,195 stochasticprocesses,173 single-labelclassification,28 Stoppingcriterion,94 single-layerattention,232 StrongCeilingPerformance,550 single-layerneuralnetwork,73 Structureprediction,49 single-layerperceptrons,71 structureprediction,27 single-roundprediction,538 structuredpruning,342 singularvaluedecomposition,147 sub-layerdropout,305 singularvalues,148 Sub-problemGeneration,495 skipconnections,89,179 Sub-problemSolving,495 SMT,240 sub-spacemethod,260 SoftMasking,287 subword,131 softprompts,519 suffixstripping,126 softwordalignmentmatrix,66 superficialalignmenthypothesis,547 712 INDEX Superviseddimensionreduction,152 TF,145 SupervisedFine-tuning,415 TF-IDF,146 supervisedfine-tuning,534 theAkaikeinformationcriterion,53 Supervisedlearning,37 thebag-of-words(BOW)model,28 supervisedlearning,367 theBayesianapproach,46 supportvectormachines,41 theBayesianinformationcriterion,53 SurfaceFormsofWordsandSentences,294 theBayesianrisk,46 surrogateobjective,565 theCBOWmodel,155 SVD,147 Thecontinuousbag-of-wordsmodel,155 SyntacticandSemanticLabels,293 Thecontinuousskip-grammodel,156 syntactichierarchy,290 thecurseofdimensionality,39 syntacticparser,63 TheExpectationStep,136 syntaxtree,63 theExpectation-Maximization(EM)algorithm, syntax-awareTransformerencoders,284 135 systemcombinationmethods,260 thekerneltrick,41 systematicerror,54 Thelearningproblem,27 TheMaximizationStep,136 t-distributed stochastic neighbor embedding, theminimummessagelength,53 168 Themodelingproblem,27 t-SNE,168 themulti-storemodel,182 T5,380 Thenofreelunchtheorem,52 tags,26 Thepredictionproblem,27 targetsequence,212 theprincipleofcompositionality,140 targetword,167 theskip-grammodel,156 target-sidesequence,212 theVapnik-Chervonenkisdimension,52 TD,560 theVCdimension,52 teacherforcing,239 thresholdpruning,248 teacher-studenttraining,341 timeseries,173 templatefilling,67 TokenDeletion,385 temporaldifference,560 TokenMasking,385 termfrequency,145 tokenpruning,344 termfrequency-inversedocumentfrequency, token-leveldepth-adaptivemodel,338 146 Tokenization,123 term-documentco-occurrencematrix,145 tokens,123 term-termco-occurrencematrix,143 Topicmodels,152 testerror,50 trainingepochs,94 textcompletion,483 trainingerror,50 textembedding,210 trainingstep,91 textencoder,352 transcription,200 textgeneration,212 transcriptionlabels,200 texttransformation,483 transcriptionunits,200 text-to-imagegeneration,213 transferlearning,341 INDEX 713 Transformer,269 VQA,237,355 Transformer-XL,319 WarmupandDecay,97 transitionprobability,204 WeakPerformance,550 Transition-likeFeatures,205 weak-to-stronggeneralization,550 translationlanguagemodeling,395 Weak-to-strongPerformance,550 transpose,19 weightdecay,100 treelinearization,284 weightsharing,85 Trees,292 WeightedDot-productAttention,220 trustregions,565 width,308 Tustin’smethod,333 Wordalignment,234 under-translation,240 wordalignment,66 undercompleteauto-encoder,110 wordalignmentweightmatrix,66 underfitting,47 wordclustering,37 undirectedgraphicalmodels,205 worddistance,164 unfolded,83 wordembedding,82,139 uni-directionalmodels,180 WordRepresentationLearning,123 unigram,134 wordsegmentation,124 unrolled,83 wordsemanticdistance,164 unseenwords,35 wordsense,137 unstructuredpruning,342 wordsensedisambiguation,142 unsupervised bilingual dictionary induction, word-documentco-occurrencematrix,145 37 word-wordco-occurrencematrix,143 unsupervisedlearning,37,367 Word2Vec,155 updategate,185 WSD,142 updaterule,91 Xavierinitialization,96 updatestep,91 XLMs,395 VAEs,115 zeromatrix,18 validationdata,48 zeropadding,282 valuefunction,262 zero-shotCOT,423 value-basedsearch,262 zero-shotlearning,414 variable,22 Zipf’sLaw,36 variance,24 variationalauto-encoders,115 Vector,18 vectordatabase,321 vectorfunction,30 VisionTransformer,353 visualquestionanswering,237,355 ViT,353 Viterbidecoding,203 vLBL,162"
  ],
  "metadata": [
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "20250423-EB-Event-Driven_Design_for_Agents_copy.pdf"
    },
    {
      "source": "sample_knowledge.txt"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    }
  ]
}