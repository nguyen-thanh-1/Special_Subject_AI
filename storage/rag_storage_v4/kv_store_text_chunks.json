{
  "chunk-e78da99a2d589a658ec77f66e1eccc2e": {
    "tokens": 1200,
    "content": "A Guide to Event-Driven\nDesign for Agents and\nMulti-Agent Systems\nBy Sean Falconer, AI Entrepreneur in Residence, Confluent\n© 2025 Confluent, Inc.\n\nContents\n3 Introduction\n4 Part I: Understanding the Evolution of AI\nThe First Wave: Predictive AI\nThe Second Wave: Generative AI\nCompound AI Bridges the Gap\nThe Third Wave: Agentic AI\nThe Power of Singular Agent Systems\n9 Part II: The Case for Event-Driven Agentic Systems\nThe Anatomy of an Agent\nPersona (Job Function)\nPerception (Sensing)\nReasoning and Decision-Making\nMemory\nPlanning\nAction\nLearning\nCoordination and Collaboration\nTool Interface\nWhy Event-Driven Matters for Agents\nFrom Singular to Multi-Agent Systems\n13 Part III: Design Patterns for Multi-Agent Systems\nMulti-Agent Design Patterns\nOrchestrator-Worker Pattern\nTraditional Approach\nEvent-Driven Approach\nHierarchical Agent Pattern\nTraditional Approach\nEvent-Driven Approach\nBlackboard Pattern\nTraditional Approach\nEvent-Driven Approach\nMarket-Based Pattern\nTraditional Approach\nEvent-Driven Approach\nThe Role of the Data Streaming Platform in Multi-Agent Systems\n23 Part IV: Building Event-Driven Systems for Agents with a Data Streaming Platform\nArchitecting Singular and Multi-Agent Systems\nChallenges and Solutions in Event-Driven Agent Design\n[Stream] Next-Level Data Streaming with a Fully Managed, Cloud-Native Service\n[Connect] Seamless Integration: Connecting Agents with Diverse Systems\n[Process] Ensuring Data Freshness: Handling Dynamic Data Streams\n[Govern] Data Quality, Security, and Compliance\nReal-World Applications of Event-Driven Agentic Systems\n1. Automating Web Scraping with AI Agents\n2. Intelligent Business Copilots\n3. Workflow Automation with a Drag-and-Drop Agent Builder\nBroader Industry Impact\nWhy a Data Streaming Platform Is Essential for AI Agents\n28 The Future of AI is Event-Driven\n29 Take the Next Step\n© 2025 Confluent, Inc. 2\n\nIntroduction\nAI has come a long way. We’ve moved from By processing data close to the source, it\npredictive models that analyze patterns to reduces latency and enhances decision-\ngenerative AI that creates new content. Now, making, allowing AI agents to operate with\nwe’re entering the next phase: agentic AI— the freshest, most relevant data, while\nsystems that don’t just generate, but have the maintaining compliance and control across\nagency to act, adapt, and collaborate, in real the organization.\ntime.\nEDA transforms AI from isolated models into\nFor AI agents to be useful, they need a dynamic system of agents that react to\nmore than just intelligence. They need events in real time. By using a data streaming\ninfrastructure. They need security. They need platform, agents can consume, process,\ngovernance. and emit events asynchronously, ensuring\nscalable, flexible, and resilient interactions.\nAn agent must consume data, use tools,\nWhether it’s a single agent handling customer\nmake decisions, and share outputs across\nqueries, or a network of agents optimizing a\nan organization. But rigid, request-driven\nsupply chain, event-driven design ensures\narchitectures can’t keep up. APIs and\nthey stay in sync, without breaking down\nsynchronous workflows create bottlenecks,\nunder complexity.\nlimit scalability, and make coordination\nbetween multiple agents a nightmare. The This ebook is a guide to building event-\nfuture of AI agents isn’t about making models driven AI agents and multi-agent systems.\nbetter, it’s about making AI systems work We’ll cover the evolution of AI, the anatomy\ntogether seamlessly. of an agent, design patterns for scalable\narchitectures, and real-world applications. At\nThis demands an event-driven architecture\nthe core of this approach is a data streaming\n(EDA) powered by a data streaming platform,\nplatform, which enables agents to consume,\nwhich enables agents to react dynamically\nprocess, and act on real-time data, while\nto changes, share state efficiently, and\nmaintaining governance and scalability. The\nscale without the constraints of traditional\ngoal is simple: to demonstrate why event-\nrequest-response patterns. A data streaming\ndriven design—powered by a data streaming\nplatform not only facilitates real-time event\nplatform—is the backbone of AI’s next\nprocessing, but also ensures governance,\nevolution, and how to architect systems that\nenforces data quality, and connects\nare not only effective today, but are built for\ndisparate data sources into a unified stream.\nlong-term scalability and adaptability.\n© 2025 Confluent, Inc. 3\n\nPart I: Understanding the Evolution of AI\nAI has evolved through three distinct phases, each unlocking new capabilities, while introducing\nits own limitations.\nThe First Wave: Predictive AI\nThe first wave of AI revolved around traditional machine learning, focusing on predictive\ncapabilities for narrowly defined tasks.\nThe Traditional Machine Learning Workflow\nBuilding these models required significant expertise, as they were crafted specifically for\nindividual use cases. They were domain-specific, with their domain specificity embedded in the\ntraining data, making them rigid and tough to repurpose. Adapting a model to a new domain often\nmeant starting from scratch—an approach that lacked scalability and slowed adoption.\nThe Second Wave: Generative AI\nGenerative AI, driven by deep learning, marked a turning point.\nInstead of being confined to single domains, these generative models were trained on vast,\ndiverse datasets, giving them the ability to generalize across a variety of contexts. They\ncould generate text, images, and even videos, opening up exciting new applications. Whereas\npredictive AI relies on traditional batch-based statistical models to solve specific problems,\ngenerative AI uses foundation models like LLMs that are broadly capable",
    "chunk_order_index": 0,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:d97b899613d0454995d6238e5b9976c3",
      "default:extract:8661c8d5f395fbf1bfe951c175ca24c8"
    ],
    "create_time": 1769937806,
    "update_time": 1769938061,
    "_id": "chunk-e78da99a2d589a658ec77f66e1eccc2e"
  },
  "chunk-f23ac140cb981aae9be85d4061cdc74f": {
    "tokens": 1200,
    "content": "ative AI\nGenerative AI, driven by deep learning, marked a turning point.\nInstead of being confined to single domains, these generative models were trained on vast,\ndiverse datasets, giving them the ability to generalize across a variety of contexts. They\ncould generate text, images, and even videos, opening up exciting new applications. Whereas\npredictive AI relies on traditional batch-based statistical models to solve specific problems,\ngenerative AI uses foundation models like LLMs that are broadly capable and reusable:\n© 2025 Confluent, Inc. 4\n\nThe Generative AI Workflow\nHowever, this wave came with its own challenges.\nGenerative models are fixed in time—unable to incorporate new or dynamic information—\nand are difficult to adapt. Fine-tuning can address domain-specific needs, but it’s expensive\nand error-prone. Fine-tuning requires vast data, significant computational resources, and ML\nexpertise, making it impractical for many situations. Additionally, since LLMs are trained on\npublicly available data, they don’t have access to domain-specific information, limiting their\nability to accurately respond to questions that require context.\nFor example, suppose you ask a generative model to recommend an insurance policy tailored to a\nuser’s personal health history, location, and financial goals.\nSimple Prompt and Response with an LLM\nIn this scenario, you prompt the LLM and it generates a response. Clearly the model can’t deliver\naccurate recommendations, because it lacks access to the relevant user data. Without it, the\nresponse will either be generic or flat-out wrong.\n© 2025 Confluent, Inc. 5\n\nCompound AI Bridges the Gap\nTo overcome these limitations, Compound AI systems integrate generative models with other\ncomponents like programmatic logic, data retrieval mechanisms, and validation layers. This\nmodular design allows AI to combine tools, fetch relevant data, and tailor outputs in a way that\nstatic models cannot.\nFor instance, in the insurance recommendation example:\n• A retrieval mechanism pulls the user’s health and financial data from a secure database.\n• This data is added to the context provided to the LLM during prompt assembly.\n• The LLM uses the assembled prompt to generate an accurate response.\nSimple RAG Architecture\nThis process, known as Retrieval-Augmented Generation (RAG), bridges the gap between static\nAI and real-world needs by dynamically incorporating relevant data into the model’s workflow.\nWhile RAG effectively handles tasks like this, it relies on fixed workflows, meaning every\ninteraction and execution path must be predefined. This rigidity makes it impractical to handle\nmore complex or dynamic tasks, where workflows cannot be exhaustively encoded. Encoding all\npossible execution paths manually is labor-intensive and ultimately limiting.\nThe limitations of fixed-flow architectures have led to the rise of the third wave of AI: agentic\nsystems.\nThe Third Wave: Agentic AI\nAs HubSpot CTO Dharmesh Shah put it, “Agents are the new apps.” Salesforce CEO Marc Benioff\nechoed this sentiment on The Wall Street Journal’s “Future of Everything” podcast, emphasizing\nthat the future of AI lies with autonomous agents—systems that can think, adapt, and act\nindependently.\nAgents bring something fundamentally new: dynamic, context-driven workflows. Unlike traditional\nAI models that follow predefined paths, agentic systems determine the best course of action on\nthe fly, adapting in real time to the challenges they face. This makes them particularly well-suited\nfor solving complex, interconnected problems, in enterprise environments.\n© 2025 Confluent, Inc. 6\n\nControl Logic: Programmatic vs. Agentic\nAgents flip traditional control logic on its head.\nInstead of rigid programs dictating every move, agents use LLMs to drive decisions. They can\nreason, use tools, and access memory—all dynamically. This flexibility allows for workflows that\nevolve in real time, making agents far more powerful than anything built on fixed logic.\nAgent Architecture (inspired by https://arxiv.org/pdf/2304.03442)\n© 2025 Confluent, Inc. 7\n\nThe Power of Singular Agent Systems\nA single agent can be powerful when designed correctly. Effective AI agents share key\ncharacteristics:\n• Autonomy: They operate without constant human intervention.\n• Adaptability: They adjust to new data and changing conditions.\n• Decision-Making: They evaluate multiple options and select the best course of action.\nHowever, traditional architectures make deploying such agents challenging.\nAI systems struggle with data freshness, integration complexity, security, governance, and real-\ntime responsiveness. Many still rely on batch-based processing, leading to decisions made based\non stale data, while fragmented data landscapes make it difficult to establish contextualized and\ntrustworthy data. These challenges create a data mess, where agents lack the reliable, real-\ntime inputs needed to make effective decisions. This highlights the need for an event-driven\nfoundation powered by a data streaming platform, where agents can securely react to live\nevents, enforce governance, and integrate data from disparate sources in real time—eliminating\nbottlenecks, and enabling AI systems to operate with speed, accuracy, and compliance.\nBefore we explore why event-driven design is critical, we need to break down the fundamental\ncomponents of an agent. Understanding the anatomy of an agent provides the foundation for\narchitecting scalable, interoperable AI systems.\n© 2025 Confluent, Inc. 8\n\nPart II: The Case for Event-Driven\nAgentic Systems\nIn artificial intelligence, agents have a long history, from early theoretical considerations by Alan\nTuring and John McCarthy, to rule-based reasoning agents in the 1960s. These agents were\ndesigned to act autonomously within a defined context, but their capabilities were limited by\nnarrow applications and rigid logic.",
    "chunk_order_index": 1,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:3c060a8448956117e9513c3344a26fd1",
      "default:extract:4755f249ce9c763d2d0b2b306d210388"
    ],
    "create_time": 1769937806,
    "update_time": 1769938061,
    "_id": "chunk-f23ac140cb981aae9be85d4061cdc74f"
  },
  "chunk-d6c229e370b8a7d2f437965f778c3850": {
    "tokens": 1200,
    "content": "ing scalable, interoperable AI systems.\n© 2025 Confluent, Inc. 8\n\nPart II: The Case for Event-Driven\nAgentic Systems\nIn artificial intelligence, agents have a long history, from early theoretical considerations by Alan\nTuring and John McCarthy, to rule-based reasoning agents in the 1960s. These agents were\ndesigned to act autonomously within a defined context, but their capabilities were limited by\nnarrow applications and rigid logic.\nToday, the emergence of foundation models has transformed what’s possible.\nThe Anatomy of an Agent\nJust like humans, agents solve problems by combining their senses, memory, reasoning, and\nability to act. But before diving into these mechanics, there’s one foundational element that\nunderpins everything: their persona.\nThe Anatomy of an Agent\n© 2025 Confluent, Inc. 9\n\nPersona (Job Function)\nThe persona of an agent defines its job function and expertise. It’s like a detailed job description\nembedded into the system prompt, shaping the agent’s behavior and responses. The system\nprompt sets expectations, and influences the model’s probability distribution over tokens, to align\noutputs with the defined role.\nPerception (Sensing)\nWith a clear role, the first step to solving any problem is understanding the environment. For\nagents, perception is their sensory input—how they gather data from the world around them.\nHumans use eyes, ears, and touch; agents use APIs, sensors, and user inputs.\nReasoning and Decision-Making\nOnce information is gathered, it needs to be processed and understood. Reasoning is the agent’s\nability to analyze data, derive insights, and decide what to do next. For humans, this happens in\nthe brain. For agents, it’s powered by models like LLMs, which dynamically adapt to inputs and\ncontexts.\nMemory\nMemory allows agents to retain domain-specific information across interactions. It’s not about\nlearning, which is a separate part of the anatomy. Humans rely on both short-term memory (like\nrecalling the start of a conversation) and long-term memory (like remembering a skill learned\nyears ago). Agents work the same way.\nShort-term memory allows the agent to keep track of the immediate context within a conversation,\nwhich might be stored temporarily in memory buffers during the session. Meanwhile, long-term\nmemory involves storing historical data, such as user preferences or past interactions. This could\nbe a vector database like MongoDB, Elasticsearch, Pinecone, or another permanent storage. A\nvector database enables semantic search, where embeddings allow the agent to retrieve relevant\ninformation efficiently.\nPlanning\nOnce the agent knows what needs to be done, it devises a plan to achieve its goal. This step\nmirrors how humans strategize: breaking a problem into smaller steps and prioritizing actions.\nAction\nPlanning is worthless without execution. Action is where agents interact with the world, whether\nby sending a message, controlling a device, or updating a database.\nThe agent’s execution handlers are responsible for ensuring these actions are performed\naccurately, and for validating the outcomes.\nLearning\nHumans improve by learning from mistakes and adapting to new information. Agents do the same,\nusing machine learning to refine their reasoning, improve predictions, and optimize actions.\nThis process may involve adjusting the agent’s context dynamically during prompt assembly,\nallowing it to refine its responses based on situational feedback, without making permanent\nchanges to the model’s weights. Alternatively, learning can also occur through reinforcement\n© 2025 Confluent, Inc. 10\n\nlearning, where decision-making is optimized using rewards or penalties tied to specific actions.\nIn many cases, adapting context provides a flexible and efficient way for agents to improve,\nwithout the overhead of fine-tuning.\nCoordination and Collaboration\nHumans rarely work alone—we collaborate, share knowledge, and divide tasks. In multi-agent\nsystems, coordination enables agents to do the same, working together to achieve shared goals.\nTool Interface\nHumans use tools to amplify their capabilities—doctors use stethoscopes, and programmers use\nintegrated development environments (IDEs). Agents are no different. The tool interface is their\nbridge to specialized capabilities, allowing them to extend their reach, and operate effectively in\nthe real world.\nThese interfaces often rely on modular API handlers or plugin architectures, allowing the agent\nto extend its functionality dynamically and efficiently. Agents sense their environment, process\ninputs, recall relevant information, make decisions, and take action. These steps mirror human\ncognition and problem-solving, but operate within digital ecosystems.\nWhy Event-Driven Matters for Agents\nAgents, at their core, function much like microservices—as modular, independent units that\nexecute specific tasks. However, unlike traditional microservices, agents don’t just process\nrequests; they reason, plan, and take actions based on stateful information. Without proper\ncoordination, this complexity can quickly spiral out of control.\nImagine deploying hundreds of microservices without guardrails—without standardized\ncommunication, state synchronization, or failure recovery mechanisms. The result would be\nchaos. The same applies to multi-agent systems: without a structured framework, agents become\nfragmented, inefficient, and unreliable.\nMicroservices architecture evolved to solve similar challenges by shifting from tightly coupled,\nrequest/response-based communication to event-driven design.\nEarly monolithic applications struggled to scale, because every component had direct\ndependencies on others. Microservices addressed this by decoupling services, allowing them to\noperate independently. But managing interservice communication through APIs still introduced\nbottlenecks. The breakthrough came with event-driven architectures (EDA), where services could\nreact to changes asynchronously, enabling real-time responsiveness and scalability.\nTightly Coupled Microservices to Event-Driven Microservices\n© 2025 Confluent, Inc. 11\n\nAgents need the same shift",
    "chunk_order_index": 2,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:ac86bf2884c00f400dc8e7260db4f433",
      "default:extract:ee1218cd2a1f50bb0dbc348a4e30b975"
    ],
    "create_time": 1769937806,
    "update_time": 1769938061,
    "_id": "chunk-d6c229e370b8a7d2f437965f778c3850"
  },
  "chunk-d95fc9af155efcfee2a297ca03d7989f": {
    "tokens": 1200,
    "content": ", because every component had direct\ndependencies on others. Microservices addressed this by decoupling services, allowing them to\noperate independently. But managing interservice communication through APIs still introduced\nbottlenecks. The breakthrough came with event-driven architectures (EDA), where services could\nreact to changes asynchronously, enabling real-time responsiveness and scalability.\nTightly Coupled Microservices to Event-Driven Microservices\n© 2025 Confluent, Inc. 11\n\nAgents need the same shift.\nInstead of rigid, API-driven interactions, they should operate in an event-driven ecosystem,\nconsuming and emitting events dynamically. EDA provides the necessary foundation for scalable,\nadaptive agents by ensuring:\n• Asynchronous Processing: Agents can process tasks as events arrive, avoiding bottlenecks\ncaused by synchronous API calls.\n• Scalability: New agents can join the system without disrupting existing workflows, much like\nadding new microservices to an event-driven infrastructure.\n• Loose Coupling: Agents interact via event streams rather than direct dependencies, reducing\nfragility and enabling modular development.\n• Real-Time Responsiveness: Agents react instantly to events, ensuring that decisions are made\nbased on the latest available data.\nBy borrowing from microservices architecture, event-driven agentic systems enable flexibility,\nresilience, and efficiency at scale. Just as microservices rely on message brokers like Apache\nKafka® for asynchronous communication, agents leverage event streaming to collaborate without\nrigid dependencies. A data streaming platform takes this further by not just streaming data, but also\nconnecting disparate data sources, processing events in motion, and enforcing governance.\nWith a data streaming platform, agents operate on real-time, contextualized data, avoiding stale\ninsights from batch processing. It enables dynamic filtering, transformation, and secure data\nsharing, ensuring decisions are made with the freshest, most relevant information. This keeps\nagent ecosystems adaptive, scalable, and ready for real-world challenges—moving beyond static,\nrequest-driven workflows, to truly autonomous AI systems.\nFrom Singular to Multi-Agent Systems\nAs powerful as a single agent can be, its capabilities are inherently limited by its scope.\nNo single agent can handle every possible task with full expertise, just as no individual worker in\na company can effectively perform every job. The real promise of agentic AI lies in multi-agent\nsystems (MAS), where multiple specialized agents work together, coordinating actions, exchanging\ninformation, and dynamically adapting to changes.\nA MAS is a network of agents that collaborate (or sometimes compete) to solve complex problems\nmore effectively than any single agent could on its own. These systems are designed to distribute\nworkloads, balance specialization, and enable decentralized decision-making.\nMulti-agent systems allow:\n• Task Delegation: Agents specialize in specific domains and distribute workloads efficiently.\n• Parallel Processing: Agents execute tasks simultaneously without bottlenecks.\n• Dynamic Role Allocation: Responsibilities shift based on changing requirements.\nTo function effectively, multi-agent systems require seamless coordination and communication.\nTraditional API-based integrations create tight coupling, where agents must know exactly which\nother agents to interact with. This approach does not scale. As the number of agents grows, the\ncomplexity of interactions increases exponentially.\nEvent-driven architecture ensures seamless coordination, enabling agents to operate as part of an\nadaptive, resilient ecosystem. In the next section, we’ll explore the design patterns that make multi-\nagent interactions scalable and efficient.\n© 2025 Confluent, Inc. 12\n\nPart III: Design Patterns for Multi-Agent\nSystems\nEnterprises require networks of agents that collaborate, share context, and execute workflows\ntogether. However, scaling from a single agent to a multi-agent system introduces significant\nchallenges:\n• Context and Data Sharing: Agents must exchange information efficiently without duplication,\nloss, or inconsistency.\n• Scalability and Fault Tolerance: Systems should handle a growing number of agents, while\nensuring recovery from failures.\n• Integration Complexity: Agents interact with diverse tools and data sources, requiring\nseamless interoperability.\n• Timely and Accurate Decisions: Agents need access to fresh, real-time data to make\ninformed decisions without delay.\n• Safety and Validation: Guardrails are necessary to prevent unintended behaviors and ensure\nreliable, high-quality outputs.\n• Security and Governance: Organizations must enforce data security, compliance, and lineage\nto maintain trust and control.\n• Operational Overhead: Managing infrastructure for data pipelines, integration, and compute\nat scale adds significant burden.\n• Lack of Global Availability: AI systems require real-time data access and processing across\nregions to support global operations.\nConfluent Data Streaming Platform is the future-proof foundation for addressing these\nchallenges. It acts as the communication layer and data enabler in the agentic AI stack:\n© 2025 Confluent, Inc. 13\n\nBy combining four key pillars—Stream, Connect, Process, and Govern—Confluent enables agents\nto seamlessly consume, process, and act on data in motion.\n• Streaming ensures real-time event flow, allowing agents to react instantly to changes.\n• Connectors integrate disparate data sources, removing bottlenecks in interoperability.\n• Stream Processing transforms and enriches data in motion, enabling contextualized decision-\nmaking.\n• Stream Governance enforces security, compliance, and data quality, ensuring trust and\nreliability.\nThis section explores key design patterns that enable multi-agent systems to function as scalable,\nresilient, and adaptable networks, leveraging a data streaming platform to unlock real-time AI\ncapabilities without infrastructure headaches.\nMulti-Agent Design Patterns\nMulti-agent design patterns define how autonomous agents communicate, collaborate, or\ncompete to solve problems. These patterns structure interactions between agents, ensuring\nefficient decision-making and workload distribution.\nBelow, we examine four essential patterns: Orchestrator-Worker, Hierarchical Agent,\nBlackboard, and Market-Based, along with how event-driven architectures transform them into\nscalable, loosely coupled systems.",
    "chunk_order_index": 3,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:35965da564890aa1b65e7277c4000011",
      "default:extract:bc40eee583ac9f0f0a241173ff7b27b8"
    ],
    "create_time": 1769937806,
    "update_time": 1769938061,
    "_id": "chunk-d95fc9af155efcfee2a297ca03d7989f"
  },
  "chunk-5c1ccd6e4169f2c696ec77c1ea699f31": {
    "tokens": 1200,
    "content": "leveraging a data streaming platform to unlock real-time AI\ncapabilities without infrastructure headaches.\nMulti-Agent Design Patterns\nMulti-agent design patterns define how autonomous agents communicate, collaborate, or\ncompete to solve problems. These patterns structure interactions between agents, ensuring\nefficient decision-making and workload distribution.\nBelow, we examine four essential patterns: Orchestrator-Worker, Hierarchical Agent,\nBlackboard, and Market-Based, along with how event-driven architectures transform them into\nscalable, loosely coupled systems. A data streaming platform not only enables any multi-agent\nsystem design pattern, but goes far beyond just Apache Kafka®, providing all the tools to connect\nand unlock data from any system, enforce governance for quality and compliance, and process\ndata into high-value streams that fuel real-time, intelligent agent decision-making.\nOrchestrator-Worker Pattern\nIn the Orchestrator-Worker pattern, a central agent assigns tasks to worker agents and manages\nexecution. This is akin to the Master-Worker pattern in distributed computing, where an\norchestrator coordinates multiple independent workers that execute specific jobs.\nOrchestrator-Worker Pattern\n© 2025 Confluent, Inc. 14\n\nTraditional Approach:\n• The orchestrator assigns tasks to worker agents.\n• Workers execute tasks and return results to the orchestrator.\n• If a worker fails, the orchestrator must reassign tasks manually.\nEvent-Driven Approach:\nUsing data streaming, we can adapt this pattern to make the agents event-driven. Confluent\nData Streaming Platform offers key-based Kafka partitioning strategies, so the orchestrator can\nuse keys to distribute command messages across partitions in a single topic. Worker agents can\nthen act as a consumer group, pulling events from one or more assigned partitions to complete\nthe work. Each worker agent then sends output messages into a second topic, where it can be\nconsumed by downstream systems.\nThe pattern now looks like this:\nEvent-Driven Orchestrator-Worker Pattern\n© 2025 Confluent, Inc. 15\n\nWhile this diagram looks more complex, it dramatically simplifies the operations of the system.\nThe orchestrator no longer has to manage its connections to worker agents, including managing\nwhat happens if one dies, or handling more or fewer worker agents. Instead, it uses a keying\nstrategy that distributes work across partitions. For events that should be processed by the\nstateful worker agent as some previous message, the same key can be used for each event in a\nsequence. The worker agents gain the benefits of any consumer group.\nThe worker agents pull from one or more partitions, and the Kafka Consumer Rebalance Protocol\nassures that each worker has similar workloads, even as worker agents are added or removed. In\nthe event of a worker failure, the log can be replayed from a given partition for a saved offset. The\norchestrator no longer needs bespoke logic for managing workers; instead, it simply specifies\nwork and distributes it with a sensible keying strategy. Similarly, the worker agents inherit the\nfunctionality of a Kafka consumer group, so they can use common machinery for coordination,\nscaling, and fault recovery.\nThis pattern allows for dynamic scaling, automatic fault recovery, and efficient workload\ndistribution without the need for complex management logic.\nHierarchical Agent Pattern\nThe Hierarchical Agent pattern organizes agents into layers, where higher-level agents oversee\nor delegate tasks to lower-level agents. This is ideal for breaking down complex problems into\nsmaller, manageable parts.\nHierarchical Multi-Agent Pattern\nTraditional Approach:\n• A central decision-making agent controls multiple subordinate agents.\n• Subordinate agents handle specialized tasks but require direct coordination.\n© 2025 Confluent, Inc. 16\n\nEvent-Driven Approach:\nTo make the hierarchical pattern event-driven, we apply the same techniques for decomposing\nwork in the orchestrator-worker pattern recursively in the agent hierarchy, such that each non-\nleaf node is the orchestrator for its respective subtree.\nEvent-Driven Hierarchical Multi-Agent Pattern\nBy making hierarchical coordination event-driven, agents publish and subscribe to event\nstreams, rather than rely on direct supervision:\n• Higher-level agents publish objectives as events.\n• Mid-tier agents consume events, break down tasks, and issue new events to lower-tier\nagents.\n© 2025 Confluent, Inc. 17\n\n• Execution agents consume low-level tasks, process them, and publish results.\n• Sibling agents form consumer groups to process shared workloads dynamically.\nWith this approach, hierarchy is no longer rigid, agents can be added or removed dynamically\nwithout modifying the system’s core logic. Asynchronous event processing ensures scalability\nwhile maintaining structured delegation.\nBlackboard Pattern\nThe Blackboard Pattern introduces a shared knowledge base—a “blackboard”—where agents\nasynchronously post and retrieve information. This pattern is widely used in complex problem-\nsolving, such as collaborative AI systems and robotics.\nBlackboard Pattern\nTraditional Approach:\n• Agents must explicitly query a database or communicate directly with other agents.\n• Coordination becomes a bottleneck, leading to synchronization challenges.\nEvent-Driven Approach:\n• The blackboard is implemented as a streaming topic in Kafka.\n• Agents publish knowledge updates as events instead of direct database writes.\n• Other agents subscribe to these updates dynamically, consuming only relevant information.\n© 2025 Confluent, Inc. 18\n\nThe event-driven version looks like this:\nEvent-Driven Blackboard Pattern\nThis approach allows real-time collaboration without agents needing to track each other’s\nstate explicitly. The blackboard acts as a memory layer, ensuring that shared context is always\navailable without excessive network calls.\nMarket-Based Pattern\nThe Market-Based Pattern models a decentralized system where agents negotiate or compete\nfor tasks and resources. This is commonly used in autonomous trading, logistics, and distributed\noptimization problems.\nMarket-Based Pattern\n© 2025 Confluent, Inc. 19\n\nTraditional Approach:\n• Agents communicate",
    "chunk_order_index": 4,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:03fc26d4e641bff9cd3ef0a31b2030c9",
      "default:extract:b7b03517679ec778a800c7795d7eefc0"
    ],
    "create_time": 1769937806,
    "update_time": 1769938283,
    "_id": "chunk-5c1ccd6e4169f2c696ec77c1ea699f31"
  },
  "chunk-fdcede6800acbf2e3e236e61d926e28b": {
    "tokens": 1200,
    "content": "allows real-time collaboration without agents needing to track each other’s\nstate explicitly. The blackboard acts as a memory layer, ensuring that shared context is always\navailable without excessive network calls.\nMarket-Based Pattern\nThe Market-Based Pattern models a decentralized system where agents negotiate or compete\nfor tasks and resources. This is commonly used in autonomous trading, logistics, and distributed\noptimization problems.\nMarket-Based Pattern\n© 2025 Confluent, Inc. 19\n\nTraditional Approach:\n• Agents communicate directly with each other to place bids or negotiate terms.\n• A central system is often required to coordinate interactions.\nEvent-Driven Approach:\n• Bidding agents publish offers and requests as events.\n• A market-making service matches events, executing transactions asynchronously.\n• Agents listen for matched events and adjust their strategies dynamically.\nThe pattern now looks like this:\nEvent-Driven Market-Based Pattern\nThis removes the quadratic complexity of direct peer-to-peer communication, as agents interact\nthrough a central event log instead of maintaining individual connections.\nFor example, in financial markets, a data streaming platform is used as a real-time event\nbroker, allowing thousands of trading agents to execute bids, match orders, and react to price\nfluctuations in milliseconds.\n© 2025 Confluent, Inc. 20\n\nEXAMPLE\nMulti-Agent AI Sales Development Representative (SDR)\nHere’s an event-driven, multi-agent system that automates the SDR workflow. Apache Flink® with\nAI Model Inference is used to orchestrate communication with a series of AI agents.\nThe system consists of the following agents:\n• Lead Ingestion Agent: Captures incoming leads from web forms, enriches them with\nexternal data (e.g., company website, Salesforce), and generates a report that can be\nused for scoring.\n• Lead Scoring Agent: Uses enriched lead information to score leads and generate a short\nsummary for how to best engage. Determines the appropriate next step and triggers\ndownstream agents.\n• Active Outreach Agent: Creates personalized outreach emails, incorporating insights\nfrom the lead’s online presence, in order to book a meeting.\n• Nurture Campaign Agent: Dynamically creates a sequence of emails based on where the\nlead originated, and what their interest was.\n• Send Email Agent: Currently sends to a terminal, but in a real application would send via\nemail relay or email service.\n→ Visit this GitHub repo to learn more.\n© 2025 Confluent, Inc. 21\n\nThe Role of the Data Streaming Platform in Multi-Agent Systems\nFor multi-agent systems to function efficiently, they must operate under a shared event-driven\nmodel that standardizes communication and decision-making. This model consists of three\nprimary components:\n1. Input: Agents consume structured events or commands.\n2. Processing: Agents apply reasoning, use tools, or retrieve additional context.\n3. Output: Agents produce new events or take actions in external systems.\nBy following this event-driven framework, multi-agent systems gain modularity, resilience, and the\nability to scale dynamically.\nMaintaining state consistency across multiple agents requires event persistence and replayability.\nThis is where immutable logs and event sourcing come into play.\n• Every event is recorded as an immutable entry, ensuring no data loss.\n• If an agent fails, it can replay events from a saved offset, restoring its state seamlessly.\n• Multiple agents can consume the same event stream, allowing parallel processing without\ninterference.\nThis model dramatically improves reliability, making it easier to debug failures, scale workloads,\nand maintain real-time synchronization.\nAs multi-agent architectures become more prevalent, event-driven patterns are essential to\nensuring they remain scalable, adaptable, and efficient.\nBy applying the lessons from microservices architecture, AI agents can be designed to operate\nin a loosely coupled, decentralized manner, reacting to events rather than relying on rigid\ndependencies. This enables the creation of AI-driven ecosystems where agents collaborate\nseamlessly across complex workflows, unlocking new possibilities for enterprise automation and\nlarge-scale AI systems.\nIn the next section, we’ll explore how to build and deploy these event-driven agents in real-world\napplications, leveraging a data streaming platform with fully managed Kafka and Flink for scalable\nexecution.\n© 2025 Confluent, Inc. 22\n\nPart IV: Building Event-Driven Systems\nfor Agents with a Data Streaming\nPlatform\nAI agents are only as effective as the infrastructure that supports them. No matter how\nsophisticated an agent’s reasoning and decision-making capabilities are, they depend on access\nto the right data, tools, and communication channels. Traditional architectures—built around\nrequest/response patterns, rigid APIs, and batch data processing—create bottlenecks that limit\nan agent’s ability to act in real time.\nEDAs solve this by treating data as a continuously moving asset, rather than a static snapshot. By\nusing Confluent Data Streaming Platform as the backbone, agents can consume, process, and\nemit events asynchronously, making them adaptable, scalable, and resilient for enterprise use\ncases.\nIn this section, we’ll explore how to architect enterprise-ready event-driven AI systems, tackle the\nchallenges of real-time agent interaction, and apply these concepts to real-world applications.\nArchitecting Singular and Multi-Agent Systems\nThe shift to event-driven architectures marks a pivotal moment in building scalable agent\nsystems. Instead of waiting for direct instructions, agents are designed to emit and listen\nfor events autonomously. Events act as signals that something has happened—a change in\ndata, a triggered action, or an important update—allowing agents to respond dynamically and\nindependently.\nAt their core, AI agents function much like microservices, operating autonomously but needing\nstructured communication to collaborate effectively. Confluent Data Streaming Platform acts\nas the “central nervous system” for agents, enabling them to function in a loosely coupled, but\nhighly coordinated manner.\n©",
    "chunk_order_index": 5,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:b5ec37c853f6ab7a736e719f8ef9dbc6",
      "default:extract:e14526fe014fca14fb883dde6c846162"
    ],
    "create_time": 1769937806,
    "update_time": 1769938339,
    "_id": "chunk-fdcede6800acbf2e3e236e61d926e28b"
  },
  "chunk-714ac346e5243ff68a2a0f7dd3afc887": {
    "tokens": 1200,
    "content": "for events autonomously. Events act as signals that something has happened—a change in\ndata, a triggered action, or an important update—allowing agents to respond dynamically and\nindependently.\nAt their core, AI agents function much like microservices, operating autonomously but needing\nstructured communication to collaborate effectively. Confluent Data Streaming Platform acts\nas the “central nervous system” for agents, enabling them to function in a loosely coupled, but\nhighly coordinated manner.\n© 2025 Confluent, Inc. 23\n\nStream Connect Process Govern\nContinuously capture Integrate disparate Use Flink stream Use data lineage,\nand share real-time data from any processing (e.g., join, quality controls, and\nevents with AI systems environment, with filter) to enrich data traceability to ensure\nand agents anywhere, 120+ pre-built and with real-time context data for agents is\nbuilt on Kora, the cloud- custom connectors, at query execution, secure and verifiable.\nnative Apache Kafka® bringing real-time enabling agentic RAG.\nengine. data to agents.\nThe Central Nervous System for Agentic Systems\nBy integrating Apache Kafka®, Apache Flink®, and an existing agent framework like LangGraph,\nwe can build architectures that scale horizontally, process high-throughput data streams, and\nensure real-time responsiveness.\nHere’s what Confluent Data Streaming Platform provides:\n• Horizontal Scalability: Agents operate independently, allowing for seamless scaling. Adding\nmore agents doesn’t require rewriting the entire system.\n• Low Latency and High Throughput: Streaming platforms ensure that agents receive and act\non fresh data without waiting for batch updates.\n• Fault Recovery and Isolation: Failures in one part of the system don’t bring down the entire\nworkflow. Agents replay events from logs, ensuring resilience.\n• Shift Left: Data is processed closer to the source, reducing latency and improving AI\nperformance.\nA tightly integrated, event-driven pipeline ensures that agents not only receive fresh data, but\nalso act on it dynamically, making real-time AI applications possible.\n© 2025 Confluent, Inc. 24\n\nChallenges and Solutions in Event-Driven Agent Design\nWhile the benefits of an event-driven approach are clear, building AI-powered agentic systems comes with its\nown set of challenges. Below, we address key roadblocks and how Confluent Data Streaming Platform provides\nsolutions.\nStream Next-Level Data Streaming with a Fully Managed, Cloud-Native Service\nIn an agent-driven system, failures, whether from network issues, hardware faults, or software crashes, are\ninevitable. The challenge is ensuring agents can recover seamlessly without disrupting workflows.\nSolution:\n• Kafka’s log-based architecture (on Kora) enables agents to replay events and recover from failures.\n• Idempotent processing ensures that retried operations don’t result in duplicate actions.\n• Dead-letter queues handle failure scenarios gracefully, allowing human oversight when needed.\nConnect Seamless Integration: Connecting Agents with Diverse Systems\nAI agents often interact with multiple external systems, such as databases, APIs, vector stores, and enterprise\napplications. These interactions require seamless, low-latency connectivity without introducing unnecessary\ndependencies.\nSolution:\n• Use connectors to integrate disparate data sources and event streams. This allows agents to consume and\nproduce events without hardcoded dependencies.\n• Leverage LangGraph, Microsoft AutoGen, CrewAI and similar frameworks for tool integration, enabling\nagents to call APIs, databases, and models dynamically.\n• Use data streaming through Kafka topics and stream processing with Flink, ensuring structured\ncollaboration between different AI agents.\nProcess Ensuring Data Freshness: Handling Dynamic Data Streams\nAgents require up-to-date, real-time data for optimal decision-making. Traditional architectures that rely on static\ndata retrieval lead to outdated responses and inefficient workflows.\nSolution:\n• Flink SQL and Table API to process incoming data streams, ensuring AI models and agents work with the\nlatest context.\n• Flink AI Model Inference ensures model predictions update dynamically as new data flows in.\n• Embedding pipelines transform unstructured text into vector representations in real time, stored in vector\ndatabases such as MongoDB, for rapid retrieval in RAG.\nGovern Data Quality, Security, and Compliance\nAI agents often interact with sensitive data, requiring strong governance, auditing, and compliance measures.\nSolution:\n• Stream Governance ensures clean, structured data flows through the system, enforcing policies and data\nlineage tracking.\n• Encryption and access control at the field level prevent unauthorized data exposure.\n• Fine-grained retention policies ensure data is handled in compliance with regulatory requirements like\nGDPR.\nBy addressing these challenges with an event-driven approach, we ensure that agents remain scalable,\nreliable, and capable of handling complex, multi-step workflows.\n© 2025 Confluent, Inc. 25\n\nEXAMPLE\nAgentic RAG\nHere’s an event-driven research agent that mines source materials and leverages RAG to create a\npodcast interview brief.\nThe workflow:\n1. Stream unstructured data (e.g., website URLs, blogs, podcasts) into Confluent,\nretrieve the text, chunk it, generate embeddings using Flink, and store them in a vector\ndatabase like MongoDB Atlas using a sink connector.\n2. For all text extracted, pull out the most interesting questions, and store those.\n3. Call the LLM to generate a research brief combining the most relevant context based on\nthe embeddings.\n→ Visit this GitHub repo and blog to learn more.\n© 2025 Confluent, Inc. 26\n\nReal-World Applications of Event-Driven Agentic Systems\nEvent-driven architectures unlock powerful AI applications across industries by ensuring agents\noperate with fresh, real-time data. Below are a few examples of how companies are using a data\nstreaming platform to build event-driven multi-agent systems.\n1. Automating Web Scraping with AI Agents",
    "chunk_order_index": 6,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:6caca960960b565a15731866bffbd883",
      "default:extract:a2eafd95605d7436fe675ad9c8fdae60"
    ],
    "create_time": 1769937806,
    "update_time": 1769938354,
    "_id": "chunk-714ac346e5243ff68a2a0f7dd3afc887"
  },
  "chunk-be8a479aabd0d0444893f31e7e29e918": {
    "tokens": 1200,
    "content": "relevant context based on\nthe embeddings.\n→ Visit this GitHub repo and blog to learn more.\n© 2025 Confluent, Inc. 26\n\nReal-World Applications of Event-Driven Agentic Systems\nEvent-driven architectures unlock powerful AI applications across industries by ensuring agents\noperate with fresh, real-time data. Below are a few examples of how companies are using a data\nstreaming platform to build event-driven multi-agent systems.\n1. Automating Web Scraping with AI Agents\nTraditional web scraping is brittle, requiring manual efforts to handle dynamic pages,\nextract relevant data, and adapt to site changes. This complexity increases when\nsupporting GenAI models, which need structured and unstructured data in real time.\nStatic scraping workflows fail to keep up, leading to outdated or incomplete information.\nReworkd tackled this problem by building an agentic system for web scraping. AI\nagents write code to extract relevant data, while test and validation agents verify that\nthe generated code is correct. These agents operate asynchronously, consuming and\nproducing events in a continuous feedback loop.\nThe agents seamlessly process and stream real-time data into downstream applications.\nThe result: a scalable, fault-tolerant system that dynamically adjusts to website changes,\nand ensures high-quality data feeds for AI models.\n2. Intelligent Business Copilots\nTeams need self-serve, real-time access to data for faster, smarter decision-making. Yet,\nbusiness stakeholders often rely on engineering and data science teams to integrate and\nquery data, introducing bottlenecks, batch-based delays, and stale insights.\nAiry transforms this process by enabling AI-powered copilots that provide a natural\nlanguage interface for exploring and working with real-time data. Agents convert plain\nlanguage into Flink jobs, continuously monitoring and processing data streams.\nBy leveraging real-time context from a data streaming platform, these agents use LLMs\nto generate precise Flink SQL queries, empowering teams to extract and analyze live data\ninstantly. This shifts data interaction from manual queries to conversational, real-time\ninsights, making knowledge more accessible than ever.\n3. Workflow Automation with a Drag-and-Drop Agent Builder\nWhen starting to build multi-agent systems, teams face the complexity of integrating and\norchestrating AI agents and tools, as well as managing intricate tasks at scale.\nAgent Taskflow provides a no-code platform that helps users get up and running with\nall the features that go into an agent–memory, knowledge bases, and tools–in just a few\nclicks. The drag-and-drop UI allows for effortless creation of workflows.\nBuilt on a data streaming platform, the solution enables agents to make context-informed\ndecisions and adapt to real-time events, for more efficient and intelligent automation.\nFrom automating customer support to marketing campaigns, this frees teams to focus\non high-value work while democratizing AI agents, making it accessible to users without\nprogramming expertise.\n© 2025 Confluent, Inc. 27\n\nBroader Industry Impact\nEvent-driven AI agents enable businesses to automate complex, real-time tasks across industries:\n• E-commerce: Agents continuously track price changes, product availability, and competitor\ntrends, ensuring businesses make informed pricing decisions.\n• Market Research: Streaming data feeds allow companies to monitor customer sentiment,\ncompetitive shifts, and industry trends in real time.\n• Finance: Agents aggregate financial data, news sentiment, and stock movements, helping\nanalysts make more accurate, timely decisions.\nBy adopting event-driven multi-agent systems, businesses gain resilient, adaptive AI workflows\nthat evolve dynamically. As this paradigm becomes standard, companies that integrate\nstreaming-first architectures will gain a competitive edge in automating complex, data-intensive\nprocesses.\nWhy a Data Streaming Platform is Essential for AI Agents\nAt the heart of every scalable agentic system is a real-time data streaming platform. Unlike\ntraditional request/response architectures that introduce bottlenecks and stale data, streaming\nenables continuous, low-latency access to the information agents need.\nWhy streaming matters:\n• Real-Time Access: Eliminates batch delays and ensures agents make decisions based on the\nlatest available data.\n• Decoupled Architecture: Agents interact through event streams rather than direct calls,\nreducing complexity and interdependencies.\n• AI-Ready Data: Streaming platforms transform unstructured data into embeddings stored in\nvector stores like MongoDB, making it AI-friendly.\n• Scalable AI Interactions: AI workloads are distributed across streaming pipelines, preventing\nbottlenecks and improving efficiency.\n• Modular and Future-Proof: Use any model, vector database, or AI framework of choice, with\nthe flexibility to swap in new technologies as they evolve, ensuring long-term adaptability.\nBy adopting a shift-left approach—moving computation closer to the data source—organizations\ncan reduce latency, improve AI performance, and create a more adaptive architecture.\nThe Future of AI is Event-Driven\nAI-powered agents will define the next era of automation, but only if they can think, act, and\ncollaborate in real time. Event-driven architectures ensure that agents are no longer limited\nby outdated batch processes, rigid APIs, or stale data. Instead, they operate dynamically—\nprocessing, analyzing, and acting on real-time events as they happen.\nAs AI adoption accelerates, companies that embrace streaming-first architectures will have a\nmassive advantage. They’ll build AI systems that are smarter, more adaptable, and infinitely\nscalable, unlocking true agentic intelligence across industries.\n© 2025 Confluent, Inc. 28\n\nTake the\nSign up for Confluent Cloud and\n1\nreceive $400 in free credits.\nNext Step\n2 Visit the GenAI hub for more resources.\nTo get started with\nbuilding event-driven\n3 Apply for Confluent’s AI Accelerator Program,\nmulti-agents:\nwhich provides technical and business\nmentorship to help AI startups fast-track\ninnovation and growth",
    "chunk_order_index": 7,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:edbfa7943883571229e7e804e31fe3c1",
      "default:extract:0b2e259f45c214da675b1a24cb84943d"
    ],
    "create_time": 1769937806,
    "update_time": 1769938354,
    "_id": "chunk-be8a479aabd0d0444893f31e7e29e918"
  },
  "chunk-1171fadc9ac9d0b83d49887e5793a469": {
    "tokens": 243,
    "content": "scalable, unlocking true agentic intelligence across industries.\n© 2025 Confluent, Inc. 28\n\nTake the\nSign up for Confluent Cloud and\n1\nreceive $400 in free credits.\nNext Step\n2 Visit the GenAI hub for more resources.\nTo get started with\nbuilding event-driven\n3 Apply for Confluent’s AI Accelerator Program,\nmulti-agents:\nwhich provides technical and business\nmentorship to help AI startups fast-track\ninnovation and growth.\nAbout Confluent\nConfluent is pioneering a fundamentally new category of\ndata infrastructure focused on data in motion. Confluent’s\ncloud-native offering is the foundational platform for\ndata in motion–designed to be the intelligent connective\ntissue enabling real-time data from multiple sources to\nconstantly stream across the organization. With Confluent,\norganizations can meet the new business imperative of\ndelivering rich digital front-end customer experiences and\ntransitioning to sophisticated, real-time, software-driven\nback-end operations.\nTo learn more, please visit www.confluent.io.\n© Confluent Inc. 2025 29\n© Confluent Inc. 2025 29",
    "chunk_order_index": 8,
    "full_doc_id": "doc-cda0e3aef4c5151a2f709f4cef7aee7e",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:00c5ed22a566d1d8626b1f85d6af1b41",
      "default:extract:1768dc86db4840dac412b39ce57aa280"
    ],
    "create_time": 1769937806,
    "update_time": 1769938365,
    "_id": "chunk-1171fadc9ac9d0b83d49887e5793a469"
  },
  "chunk-04a91a22ba38bfcc9bb0e1df32b15611": {
    "tokens": 78,
    "content": "RAGAnything là một framework RAG tất cả trong một.\n        Nó hỗ trợ xử lý đa phương thức (hình ảnh, bảng biểu, công thức).\n        Việc sử dụng Local LLM giúp bảo mật dữ liệu và tiết kiệm chi phí API.\n        Llama 3.1 8B là một mô hình ngôn ngữ mạnh mẽ của Meta.",
    "chunk_order_index": 0,
    "full_doc_id": "doc-04a91a22ba38bfcc9bb0e1df32b15611",
    "file_path": "unknown_source",
    "llm_cache_list": [
      "default:extract:fff6f6a6cd14ffbe79aac6150250af01",
      "default:extract:1d0471885dd9c662d861d2341f68490f"
    ],
    "create_time": 1769938365,
    "update_time": 1769938380,
    "_id": "chunk-04a91a22ba38bfcc9bb0e1df32b15611"
  }
}