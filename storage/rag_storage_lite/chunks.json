{
  "chunks": [
    "Tong Xiao\nJingbo Zhu\nNatural Language Processing\nNeural Networks and Large Language Models\nNATURAL LANGUAGE PROCESSING LAB\nNORTHEASTERN UNIVERSITY\n&\nNIUTRANS RESEARCH\nhttps://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\n\nCopyright©2021-2025TongXiaoandJingboZhu\nNATURAL LANGUAGE PROCESSING LAB, NORTHEASTERN UNIVERSITY\n&\nNIUTRANS RESEARCH\nhttps://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nLicensedundertheCreativeCommonsAttribution-NonCommercial4.0UnportedLicense(the\n“License”). YoumaynotusethisfileexceptincompliancewiththeLicense. Youmayobtain\nacopyoftheLicenseathttp://creativecommons.org/licenses/by-nc/4.0.\nUnless required by applicable law or agreed to in writing, software distributed under the\nLicense is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\nANY KIND, either express or implied. See the License for the specific language governing\npermissionsandlimitationsundertheLicense.\nDecember1,2025",
    "3\nPreface\nNaturallanguageprocessing(NLP)isoneofthecoresubfieldsofartificialintelligence(AI).\nForalongtime,researchinNLPprimarilyfocusedonsolvingspecificproblemsinlanguage\nunderstanding and generation, such as parsing and machine translation. This task-driven\nresearch approach dominated the development of NLP for several decades. However, with\ntheriseofdeeplearning,theresearchparadigmofNLPhasundergoneafundamentaltrans-\nformation. The application of deep neural networks has enabled us to tackle increasingly\ncomplextasks. Moreimportantly,researchershavediscoveredthatbyconductinglarge-scale\npretrainingonabasemodelwithmassivedatasets,andthenfine-tuningitwithasmallamount\noftask-specificdataandknowledge,itispossibletoconstructgeneral-purposemodelscapable\nofhandlingmultipletaskssimultaneously. Thisnewparadigmisgreatlychangingtheresearch\nlandscapeofNLP,andeventhebroaderfieldofAI.\nThisbookfocusesonmodernNLPmethodscenteredaroundneuralnetworksandfounda-",
    "tionmodels. Itaimstoprovideapracticalguidetounderstanding,building,andapplyingthese\npowerfulmodels. UnliketraditionalNLPtextbooksthatorganizechaptersbasedonspecific\ntasks,thisbookisstructuredfromtheperspectiveofconstructingneuralNLPmodelsandis\ndividedintothreemainparts:\n• FoundationsofMachineLearningandNeuralNetworks(Chapters1-2): Thispart\nintroduces the core concepts and methods of machine learning and neural networks,\nlayingafoundationforthesubsequentchapters. Itisrelativelyself-containedandcan\nbestudiedindependentlyorusedasbackgroundmaterialwhenneeded.\n• Basic Neural Models for Natural Language Processing (Chapters 3-6): This part\nexplainstheneuralnetworksusedinNLPtasks,includingwordrepresentationmodels,\nsequence models, and sequence-to-sequence models. In addition, Transformers are\nintroducedinadedicatedchapter. ThesemodelsarenotlimitedtoindividualNLPtasks;\nrather,theyserveasgeneral-purposetoolsacrossmanyapplications.",
    "• LargeLanguageModels(Chapters7-11): Thispartfocusesonlargelanguagemodels\n(LLMs), coveringtopicssuchaspretraining, generativemodels, promptengineering,\nalignment,andinference.\nThisbookisintendedforseniorundergraduates,graduatestudents,researchersinrelated\nfields, and anyone interested in NLP. We strive for clear and accessible writing, aiming to\nintroducecoreconceptsandfundamentalmethodsratherthanprovidinganin-depthexploration\nofallcutting-edgetechniques. Therefore,thisbookcanservebothasanintroductorytextfor\nnewcomersandasareferencemanualforkeyconceptsandmethodsinNLP.\nThe content of this book has gradually taken shape through our years of teaching and\nresearchexperience. Initially,weonlyplannedtowritethefirsttwoparts. However,therapid",
    "4\nriseandgrowingimportanceofLLMsledustoincludethistopicasakeypartofthebook. At\nthesametime,wearedelightedtowitnesstherapiddevelopmentofNLP,andthewritingof\nthisbookisalsoourresponsetothisexcitingtrend.\nSomechaptersofthisbookhavebeenpreviouslypublishedonline,suchasIntroductionto\nTransformers: AnNLPPerspective(Chapter6)andFoundationsofLargeLanguageModels\n(Chapters7-11),andwearegratefulforthevaluablefeedbackfrommanyreaders,whichhas\ngreatly contributed to the refinement of the book. Furthermore, during the writing process,\nwedrewsignificantinspirationfromclassicworks,includingMachineLearningbyMitchell\n[1997], Foundations of Statistical Natural Language Processing by Manning and Schütze\n[1999],PatternRecognitionandMachineLearningbyBishop[2006],andSpeechandLan-\nguageProcessingbyJurafskyandMartin[2008]. Manyinsightsfromtheseworksprofoundly\ninfluencedthewritingapproachofthisbook.\nLastly,wewouldliketoexpressourheartfeltthankstoallthosewhoprovidedsuggestions",
    "andrevisionstothecontentofthisbook. Theyare: WeiqiaoShan,YongyuMu,Chenglong\nWang,KaiyanChang,YuchunFan,HangZhou,ChuanhaoLv,XinyuLiu,TaoZhou,Huiwen\nBao,TongZheng,JunhaoRuan,YingfengLuo,YuzhangWu,andYifuHuo.\nTongXiaoandJingboZhu\nJune,2025",
    "NaturalLanguageProcessingLab,NortheasternUniversity\nThisbookisdedicatedtoourfamilies.",
    "Contents\nI Preliminaries\n1 Foundations of Machine Learning ........................... 17\n1.1 Math Basics ................................................ 18\n1.1.1 LinearAlgebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n1.1.2 ProbabilityandStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1.2 Designing a Text Classifier.................................... 26\n1.2.1 ProblemStatement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n1.2.2 DocumentsasFeatureVectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n1.2.3 LinearClassifiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n1.2.4 GenerativevsDiscriminative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32",
    "1.2.5 OOVWordsandSmoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n1.3 General Problems ........................................... 37\n1.3.1 SupervisedandUnsupervisedModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n1.3.2 InductiveBias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n1.3.3 Non-linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n1.3.4 TrainingandLossFunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n1.3.5 OverfittingandUnderfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n1.3.6 Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n1.4 Model Selection and Evaluation................................ 50",
    "1.4.1 StrategiesforModelSelection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n1.4.2 Training,ValidationandTestData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n1.4.3 PerformanceMeasure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n1.4.4 SignificanceTests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n1.5 NLP Tasks as ML Tasks....................................... 59\n1.5.1 Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n1.5.2 SequenceLabeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n1.5.3 LanguageModeling/WordPrediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n1.5.4 SequenceGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62",
    "1.5.5 TreeGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n1.5.6 RelevanceModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64",
    "1.5.7 LinguisticAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n1.5.8 Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n1.5.9 Others . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n1.6 Summary................................................... 68\n2 Foundations of Neural Networks ............................ 71\n2.1 Multi-layer Neural Networks ................................... 71\n2.1.1 Single-layerPerceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n2.1.2 StackingMultipleLayers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n2.1.3 ComputationGraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n2.2 Example: Neural Language Modeling........................... 78",
    "2.3 Basic Model Architectures .................................... 83\n2.3.1 RecurrentUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n2.3.2 ConvolutionalUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n2.3.3 GateUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n2.3.4 Normalization(Standardization)Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n2.3.5 ResidualUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n2.4 Training Neural Networks ..................................... 90\n2.4.1 GradientDescent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n2.4.2 Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94",
    "2.4.3 ParameterInitialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n2.4.4 LearningRateScheduling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n2.5 Regularization Methods ...................................... 99\n2.5.1 Norm-basedPenalties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n2.5.2 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n2.5.3 EarlyStopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n2.5.4 SmoothingOutputProbabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n2.5.5 TrainingwithNoise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n2.6 Unsupervised Methods and Auto-encoders..................... 108",
    "2.6.1 Auto-encoderswithExplicitRegularizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n2.6.2 DenoisingAuto-encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n2.6.3 VariationalAuto-encoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n2.7 Summary.................................................. 119\nII Basic Models\n3 Words and Word Vectors .................................. 123\n3.1 Tokenization ............................................... 124\n3.1.1 TokenizationviaRulesandHeuristics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n3.1.2 TokenizationasLanguageModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126",
    "3.1.3 TokenizationasSequenceLabeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n3.1.4 LearningSubwords . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n3.2 Vector Representation for Words.............................. 137\n3.2.1 One-hotRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n3.2.2 DistributedRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n3.2.3 CompositionalityandContextuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n3.3 Count-based Models ........................................ 142\n3.3.1 Co-occurrenceMatrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n3.3.2 TF-IDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146",
    "3.3.3 Low-DimensionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n3.4 Inducing Word Embeddings from NLMs........................ 153\n3.5 Word Embedding Models .................................... 154\n3.5.1 Word2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n3.5.2 GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n3.5.3 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n3.6 Evaluating Word Embeddings ................................ 163\n3.6.1 ExtrinsicEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n3.6.2 IntrinsicEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164",
    "3.6.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n3.7 Summary.................................................. 168\n4 Recurrent and Convolutional Sequence Models ............. 171\n4.1 Problem Statement ......................................... 172\n4.2 Recurrent Models........................................... 173\n4.2.1 AnRNN-basedLanguageModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\n4.2.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n4.2.3 LayerStacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n4.2.4 Bi-directionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n4.3 Memory ................................................... 181",
    "4.3.1 MemoryasASystem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n4.3.2 LongShort-TermMemory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183\n4.3.3 GatedRecurrentUnits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n4.4 Convolutional Models ....................................... 187\n4.4.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n4.4.2 CNNsforSequenceModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190\n4.4.3 HandlingPositionalInformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n4.5 Examples.................................................. 198\n4.5.1 TextClassification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198",
    "4.5.2 End-to-EndSpeechRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200\n4.5.3 SequenceLabelingwithLSTMandGraphicalModels . . . . . . . . . . . . . . . . . . 203",
    "4.5.4 HybridModelsforLanguageModeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n4.6 Summary.................................................. 207\n5 Sequence-to-Sequence Models ............................ 211\n5.1 Sequence-to-Sequence Problems ............................. 212\n5.2 The Encoder-Decoder Architecture............................ 213\n5.2.1 EncodingandDecoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n5.2.2 Example: NeuralMachineTranslation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n5.3 The Attention Mechanism.................................... 218\n5.3.1 ABasicModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219\n5.3.2 TheQKVAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n5.3.3 Multi-headAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226",
    "5.3.4 HierarchicalAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n5.3.5 Multi-layerAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n5.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n5.4 Search .................................................... 238\n5.4.1 TheLengthProblem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n5.4.2 PruningandBeamSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242\n5.4.3 OnlineSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250\n5.4.4 ExactSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254\n5.4.5 DifferentiableSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256",
    "5.4.6 HypothesisDiversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n5.4.7 CombiningMultipleModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n5.4.8 MoreSearchObjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262\n5.5 Summary.................................................. 265\n6 Transformers ............................................. 269\n6.1 The Basic Model............................................ 269\n6.1.1 TheTransformerArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n6.1.2 PositionalEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n6.1.3 Multi-headSelf-attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\n6.1.4 LayerNormalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276",
    "6.1.5 Feed-forwardNeuralNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n6.1.6 AttentionModelsontheDecoderSide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n6.1.7 TrainingandInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281\n6.2 Syntax-aware Models........................................ 283\n6.2.1 Syntax-awareInputandOutput . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n6.2.2 Syntax-awareAttentionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285\n6.2.3 Multi-branchModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n6.2.4 Multi-scaleModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\n6.2.5 TransformersasSyntaxLearners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291",
    "6.3 Improved Architectures...................................... 295\n6.3.1 LocallyAttentiveModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295\n6.3.2 DeepModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299\n6.3.3 NumericalMethod-InspiredModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305\n6.3.4 WideModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\n6.4 Efficient Models ............................................ 312\n6.4.1 SparseAttention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n6.4.2 RecurrentandMemoryModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n6.4.3 Low-dimensionalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322",
    "6.4.4 ParameterandActivationSharing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n6.4.5 AlternativestoSelf-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n6.4.6 ConditionalComputation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336\n6.4.7 ModelTransferandPruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n6.4.8 SequenceCompression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n6.4.9 HighPerformanceComputingMethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n6.5 Applications ............................................... 347\n6.5.1 LanguageModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348\n6.5.2 TextEncoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349",
    "6.5.3 SpeechTranslation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n6.5.4 VisionModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353\n6.5.5 MultimodalModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n6.6 Summary.................................................. 357\nIII Large Language Models\n7 Pre-training .............................................. 365\n7.1 Pre-training NLP Models..................................... 366\n7.1.1 Unsupervised,SupervisedandSelf-supervisedPre-training . . . . . . . . . . . . . . 366\n7.1.2 AdaptingPre-trainedModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368\n7.2 Self-supervised Pre-training Tasks ............................ 372\n7.2.1 Decoder-onlyPre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372",
    "7.2.2 Encoder-onlyPre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n7.2.3 Encoder-DecoderPre-training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380\n7.2.4 ComparisonofPre-trainingTasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\n7.3 Example: BERT ............................................ 388\n7.3.1 TheStandardModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n7.3.2 MoreTrainingandLargerModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n7.3.3 MoreEfficientModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n7.3.4 Multi-lingualModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394",
    "7.4 Applying BERT Models ...................................... 396\n7.5 Summary.................................................. 401\n8 Generative Models ........................................ 403\n8.1 A Brief Introduction to LLMs ................................. 404\n8.1.1 Decoder-onlyTransformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n8.1.2 TrainingLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n8.1.3 Fine-tuningLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n8.1.4 AligningLLMswiththeWorld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n8.1.5 PromptingLLMs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n8.2 Training at Scale............................................ 425",
    "8.2.1 DataPreparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n8.2.2 ModelModifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427\n8.2.3 DistributedTraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430\n8.2.4 ScalingLaws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n8.3 Long Sequence Modeling.................................... 436\n8.3.1 OptimizationfromHPCPerspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437\n8.3.2 EfficientArchitectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\n8.3.3 CacheandMemory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n8.3.4 SharingacrossHeadsandLayers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450",
    "8.3.5 PositionExtrapolationandInterpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n8.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463\n8.4 Summary.................................................. 466\n9 Prompting ................................................ 467\n9.1 General Prompt Design...................................... 468\n9.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n9.1.2 In-contextLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471\n9.1.3 PromptEngineeringStrategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473\n9.1.4 MoreExamples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\n9.2 Advanced Prompting Methods................................ 489",
    "9.2.1 ChainofThought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489\n9.2.2 ProblemDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492\n9.2.3 Self-refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499\n9.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505\n9.2.5 RAGandToolUse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n9.3 Learning to Prompt ......................................... 515\n9.3.1 PromptOptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515\n9.3.2 SoftPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519\n9.3.3 PromptLengthReduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528",
    "9.4 Summary.................................................. 530",
    "10 Alignment ................................................ 533\n10.1 An Overview of LLM Alignment ............................... 534\n10.2 Instruction Alignment ....................................... 535\n10.2.1 SupervisedFine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\n10.2.2 Fine-tuningDataAcquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541\n10.2.3 Fine-tuningwithLessData . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n10.2.4 InstructionGeneralization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547\n10.2.5 UsingWeakModelstoImproveStrongModels . . . . . . . . . . . . . . . . . . . . . . . 549\n10.3 Human Preference Alignment: RLHF .......................... 553\n10.3.1 BasicsofReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553",
    "10.3.2 TrainingRewardModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560\n10.3.3 TrainingLLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\n10.4 Improved Human Preference Alignment........................ 568\n10.4.1 BetterRewardModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n10.4.2 DirectPreferenceOptimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575\n10.4.3 AutomaticPreferenceDataGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 578\n10.4.4 Step-by-stepAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n10.4.5 Inference-timeAlignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n10.5 Summary.................................................. 584\n11 Inference ................................................. 587",
    "11.1 Prefilling and Decoding...................................... 588\n11.1.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 588\n11.1.2 ATwo-phaseFramework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593\n11.1.3 DecodingAlgorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596\n11.1.4 EvaluationMetricsforLLMInference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607\n11.2 Efficient Inference Techniques................................ 608\n11.2.1 MoreCaching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608\n11.2.2 Batching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609\n11.2.3 Parallelization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619",
    "11.2.4 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619\n11.3 Inference-time Scaling....................................... 621\n11.3.1 ContextScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622\n11.3.2 SearchScaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623\n11.3.3 OutputEnsembling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623\n11.3.4 GeneratingandVerifyingThinkingPaths . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n11.4 Summary.................................................. 632",
    "I\nPreliminaries\n1 Foundations of Machine Learning .......... 17\n1.1 MathBasics\n1.2 DesigningaTextClassifier\n1.3 GeneralProblems\n1.4 ModelSelectionandEvaluation\n1.5 NLPTasksasMLTasks\n1.6 Summary\n2 Foundations of Neural Networks ........... 71\n2.1 Multi-layerNeuralNetworks\n2.2 Example: NeuralLanguageModeling\n2.3 BasicModelArchitectures\n2.4 TrainingNeuralNetworks\n2.5 RegularizationMethods\n2.6 UnsupervisedMethodsandAuto-encoders\n2.7 Summary",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 1\nFoundations of Machine Learning\nThegoalofmachinelearningistodevelopmethodsthatcanautomatically\ndetectpatternsindata,andthentousetheuncoveredpatternstopredict\nfuturedataorotheroutcomesofinterest.\n–Murphy[2012]\nMachinelearningcanbebroadlydefinedascomputationalmethodsusing\nexperiencetoimproveperformanceortomakeaccuratepredictions.\n–Mohrietal.[2018]\nData-drivenNLPfitstheabovedefinitions1. Itteachescomputerstolearnlanguageexperience\nfromcorpora,andtounderstandandutilizelanguagebasedonthatexperience. Connecting\nmachine learning (ML) with natural language processing is much more than a means that\nmakescomputersmimichumanlanguageintelligencefromdata. Itisleadingarevolutionin\nbothareas: naturallanguageprocessingevolvesbyusingapowerfultoolofderivingmeaning\nfromcorpora,andmachinelearningevolvesbyaddressingtheNLPchallengesandtestingon\nreal-worlddata.",
    "Inthischapter,wepresentseveralbasicconceptsandmodelsinmachinelearning. There\narenotoughbitsbutsomepreliminariesforthesubsequentchapters. Herewefocusonhow\nto apply machine learning to NLP problems, in particular how to define an NLP problem\nas a statistical learning problem. To do this, we start with classification — one of the most\nwidely-usedexamplesinmostintroductorybooks. Wethenpresentseveralfundamentalissues\nofmachinelearning. TheyarefollowedbyadiscussiononNLPproblemsfromthemachine\nlearningperspective.\n1Wedropthetermdata-drivenfromnowonandassumethatallNLPmodelsaredata-drivenintheremainder\nofthisdocument.",
    "18 Chapter 1. Foundations of Machine Learning\n1.1 Math Basics\nIntheremainderofthischapterandthefollowingchapters,wewilltalkaboutmachinelearning\nproblems using the tool of applied mathematics. Here are the math basics. If you find the\ndetailstrivial,youcanskiptoSection1.2directly.\n1.1.1 Linear Algebra\n1. Vectors and Matrices\nScalarmayormaynotbethesimplestconceptinlinearalgebra,butissurelythemostcommon\nconceptthatonelearnsinhighschoolorinuniversity. Ascalarisanumber. Itisaquantity\nthathasamagnitudebuthasnodirection. Forexample,height,weight,distance,temperature\nareallexamplesofscalars. Hereweuseanitalicnumbertodenoteascalar,forexample,a,b,\nx,A,andsoon.\nVectorandmatrixaredefinedontopofscalar. Avectorisanarrayofscalars,orsimplya\nnumberlist. Amatrixisarectangulararrayofscalars. Inthisbook,wefollowtheconvention\nofusingboldletterstodenotevectorsandmatrices. Forexample,ann-dimensionalvectorcan\nbewrittenas\n(cid:104) (cid:105)\na = a a ... a (1.1)\n1 2 n",
    "where{a ,a ,...,a }aretheelements(orentries)ofthevector. Eachindicatesadimension.\n1 2 n\nForconvenienceofnotation,wewritea asa(i)sometimes. Avectorisareal-valuedvector\ni\nonlyifalltheelementsarerealnumbers(i.e.,a ∈Rforeachi),denotedasa∈Rn. Likewise,\ni\nwecanwriteanm×nmatrixas\n \nA A ... A\n11 12 1n\nA A ... A \nA =  21 22 2n (1.2)\n ... ... ... ... \n \nA A ... A\nm1 m2 mn\nwhere m is the number of rows and n is the number of columns. A is the entry (i,j) of\nij\nthe matrix. A real-valued matrix is denoted as A∈Rm×n. Occasionally, we use A to\nm×n\nemphasizethattheshapeofthematrixism×n.\nThereareafewspecialmatrices. Forexample,amatrixwhoseelementsareallzerosisa\nzeromatrix,denotedas0. Anotherexampleisidentitymatrix,denotedasI. Itisasquare\nmatrixwhosediagonalelementsareall1,andotherelementsare0. Vectorscanbetreatedasa\nspecialsortofmatrices,too. Forexample,thevectorinEq. (1.1)isamatrixwithonlyonerow.",
    "1.1 Math Basics 19\n2. Matrix Transpose\nThetransposeofamatrixA isamatrixB subjecttoA =B foreachpairofiand\nm×n n×m ij ji\nj. Often,A’stransposeisdenotedasAT. Forexample,foramatrix\n(cid:34) (cid:35)\n8 0 0\nA = (1.3)\n2 9 7\nthetransposeis\n \n8 2\nAT = 0 9 (1.4)\n \n0 7\nOnecantransposeavectoraswell. Foravector\n(cid:104) (cid:105)\na = 1 9 7 3 (1.5)\nthetransposeis\n \n1\n9\naT =   (1.6)\n7\n \n3\nIngeneral,avectorwithonlyonerowiscalledarowvector(asinEq. (1.5)),andavector\nwithonlyonecolumniscalledacolumnvector(asinEq. (1.6)). Inthisbook,allvectorsare\nrowvectorsbydefault.\n3. Element-wise Operations on Matrices\nSupposeAandBaretwomatricesofthesameshape,sayA,B∈Rm×n. Thematrixaddition\nof A and B is written as A+B. A+B is a matrix in Rm×n such that each element is the\nsumofthecorrespondingelementsofAandB. Hereisanexample.\n(cid:34) (cid:35) (cid:34) (cid:35)\n8 0 0 1 1 1\nA+B = +\n2 9 7 1 0 4\n(cid:34) (cid:35)\n9 1 1\n= (1.7)\n3 9 11\nInasimilarway,wecandefineelement-wiseminus(A−B),product(A⊙B),division",
    "(A⊘B)andotheroperations. Aspecialcaseofelement-wiseproductisthatwemultiplya\nmatrixAwithanothermatrixwhoseelementsareallthesame(sayk). ItisequaltoscalingA\nwithascalark,denotedask×AorkA. Thisisalsocalledscalarproduct. Seebelowforan",
    "20 Chapter 1. Foundations of Machine Learning\n(cid:34) (cid:35)\n8 0 0\nexamplefork=2andA= .\n2 9 7\n(cid:34) (cid:35)\n8 0 0\nkA = 2\n2 9 7\n(cid:34) (cid:35)\n16 0 0\n= (1.8)\n4 18 14\nLetA,BandCbematricesinRm×n,andk andlbescalarsinR. Somepropertiesofthe\nmatrixoperationsare:\n• Propertyofthezeromatrix:\nA = A+0\n= 0+A (1.9)\n• Commutativity:\nA+B = B+A (1.10)\nklA = lkA (1.11)\n• Associativity:\n(A+B)+C = A+(B+C) (1.12)\n(kl)A = l(kA) (1.13)\n• Distributivity:\nk(A+B) = kA+kB (1.14)\n(k+l)A = kA+lA (1.15)\n4. Dot Product\n(cid:104) (cid:105) (cid:104) (cid:105)\nThedotproductoftwosame-sizedvectorsa= a a ... a andb= b b ... b\n1 2 n 1 2 n\nisdefinedtobe:\nn\n(cid:88)\na·b = a b\ni i\ni=1\n= a b +a b +...+a b (1.16)\n1 1 2 2 n n\nIngeometry,areal-valuedvectoracanbeseenasageometricobjecthavingbothmagnitude\n(denotedas|a|)anddirection. Thedotproductofaandbcanalsobedefinedas\na·b = |a|×|b|×cos(θ) (1.17)",
    "1.1 Math Basics 21\nwhereθ istheanglebetweenaandb.\n5. Matrix Product\nMatrix product (or matrix-matrix product) operates on two matrices. Given a matrix\nA ∈ Rm×p and a matrix B ∈ Rp×n, the matrix product of A and B produces a matrix\nC∈Rm×n whoseelementsaredefinedas:\np\n(cid:88)\nC = A ×B\nij ik kj\nk=1\n= A 1×B +A ×B +...+A ×B (1.18)\ni 1j i2 2j ip pj\nMatrixproductrequiresthatthenumberofcolumnsinAisexactlythesameasthenumber\nofrowsinB. InthisbookweuseABtodenotethematrixproductofAandB. Herearea\nfewpropertiesofmatrixproduct.\n• Distributivity. ForA∈Rm×p andB,C∈Rp×n,theleftdistributivityisdefinedas\nA(B+C) = AB+AC (1.19)\nForA,B∈Rm×p andC∈Rp×n,therightdistributivityisdefinedas\n(A+B)C = AC+BC (1.20)\n• Associativity. ForA∈Rm×p,B∈Rp×q andC∈Rq×n,theassociativitydefines\n(AB)C = A(BC) (1.21)\n• Transpose. ForA∈Rm×p andB∈Rp×n,wehave\n(AB)T = BTAT (1.22)\nMatrixproductisnotcommutative,i.e.,wedonothaveAB=BAforallAandBeven\nifAandBaresquarematriceswiththesameshape. Basedonmatrix-matrixproduct,wecan",
    "definevector-matrixproductandmatrix-vectorproductaccordingly. Thisistrivialbecauseall\nweneedistoseeavectorasamatrixinmultiplication.\n6. Norm\n(cid:104) (cid:105)\nInavectorspace,normisameasureofvector“length”. Givenavectora= a a ... a ∈\n1 2 n\nRn,thenormonaisafunctionthatmapsfromRn toR. Itiswrittenas∥·∥ ,orl forshort.\np p\n∥a∥ isdefinedas\np\n(cid:16) (cid:17)1/p\n∥a∥ = (cid:80)n |a |p (1.23)\np i=1 i\nItiscalledp-normorl norm. Thepopularversionsofp-normarethosewhenp=1,2and\np",
    "22 Chapter 1. Foundations of Machine Learning\n∞:\nn\n(cid:88)\n∥a∥ = |a | (1.24)\n1 i\ni=1\n(cid:118)\n(cid:117) n\n(cid:117)(cid:88)\n∥a∥ 2 = (cid:116) |a i |2 (1.25)\ni=1\n∥a∥ = max{|a |,|a |,...,|a |} (1.26)\n∞ 1 2 n\n2-normand∞-normarealsocalledEuclideannormandmaximumnorm. p-normcanalso\nbeusedinmeasuringthedistanceoftwopointsinann-dimensionalspace. Letbbeanother\nvectorinRn. Thep-normdistancebetweenaandbisgivenbytheequation:\n(cid:16) (cid:17)1/p\n∥a−b∥ = (cid:80)n |a −b |p (1.27)\np i=1 i i\n1.1.2 Probability and Statistics\n1. What is Probability\nProbabilityisamatterofuncertainty. Itisaquantitythatdescribeshowlikelyaneventisto\nhappen. Forexample,iftheeventiscertaintohappen,wewillsaythattheprobabilityis1;if\ntheeventwillneverhappen,wewillsaythattheprobabilityis0.\nThen,whatisanevent? Inshort,aneventisanexperimentaloutcome. Itcouldbesimply\nthe result of everything. For example, an event could be the action that you raised your",
    "arms, the scene that you were seeing the sunset, the result that you figured out for a math\nquiz, andsoon. Asetofrelatedeventsisdescribedbyarandomvariableorvariablefor\nshort. For example, we can define the outcome of tossing a coin as a variable x. As there\nare two outcomes (heads or tails), we have two choices for the value of x. We can define\nthatx=1whenthecoinlandsheads,andx=0otherwise. Hence,xisabinaryvariableor\nmorepreciselya0-1variable. Avariablechoosingavaluemeansthataneventhappens. For\nexample,x=1meanstheeventofthecoinlandingheads.\nInmathematics,probabilityisameasureontheprobabilityspacecomprisingevents(call\nitaprobabilitymeasure). Asameasure,probabilityshouldsatisfycertainproperties,such\nas countable additivity [Ash and Doléans-Dade, 1999]. This means that not all functions\ndefinedontheinterval[0,1]couldbeaprobabilitymeasure. Herewedonotdiscusstheprecise\ndefinition of probability measure. We just simply treat it as a function that outputs a real\nnumberin[0,1].",
    "Usually,aprobabilitymeasureisdenotedasafunctionPr(·),calledaprobabilityfunction.\nWhen the input of Pr(·) is defined on a discrete set of events, the output of the function is\nthe probability that an event happens. For example, Pr(x=1) means the probability of x\nequalling1. NotethatPr(x)isafunctionthatvariesitsoutputbychoosingdifferentvalues\nof x. Suppose x is a value that x can take. When we write Pr(x ), it means Pr(x=x ).\n1 1 1\nBecausetheprobabilityoveralleventsshouldbe1,anyprobabilityfunctionshouldbesubject",
    "1.1 Math Basics 23\nto:\n(cid:88)\nPr(x ) = 1 (1.28)\ni\nxi∈X\nwhere X is the set of all events. A probability function can be defined on two variables or\nmore. Hereareafewwidely-usedcases.\n• JointProbability. Itistheprobabilitythattwoeventsx andy happenatthesame\n1 1\ntime,denotedasPr(x ,y ). Asaspecialcase,thejointprobabilitywillbedecomposed\n1 1\nintotheproductoftheprobabilitiesofx andy ,ifx andy areindependentofeach\n1 1 1 1\nother.\nPr(x ,y )=Pr(x )Pr(y ) (1.29)\n1 1 1 1\n• Conditional Probability. It is the probability that x happens in the presence of y\n1 1\nhappening,denotedasPr(x |y ). Pr(x |y )canbedefinedas:\n1 1 1 1\nPr(x ,y )\n1 1\nPr(x |y ) = (1.30)\n1 1\nPr(y )\n1\n• MarginalProbability. Itisanotherwaytodefinetheprobabilityofasinglevariable.\nGiventhejointprobabilityontwovariables,themarginalprobabilitydefinesthat\n(cid:88)\nPr(x ) = Pr(x ,y ) (1.31)\n1 1 j\nyj∈Y\nwhereY istheeventspaceofy . Eq(1.31)saysthatPr(x )isunconditionedonY.\nj 1",
    "Another note on joint probability. In some cases, one would like to use conditional\nprobabilitiestorepresentajointprobability. Tothisend,onecanrewritethejointprobability\nbytheproductruleorthechainrule,likethis\nPr(x ,y )=Pr(x |y )Pr(y ) (1.32)\n1 1 1 1 1\nSofar,wehavedefinedseveralkindsofprobabilityondiscretevariables. Forcontinuous\nvariables,wedonothavea“probability”atacertainpoint. Instead,wehaveadensityforthat\npoint. More formally, given a continuous variable x, a probability density of x is written\nas Pr(x). Suppose x∈R. The probability of x lying in the interval [a,b] is defined via an\nintegral:\n(cid:90)\nb\nPr(x∈[a,b]) = Pr(x)dx (1.33)\na",
    "24 Chapter 1. Foundations of Machine Learning\nObviously,wehave\n(cid:90)\n+∞\nPr(x)dx = 1 (1.34)\n−∞\nForotherproperties,suchasjointprobabilityandconditionalprobability,theformsfor\ncontinuousvariablesarealmostthesameasthosefordiscretevariables. Wejustneedtoreplace\nthesumsintheformulaswiththeintegrals.\n2. Distribution and Expectation\nAprobabilitydistribution(ordistributionforshort)istheprobabilitiesofdifferentvalues\nfor a variable. It is defined by probability functions (for discrete variables) or probability\ndensityfunctions(forcontinuousvariables). Forexample,auniformdistributiononadiscrete\nvariable x that chooses values from {x ,x } can be described as Pr(x ) = 1/2 because\n1 2 i\nPr(x )=Pr(x )andPr(x )+Pr(x )=1;Auniformdistributiononacontinuousvariable\n1 2 1 2\nin[−2,2]canbedescribedasPr(x)=1/4becausePr(x)isaconstantforanyx∈[−2,2]and\n(cid:82)\n2\nPr(x)dx=1. Statisticians have developed many distributions for describing the world\n−2",
    "we are living in, such as binomial distribution, Bernoulli distribution and Gaussian/normal\ndistribution. Onecanfinddetailsofthesedistributionsinmosttextbooksonstatistics[McClave\nandSincich,2006;Freedmanetal.,2007].\nFordescribingpropertiesofavariable,apopularmeansistocomputetheexpectedvalue\norexpectationofthevariable. Letxbeadiscretevariablethattakesvaluesfrom{x ,...,x },\n1 n\nandPr(x)beadistributiononx. Theexpectedvalueofxisdefinedtobe\nn\n(cid:88)\nE (x) = x ·Pr(x ) (1.35)\nx∼Pr(x) i i\ni=1\nwherethesubscriptx∼Pr(x)indicatesthatxfollowsthedistributionPr(x). Inmanycases,\nwe can drop the subscript and rewrite it as E(x). E(x) is essentially the weighted average\nvalueofxunderthedistributionPr(x). Itisameasureofcentraltendency,andissometimes\ncalledthemeanofavariable(denotedasµ).\nThen, we can define the variance of a variable as the squared variation of the variable\nfromthemeanvalue,likethis\nVar(x) = E[(x−E(x))2] (1.36)",
    "Informally,itdescribeshowfarthevaluesarefromthemean. Var(x)isusuallywrittenasσ2,\nwhereσ iscalledstandarddeviation.\nForacontinuousvariablex∈R,theexpectedvalueisdefinedas:\n(cid:90)\n+∞\nE(x) = x·Pr(x)dx (1.37)\n−∞\nwherePr(x)isaprobabilitydensityfunction. Forcomputingthevarianceofx,wejustreuse",
    "1.1 Math Basics 25\nEq. (1.36).\n3. Entropy\nEntropy is one of the most important tools of describing random variables and processes\n[Shannon, 1948b]. It is a measure of expected surprise. The more deterministic the events\noccur, the less surprise and less information there will be. For simplicity, we restrict the\ndiscussionondiscretevariableshere2. LetxbeavariableandPr(x)beadistributiononx.\nTheentropyiswrittenas:\nn\n(cid:88)\nH(x) = − Pr(x )·log Pr(x ) (1.38)\ni b i\ni=1\nwherebisthebaseofthelogarithmfunction. Thevalueofbistypicallysetto2,10ande.\nInadditiontoobtainingtheentropyofasingledistribution,wecandeterminethesimilarity\noftwodistributionsfromtheentropypointofview. Supposep(x)andq(x)aredistributions\nonx. Therelativeentropyofpwithrespecttoq isdefinedtobe:\nn\n(cid:88) p(x i )\nD (p||q) = p(x )·log (1.39)\nb i b q(x )\ni\ni=1\nWe can treat p(x ) as a weight to the log likelihood ratio log\np(xi)\n. Hence, D (p||q) is a\ni b q(xi) b",
    "weighted sum of the likelihood ratios over all possible values. A smaller value D (p||q)\nb\nindicates that distributions p and q are closer. For example, p and q will be identical if\nD (p||q)=0. TherelativeentropyisalsocalledtheKullback-Leibler(KL)divergence. Note\nb\nthattherelativeentropyisasymmetric,i.e.,wecannotguaranteeD (p||q)=D (q||p).\nb b\nAnotherconceptthatispopularinmachinelearningiscross-entropy. Itisameasureof\ntheinformation(intermsofthetotalnumberofbits)thatweneedtotransittheeventsfroma\nsourceinonedistributionwithanotherdistribution. Moreformally,wewritethecross-entropy\nofthedistributionpwiththedistributionq asH (p,q). Itisdefinedtobe:\ncross\nn\n(cid:88)\nH (p,q) = p(x )·log q(x ) (1.40)\ncross i 2 i\ni=1\nLikerelativeentropy,cross-entropyisasymmetrictoo. Bothrelativeentropyandcross-entropy\narewidelyusedindesigningtheobjectiveoflearningNLPsystemsalthoughtheyaredifferent\nquantities. Thedifferenceliesinthatrelativeentropycalculatestheaveragenumberofbits",
    "whenreplacingpwithq,whilecross-entropycalculatesthetotalnumberofbitsinthesame\nprocess.\n2Forcontinuousvariables,wehavesimilarcalculations.",
    "26 Chapter 1. Foundations of Machine Learning\n1.2 Designing a Text Classifier\nClassificationisoneofthemostcommonproblemsinmachinelearning. Itaimsatautomat-\nically categorizing something into a set of classes. These classes are called labels, or tags,\nor categories. In general, a program of classification is called a classifier or classification\nsystem. Thereareavastnumberofpracticalapplicationsofclassifiers. Asimpleexampleis\nspamfilteringinthatoneneedstolabelanemailas“spam”or“not-spam”. Morechalleng-\ningexamplesincludeclassifyingcomputedtomographyimagesoforgansinto“normal”or\n“not-normal”,determiningwhetherapieceofChinesetextiswrittenbynativespeakersornot,\nlabelingapatentapplicationwithasetofIPCcodesitbelongsto,andsoon.\nManymachinelearningtheoriesandalgorithmsaremodeledandtestedonclassification\ntasks. Followingthisconvention,weconsidertextclassificationasanexampletogetstarted.\nAssumethatwehaveacorpuslikethis.\nText Label\nThegamewaswonderful. Not-food",
    "I’vetriedmybesttorecreateitinmykitchen. Ittastesheavenly. Food\nForcenturiesseaweedwasconsideredafoodfornormalpeople. Food\nHaveyoufinishedyourcodingworktoday? Not-Food\nIwaswonderinghowyoucouldmissthebus. Not-Food\nIlikefruitbecauseitisgoodforhealth. Food\nNaturallanguageprocessingresearchisamazing. Not-Food\n... ...\nEachlineofthecorpusisatupleofapieceoftext(wesimplycallitadocument)anda\nlabelthatindicateswhetherthetextisaboutfoodornot. Wecallsuchtuplessamples,ormore\npreciselylabeledsamples. Labeledsamplesareessentiallyquestion-answerpairsalthough\ntheydonotstrictlyfollowthegeneralformsofquestionsandanswers. Forexample,inthe\nsamples presented above, one can take a document as a question and take its label as the\nanswer. Inthenextfewchapters,wewillshowthatsuchaformofdescribingmachinelearning\nproblemsisgeneralandfitsmostproblemsinNLP.\nNext, let us assume that we have a classifier that learns from those samples the way of",
    "labeling documents. The classifier is then used to label every new document as “Food” or\n“Not-Food”. Forexample,forthetext\nFruitisnotmyfavoritebutIcanenjoyit.\ntheclassifierwouldcategorizeitas“Food”.\nHowever, text classification, though seems simple on the surface, is much more than\nclassifying or sorting unlabeled samples into classes. It presents a wide variety of issues,\nespeciallywhenconsideringtheambiguitiesandrichnessoflanguage. Modernclassifiersare\nnot a system comprising a set of hand-crafted rules. They instead model the classification\nprobleminaprobabilisticmanner,makingitpossibletolearntheabilityofclassificationfrom\nlarge-scalelabeleddata.",
    "1.2 Designing a Text Classifier 27\n1.2.1 Problem Statement\nLetxbeadocumentandcbealabel. Hereweassumeaprobabilisticclassifierwhichwould\nestimatehowlikelywechoosecasthelabelofx,denotedasPr(c|x). Pr(c|x)isingenerala\nclassificationmodel. Itdescribesadistributionoverthesetofallpossiblelabels,satisfying\n(cid:88)\nPr(c|x) = 1 (1.41)\nc∈C\nwhereC isthelabelset. Foranydocument,wechoosethemostprobablelabelasoutputvia\ntheclassificationmodel,likethis\ncˆ = argmaxPr(c|x) (1.42)\nc∈C\nwherecˆisthe“best”labelpredictedbythemodel. argmaxistheabbreviationofthearguments\nofthemaxima. Itreturnsthevalueoftheargumentthatmaximizessomefunction.\nEq. (1.42)isthefundamentalequationofclassification. Itimpliesthreeproblems\n• Themodelingproblem. Pr(c|x)isacomputationalchallengebecauseitisnotobvious\nhow to obtain the value of Pr(c|x) for each pair of x and c. To make an adequate\nmodel,onemayneedtorepresentxandcinsomewaythatiseasytouse,andmayneed",
    "to develop some mathematical form connecting x and c together with the algorithms\nnecessarytocomputetheform.\n• Thelearningproblem. Fromastatisticallearningpointofview,thegeneralformof\nPr(c|x)representsarangeofmodelsconfiguredwithdifferentvariablesorparameters.\nThesemodelsareessentiallyofthesameformbutwouldbehavedifferentlyifwechoose\ndifferentvaluesofthoseparameters. Thus,weneedtochoosea“good”modelamong\nthem. Thisistypicallyaddressedbyoptimizingtheparametersonlabeleddatabysome\ncriterion.\n• The prediction problem. We are addressing a binary classification problem here.\nPredictingdocumentclassisthustrivialaswejustneedtodeterminewhichclassismore\nprobablethantheother. However,onecanhardlyimaginehowdifficulttheprediction\nproblemisintherealworld,especiallywhenpredictingtreeorgraph-likestructuresand\nothernon-linearstructures3. FormanyNLPproblems,predictionneedseffectiveand\nefficientsearchalgorithms.\nThese problems are general and cover many machine learning and natural language",
    "processingtasks. Binaryclassification,thoughisoneofthesimplestcases,canfullycomplete\nthegoalofgettingfamiliarwithmachinelearning. Ontheotherhand,classificationhasseveral\nvariants. Herearetwoexamples.\n• Multi-classclassification. Itisanupdatedversionofbinaryclassification. Inmulti-class\n3Predictingtreesorotherstructuresisnotrecognizedasastandardsub-problemofclassification.Itistypically\nreferred to as structure prediction. We will show in the later sections that both classification and structure\npredictioncanshareasimilarmachinelearningparadigm.",
    "28 Chapter 1. Foundations of Machine Learning\nclassification,oneneedstoclassifysamplesintooneofthreeormoreclasses.\n• Multi-labelclassification. Thismightbeconfusingbecausemulti-classclassification\nand multi-label classification seem to be the same thing. By conventional use of the\nterms,multi-labelclassificationisreferredtoasassigningmultiplelabelstoasample.\nBycontrast,theproblempresentedinEq. (1.42)isasingle-labelclassificationproblem.\nClassification would be more interesting and challenging if we extend it to the case of\ndealingwithhierarchicaldata. Forexample,forbiologicalandpatentdata,someclassescan\nbegroupedintoasuper-class. Thismakesahierarchyoftheclassesandrequiresahierarchical\nclassificationschema.\n1.2.2 Documents as Feature Vectors\nThe first problem we confront in designing text classification models is how to represent a\ndocument. Treatingxasastringissimplynotagoodsolution. Onemaywantarepresentation\nbywhichahumanbeingcanunderstandthetext. Forexample,wecanparseeachsentencein",
    "adocumentintoasyntaxtreeandusetreesasatextrepresentation. This,however,requires\neffortsfordevelopingadditionalNLPtools(suchassyntacticparsers).\nRepresentation, of course, is a fundamental issue in NLP. We skip here those diverse,\nstate-of-the-artmodels,butpresentasimpleandeffectivemodel—thebag-of-words(BOW)\nmodel. The bag-of-words model is a feature-based model of representing documents. In\nmachinelearning,afeatureisapropertyofasample. Onecandefineafeaturenotonlyas\nsomeconcreteattribute,suchasanameandagender,butalsoasaquantitythatiscountable\nformachinelearningsystems,suchasarealnumber.\nInthebag-of-wordsmodel,afeaturecorrespondstotheoccurrencetimesofaword. LetV\nbeavocabulary. Adocumentcanberepresentedasa|V|-dimensionalfeaturevector. Each\ndimensiondescribesawordcountfeature. Itcountstheoccurrenceofthei-thwordofV inthe\ndocument. Moreformally,letxbeafeaturevector. Thei-thentryofxisdefinedas:\nx = count(V ) (1.43)\ni i",
    "wherecount(·)isacountingfunction. Consider,forexample,thefollowinglinesoftext4.\nAsIwenttoBonner\nImetapig\nWithoutawig,\nUponmywordandhonor.\n4ThetextisfromMotherGooserhymes.",
    "1.2 Designing a Text Classifier 29\nWethenhaveavocabularyextractedfromthecorpus5,likethis\nV = {“a”, “and”, “as”, “Bonner”, “honor”,\n“I”, “met”, “word”, “my”, “pig”,\n“to”, “upon”, “went”, “wig”, “without”,\n“,”, “.”}\nEachlineofthetextcanbeseenasadocumentandrepresentedasafeaturevector. See\nbelowforthefeaturevectorsgeneratedbyusingthebag-of-wordsmodel.\na\ndna\nsa\nrennoB\nronoh\nI\ntem\ndrow\nym gip\not\nnopu tnew\ngiw\ntuohtiw\n, .\nAsIwenttoBonner [ 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 ]\nImetapig [ 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 ]\nWithoutawig, [ 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 ]\nUponmywordandhonor. [ 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 ]\nThebag-of-wordsmodeldefinesavectorspace6. Inthisspace,thesimilarityoftwovectors\nismeasuredinsomewaylikedot-product. Ithelpswhenonewantstoestablishtherelationship\nbetweendocuments—twodocumentswithmoreoverlappingwordsaremoresimilar. This\nintuitivepicturehasguidedmanypeoplewhenbuildingclassificationsystems.",
    "The beauty of the bag-of-words model comes from its simple form in that any word\nis independent of other words. The independent assumption makes it possible to encode a\ndocument with almost infinite word relations as a countable, feasibly sized vector. On the\notherhand,representingadocumentasafeaturevectorofwordcountsisnottheonlyoption.\nAsanimprovement,onemighttakemorecontextintoaccount,and/ordesignmorepowerful\nfeatures. Thisleadstoanactivelineofresearchontextrepresentationmethods,rangingfrom\nheuristics-basedmethodstorepresentationlearningmethods. Wewillseeafewoftheminthe\nsubsequentchapters.\n1.2.3 Linear Classifiers\nLinearclassifierisoneofthesimplestclassificationmodels. Supposewetakeafeaturevector\n(cid:104) (cid:105) (cid:104) (cid:105)\nx= x ... x as input, and take a weight vector w= w ... w and a scalar b as\n1 n 1 n\nparameters. Alinearfunctionhasaformlikethis\ns(x,w,b) = w·x+b\n= w x +w x +...+w x +b (1.44)\n1 1 2 2 n n\n(cid:104) (cid:105)",
    "wherebisabiasterm. Thedotproductx·wisalinearcombinationof x ... x where\n1 n\neachx isweightedbyw . Foramorecondensedformulation,wecandefineanewinputvector\ni i\n5Weremovedthecaseofthewordatthebeginningofeachline.\n6Avectorspaceshouldbeclosedundervectoradditionandscalarmultiplication.",
    "30 Chapter 1. Foundations of Machine Learning\n(cid:104) (cid:105) (cid:104) (cid:105)\nx′= x ... x 1 andanewweightvectorw′= w ... w b . WethenrewriteEq.\n1 n 1 n\n(1.44)as:\ns(x′,w′) = s(x,w,b)\n= w′·x′ (1.45)\nInthefollowing,wedropthebiastermbforsimplicityanduses(x,w)todenotealinear\nfunction. Forclassification,alinearfunctionisusedtodescribeclassmembership. Eachclass\nisassignedascorebythefunctionequippedwithauniqueweightvector. Consideragainthe\nbinaryclassificationasanexample. Letc andc betwoclasses. Wecandefinetwoweight\na b\nvectorsw andw sothatthefunctioncandiscriminatebetweenc andc .\na b a b\nForprediction,wecaninferaclassbasedons(x,w). Toachievethis,activationfunctions\nψ(·)areingeneralusedformappingthevalueofs(x,w)toaclass. Forexample,forbinary\nclassification,wecandefineanactivationfunctionlikethis,\n(cid:40)\nc x>0\na\nψ(x) = (1.46)\nc otherwise\nb\nThen,wemakeapredictionby\n(cid:40)\nc s(x,w )−s(x,w )>0\na a b\nψ(s(x,w )−s(x,w )) = (1.47)\na b\nc otherwise\nb",
    "As s(x,w )−s(x,w )=s(x,w −w ), the final prediction function is ψ(s(x,w −w )).\na b a b a b\nWe call it a discriminant function. Note that s(x,w −w ) is linear. So this is a linear\na b\ndiscriminantfunction.\nAdiscriminantfunctionassignsaninputvectorxdirectlytoaclassbutitdoesnotdescribe\nhowlikelyaclasswouldappeargivenx. Thereareotheractivationfunctionsforgeneratinga\ndesirableoutput. Forexample,wemaywantaprobability-likeoutput(seeEq. (1.42)),and\nthusdefineψ(·)asanormalizedfunction7. Then,theclassificationprobabilitiesaregivenby\ntheequation\n(cid:104) (cid:105) (cid:16)(cid:104) (cid:105)(cid:17)\nPr(c |x) Pr(c |x) = ψ s(x,w ) s(x,w )\na b a b\n(cid:104) (cid:105)\n=\ns(x,wa) s(x,w\nb\n)\n(1.48)\ns(x,wa)+s(x,w\nb\n) s(x,wa)+s(x,w\nb\n)\nwhere ψ(·) is a vector function8. It normalizes the entries of the input vector by the sum\nof these entries. The decision rule is simple: we predict c if Pr(c |x) > Pr(c |x), and\na a b\nc otherwise. Since\ns(x,wa)\nand\ns(x,w\nb\n)\nshare the same denominator, the\nb s(x,wa)+s(x,w",
    "b\n) s(x,wa)+s(x,w\nb\n)\npredictioncanalsobemadebycomparingthenumerators,i.e.,wearedoingthesamething\nas that in Eq. (1.47). In subsequent chapters, we will show that the trick of transforming\n7Anormalizedfunctionisafunctionwhoseintegraloveritsdomainisequalto1.\n8Avectorfunctionreadsavectorandreturnsanewvector.",
    "1.2 Designing a Text Classifier 31\n8\n7\n6\n5\n4\n3\n2\n1\n0\n0 1 2 3 4 5 6 7 8\nx\n1\nx\n2\nFood Not-food\n2\nHyperplane3\nHyperplane\n1\ne\nn a\npl\ner\np\ny\nH\nFigure1.1: Datapointsandseparatinghyperplanesintwodimensions. Therearetwoclasses\nof data points (food and non-food). Both hyperplanes 1 and 2 separate the space into two\nsub-spaceswherethetwoclassesofdatapointsareisolated. Inthissense,theproblemhereis\nlinearlyseparable. Ontheotherhand,hyperplane3failstoseparatethetwoclasses,thatis,the\ndatapointsinthesameclassareclassifiedintotwodifferentclasses.\ncomparingprobabilitiestocomparingreal-valuedscoresisfrequentlyusedforaddressingNLP\nproblems.\nForeaseofunderstanding,onecanseealinearclassificationmodelasahyperplane(or\ndecision surface, or decision boundary) that separates data points into different groups.\nFigure1.1showsexamplehyperplanesina2Dspacewhereeachxisadatapoint. Hyperplanes\n1and2successfullyseparatethedatapointsintothecorrectclasses,whilehyperplane3fails",
    "todoso. Inthissense,thetaskofclassificationistofindhyperplanesthatmakethecorrect\nseparationofdatapoints.\nItisworthyofnotethatlinearityisthebasisofmanyclassifiersalthoughmostofthem\ndo not exist in the same form as Eqs. (1.44 - 1.45). A linear model can be nearly a perfect\nsolutioniftheproblemislinearlyseparable9. Evenfornon-linearlyseparableproblems,linear\nmodelscanbemarriedtoothermodelswithanon-linearseparationability. Agoodexample\nisthatonecanachievenon-linearclassificationbymarryingalinearmodelwithanon-linear\nactivationfunction.\n9Linearseparabilitychecksifthereisawaythatweputahyperplanetoseparateagroupofdatapointsfromthe\nremainingdatapoints.Forexample,theproblemshowninFigure1.1islinearlyseparable.",
    "32 Chapter 1. Foundations of Machine Learning\n1.2.4 Generative vs Discriminative\nThere are two ways, though not restricted to linear models, to make use of linearity in\nclassification — generative models and discriminative models. While most statistical\nclassifiersareofthemodelingvariety,theychoosethebackbonedesignfromeitherorbothof\nthesetwotypesofmodels.\n1. Generative Models\nA goal of classification is to learn Pr(c|x). Generative models do not explicitly model this\nconditionalprobability. Instead,theymodelthejointprobabilityPr(x,c),andusetheBayes’\nruletocomputePr(c|x). Thisisgivenbythefollowingequation.\nPr(x,c)\nPr(c|x) =\nPr(x)\nPr(c)Pr(x|c)\n= (1.49)\nPr(x)\nwherePr(x,c)isrewrittenasPr(x|c)Pr(c). Foranoptimalclasscˆ,wechooseaclasscby\nmaximizingPr(c|x)(seeEq.(1.42)). Sincethedenominatorxisaconstantforanyc,wejust\nneedtomaximizethenumerator. Then,werewriteEq.(1.42)intheform\ncˆ = argmaxPr(c|x)\nc∈C\nPr(c)Pr(x|c)\n= argmax\nPr(x)\nc∈C\n= argmaxPr(c)Pr(x|c) (1.50)\nc∈C",
    "wherePr(c)isthepriorofc,andPr(x|c)istheconditionalprobabilityoftheinputdocument\nvectorxgivenc. ComputingPr(c)iseasy. Forexample,themaximumlikelihoodestimation\n(MLE)definesPr(c)asarelativefrequency:\ncount(c)\nPr(c) = (1.51)\n(cid:80) count(c′)\nc′∈C\nwherecount(c)countstheoccurrencesofcinacorpus.\nBut computing Pr(x|c) is non-trivial as data sparseness prevents us from accurately\nestimatingtheprobabilityofahigh-dimensionaldocumentvector. Recallthatthebag-of-words\nmodeldefinesx asthewordfrequencyofV inthedocument. Weassumeherethatthefeature\ni i\n(cid:104) (cid:105)\nvectorx= x ... x isgeneratedbyamultinomial(p (c),p (c),...,p (c)),wherep (c)\n1 n 1 2 n i\nistheprobabilityofV occurringgivenc. BasedonMLE,wecanestimatep (c)bytherelative\ni i\nfrequencyestimation:\ncount(V ,c)\ni\np (c) = (1.52)\ni (cid:80)\ncount(V ,c)\n1≤i′≤n i′",
    "1.2 Designing a Text Classifier 33\nwhere count(V ,c) is the number of occurrences of V in all documents labeled as c. Then,\ni i\nPr(x|c)isgivenby\nPr(x|c) =\n(cid:0)(cid:80)n\ni=1 x i\n(cid:1)\n! · (cid:89)\nn\np (c)xi (1.53)\ni\nx !·x !·...·x !\n1 2 n\ni=1\nSubstitutingEq. (1.53)intoEq. (1.50),wehave\ncˆ = argmax Pr(c)·\n(cid:0)(cid:80)n\ni=1 x i\n(cid:1)\n! · (cid:89)\nn\np (c)xi (1.54)\ni\nx !·x !·...·x !\nc∈C 1 2 n\ni=1\n(cid:0) (cid:1)\nNote that\n(cid:80)n\ni=1\nxi !\nis independent of any c. We drop it in argmax, and rewrite the\nx1!·x2!·...·xn!\nright-handsideoftheequationinlogscale:\nn\n(cid:88)\ncˆ = argmax log(Pr(c))+ x ·log(p (c)) (1.55)\ni i\nc∈C\ni=1\nObviously,thisisalinearmodel. Itdefinesthefeaturevectorandweightvectorasbelow\n(cid:104) (cid:105)\nx = 1 x ... x (1.56)\n1 n\n(cid:104) (cid:105)\nw = log(Pr(c)) log(p (c)) ... log(p (c) (1.57)\n1 n\nSuchaformissometimescalledalog-linearmodel,asthelinearitycomesfromtransforming\ntheoriginalproblemviaalogisticfunction.",
    "Ingeneral,Eq(1.55)iscalledthemultinomialnaiveBayesapproach. Thereare,ofcourse,\nthe naive Bayes variants for other types of feature vectors. For example, for binary value\nfeaturevectors,onecanassumeaBernoullidistributiononeachentryofavectoranddesign\na Bernoulli naive Bayes classifier; for vectors with continuous features, one can assume a\nGaussiandistributionovercontinuousdataanddesignaGaussiannaiveBayesclassifier.\n2. Discriminative Models\nPr(x,c) Pr(c)Pr(x|c)\nThe model defined by Pr(c|x)= = (see Eq. (1.49)) is called generative\nPr(x) Pr(x)\nbecauseitassumessomewayofgeneratingdataxgivenlabelc. TheideaistousePr(x,c)as\napivottocomputePr(c|x). AsanalternativepossibilityformodelingPr(c|x),discriminative\nmodelsdonottrytomodelthedistributionofxbutestimatePr(c|x)directly. Anexampleis\nlogisticregression. Forbinarytextclassification,anaiveBayesclassifierpredictslabelc for\na\nagivendocumentxonlyifthefollowingfunctionispositive:\nPr(c |x)\na\nf (x) = log (1.58)\na\nPr(c |x)\nb",
    "34 Chapter 1. Foundations of Machine Learning\nOnecanassumethatthisquantityfollowsalinearmodel:\nPr(c |x)\na\nlog = w·x (1.59)\nPr(c |x)\nb\nSincePr(c |x)=1−Pr(c |x),wehave\nb a\n1\nPr(c |x) = (1.60)\na\n1+exp(−w·x)\nThis is a logistic function, or more precisely a Sigmoid function. With such a model, we\npredictc onlyifw·xispositive. Eq. (1.60)isalsocalledthelogisticregressionclassifier. It\na\nissimplyadiscriminativeanalogofthenaiveBayesclassifier.\nAnadvantageofdiscriminativemodelsisthattheyofferflexibilityinviewingclassification\n(orothermachinelearningproblems)fromdifferentangles. Whilegenerativemodelstryto\nestimatethedatadistributionofx,discriminativemodelstrytofindagoodboundarybetween\nclasses. Discriminative models, therefore, care more about which class is prioritized over\nanother given x, or in possibility language which class is more likely to appear, instead of\nmakingassumptionsonindividualdatapoints. Thismakesitpossibletolearnaclassifierby",
    "minimizingthenumberofsomeerrors,notnecessarilyguaranteeingthemaximumlikelihood\nonthosedatapoints. Thisapproachisgenerallycallederror-drivenlearning.\nTherearemanywaystodefineerrors. Likegenerativemodels,onecanlearnadiscrim-\ninative model by fitting parameters w to maximize the likelihood on the training data. Let\n{(x(1),c(1)),...,(x(K),c(K))} be a set of labeled documents, where x(k) is a document and\nc(k) isthecorrespondingclass. Thebestparametervectorwˆ isgivenbytheequation\nK\n(cid:88)\nwˆ = argmax logPr(c(k)|x(k)) (1.61)\nw\nk=1\nTakingEq. (1.60),theprocesscanbeseenasmaximizingthelikelihood\nK\n(cid:88)\nwˆ = argmax logδ(c ,c(k))Pr(c |x)+logδ(c ,c(k))Pr(c |x)\na a b b\nw\nk=1\nK\n(cid:88) 1\n= argmax logδ(c ,c(k)) +\na\n1+exp(−w·x(k))\nw\nk=1\n(cid:16) 1 (cid:17)\nlogδ(c ,c(k)) 1− (1.62)\nb\n1+exp(−w·x(k))\nwhere δ(·,·) is an indicator function that returns 1 if the two arguments are equal, and 0\notherwise.\nAlternatively,wecantrainthemodelbyminimizing0-1errors,thatis,wecountanerror",
    "1.2 Designing a Text Classifier 35\nwhentheoutputisnotthecorrectlabel. Thisprocesscanbeformulatedas\nK\n(cid:88)\nwˆ = argmin δ(cˆ(k),c(k)) (1.63)\nw\nk=1\nwherecˆ(i) isthepredictionmadebytheclassifier. The0-1errorcanbefurtherextendedby\ntakingtheposteriorintoaccount:\nK\n(cid:88)\nwˆ = argmin Pr(cˆ(k)|x(k))·δ(cˆ(k),c(k)) (1.64)\nw\nk=1\nThetrainingobjectiveplaysanimportantroleindiscriminativemodels. Thistopic,however,\nis so broad and beyond the scope of this section. We will present some in Section 1.3.4.\nAs another bonus, discriminative models do not restrict features to forming a probabilistic\ngenerative story. In a broader sense, x could be any feature vector that is designed by\nresearchers and engineers. One does not even need to guarantee the probabilistic meaning\nfor these features. For example, let g(x) be the output of another system, say some scores.\nFollowingEq. (1.60),anewbinaryclassificationmodelcanbedesignedinalogisticregression\nmanner:\n1\nPr(c |x) = (1.65)\na\n1+exp(−w·g(x))",
    "Forlearningg(x),onecaneitherpre-trainitonsomeadditionaldata,ortrainitjointlywithw\n(i.e.,theparametersoftheupper-levelmodelPr(c |x)).\na\n1.2.5 OOV Words and Smoothing\nTheout-of-vocabulary(OOV)problemoccurswhensomeofthewordsofadocumentarenot\nfoundinthevocabulary. OOVwordsarecommoninNLPbecausenewwordsarealwaysthere\nnomatterhowmuchtextwehaveseen. Figure1.2givestwocurvestoillustratethisproblem.\nAsshowninFigure1.2(a),newwordscontinuouslyappearwhenmoredataisavailable. When\nwefixthedatathatisusedfortestingthecoverageofavocabulary,OOVwordsremainevenif\nwehaveanextremelylargevocabulary(Figure1.2(b)).\nForpracticalsystems,OOVwordsarecommonbecausethevocabularyisoftenrestricted\ntoa“small”numberofentries. Astandardmethodistokeepthetop-nmostfrequentwords\nand discard the rest. In this case, OOV words are treated as unknown words. For example,\na new symbol <unk> is introduced into the vocabulary so that all OOV words are denoted\nas<unk>. Amoreaggressiveideaistobuildanopen-vocabularysystemthatacceptsevery",
    "possibleword,butitwouldrequiremoresophisticatedalgorithmsandprobablyatask-specific\ndesignofdatastructures. The<unk>trickisstillthedefactostandardforthedevelopmentof\ncurrentNLPsystems.\nForwordsthatarealreadyinthevocabulary,therearealsounseenwordsthatareabsent\nintheparameterestimationphasebutappearinanewdocument. Anaiveimplementationof",
    "36 Chapter 1. Foundations of Machine Learning\n20\n16\n12\n8\n4\n0\n200 400 600 800 1,0001,200\nDatasize\nezisyralubacoV\n(·104)\n100\n80\n60\n40\n20\n(·105)\n0 4 8 12 16 20\nVocabularysize\negatnecrepVOO\n(%)\n(·104)\nFigure 1.2: Data size (in number of words), vocabulary size and percentage of OOV. The\nstatisticsarecollectedontheEnglishdataprovidedintheWMT21Zh-Entranslationtask. The\nmoredataweuse,thelargervocabularywehave. Theincreaseinvocabularysizecontinues\nevenifwebuildthevocabularyondataofmorethan100millionwords. However,theslope\nofthecurvetendstobesmallerasmoredataisinvolved. Interestingly,theOOVpercentage\nconvergestoacertainlevelasthevocabularysizeincreases,indicatingthatnewwordsalways\noccurnomatterhowmanywordswehaveobserved.\nthemodeldescribedintheprevioussectionwouldbetoughwhendealingwithunseenwords.\nForexample,Eq. (1.52)willsimplygiveazeroprobabilityifthewordV doesnotoccurin\ni\nanytrainingexamplelabeledwithc. Inconsequence, foranewexamplecontainingV , Eq.\ni",
    "(1.53)willassignitazeroprobability. Thisresultisobviouslyunreasonable. However,we\nshouldnotsimplyattributeittothemodeldesignitself. Instead,aprimaryreasonforthisis\ntheinsufficientdatausedforparameterestimation. ThisisalsoexplainedbyZipf’sLaw: a\nsmallnumberofwordsoccurquiteoften,whilealargenumberofwordsoccurrarely.\nHowever,wecannotsupposethatwealwayshaveaccesstosomedatawhereeveryword\noccurs sufficiently. Alternatively, one couldadopt smoothing techniquesto redistribute the\nprobability over the vocabulary. Consider Eq. (1.52) as an example. We can add a small\nnumbertoeachV ,andrewritetheequationas:\ni\ncount(V ,c)+α\ni\np (c) = (1.66)\ni (cid:80)\ncount(V ,c)+n·α\n1≤i′≤n i′\nwhereαisthedefaultcountthatweassigntoeachword. Inthisway,p (c)givesanon-zero\ni\nprobability even if count(V ,c)=0. Eq. (1.66) is doing something like subtracting word\ni\ncounts from high-frequency words and reassigning the subtracted counts to low-frequency",
    "words. This method is called additive smoothing or add-α smoothing. It is one of the\nsimplestsmoothingmethods. Forothermethods,wereferthereadertolanguagemodeling\npaperswheresmoothingisinheavyuse[ChenandGoodman,1999].",
    "1.3 General Problems 37\n1.3 General Problems\nBuildingasimpletextclassifierisagoodstartbutnotenoughforsolvingcomplicated,diverse\nreal-worldproblems. Foramoregeneralpictureofhowmodernmachinelearningsystems\nwork,wenowdiscusssomeproblemsthatareimportantwhendesigningsuchsystems.\n1.3.1 Supervised and Unsupervised Models\nSupervised learning deals with labeled samples, by which we mean that an input x is\nassociatedwithanoutputy. Givenasetofinput-outputpairs{(x(1),y(1)),...,(x(K),y(K))},\nthetaskhereistolearnafunctionf(·)thatmapseachx(k) toy(k):\ny(k) = f(x(k)) (1.67)\nThisprocessiscalledsupervisedbecauselearningf(·)isguidedbythemanuallyannotated\nanswery(k) forx(k). Ingeneral,y(k) iscalledtheground-truthorgold-standardlabelofx(k).\nAfterthat,whenanewinputx comes,weusethelearnedfunctionf(·)topredicttheoutput.\nnew\nWewillsaythatthesupervisedlearningsucceedsifthepredictionf(x )isthesameasthe\nnew\nground-truthy .\nnew\nThe vast majority of NLP can be framed as supervised learning problems. Assigning a",
    "classtoadocumentisnodoubtoneofthesimplestcases. OtherNLPtasksincludebutarenot\nlimitedtoproducingasequenceoflabels,apieceoftext,asyntaxtree,andagraph.\nIncontrasttosupervisedlearning,unsupervisedlearningdealswithunlabeledsamples,in\notherwords,foreachsample,wehaveaninputxintheabsenceofthecorrectoutputy. Inthis\ncase,weneedanalgorithmthatlearnsfromtheunlabeleddata{x(k)}themappingfunction\nfromx(k) tosomeoutput. Sincethereisnohumaninterventionontheoutputofthefunction,\nalgorithmsofthiskindneedtodiscoverpatternsin{x(k)}andoptimizethewaywerepresent\n{x(k)}andfunctionoutputsbysomecriteria. Thesecriteriaaretypicallyinspiredbyhuman\npriorknowledgesothattheresultingfunctioncouldoutputsomethingthatweexpect.\nAcommonexampleinunsupervisedlearningiswordclustering. Itgroupsasetofwords\nbyassigningsimilarwordsintothesamecluster10. Basedonsuchacriterion,wecanbiasf(·)\ntowardsoutputtingthesameclusterforsimilarwords. Amoredifficultcaseisunsupervised",
    "bilingual dictionary induction. It learns a word-level mapping between two languages\nwithouttheneedofparalleldata. Theproblemisusuallyaddressed,inpart,bymakinguseof\ntheisomorphismofwordrepresentationspacesofdifferentlanguages.\nA halfway between supervised learning and unsupervised learning is semi-supervised\nlearning. It deals with the case in which some of the input data is labeled and the rest is\nunlabeled. Thus, it can receive benefits from both supervised and unsupervised learning.\nFor example, in machine translation, we may have a certain amount of parallel data (i.e.,\nlabeleddata)andordersofmagnitudelargermonolingualdata(i.e.,unlabeleddata). Often,\na base system is learned on the parallel data in a supervised learning fashion. On top of\nit, improvements can be made from training components of the base system on large-scale\n10Itmightbedifficultandambiguoustodetermineiftwowordsaresimilarornot.WeleavethisissuetoChapter\n3.",
    "38 Chapter 1. Foundations of Machine Learning\nmonolingualdata. Thisisimplementedbyeithercombiningatranslationmodellearnedon\nthe bilingual data and a language model learned on the target-language monolingual data,\norpre-trainingpartsofthetranslationmodelonmonolingualdataandfine-tuningtheentire\nmodelonthebilingualdata.\nBroadlyspeaking,alllearningalgorithmsneedsupervision. Thissoundsweirdbecause\nunsupervisedlearningseemstonotbesignaledbyanyground-truthdata. However,froma\ngeneral learning perspective, we should not restrict ourselves to labeled data for receiving\nsupervisionsignals. Evenforanunsupervisedlearningproblem,westillneedtosupervisethe\nlearningprocessbypriorknowledgeandhiddenpatternsintheinputdata{x(k)}. Inthissense,\nunsupervisedlearningisnot“learningwithoutsupervision”.\nTaking “supervision” as a concept in a broader sense, more paradigms can be seen as\ninstancesofmachinelearning,thoughnotnecessarilybelongingtoeithersupervisedlearningor",
    "unsupervisedlearning. Anexampleisreinforcementlearning. Itmodelshowasystemmakes\nasequenceofdecisions. Thisisachievedbyoperatinganagentinanenvironment. Theagent\nreceivesafeedback(orareward)fromtheenvironmentwhenmakingadecision(ortakingan\naction). Thegoalofreinforcementlearningistolearnadecisionmodelthatmaximizesthe\nrewardalongthestepstheagenttakes. Therealrewardhereisavailableonlywhentheagent\nreachessomestate,suchastheendofagame. Assuch,reinforcementlearningcandescribe\nproblemswheretherewardisoveralongerperiod(callitdistantreward). Thisdifferentiates\nreinforcementlearningsharplyfromstandardsupervisedlearning,traditionallyconcernedwith\ninstantsupervisionsignalsthatareencodedinlabeleddatainadvance. Thischaracteristicfits\nmanyNLPproblems. Forexample,atextgenerationsystemgenerallygeneratesasequenceof\nwords from left to right, but it is hard to determine if a word is properly predicted until the\nwholetexthasbeengenerated.",
    "Anotherexampleisself-supervisedlearning. Itaddressesunsupervisedlearningproblems\ninasupervisedlearningmanner. Ageneralideaistoframetheunsupervisedlearningtaskasa\npretexttaskthatcanbeusedinsolvingtheoriginalproblem. Inthepretexttask,groundtruth\ncanbegeneratedfrominputdata. Thelearningthereforereceivessupervisionfromself-made\nsignalsinsteadofmanuallabels.\nSelf-supervisedlearninghasindeedbeenquitesuccessfulinNLP.Perhapspre-training\nisoneofthosewhichhavemadethemostincredibleprogress. Inpre-training,onecantrain\nsomemodel(suchasalanguagemodel)viaself-supervisedlearning,andthenapplypartsof\nthemodeltosomedownstreamsystem(suchasasentimentanalysissystem). Itofferstwo\nadvantages. First,theself-supervised,pre-trainedmodelscanbescaledtoahugeamountof\ndata because they require no labeled data. Second, pre-training is general itself and can be\napplied to a wide range of downstream tasks. In Chapter 7, we will see a few examples of\napplyingself-supervisedlearningtoNLPmodels.\n1.3.2 Inductive Bias",
    "Weinformallydescribe(supervised)machinelearningproblemsasaninductivereasoning\n(orinductiveinference)process: weusespecificobservations(suchaslabeleddocuments)\ntomakeageneralizedmodel(suchasaclassifier). Forexample,wemayobservethatword",
    "1.3 General Problems 39\ncookingfrequentlyoccursinsomedocumentstalkingaboutfood. Wewouldsaythat,basedon\ninductivereasoning,cookingisanimportantindicatorforallfooddocuments. This“specific-\nto-general”methodisalsocalledinductionsometimes. Oncewehaveaninducedmodel,we\ncanapplyittodescribenewobservations,asadeductionprocess.\nInduction is the most widely-used principle in designing learners of modern machine\nlearning systems11. Imagine that there is a hypothesis space (or model space, or learnable\nfunction space) consisting of all possible models that we could make. Learning a model is\nthusthesameasselectingamodelinthehypothesisspacebyinducingfromthegiventraining\nsamples. However,wecannotsimplyassumeanoraclemodelthatworkswellonallunseen\nsamples. Areasonforthisisthatsearchingforthe“best”modelinahugehypothesisspaceis\ncomputationallyinfeasible. Therewouldbeaninfinitenumberofdimensionsalongwhichwe\ncandesignmodelsifthehypothesisspaceisunconstrained. Thisproblemisessentiallysome",
    "sortofthecurseofdimensionality12. Anotherreasonisthatmanymodelsinthehypothesis\nspacecanfittrainingsampleswell,butonlysomeofthemcanfitunseensamples. Thereisa\nriskthatweselecta“weak”modelfortestdataalthoughitis“strong”fortrainingdata. Thisis\nrelevanttooverfitting,aconceptthatwewilldiscusslater.\nAnaturalsolutionistodefinepriorsonthehypothesisspaceinawaythatallowssome\nmodelstobemorepreferablethanothers. Asimpleexampleisthatwerestrictclassifiersto\nlinearmodels(seeSection1.2.3). Itisdoingsomethinglikeweimposeapriorthatexcludes\nallnon-linearmodelsfromthehypothesisspace.\nSuchapriorisgenerallycalledaninductivebias. Inanutshell,aninductivebiasisaset\nofassumptionsontheproblem13. Forexample,onecandesignmodelsincertainmathematical\nforms(i.e.,modelbias);onecanchoosespecificalgorithmsforlearningamodel(i.e,algorithm\nbias);onecanassumethewayofgeneratingsamples(i.e,samplebias),andsoon14.\nInductive biases try to tell in what way we should describe a problem. Better results",
    "aregenerallyfavorablewheninductivebiasesmeetwhatreallyhappens. Thisexplainswhy\nsolutionstosomeproblemsprefercertainmodelarchitectures(ormodelbiases). Ofcourse,\nmoreandstrongerinductivebiasescouldmakeiteasiertosolveaproblem. However,inductive\nbiasesarenotalwayshelpful,especiallywhentheyarenotclosetothereality.\nLetusconsideradicerollinggame. Supposeyouhavea6-sideddice. Beforerollingthe\ndice,youguessaside(sayanumberfrom1to6). Youwillwinifthedicelandsonthesame\nsideyouguess. Youareagamblerandtrytowinasmanytimesaspossible. Inyourexperience,\narandomguessisthebestchoiceinthisgamebecauseallsidesshouldhaveanequalchance\nofappearing(i.e. achanceof1/6). Thisistruewhenyouplayfairdice. However,oneday,\n11Notallmachinelearningmethodsshouldfollowaninductionprocessforlearningamodel.Thereareother\noptionsfordifferenttypesofproblems,includingdeductivereasoning,abductivereasoning,analogicalreasoning\n(ortransduction),andsoon[Hurley,2011].",
    "12Thecurseofdimensionalityreferstotheproblemsthatgenerallyappearasthedimensionalityofthehypothesis\nspaceincreases.Forexample,datasparsenessisacommonproblemthatariseswhenprocessinghigh-dimensional\ndata,andisthusakindofthecurseofdimensionality.\n13Amoreformaldefinitioncanbefoundinmachinelearningtextbooks[Mitchell,1997]\n14Asanasideitisworthnotingthatthetermbiasisusedinmanydifferentways,andthereareothermeanings\nforbiasincertaincontexts.Wewillmakeitclearwhenadifferentmeaningisused.",
    "40 Chapter 1. Foundations of Machine Learning\nweplayedweighteddice,anditwasnoteasytowinasbefore. Youfoundthattheappearance\nof different sides did not follow a uniform distribution. Then, you assumed a multinomial\ndistribution(becauseyouhadtheexperienceofdevelopingnaiveBayesclassifiers). Beforethe\ngamestarted,yourolledthedice100times. Youchosethemostfrequentsidefornewgames,\nandyouwonmore. Inthisexample,youmadeaninitialassumptionthatallsixofthesidesare\nequallylikelytooccur. Thisisaverystronginductivebiasbecauseyourmodelhas0degrees\noffreedom. Itseemstobeobviousbutdoesnotworkforweighteddice. Thesecondinductive\nbias,thoughseemsmorecomplicated,isactuallyaweakerassumption,becauseamultinomial\ndistributiondefinesalargerfamilyofmodelsandgivesroomtofindingappropriatemodels.\nIngeneral,allmachinelearningmodelsneedsomesortofinductivebias. Manyofthemare\nimplicitassumptions. Sometimes,weareevennotawarethatwearemakingtheseassumptions",
    "becausetheyareso“obvious”and“logical”. Ontheotherhand,iftheassumptioniswrongthen\nitisharmfultoproblem-solving. Sowestillneedsomeexperiencetoavoideasilyneglected\nmistakes.\n1.3.3 Non-linearity\nNon-linearityisthenatureofmostreal-worldproblems,whereasitisnoteasytousealinear\nmodel to solve a non-linear problem. See Figure 1.3 for examples of varying degrees of\nclassificationdifficulty. InFigure1.3(a),thetwoclassescanbeseparatedbyahyperplane. In\nthiscase,theproblemislinearlyseparablebecausethedecisionboundarycanberepresented\nasalinearfunction. Incontrast,inFigure1.3(b),wecannotdrawhyperplanestoperfectly\nseparatethetwoclasses. Instead,weneedsomenon-linearityforbetterseparation,suchas\nhyperspheres. AmoredifficultcaseisshowninFigure1.3(c)wherethedecisionboundaryis\nhighlycomplex.\nAlthoughthetheoryofnon-linearsystemsstillhasnotbeenfullystudied,thereareseveral\nmethodsthathelpusintroducenon-linearityintomachinelearningsystems.",
    "• Featuremappingandkernelmethods. Recallthatalinearclassifiercanbeformulated\nasafunctionf(w·x), wherew istheweightvector, xisthefeaturevector, andf(·)\nis the function that returns one class (say c ) if its argument >0 and the other class\na\n(say c ) otherwise. The idea of feature mapping is that we map the feature vector x\nb\ninto a higher-dimensional space so that the problem is linearly separable in the new\n(cid:104) (cid:105)\nspace. Forexample,letϕ(·):R2→R4 beamappingfunctionandx= x x bea\n1 2\n2-dimensionalvector. Weassumethat\n(cid:104) (cid:105) (cid:104) (cid:105)\nϕ( x x ) = x2+x2 x x 1 (1.68)\n1 2 1 2 1 2\n(cid:104) (cid:105)\nBychoosingw= 1 −8 −8 28 ,wegetanewclassifier:\n(cid:104) (cid:105) (cid:104) (cid:105)\n(cid:0) (cid:1)\nf(w·ϕ((x))) = f 1 −8 −8 28 · x2+x2 x x 1\n1 2 1 2\n= f (cid:0) (x −4)2+(x −4)2−4 (cid:1) (1.69)\n1 2",
    "1.3 General Problems 41\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nLinearity\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nNon-linearity\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nNon-linearity\n(moredifficult)\nFigure 1.3: Linearity and non-linearity in binary classification. The first problem (left) is\nlinearlyseparablebecausethereexists(atleast)ahyperplanethatperfectlyseparatesthedata\npoints in the two classes. The property of linear separability does not hold in the second\nproblem(middle). Rather,weneedacircle-likedecisionboundary. Thedecisionboundary\nwouldbemorecomplexifthereareareaswherethetwoclassesofthedatapointsaremixed\nandmoreorlessindistinguishable(right).\nIt defines a decision boundary (i.e., a hypersphere (x −4)2+(x −4)2 = 4) that\n1 2\nperfectlyclassifiesthesamplesinFigure1.3(b). Inotherwords,weusealinearmodel\non the mapped feature space to create a non-linear model. However, computing the\nmapping function might be inefficient. This is typically addressed by using kernel",
    "methods. Inkernelmethods,thecalculationofvectordot-productinthenewspaceis\nperformedefficientlybyusingakernelfunctionintheoldspace. Thismethodiscalled\nthekerneltrick. Ithasbeensuccessfullyadoptedinclassificationandothermachine\nlearningmodels,suchassupportvectormachines[CortesandVapnik,1995].\n• Non-linearactivationfunctions. Anotherwaytoaddnon-linearityistouseactivation\nfunctions. A common method is to stack a non-linear activation function on top of a\nlinearmodel. Forexample,thefunctionf(·)usedintheaboveexampleisitselfanon-\nlinearfunction. Therearemanykindsofnon-linearactivationfunctions. Wecanchoose\nfrom them, depending on what form of the output we want. For more sophisticated\nmodels,moreactivationfunctionscanbeinsertedintotheintermediatecomputingsteps\ntodevelopamorepowerfulandexpressivemodel. Forexample,adeepneuralnetwork\nisastackofsub-models(callthemlayers)whereeachsub-modelmayinvolveoneor\nmoreactivationfunctions.",
    "• Non-parametricmethods. Non-parametricisatermthatisoriginatedfromstatistics.\nInnon-parametricstatistics,statisticalinferencesaremadewithoutanyassumptionon\nunderlyingdistributionsofdata. Inmachinelearning,non-parametricmethodsfollow\nthe same idea. They do not assume any mapping function from input to output as\nEq. (1.69). Thisdifferentiatesthemfromparametricmethodsthatexplicitlylearna\nmathematicalformofvariables(orparameters)todescribetheproblem. Anexampleof",
    "42 Chapter 1. Foundations of Machine Learning\nnon-parametricmethodsisk-nearestneighbors. Itmakesapredictionforanewsample\nbasedonthek nearestneighboringsamplesintrainingdata. Notethatnon-parametric\ndoesnotmeanparameter-free. Rather,itmeansthatparameterscanchange. Onanother\nhand,non-parametricmethodsdonotensureafixedmodel. Theygrowinmodelsize\nasmoretrainingsamplesareavailable. Asareward,theycanhandlehighlynon-linear\nproblemswhentrainingsamplesaresufficient.\nStill,non-linearmethodsdonotworkalone. Linearityissurelyanimportantcomponent\nformostpracticalmachinelearningsystems. Thishastwoflavors. First,morenon-linearityis\nnotalwaysbetter. Wedonotneedtocomplicatethemodelingifalinearmodelisenoughfor\nsolvingtheproblem. Anexampleisthatmoststate-of-the-artmachinelearningmodelsarea\ncombinationoflinearandnon-linearsub-models. ThisisalsoaninstanceofOccam’sRazor—\nthesimplestsolutionisalmostalwaysthebest. Thesecondflavoristhelinearapproximationof",
    "non-linearbehaviors. Linearmodelsareagoodalternativeifthenon-linearityoftheproblemis\nnotobvious. Insuchcases,usinglinearfunctionstoapproximateprecisesolutionsisprobably\nmoreefficientforpracticalpurposes.\n1.3.4 Training and Loss Functions\nAlmostallmachinelearningalgorithmsinvolveatrainingstep. Typically,itreferstotheprocess\nofestimatingthemappingfunctionandtheassociatedparametersfromdata. Herewefollow\na conventional definition of the training problem: given a model or mapping function, we\nimprovesomeobjectivebyevaluatingthemodelthroughsometrainingexperience[Mitchell,\n1997]. Forexample,traininganaiveBayestextclassifierrequiresmaximizingalikelihood\nfunction(i.e.,theobjective)onanumberoflabeleddocuments(i.e.,thetrainingexperience).\nOften, the training problem can be framed as an optimization problem. As such, we\noptimizesomeobjectivefunctionviasometrainingalgorithm. Althoughanidealobjective\nfunctionisaperformancemeasureontestsamples,wecannottakeitinoptimizationsincethe",
    "test samples and corresponding labels are assumed to be inaccessible in the training phase.\nPracticalobjectivefunctionsareinsteaddefinedasasurrogateforthemeasureontestdata. On\ntheotherhand,theseobjectivefunctionsarenotnecessarilysomesortofperformancemeasure,\nbutsomemetricsthatareassumedtocorrelatewiththeperformanceontestdata.\nLetusconsiderageneralcase. Supposey =f (x)isamodelthatreadsafeaturevector\nθ θ\nx and produces an n-dimensional vector y . For example, in text classification, x is the\nθ\nbag-of-wordsrepresentationofadocument, andy isadistributionoverasetofclasses. θ\nθ\nis the parameters of the model. The subscript emphasizes that the model is determined by\nθ. We further suppose that y is the gold-standard vector. Then, we define the objective\ngold\nfunctionasafunctionthatcountserrorsiny withrespecttoy ,denotedasL(y ,y ). It\nθ gold θ gold\nmeasureshowbaditwouldbeifwepredicty insteadofy . Givenamodel,thetraining\nθ gold",
    "problemcanbedescribedasfindingthe“best”parametersθˆ sothatL(y ,y )isminimized:\nθˆ gold\nθˆ = argminL(y ,y ) (1.70)\nθ gold\nθ",
    "1.3 General Problems 43\nThisformulationcanbeeasilyextendedtothecaseofK trainingsamples:\nK\nθˆ = argmin 1 (cid:88) L(y (k) ,y (k) ) (1.71)\nK θ gold\nθ\nk=1\nOnceweobtainθˆ ,wecanusef (x)asafixedmodelforprediction.\nθˆ\nL(y ,y )and 1 (cid:80)K L(y (k) ,y (k) )areusuallycalledlossfunctions(orcostfunc-\nθ gold K k=1 θ gold\ntions). A loss function can be defined in many ways, depending on what type of problem\nweaddressandwhatpriorwewanttoimposeupontraining. Here,wefirstconsiderthecase\nin which y is a probability distribution. It is quite common in NLP, e.g., y could be a\nθ θ\ndistributionoveravocabulary,adistributionoveralistofdocuments,adistributionoveraset\nofsyntacticlabels. Forsuchatypeofmodeloutput,themostcommonly-usedlossfunctions\naremeasuresofdivergence:\n• Divergence-basedLoss. Divergence-basedlossfunctionscomputethedegreeofdif-\nference between the two distributions y and y . For example, cross-entropy (see\nθ gold\nSection1.1.2)isoneofthemostpopularlossfunctionsusedinNLP.Onecan,ofcourse,",
    "choose other divergence-based measures, such as the KL divergence and the Jensen-\nShannon (JS) divergence, which can be found in most statistics textbooks. Note that\nMLEisalsoaspecialinstanceofthedivergence-basedobjective. Itisthesameasthe\ncross-entropylossify isaone-hotvectorwheretheentryofthecorrectlabelis1\ngold\nandotherentriesareall0.\nHowever,machinelearningsystemsarenotalwaysrestrictedtodistribution-likeoutput.\nRather, y could be a vector in Rn. An example is the discriminant functions used in clas-\nθ\nsification (see Section 1.2.3). They assign a score to each class, indicating how strong the\nmodelbelievesitistheanswer. Onewaytodefinethelossfunctionsonreal-valuedvectors\nis to transform them into distribution-like forms15, and resort to the divergence-based loss.\nHowever,normalizationisnotalwaysnecessary,especiallywhenweneedascoreoutofthe\nrangeof[0,1]. Itismorecommontocomputelossesontherawoutputofthesemodels. Here\naresomeexamples.",
    "• Distance-basedLoss. Itisnaturaltotakelossassomesortofdistanceingeometry. A\ngeneralexampleisthep-normdistance(seeSection1.1.1):\nn\n(cid:16)(cid:88) (cid:17)1/p\nL(y ,y ) = |y (i)−y (i)|p (1.72)\nθ gold θ gold\ni=1\nFor example, we would have a Euclidean distance-based loss function if p=2. The\ndistance-based loss intrinsically describes a curve fitting problem: we learn a curve\ny =f (x) to fit the points {(x(k),y (k) )}. It is also called regression 16. A simple\nθ θ gold\n15Forexample,wecannormalizetheentriesofavectorbythesumoftheseentries.\n16Whenthemodeloutputisavectorwithtwoormoredimensions,theproblemiscalledmultivariateregression.",
    "44 Chapter 1. Foundations of Machine Learning\nexampleisqualityestimationofmachinetranslation17. Itlearnstopredicttranslation\nquality (i.e., y ) for any pair of source and target sentences (i.e., x). We would say\nθ\nthat the prediction is accurate if the predicted score is close to that made by humans.\nBy using Eq. (1.72), one can design many loss functions for regression models. For\nexample,meansquareerror(MSE)isapopularregressionlossfunction. Itisthesum\nofsquaredEuclideandistancesbetweenthepredictionandthegoldstandard:\nn\n(cid:88)\nL(y ,y ) = |y (i)−y (i)|2 (1.73)\nθ gold θ gold\ni=1\nAnotherexampleismeanabsoluteerror(MAE).ItispreciselytheformofEq. (1.72)\nwhenp=1.\n• 0-1 Loss. The 0-1 loss is widely used in classification problems. It chooses a value\nof either 1 (penalty) or 0 (no penalty), and penalizes the case in which the predicted\nlabel and the gold-standard label are not the same. Let c = argmax y (c) be the\nθ c θ",
    "labelthatispredictedbyselectingtheentryiny withthehighestvalue. Likewise,let\nθ\nc =argmax y (c)bethegold-standardlabel. The0-1lossisdefinedtobe:\ngold c gold\nL(y ,y ) = L (c ,c )\nθ gold 0-1 θ gold\n(cid:40)\n1 c ̸=c\nθ gold\n= (1.74)\n0 c =c\nθ gold\n• Margin-basedLoss. Amarginisthedifferencebetweenthepredictedscoresofthe\ncorrectlabelc andanincorrectlabelc:\ngold\nmagin(c,c ) = y (c )−y (c) (1.75)\ngold θ gold θ\nIt indicates a distinction between c and c. So, a natural idea is to ensure that the\ngold\nmarginissufficientlylarge,oratleastexceedsaminimum. Thisiscalledlarge-margin\ntraining. Let △(c,c ) be a predefined cost of replacing label c with label c,\ngold gold\nsatisfying△(c,c )≥0,and△(c,c )=0onlyifc=c . Ourgoalistoenlarge\ngold gold gold\nmargin(c,c )−△(c,c ), in other words, the larger this value is, the smaller the\ngold gold\nlossis. Then,themargin-basedlossisgivenby:\n(cid:16) (cid:17)\n(cid:0) (cid:1)\nL(y ,y ) = max 0,max− magin(c,c )−△(c,c )\nθ gold gold gold\nc\n(cid:16) (cid:17)\n(cid:0) (cid:1)",
    "= max 0,max y (c)−y (c )+△(c,c )\nθ θ gold gold\nc\n(cid:16) (cid:17)\n= max 0,y (c)−y (c )+△(c,c ) (1.76)\nθ θ gold gold\nc\n17Inmachinetranslation,qualityestimationcomprisesseveraldifferenttasks(seehttps://www.statmt.\norg/wmt21/quality-estimation-task.html).Hereweusethetermtorefertothetaskthatpredicts\nanevaluationscoredirectly.",
    "1.3 General Problems 45\nDesigning△(c,c )dependsontheproblem. Asimplechoiceis△(c,c )=1for\ngold gold\nc̸=c . ThismakesEq. (1.76)atypeofthehingeloss. AnothervariantofEq. (1.76)\ngold\nisusingasuminsteadofamax:\n(cid:88) (cid:16) (cid:17)\nL(y ,y ) = max 0,y (c)−y (c )+△(c,c ) (1.77)\nθ gold θ θ gold gold\nc\n• Ranking-basedLoss. Ranking-basedloss(orrankingloss)isusedinseveraldifferent\nareas, such as information retrieval, classification and metric learning. It deals with\ntheproblemwherewewanttoorderasetofscoreditems. Supposethemodeloutput\ny corresponds to a set of n items {c }, each for an entry of y . We define {ψ (c )}\nθ i θ θ i\nas the order of {c } by {y (i)}. For example, given {y (i)}={0.3,−2,1}, we have\ni θ θ\nψ (c )=2, ψ (c )=3 and ψ (c )=1. Likewise, we can define {ψ (c )} as the\nθ 1 θ 2 θ 3 gold i\ngold-standardranks. Notethat{ψ (c )}canbeinducedinsomewaywithouttheneed\ngold i\nofy iftheproblemonlyrequiresorders,ratherthanscores. Anideaofranking-based\ngold",
    "lossistomodeltherankingmistakesin{ψ (c )}withrespectto{ψ (c )}. Thereare\nθ i gold i\nmanywaysto“count”themistakes. Asimplemethodistopenalizethecaseinwhicha\npairofitemsareorderedincorrectly. Assuch,theranking-basedlosssomewhatshares\nthesamespiritofthatusedinbinaryclassification—wecategorizeapairofitemsas\ncorrectorincorrect. LetΩbeasetofordereditempairs:\nΩ = {(i,j)|ϕ (i)<ϕ (j)} (1.78)\ngold gold\nThelossfunctionisgivenbytheequation:\n(cid:88)\nL(y ,y ) = L (y (i),y (j)) (1.79)\nθ gold pair θ θ\n(i,j)∈Ω\nwhereL (y (i),y (j))isaclassificationloss,suchasthehingelossusedin[Collobert\npair θ θ\nandWeston,2008]:\n(cid:0) (cid:1)\nL (y (i),y (j)) = max 0,y (i)−y (j)+1 (1.80)\npair θ θ θ θ\nThis method is called the pairwise method. Also, one can define the ranking-based\nloss in a pointwise or listwise manner. These loss functions are extensively used in\ndevelopingsystemstorankobjects.\n• ContrastiveLoss. Contrastivelossistypicallyusedincontrastivelearning. Itassumes",
    "that, given a sample, there is a similar sample that is labeled as “positive”, and there\nare a number of dissimilar samples that are labeled as “negative”. A natural idea is\nto minimize the distance between similar samples and simultaneously maximize the\ndistancebetweendissimilarsamples. Returntotheformulationhere. Foramodeloutput\ny ,lety+ bethepositiveoutputandY−={y−}bethesetofnegativeoutputs. Also,\nθ\nweusey todenotethetupleofy+ andY− insteadofasinglegold-standardvector.\ngold",
    "46 Chapter 1. Foundations of Machine Learning\nAformofthecontrastivelossfunctionisgivenbytheequation:\nL(y ,y ) = L(y ,y+,{y−})\nθ gold θ\n(cid:88)\n= logD(y ,y+)−log D(y ,y−)\nθ θ\ny−∈Y−\nD(y ,y+)\nθ\n= log (1.81)\n(cid:80) D(y ,y−)\ny−∈Y− θ\nwhere D(α,β) is a measure of the distance between α and β. For example, we can\ndefine D(α,β) as the Euclidean distance (see Eq. (1.72)). A problem here is how to\ngeneratepositiveandnegativemodeloutputs. Inthesupervisedlearningsetup,onecan\nsimplytreatthegold-standardvectorasthepositiveoutput. Fornegativeoutputs,the\nmodelf(x)canoutputanumberofythroughacceptingdifferentx. Intheunsupervised\nlearningsetup,y+ andY− areoftendefinedbasedonsome“natural”annotation. For\nexample,f(·)canbeafunctionthatmapsxtosomethingandbacktox(callitauto-\nencoding). Then,y+ isxitselforsomeneighborsofx,andY− isasetofrandomly\ngeneratedvectors.\n• Error-basedLoss. Evaluationmetrics,asgenerallyusedincountingerrorsinsystem",
    "output,canalsobetakentobepartofalossfunction. Forexample,inmachinetranslation,\napopularevaluationmetricisBLEU18.Thus,wecantakeminimizing1−BLEUasthe\nobjective. LetError(y ,y )bethe“number”oferrorsincomparingy withy .\nθ gold θ gold\nTheerror-basedlossisjustthesameasthisnumber:\nL(y ,y ) = Error(y ,y ) (1.82)\nθ gold θ gold\nSo far wehave presented several lossfunctions fora wide variety of problems, such as\nclassification, regression, andranking. Aswewillseeinthisbook, differentlossfunctions\nhavedifferenteffectsonmodelbehavior. However,testingallpossiblelossfunctionsissimply\nimpracticalbecause there aresomanyofthem. Usersinsteadneed tochooseor designthe\nmostsuitablelossfunctionsfortheirownproblems. Thismaytaketimebutisnecessary.\nOnanotherhand,therearegeneralmethodstoimprovethedesignoflossfunctions. For\nexample, wecanassumethatthemodeloutputy isnotasinglevectorbutavariablewith\nθ\nsomeprobability. ThelossL(y ,y )isthustreatedasavariabletoo. Then,weredefinethe\nθ gold",
    "lossfunctionastheexpectationofL(y ,y )underthedistributionofy :\nθ gold θ\nL({y },y ) = E (cid:2) L(y ,y )·Pr(y |x) (cid:3) (1.83)\nθ gold y ∼Pr(y |x) θ gold θ\nθ θ\nwhere the use of {y } means that y is not fixed. By accessing the space of possible y , it\nθ θ θ\noffersabetterestimationoftheloss. ThisisessentiallyaninstanceoftheBayesianapproach.\nL({y },y )iscalledtheBayesianriskorriskforshort,sometimes.\nθ gold\nAnotherwaytoimprovetrainingisintroducingpriorsintotheobjective. Atypicalmethod\n18BLEUisaprecision-likescorebetween0and1.Thehigherthebetter.",
    "1.3 General Problems 47\nistoaddaregularizationtermRtotheobjective,likethis:\nθˆ = argminL(y ,y )+α·R (1.84)\nθ gold\nθ\nwhere R could be another function that describes some aspect of the problem, such as the\nnumberofparameters. αisahyperparametercontrollinghowmuchwerespectRintraining.\nThedesignofRisitselfanimportantproblemformanypracticalmachinelearningsystems.\nAlthoughwedonotdiscussthemhere,wewilllookatafewlaterinthisbook.\nOncetheobjectiveisdetermined,weneedsometrainingalgorithmtoperformoptimization.\nThisisaverybroadtopicinmachinelearning,suchthatwedonoteventrytodescribeanyof\nthemindetailinthischapter. Anyway,oneshouldnotexpectauniversalalgorithmthatcan\nsolvealltrainingproblems,andthereareindeedsomealgorithmsthataresuitableforcertain\ntypesofproblems. Forexample,wecanusegradientdescenttotrainaneurallanguagemodel\nwiththecrossentropy-basedloss[Bengioetal.,2003a],canusequadraticprogrammingto\ntrainanSVMmodelwiththehingeloss[CortesandVapnik,1995],andcanuseminimum",
    "error-ratetraining(MERT)totrainastatisticalmachinetranslationmodelwiththe1−BLEU\nloss[OchandNey,2002].\n1.3.5 Overfitting and Underfitting\nThe standard process of (supervised) machine learning comprises a training step and a test\nstep. Whileonemaytrytominimizethelossontrainingsamples,thelearnedmodelisused\nto deal with new samples that are never seen before. It is like what we experienced in our\nlives,forexample,astudentstudieshardandwishestogetgoodgradesinfinalexams. Yes,\nstudying hard = good grades should always be true, but it does not mean that memorizing\nallthequestionsandanswersintextbooksisagoodwaytoperformwellinexams. Italways\nhappensthatthetestquestionsaresomethingdifferentfromwhatwelearned. Wetherefore\nneedsomeabilityofgeneralization.\nInmachinelearning,generalizationisusedtodescribehowwellamodellearnedthrough\nexperiencepredictsonnewdata. Asystemisthoughttobeofexcellentgenerationperformance\nifitlearnslittlefromtrainingdatabutformsitspredictionabilitybasedonsome“god”inductive",
    "biases on the problem. However, good generalization does not mean less training. Instead,\npractitionerswouldliketotrainamachinelearningmodelonmoretrainingdatatoprevent\nit from memorizing all the things. Generalization is a very complex issue determined by\nseveralfactors, includingproblemcomplexity, modelarchitecture, amountoftrainingdata,\ntrainingalgorithmandsoon. Whiletherearenostandardrulestoensuregoodgeneralization,\nresearchersalwaystrytoaddressitsomehow.\nTo describe how well a model generalizes to new data, there are two important terms,\nunderfittingandoverfitting. Underfittingreferstothephenomenonthatamodeldoesnotlearn\nsufficientlyfromthetrainingdataandthushaspoorperformanceonnewdata. Forexample,\nweinterrupttrainingaccidentallyanddeploytheimmaturemodelforprediction. Themodel\ncannotperformwelloneitherthetrainingdataorthetestdata. Ifamodelunderfitsthetraining\ndata,thenonecouldimproveitinsomesimpleways. Forexample,onecouldtrainthemodel",
    "48 Chapter 1. Foundations of Machine Learning\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nUnderfitting\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nFittingjustright\n8\n6\n4\n2\n0\n0 2 4 6 8\nx\n1\nx\n2\nOverfitting\nFigure1.4: Decisionboundariesofabinaryclassificationproblem. left=underfitting,right=\noverfitting,andmiddle=fittingjustright. Intheunderfittingcase,thereareseveralobvious\nmistakesthataremadeinseparatingthetwoclassesofdatapoints. Byshiftingthedecision\nboundaryupabit(middle),weobtainasatisfactoryseparationresult,wheremostofthedata\npointsbelongingtothesameclassareplacedonthesamesideofthedecisionboundary. By\ncontrast,aperfectseparationrequiresahighlycomplexdecisionboundaryinstead(right).\nforalongertime;onecouldremoveunimportantportionsfromthetrainingdata;onecould\nuseamodelwithasimplerarchitectureinstead.\nIn contrast to underfitting, overfitting refers to the phenomenon that a model fits the\ntrainingdatawellbutgeneralizespoorlyonthetestdata(seeFigure1.4). Asimpleexample",
    "of overfitting here is the OOV problem (see Section 1.2.5). It would be a disaster if a text\nclassificationmodeljustfitsthosewordsthathavebeenseenbutgetsstuckwhennewwords\nappear.\nThecausesofoverfitting arediverse. Anexampleislearningacomplexmodelonasmall\ntraining dataset. The model complexity often matters when we design a machine learning\nmodel. If the model is complex and has many parameters, then it would be much easier to\noverfitasmallnumberofsamples(seeFigure1.4). Theproblemwouldbemoredifficultif\nthereisnoisydata,becauseoftheerrorsof“garbagein,garbageout“intraining. Inaddition,\nexcessivetrainingisanothercauseofoverfitting. Forexample,wecanheavilytuneasystem\ntoenforceittomodelthedatawithnoerrors. Thesystemwouldbefragilefornewsamples,\nevenwhentherearesmallfluctuationsininput.\nOverfittingcanbealleviatedinmanyways. Herearesomecommonly-usedtechniques.\n• Usingmore(high-quality)trainingdata. Large-scaletraininghelpsthemodelcapture",
    "thetruepatternsindata. However,addingnoisydatawoulddothisinanegativeway.\n• Using validation data. Validation data is some test data but used in training. For\nexample,adatasetcanbedividedintoheld-outdataandtrainingdata. Onecansimply\nearlystopthetrainingprocesswhentheperformancedropsontheheld-outdata.\n• Usingsimplermodelarchitectures. Asnotedpreviously,Occam’sRazorisaprinciple",
    "1.3 General Problems 49\nwe can follow in model design. Models with more complex architectures, though\npowerful, would be more likely to fit the noisy data points if the problem is not so\ndifficultitself. Usingasimplermodelarchitectureinsteadcouldmakeiteasiertomodel\nthedominantpatternsinthedata.\n• Regularization. Regularizationisanotherwaytocontrolthemodelcomplexity. Typi-\ncally,itregularizesmodelparametersbypriors. Anexampleissmoothing(seeSection\n1.2.5). Itre-estimatesthedistributionofwordsaftertraining. Amoregeneralmethod\nisregularizedtraining(seeEq. (1.84)). Forexample,wecandefinetheregularization\nfactorasthel normoftheparameters,andbiasthemodeltothosewhoseparameters\n1\narenotinlargeabsolutevalues.\n• Combining multiple models. A better prediction can also be made by ensembling\nmultiplemodels. Thesemodels(callthemcomponentmodels)areingeneralofdifferent\nparametersorarchitectures,and/oraretrainedwithdifferentportionsofthedata. The",
    "varianceinmodelscanreducetheriskthatallthesemodelsoverfitthedatainexactly\nthesamemanner. Thesemodelsare,therefore,lesslikelytomakesimilarmistakesin\nprediction.\n1.3.6 Prediction\nAlthough we restricted our discussion to classification in previous sections, (supervised)\nmachinelearningisnotjustataskofpredictingalabelforaninputobject. Therearemany\ntypesofmachinelearningproblems,dependingonwhatformofthepredictionisdefined.\n• Classification. Classification is perhaps one of the most common machine learning\nproblems. Aclassificationsystemisrequiredtoassignoneormoreclassestoaninput\nobject.\n• Regression. In statistics, regression studies the relationship between a dependent\nvariable (or an outcome) and an independent variable. While regression has many\napplications,itisoftenframedasscorepredictioninNLP.Forexample,takingamovie\nreviewasinput(i.e.,anindependentvariable),theregressionmodellearnstopredicta\nrecommendationscore(i.e.,adependentvariable).",
    "• Ranking. Arankingmodelistopredicttheorderofasetofinputobjects. Forexample,\namodelranksanumberoftranslationsintermsoftranslationquality.\n• Structureprediction. Manymachinelearningmodelsarerequiredtooutputnotonlya\nrealvalueoraclassbutatreeorasequence. Thetaskofpredictingstructuredoutputs\niscalledstructureprediction. Forexample,asyntacticparserisastructureprediction\nsystem,asitsoutputisatreestructure.\nInadditiontothese,miningisatermthatisfrequentlyusedinthecommunity,although\nit is somehow not a standard machine learning problem. The problem of mining refers to\ndiscoveringunknownpatternsinthedata. Anexamplewewouldliketocategorizeintothis\niswordclustering. Givenanumberofwords,theclusteringsystem“predicts”theclusterfor\neachword. Theoutputofsuchsystemsisnotpre-defined. Patternsindataarethemselveshard",
    "50 Chapter 1. Foundations of Machine Learning\ntodescribe. Thus,theterm“mining”couldcoverarangeofproblems. Toavoidconfusion,we\nwillusemorespecificterms(suchaswordclustering)torefertomining-relatedproblems.\nDespiteafundamentalaspectofmachinelearning,predictionisconventionallyassumed\nto be trivial, given that many models and methods are tested on standard classification and\nregressiontasks. Ontheotherhand,predictionisnon-trivialinstructureprediction,suchas\nparsingandmachinetranslation,whichareverycommoninNLP.Essentially,predictingatree\norasequenceisasearchproblem. Forexample,thereexistatheoreticallyinfinitenumber\noftranslationsgivenasource-languagesentence. Evenifwehaveamodeltoevaluateevery\ntranslation,findingtheoptimaltranslationinthesearchspaceisobviouslyacomputational\nchallenge. In such cases, we need some way to make it feasible to perform search. This is\nimplementedbyeitherresortingtothegeneralsearchalgorithmsinartificialintelligenceor",
    "developingnewalgorithmsforspecificproblems. Asanaside,thestudyonthesearchproblem\noffersanewviewonthemistakesmadebyamachinelearningmodel: someoftheerrorsare\nduetoinaccuratemodeling(callthemmodelerrors),andtherestareduetoinaccuratesearch\n(callthemsearcherrors). Forprediction,eliminatingsearcherrorsisagoalbutoftenatthe\ncostofaconsiderablylargeamountofsearcheffort. Wesometimesmusttradeoffbetween\nefficiencyandaccuracyifamachinelearningmodelisdeployedforpracticalpurposes. We\nwillseeafewexamplesinChapter5.\n1.4 Model Selection and Evaluation\nFormostmachinelearningproblems,thegoalistofindamodelthatwouldperformthebest\nonnewdata. Twoproblemscanbeseparatedoutfromthisgoal[Hastieetal.,2009]:\n• Modelselection. Selectingthebestmodelontrainingdatabysomecriteria.\n• Modelevaluation. Estimatingtheperformanceofagivenmodelonnewdata.\nAsnotedinSection1.3.4,lossfunctions(orerrorfunctions)arecommonwaysofmeasuring\nerrors in a prediction y =f (x) with respect to a gold-standard y . Given K labeled\nθ θ gold",
    "trainingsamples{(x(1),y (1) ),...,(x(K),y (K) )},thetrainingerrorisgivenby\ngold gold\n(k) (k)\nErr = L({y },{y }) (1.85)\ntrain θ gold\n(k) (k)\nwhere{y }arethepredictionsoverthetrainingdataset,and{y }arethecorresponding\nθ gold\n(k) (k)\ngold-standards. L({y },{y })isingeneraldefinedastheaveragedlossoveralltraining\nθ gold\nsamples:\nK\n(k) (k) 1 (cid:88) (k) (k)\nL({y },{y }) = L(y ,y ) (1.86)\nθ gold K θ gold\nk=1\nordefinedasasinglemeasureontheentiresetoftrainingsamples. Likewise,wecandefinethe\ntesterroronthetestdataset,denotedasErr . Err isalsocalledgeneralizationerror.\ntest test\nItindicateshowwellamodelgeneralizestonewdata.",
    "1.4 Model Selection and Evaluation 51\n8\n6\n4\n2\n0 10 20 30 40 50 60\nTimeElapsed(intrainingepochs)\nssoL\n75\n70\n65\n]%[UELB−1\nTrainingerror(Err =cross-entropy) train\nTesterror(Err =1−BLEU)\ntest\nFigure1.5: Curvesoftrainingerrorandtesterrorforamachinelearningsystem. Thetraining\nerrorismeasuredintermsofthecross-entropyloss,andthetesterrorismeasuredintermsof\n1−BLEU. Allstatisticsarecollectedbyrunninganeuralmachinetranslationsystemonthe\nIWSLTDe-Enbenchmark. Thetrainingerrorcontinuestodropasmoretrainingepochsare\ninvolved. Thetesterror,ontheotherhand,followsatrendoffirstgoingdownandthengoing\nup. Whenthetesterrorstartstoincrease,themodelislikelytooverfitthetrainingdata.\nIntheprecedingsectionsweassumedthatminimizingErr istheobjectiveoftraining,\ntrain\ni.e., θˆ=argmin Err . However, we cannot assume that f (·) can obtain the minimum\nθ train θˆ\nErr inthesameway. SeeFigure1.5forlearningcurvesofamachinetranslationsystem.\ntest",
    "Clearly, Err does not correlate with Err well. The training error keeps reducing as\ntest train\ntrainingproceeds. However,thetesterrorgoesupafterfollowingthesametrendasthetraining\nerrorforaperiodoftime,indicatingoverfittingofthemodel. Thismakestheproblemabit\nmorecomplicated,aswecannotalwaystrustErr althoughitisandshouldbethemeasure\ntrain\nofthegoodnessoftraining. Surely,weneedsomewaytoselectabettermodel,inadditionto\nlookingatErr only.\ntrain\n1.4.1 Strategies for Model Selection\nChoosingtheoptimalmodelonthetrainingdataischallengingbecausethemotivationhereis\n“greedy”itself—wehopethatamachinelearningmodelcangeneralizefromafinite,evena\n“small”numberofsamples. Fromthestatisticallearningpointofview,thechallengeisdueto\nthewaywedefinethelearningproblem. Animplicitassumptioninmachinelearningisthatall\ndataisgeneratedbysomedistribution. Thus,thelearningproblemisdeterminedbygenerating\nthetrainingdataviaadata-generationdistributionandthetestdataviaanotherdistribution.",
    "Forexample,ifboththetrainingandtestdatasetsaresufficientlylargeandobtainedviathe\nsamedata-generationdistribution,thenthelearnedmodelcanperformonthetestdataaswell\nasonthetrainingdata. Inthiscase,itiseasytogeneralizethemodelfromthetrainingdatato\nthetestdata. Bycontrast,ifalltrainingandtestdataisgeneratedinanarbitrarymanner(saya\nuniformdistributionovertheentirespaceofdatapoints),thenthemodelwillfailtogeneralize,\naseverythinglearnedonthetrainingdatadoesnothingwiththetestdata.",
    "52 Chapter 1. Foundations of Machine Learning\nItwillbemoreinterestingifweconsiderallpossibleproblems. Thenofreelunchtheorem\nstatesthatalllearningalgorithmswillperformequallywellifweaveragethetesterrorover\nallproblems19. Inotherwords,alllearningwillmakenosenseifthereisnopreferencefor\ncertain problems. However, developing a universally good machine learning model on all\nproblemsisidealistic. Inreal-worldapplications,thetrainingandtestdataisalwaysassumed\ntoatleastinpartfollowsomedistribution. Therefore,thereareindeedsomewaystocapture\nthisdistributionandimprovethegeneralizationabilityofamodel. Twoscenariosaregenerally\nconsideredinimprovingmachinelearningsystems:\n• Giventhemodeldesignandthetrainingalgorithm, howtodeveloporselecttraining\ndatatoreducethetesterror.\n• Giventhetrainingdata,howtodeveloporselectamodeltoreducethetesterror.\nThefirstscenarioiscomplicatedandrelatestomanypracticalissues,e.g.,annotation,data",
    "cleaning, data quality estimation and so on. Since these issues are not the focus for model\nselection,wedonotdiscussthembutleavesometosubsequentsections. Here,wefocuson\nthemodelselectionprobleminthesecondscenario.\n1. Model Complexity\nThe simplest method of model selection might be testing the models on validation data.\nTypically, this data does not overlap with either training or test data, but is assumed to be\ngeneratedinthesamewayasthetestdata. However,suchdataisnotalwaysavailable. Insome\ncases,wedonotevenknowanythingaboutthetestdata. Somanymodelselectionmethods\narevalidation-free.\nAcommonwayistousemodelcomplexity(ormodelcapacity)asanindicatorofthe\nselection. Inmachinelearning,modelcomplexitycanbeinterpretedinseveraldifferentways.\nFor example, a non-linear model is intuitively more complex than a linear model. Also, a\nmodel with more parameters is more complex than a model with fewer parameters under\nthesamemodelarchitecture. Moreformaldefinitionscouldbefoundinthetheoreticalpart",
    "ofmachinelearning, suchastheVapnik-ChervonenkisdimensionortheVCdimension\n[VapnikandChervonenkis,1971]. Herewesimplytreatmodelcomplexityasameasureofthe\nexpressivepowerofamodel,i.e.,ahighermodelcomplexityindicatesmorehypothesesthat\nthemodelcanexpress.\nWhile complex models are usually assumed to be more powerful, higher model com-\nplexitiesarenotalwayshelpful. Infact,complexmodelsaremorelikelytooverfitthedata,\nespeciallywhenasmalldatasetisusedfortraining. Bycontrast,toosimplemodelsareoften\npronetounderfitting. Wethereforeneedtoseekan“optimal”levelofmodelcomplexity. Figure\n1.6plotstrainingandtesterrorsagainstmodelcomplexity. An“optimal”complexitycanbe\nchosen when the training error tends to convergence. While Figure 1.6 shows an intuitive\nexample,itisstillhardtosayatwhatpointwecanchoosethemodel. Thecommonpractice,\nthoughnotformallydescribedinmostcases,istochooseamongthose“good”modelsbyusing\n19Thenofreelunchtheoremwasoriginallypresentedinaclassificationscenario[Wolpert,1996],andwas",
    "furtherextendedtosearchandoptimizationproblems[WolpertandMacready,1997].",
    "1.4 Model Selection and Evaluation 53\nModelComplexity\nrorrE\nTrainingerror\nTesterror\nFigure1.6: Curvesoftrainingerrorandtesterrorunderdifferentmodelcomplexities. Complex\nmodelshelpinreducingthetrainingerrorastheycancomputecomplexfunctionsinfitting\ndatapoints. However,atoolargemodelcomplexityismorelikelytoleadtooverfittingandis\nharmfultothegeneralizationabilityofthemodels. Forexample,thetesterrorincreasesas\nmorecomplexityisadded.\nOccam’sRazor. Supposewehaveasetofmodelsthatperformcomparablywellonthetraining\ndatabutareofdifferentcomplexities. AccordingtoOccam’sRazor,thesimplestmodelisthe\n“best”choice. Manycriteriaareavailabletomeasurethemodelcomplexity. Forexample,\n• Numberofparameters. Thoughverysimple,countingthenumberofparametersisthe\nmostintuitiveyeteffectivemethod. Itcanbeextendedtocountingtheeffectivenumber\nofparameterswhichisdefinedtobethetraceofthematrixusedtotransformy to\ngold\ny .\nθ\n• p-normofparameters. Thep-normofaparametermatrixisalsoanindicatorofhow",
    "complexamodelis(seeSection1.1.1). Forexample,accordingtothel norm,amodel\n1\nwithlargerabsolutevaluesforparametersismorecomplex.\n• Descriptionlength. Descriptionlengthisatermusedindatacompression. Forexample,\nitcouldbethenumberofbitsusedtostoreamodel. Thus,theminimumdescription\nlength(orMDL)indicatesthemostcompressedmodel.\n• TheVCdimension. Itisoriginallyfromcomputationallearningtheory. Inshort,theVC\ndimensioncanbedefinedasthemaximumnumberofdatapointsthatcanbeshattered\nbytheclassifier.\nInaddition,thereareotherchoicesfordefiningthecriterion,includingtheAkaikeinfor-\nmationcriterion(AIC),theBayesianinformationcriterion(BIC),theminimummessage\nlength(MML)andsoon. Theycanbefoundinmosttextbooksonstatisticsand/orstatistical\nlearning[BurnhamandAnderson,2002;KonishiandKitagawa,2007;Hastieetal.,2009].",
    "54 Chapter 1. Foundations of Machine Learning\n2. Bias-Variance Tradeoff\nControllingmodelcomplexitytoavoidoverfittingandunderfittingisalsolinkedtothetradeoff\nbetweenbiasandvariance. Bias(orpredictionbias)istheamountthatthemodelprediction\ndiffers from the true value. In statistics, bias is a systematic error that cannot cancel out\nevenifwerunalargenumberofrepeatedexperiments. Ingeneral,biaserrorresultsfromthe\nwrongassumptionsabouttheproblem,suchasapproximatinganon-linearproblemviaalinear\nmodel. Thisisveryinteresting! Wecanestablishtheconnectionofthebiaserrorherewith\ntheinductivebiasusedinmodedesign(seeSection1.3.2). Forexample,giventrainingdata,\nalargebiasmodelisusuallyduetothefactthattherearemoreassumptionsandthemodel\nis not complex enough. To make it simple, we would say that more (or stronger) inductive\nbiasescanresultinalowermodelcomplexityandmorebiaserrorinprediction. Occasionally,\nthetermbiasisusedasashortforbothbiasinprediction(fromastatisticsperspective)and",
    "inductivebias(fromamodeldesignperspective),althoughtheyareconsideredtohavedifferent\nmeanings20.\nVariance,ontheotherhand,describeshowspreadthepredictioniswhentherearevariations\nin training data. The variance error also correlates with model complexity. For example, a\ncomplexmodeltendstoexhibithighervariance.\nBoth bias and variance are sources of errors of a system. A common example is the\nbias-variancedecompositionofmeansquarederror. Hereweusesomenotationthatdiffers\nslightlyfromthatusedinprevioussections. LetD beasetofK trainingsamplesandf (·)\nθˆ(D)\nbeamodelleanedonD. Further,givenanewsamplex,lety =f (x)bethemodel\nθˆ(D) θˆ(D)\npredictionandy bethe“true”prediction. Thebiasandvariancearedefinedas:\ngold\nbias = E [y ]−y (1.87)\nD θˆ(D) gold\nvariance = E (cid:2)(cid:0)E [y ]−y (cid:1)2(cid:3) (1.88)\nD D θˆ(D) θˆ(D)\nwhereE [y ]isthemeanofy overallpossibleK sampletrainingdatasets. Thus,the\nD θˆ(D) θˆ(D)\nbiasissomesortofdifferencebetweenthemeanandthetruevalue,andthevarianceissome",
    "sortofdifferencebetweenthemeanandthepredictedvalue. Takingthemeansquarederroras\ntheerrormeasure,wecanwritetheexpectederroras:\nerror = E (cid:2)(cid:0) y −y (cid:1)2(cid:3)\nD θˆ(D) gold\n= bias2+variance (1.89)\nForlowermeansquarederror,reducingbothbiasandvariancesimultaneouslyisobviously\nanidealgoal. However,itisdifficulttomakeamodelthatexhibitsbothlowbiasandvariance.\nWhen one of the two decreases, the other increases (see Figure 1.7). Researchers must\nchoosetheoptimallevelofmodelcomplexitywhilepreventingtrainingfromoverfittingand\nunderfitting. This also depends on the problem we intend to solve. For example, a simple\n20Biasismoreoftenusedinstatisticstodescribesomeaspectofanestimator.",
    "1.4 Model Selection and Evaluation 55\nTestError\nBias\nVariance\nComplexity\nFigure1.7: Biasandvarianceagainstmodelcomplexity[Goodfellowetal.,2016]. Thecurves\nshowaconflictinreducingthebiaserrorandthevarianceerrorsimultaneously. Byvarying\nthemodelcomplexity,onecanobtaineitheralow-bias,high-variancemodelorahigh-bias,\nlow-variancemodel. Bothofthetwocasesexhibithightesterror. Forexample,ahigh-variance\nmodelisoftenofalargermodelcomplexity. Whilesuchamodelisabletodealwithcomplex\nproblems, it is more likely to overfit the data. On the other hand, a high-bias model often\nmeansasimplermodelbuttendstounderfitthedata. Toimprovethegeneralizationontest\ndata,onecanseekatradeoffbetweenbiasandvariance. Forexample,thereislowtesterror\nwhena“middlesized”modelischosen.\nmodel generally has low variance but high bias. However, if we use the simple model (say\nalinearmodel)todescribeacomplexproblem(sayanon-linearproblem),thenunderfitting\nwouldprobablyoccurbecausetheproblemistoo“hard”forthemodel.",
    "Returningtothemodelselectionproblem,thebias-variancetradeoffisnotaruleformodel\nselection,butaprinciplewemustkeepinmind. Often,oneneedstomakecompromisesto\ncreateamodelthatmakesreasonablygoodpredictions. Itisalsoworthnotingthat,inmany\napplications, complex models are usually accompanied with the inefficiency problem. An\nappropriatemethodmightbetostartwithasimplemodelandonlyaddcomplexitywhenitis\nneeded.\n3. Model Combination\nSelectingfromasetofmodelsisnottheonlywaytoreducegeneralizationerror. Alternatively,\nonecandothisintheoppositeway,andcombinethesemodelsfora“stronger”model. Sucha\nmethodiscalledensemblelearning[Senietal.,2010;Zhou,2012a]. Akeyideaofensemble\nlearningistocreateasetofcomponentmodels(orensemblemodels),suchthattheycanvote\nforabetterprediction. Thesimplestoftheseisamixturemodelthataveragesthepredicted\nscoresofmultiplecomponentmodels(callitmodelaveraging),whereasamoresophisticated\nmethodcancombinethesub-structuresofthesemodels.",
    "Component models are in general generated in some way that they can exhibit some\ndiversity. For example, they can be learned on different portions of the training data, or by\nusingdifferentinitializationsformodelparameters. Interestingly,itisfoundthatsuchmethods",
    "56 Chapter 1. Foundations of Machine Learning\ncanguaranteethereductionofgeneralizationerrorsomehow. Forexample,bagginghelpsto\nlowervariance[Breiman,1996],andboostinghelpstolowerbias[Schapire,1990]. Theseare\nlinkedbacktowhatwepresentedinSection1.4.1: thegeneralizationerrorcanbereducedby\neitherreducingthebiaserrororreducingthevarianceerror.\nButdiscussinghowtocombinemodelsisbeyondthescopeofthischapter. Whileitiseven\nnotappropriatetocategorizemodelcombinationasatopicrelatedtomodelselection,itcanbe\nseenasameansofimprovingthegeneralizationability. Inthissense,bothmodelcombination\nand model selection address problems on a similar theme. In fact, model combination is\nremarkablyeffectiveformanyNLPtasks. Forexample,moststate-of-the-artsystemsinNLP\narebasedonthecombinationofmultiplemodels.\n1.4.2 Training, Validation and Test Data\nWeturnnowtothedataproblem. Asdiscussedintheprevioussections,inthetrainingstage,a\ntrainingdatasetisusedtofittheparametersofthemodel. Intheteststage,atestdatasetisused",
    "toevaluatethelearnedmodel. Closelyrelatedtotestdataisvalidationdata,whichhascome\nupafewtimesinthischapter. Avalidationdatasetisatestdatasetaswellbutcanbeusedin\nthetrainingstage. Itiscommonlyusedformodelselectionandtuninghyperparameters.\nInmanycases,onemayimaginethatthereissomedatafortrainingandsomeadditional\ndata for validation and test. This assumption, however, is not realistic in many real-world\napplications. For example, developers cannot always access the data of system use after\ndeploying a system. From a scientific point of view, there is no “real” new data for test —\nwhenyouseenewdata,itisnotnewanymore. Therefore,whatweaddressisessentiallyan\nanalogueoftheproblem.\nA simple method, as in many research papers, is to verify machine learning models on\nbenchmarktasks. Inthesetasks,alldataispreparedinadvance,andallyouneedistorunyour\nmodelsonthedata. Suchamethodmakesiteasytocomparedifferentsystemsdirectly,asall",
    "thesesystemsaretrainedandtestedonthesamedatasets. Occasionally,wearejustgivena\nnumberofsamplesbutnottoldwhicharefortrainingandwhicharefortest. Insuchcases,the\ndatacanbedividedintopartseachofwhichisusedforsomepurposes. Forexample,asplit\ncouldbe60%fortraining,20%forvalidation,and20%fortest.\nWhiledatasplittingprovidesawaytoassesstheperformanceofamodel,theassessment\nresultisnotalwaysstableduetosamplingbias. Sometimes,theperformancevariesgreatly\nacrossdifferentrunsofdatasplitting. Theproblemismoreobviouswhenthedatasetistoo\nsmalltoperformsufficienttrainingortest.\nAcommonwaytoweakentheeffectofthisbiasiscross-validation. Cross-validationisa\nresamplingmethod. Eachroundofcross-validationisanewsplitofdataandtheresultisthe\ncombinationoftheassessmentovertherounds. Asimplemethodisrandomsubsamplingthat\nrepeatsrandompartitionofthedataandaveragestheperformanceoverruns. Anothermethod\nisk-foldcross-validation. Itdividesthedataintok parts. Ineachroundofcross-validation,",
    "somepartsareusedastrainingdata,andotherpartsareusedasvalidationandtestdata. For\nexample,in10-foldcross-validation,amodelcanbetrainedandvalidated/testedfor10times,\neachchoosingoneofthetenpartsasthetestdataset.",
    "1.4 Model Selection and Evaluation 57\nAnothernoteonthescaleofdata. Forpractitioners,oneofthemostfrequentquestions\nishowmanysamplesareenoughforlearningagoodmodel. Thismaybethemostdifficult\nquestiononwhichdifferentpeoplecanhaveconsensusanswers. Therearemanytheoretical\nresultsthatcantelltheboundoferrorsgivenacertainamountofdata,whereasinmostcases\nwe just simply follow the “the more the better” idea. In another line of thought, a system\ncould be sample efficient. In general, a sample efficient system can reach a good level of\nperformancebyusingfewersamplesorseeingthesamesampleforfewertimes. Forexample,\ntuningapre-trainedmodelissampleefficientbecausethesamplesarenotusedforlearning\nfromscratchbutamodestupdateofthemodel. Anotherexampleisfew-shotlearning. Itaims\ntogeneralizefromobservingveryfewsamplesforatask.\n1.4.3 Performance Measure\nAsanessentialpartofeverymachinelearningproblem,aperformancemeasuredescribeshow",
    "well a system performs given some data. Usually it is used in either designing the training\nobjectivesorevaluatingtheresultofthefinalsystem. Forexample,allthoselossfunctions\ndescribedinSection1.3.4aresomekindsofperformancemeasures.\nAsforevaluatingtheperformanceontestdata,ameasureisoftendesignedinawaythat\nwecancounttherealerrors. Thus,re-usingthelossfunctionsintrainingmightnotbeagood\nchoiceforreportingthefinalscore. Forexample,thewidely-usedmeasuresforclassification\nproblems are precision, recall and F score. They are proposed to quantify the ability of a\n1\nclassification system in certain aspects: given a class c, precision computes the fraction of\ncorrectpredictionsinpredictingc,andrecallcomputesthefractionofcorrectpredictionson\nallsampleslabeledasc. TheF scoreisameasurethatcombinesprecisionandrecall.\n1\nNoticethatperformancemeasuresarenotnecessarilydesignedforoptimization. Inthis\nsense, they may not guarantee some mathematical properties, such as differentiable and",
    "continuousfunctions. AnexampleistheBLEUmetricusedinmachinetranslation. BLEUisa\nfunctioncombiningprecisionscoresandapenaltyscore[Papinenietal.,2002]. Thisinturn\nmakesthemetricnon-differentiableanddiscontinuous. InNLP,therearemanysuchevaluation\nmeasures that are ad-hoc for certain tasks. These raise an interesting problem that the loss\nfunctionusedintrainingmaydifferfromwhatweactuallyuseinevaluatingthefinalmodel.\nThus, one sometimes needs to take into account the discrepancy between the objectives of\ntrainingandtest.\nAnotherproblemwithperformancemeasuresinNLPisthattheremightbetwoormore\n“answers”forthesame“question”. Forexample,therearegenerallymultiplegoodtranslations\nforasource-languagesentence. Onesolutionistotakemultiplegold-standardsintoaccount\nwhendesigningaperformancemeasure. BLEUissuchacase. Itcountsthemaximumnumber\nofthecorrecttranslationsegmentsoverallreferencetranslations. Thesecondsolutioninvolves\nhumanevaluation. Suchawayofevaluationismoreaccuratebutofcourseismoreexpensive.",
    "When developing practical systems, practitioners usually train and tune the systems using\nautomaticmeasures,andcallforhumanevaluationsforthefinaltest.",
    "58 Chapter 1. Foundations of Machine Learning\n1.4.4 Significance Tests\nNow, assuming you are improving a system in some way, you might be wondering if the\nimprovement is significant enough or not. All you have is a performance measure. So you\ncantelltheperformancedifferencebetweenanytwopointsindevelopingthesystem,butyou\ncannottellifthedifferenceisrealorhappensbychance.\nInthisexample,youimplicitlytrytorejectoracceptaclaimthatasystemisbetterthan\nanothersystem(ornot). Instatistics,significancetestsareamethodtomodelthisproblem.\nSuppose we have two systems A and B. And there are a number of datasets on each of\nwhichweevaluatethetwosystemsviathesameperformancemeasure. Then,wemaketwo\nhypotheses\nH : SystemAperformsworsethanorequallywellassystemB.\n0\nH : SystemAperformsbetterthansystemB.\n1\nwhereH isthenullhypothesis,andH isthealternativehypothesisthatiscontradictoryto\n0 1\nthenullhypothesis. Bytestingthesehypotheses,wecanclaimthatsystemAissignificantly",
    "better than system B (i.e., reject H and accept H ) or not (i.e., accept H and reject H ).\n0 1 0 1\nWeprobablymakeerrorsinthetest,forexample,incorrectlyrejectingatruenullhypothesis\n(typeIerror),orincorrectlyacceptingafalsenullhypothesis(typeIIerror). Thetwotypes\noferrorsareatoddswitheachother. Adecreaseofonemayleadtoanincreaseoftheother.\nAlternatively,wecandecreaseonewhileguaranteeingthattheotherisupperbounded. For\nexample,wecanreducethetypeIIerrorasmuchaspossible,andkeepthetypeIerrorbelow\na constant α. α is typically called the significance level of a test. It is standard practice to\nchoose the significance level in the interval [1%,5%]. When conducting statistical testing,\nwe can obtain the probability of the type I error (call it a p-value). A p-value that is lower\nthan the significance level can make a rejection of the null hypothesis. For example, in the\naboveexample,withasignificancelevelof5%,ap-value=3%meansthattheimprovement",
    "isstatisticallysignificant. Formoreinformationaboutthep-value,wereferthereadertoother\nbooksonstatistics[McClaveandSincich,2006;Freedmanetal.,2007;Freedman,2009].\nNote that the conclusion of significance tests depends on several factors, such as the\nnumber of experiments and the variance in the results of experiments. A problem with\napplyingsignificanceteststoNLPtasksisthatthereareoftenveryfewdatasetsforrunningthe\nexperiments[Droretal.,2020]. Ideally,weknowthetruedatadistributionandcanconsider\nit in the test. This method is called the parametric test. If we cannot find the true data\ndistribution,then,asanon-parametrictestmethod,wecangenerateanumberofexperiments\nbysamplingoveradatasetoraddingrandomnessintothetest.\nSignificancetestsareimportantfordrawingconvincingconclusionsindevelopingmachine\nlearningsystems,althoughtheyareoftenignoredunintentionally. Figure1.8showsevaluation\nresultsofthreemodels. Eachofthemisrunforseveraltimeswithdifferentinitialparameters.",
    "WhilesystemAissuperiortosystemB intermsoftheaveragedperformance,therearelarge\nvariancesintheirresults. Thesignificancetestindicatesthatthedifferenceisnotsignificant.\nBy contrast, the difference between system A and system C is significant because their\nperformancediffersgreatlyenoughinmostcases. Ontheotherhand,researchershavefound",
    "1.5 NLP Tasks as ML Tasks 59\nsystemC\nsystemB\nsystemA\nPerformance\nFigure1.8: Performanceofthreemachinelearningsystems. Foreachsystem,therearemany\ndifferent results because we introduce some randomness into training (e.g., data shuffling,\nrandomstartingpoints,etc). AlthoughitseemsthatSystemAoutperformsSystemB,there\nis no real distinction between them, because they overlap a lot in the distributions of the\nperformance(seethebottomofthefigure). WhencomparingSystemCwithSystemAorB,\nthedifferenceinperformanceissignificantbecausewecouldaccepttheH hypothesis(i.e.,\n1\nSystemCoutperformsSystemAorB)givenalargenumberofexperiments.\nthatthereareindeedsomethresholdsofperformancegaintoindicatesignificanceundercertain\ncircumstances. Forexample,wewouldsaythatthesignificancecanberoughlyindicatedbya\ncertainmetricgainifwecomparesimilarsystems[Berg-Kirkpatricketal.,2012].\n1.5 NLP Tasks as ML Tasks\nWhile there are a wide variety of NLP tasks, many of them can be formulated as the same",
    "machine learning problem. This enables a universal solution to a group of NLP problems\nby using a general machine learning approach. Typically, an NLP task can be described as\nlearningtomaplanguageunitstosomeoutput. Followingthenotationusedinthischapter,we\nusextodenotetheinputfeaturevector(ormatrix)ofanNLPtask,andusef(x)todenotethe\nfunctionthatislearnedtoprocessx. HerearesomeofthecommontasksinNLP.\n1.5.1 Classification\nSupposethereareasetofclassesorlabelsC. Eachclassisrepresentedbyadistinctintegerin\n{1,...,|C|}. Aclassificationmodelisafunctionthatmapstheinputxtoa|C|-dimensional\nvector y, i.e., y =f(x). Each entry of y is a score corresponding to class i, denoted by\ny(i). Thetaskhereistoassignxtooneormoreclasseshavingthehighestscores. Consider\nsingle-labelclassificationasanexample. Thepredictionisgivenbytheequation\ncˆ = argmaxy(i) (1.90)\n1≤i≤|C|\nwherecˆisthe“best”classassignedtox. Sometimes,oneneedsaprobability-likeoutput(see",
    "60 Chapter 1. Foundations of Machine Learning\nSection1.2.1). Letψ(·)beafunctionthatnormalizesavectorintoadistribution21. Wethen\nobtainaprobabilisticclassifier:\ny = ψ(f(x)) (1.93)\nClassification may be the most common problem in NLP. There are many applications\ninadditiontocategorizingdocumentsintopredefinedclasses. Amongthemarechoosinga\nsenseforaword[Yarowsky,1994],determiningthepolarityofasentence[Pangetal.,2002],\nchecking whether two entities should be linked [Krebs et al., 2018], classifying the way of\nassociatingasemanticargumentwithaverb[GildeaandJurafsky,2002],andsoon. When\nadaptingaclassificationmodeltothesetasks,allyouneedistodesigntheformofxandthe\nsetofclasses.\n1.5.2 Sequence Labeling\nAnextensiontostandardclassificationistoclassifyasetofsamplessimultaneously. Sequence\nlabelingisanexampleofsuchaproblem. Insequencelabeling,theinputisasequenceofn\ntokens,suchasasequenceofnwords. Asequencelabelingsystemisrequiredtoassigneach",
    "input token x(i) a label l(i). Here the boldface in x(i) is used to emphasize that the token\nis represented as a feature vector. For convenience, we write x(i) as x and l(i) as l . The\ni i\nfunctionf(·)mapsthesequencex ...x intoanothersequencey ...y ,wherey istheoutput\n1 n 1 n i\nvectorcorrespondingtox . Thiscanbeformulatedas:\ni\n(cid:104) (cid:105) (cid:104) (cid:105)\ny ... y = f( x ... x ) (1.94)\n1 n 1 n\nForvectory ,eachentryy (c)correspondstothepredictionscoreofaclassc∈C. Note\ni i\nthatf(·)allowsfortheuseofalargercontext. Forexample,onecanconditiontheprediction\ny ontheentireinputsequence[Laffertyetal.,2001]. Thefinaloutputofthesystemcanbe\ni\ndefinedasthe“optimal”labelsequenceinducedfromy ...y . Asimplemethodistochoose\n1 n\nthelabelsequencethatmaximizesthesumofthescoresoverallpositions,likethis\nn\n(cid:104) (cid:105) (cid:88)\nˆl ... ˆl = argmax y (l ) (1.95)\n1 n i i\nl1,...,ln∈C\ni=1\nAstraightforwardapplicationofsequencelabelingtoNLPistotageachtokenoftheinput",
    "sequence,suchaspart-of-speechtagging(orPOStagging). Furthermore,sequencelabeling\n21AsimilarideacanbefoundinEq.(1.48).Givenavectora= (cid:2) a(1) ... a(n) (cid:3) ,thenormalizationfunction\nhastheform:\n(cid:104) (cid:105)\nψ(a) = (cid:80)n a(1 a ) (i) ... (cid:80)n a(n a ) (i) (1.91)\ni=1 i=1\nAnotherwayisusingtheSoftmaxfunction:\n(cid:104) (cid:105)\nψ(a) = (cid:80)n exp e ( x a p (1 (a )) (i)) ... (cid:80)n exp e ( x a p (n (a )) (i)) (1.92)\ni=1 i=1",
    "1.5 NLP Tasks as ML Tasks 61\nTokens: Most are expected to fall below previous- levels .\nmonth\nPOS tags: JJS VBP VBN TO VB IN JJ NNS .\nChunk tags: B-NP B-VP I-VP I-VP I-VP B-PP B-NP I-NP O\nNP VP PP NP\nFigure1.9: AnexampleofsequencelabelingforPOStaggingandchunking. Theexample\nisfromthetrainingdataoftheCoNLL2000sharedtask. EachtokenislabeledwithaPOS\ntagandachunktag. Achunktaghasaninitialcharacterchosenfrom{B,I,O}, whereB=\nbeginningofachunk,I=insideachunk,andO=outsideachunk. So,achunkalwaysstarts\nwitha“B”tag,optionallyfollowedby“I”tags. Forexample,theVP(verbphrase)chunkin\ntheexamplespansoverthechunktagsequence“B-VPI-VPI-VPI-VP”.\nisabletodealwithmorecomplexproblemsbyusinglabelsinacleverway. Awell-known\nexampleistheuseofthe“IOB”labelformatinidentifyingchunksspanningmultipletokens\n(callitchunking). Inthismethod,“I”,“O”and“B”standforatokeninsideachunk,atoken\noutsideachunk,andtheleftmosttokenofachunk[RamshawandMarcus,1995]. Assuch,a",
    "chunkalwaysstartswitha“B“andendsjustbeforethenext“B”oranew“O”.SeeFigure1.9\nforPOStaggingandchunkingresultsonanexamplesentence. Assequencelabelingallows\nthelabelingofbothtokensandspans,ithasbeenappliedwithstrongresultstomanytasks,\nincludingPOStagging[BahlandMercer,1976],chunking[TjongKimSangandBuchholz,\n2000],namedentityrecognition(NER)[TjongKimSang,2002],andsoon.\n1.5.3 Language Modeling/Word Prediction\nStatistical language modeling (or language modeling for short) is a task of assigning a\nprobabilityPr(w ,...,w )toasequenceofwordsw ...w . Thisjointprobabilityisgenerally\n1 n 1 n\ndecomposedintoaproductofconditionalprobabilities,byusingthechainrule:\nPr(w ,...,w ) = Pr(w )·Pr(w |w )···Pr(w |w ,...,w )\n1 n 1 2 1 n 1 n−1\nn\n(cid:89)\n= Pr(w |w ,...,w ) (1.96)\ni 1 i−1\ni=1\nEq. (1.96)describesaprocedurethatgeneratesawordsequencefromlefttoright(call\nitauto-regressivegeneration). EstimatingPr(w |w ,...,w )isessentiallyamissingword\ni 1 i−1",
    "predictionproblem: wemaskoutthelastwordofasequenceandguidethelanguagemodelto\npredictthecorrectwordatthatposition. Seebelowforawordsequencewherethelastwordis\nmissing.\nPrideandprejudiceisoneofthebestknown\nWe can reuse the idea in classification to model the probability distribution Pr( |",
    "62 Chapter 1. Foundations of Machine Learning\nPride,and,...,known). Letx bethevectorrepresentationofw . Wecandefineafunctionthat\ni i\nreadsx ...x andproducesavectorh :\n1 i−1 i\nh = f(x ,...,x ) (1.97)\ni 1 i−1\nwhere h is the intermediate states of the word distribution at position i. For a sounding\ni\ndistribution,wenormalizeh bysomenormalizationfunctionψ(·). Thus,thedistributionat\ni\npositioniwouldbe\ny = ψ(h )\ni i\n= ψ(f(x ,...,x )) (1.98)\n1 i−1\nObviously,y (w )istheprobabilityofw givenpreviouswords,i.e.,y (w )=Pr(w |w ,...,w ).\ni i i i i i 1 i−1\nNotethatEq. (1.96)onlyconsiderstheleftcontextwhenpredictingaword. Anaturalextension\ntothisistoconditionthepredictiononallavailablecontext. Consider,forexample,asentence\nwithamaskedwordinthemiddle.\nPrideand isoneofthebest-knownnovels\nInthisexample,wecanpredictthemaskedwordbyusingboththeleftcontext(Prideand)\nandtherightcontext(isoneofthebestknownnovels):\ny = ψ(f(x ,...,x ,x ,...,x )) (1.99)\ni 1 i−1 i+1 n",
    "Thisisabidirectionalmodel,andiscommonlyusedinauto-encodingmethodsforlearning\nsequencerepresentationmodels[Devlinetal.,2019].\n1.5.4 Sequence Generation\nSequencegenerationcoversarangeofNLPproblems, includingmachinetranslation, sum-\nmarization, questionanswering, dialoguesystems, andsoon. Usually, itreferstomapping\nsome data to a sequence. Here we focus on the sequence-to-sequence problem, in that a\nsource-sidesequenceistransformedtoatarget-sidesequence,althoughsequencegenerationis\nnotspecializedtoworkwithchainstructuresonthesource-side.\nFor notation convenience, we use boldface variables to denote sequences from now on.\n(cid:104) (cid:105)\nFor example, a is a sequence of size n. It can be written as either a ... a or a ...a .\n1 n 1 n\nLets=s ...s andt=t ...t bethesequencestotransformfromandto. Thesequence-to-\n1 m 1 n\nsequenceproblemcanbedescribedasfindingatarget-sidesequencethatmaximizesPr(t|s):\nˆt = argmaxPr(t|s) (1.100)\nt",
    "1.5 NLP Tasks as ML Tasks 63\nLikelanguagemodeling,Pr(t|s)canbeformalizedinanauto-regressivefashion:\nPr(t|s) = Pr(t ,...,t |s)\n1 n\n= Pr(t |s)·Pr(t |s,t )···Pr(t |s,t ,...,t )\n1 2 1 n 1 n−1\nn\n(cid:89)\n= Pr(t |s,t ,...,t ) (1.101)\ni 1 i−1\ni=1\nEq. (1.101) differs from Eq. (1.96) only in the additional condition (i.e., s) introduced\ntotheseprobabilities. Inthissense,wecanuseEqs. (1.97-1.98)tosolvePr(t |s,t ,...,t ).\ni 1 i−1\nOn the other hand, involving s makes the problem more difficult, as we need to model the\ncross-sequence relationship between s and t . A recent trend in sequence generation is to\ni\nformulate Pr(t |s,t ,...,t ) in the encoder-decoder paradigm. There are two steps: an\ni 1 i−1\nencoderisfirstusedtorepresentsassomeintermediateform(e.g.,avector),andadecoderis\nthenusedtomodelboththetarget-sidewordsandthecorrelationbetweentheencoderoutput\nandthetarget-sidewords. Puttingthesetogether,theoutputoftheencoder-decodermodelcan\nbedefinedtobe\ny = Dec(Enc(s),t ,...,t ) (1.102)\ni 1 i−1",
    "where Enc(·) is the encoder, and Dec(·) is the decoder. y is a distribution of the target-\ni\nside word at position i, i.e., y (t )=Pr(t |s,t ,...,t ). Chapter 5 will provide a detailed\ni i i 1 i−1\ndescriptionoftheencoder-decodermodel.\n1.5.5 Tree Generation\nInNLP,treesareusuallyusedtorepresentthestructuresormeaningsofsequentialdata. For\nexample,asyntacticparseranalyzesasentencetoformasyntaxtreeorparsetree. More\nformally,givenasequenceofwordss=s ...s ,theparsingproblemcanbedefinedas:\n1 m\ndˆ = argmaxPr(d|s) (1.103)\nd∈D\nwheredisaparsetree,andDisthesetofallparsetreesyieldings ...s . ComputingPr(d|s)is\n1 m\nchallenging,asthemodelingcomplexityincreasesexponentiallywhenmovingfromsequences\ntotrees. Instatisticalparsing,asolutionistomodeldasaderivationofsyntacticrules. In\nthisway,Pr(d|s)canbeformulatedasaproductofruleprobabilities. Figure1.10presents\nanexampleofparsingwithcontext-freegrammar(CFG)rules. Alternatively,Pr(d|s)canbe",
    "modeledinanend-to-endmanner. Forexample,somerecentapproachesperformparsingby\ndefininganeuralnetworkovertheparsetree. Theprobabilityofasub-treerootingatanodeis\ncomputedbyconsideringtheinteractionbetweenthisnodeandchildnodes.\nAnotherideaistoframeparsingassequencegeneration. Forexample,onecanlinearizea\nparsetreeandrepresentitasasequenceofwordsandsyntacticlabels,ortransformthetree\ngeneration process as a sequence of actions. This allows the use of sequence-to-sequence\ntechniquesinaddressingasequence-to-treeproblem.",
    "64 Chapter 1. Foundations of Machine Learning\nParse Tree: CFG Rules:\nS r : PRP → He\n1\nr : VBD → declined\n2\nNP VP .\nr : TO → to\n3\nr : VB → elaborate\nPRP VBD S 4\nr : . → .\n5\nHe declined VP . r 6 : NP → PRP\nr : VP → VB\n7\nTO VP r : VP → TO VP\n8\nr : S → VP\n9\nto VB\nr : VP → VBD S\n10\nr : S → NP VP .\nelaborate 11\nP(d|s)=P(PRP→He)·P(VBD→declined)·P(TO→to)·P(VB→elaborate)·\nP(.→.)·P(NP→PRP)·P(VP→VB)·P(VP→TO VP)·P(S→VP)·\nP(VP→VBDS)·P(S→NP VP .)\n=\n(cid:81)11\nP(r )\ni=0 i\nFigure1.10: AnexampleparsetreeandCFGrules. Thesentenceisfromthetrainingdataof\ntheCoNLL2000sharedtask. TheparsetreeisrepresentedasaderivationofCFGrules. The\nprobabilityoftheparsetreeisdefinedastheproductofruleprobabilities.\nInlinguisticsandNLP,treestructuresareinheavyuseforsyntacticanalysis. Inaddition\ntoparsingsentences,theyarealsoattributedtowords,phrases,anddiscourses. Ontheother\nhand,treesarenottheonlywayofvisualizingcomplexnon-linearstructures. Amoregeneral",
    "conceptisagraph. Whiletreescanbethoughtofasspecialgraphs,therearecasesthattrees\ncannothandle[Fellbaum,2005;Singhal,2005;Banarescuetal.,2013]. Forexample,inthe\nsemanticrepresentationofasentence,weoftenneedagraphtoconnectverbsandarguments.\nWhilelearninggeneralgraphsisharderthanparsingasentenceintoatree,wecanreusemany\nofthemethodsdevelopedinsequenceandtreegeneration.\n1.5.6 Relevance Modeling\nGenerallyspeaking,relevanceisreferredtoashowwellathingrelatestoanother. Theconcept\nofrelevanceisusedinmanydifferentsub-fieldsofNLPandinformationscience. Forexample,\nin information retrieval, relevance is used to describe to what extent a retrieved document\nmeetsthequery. Additionalusesofthisconceptcanbefoundinquestionanswering,dialogue\nsystems,semanticanalysis,andallothertasksthatrequireamatchingorretrievalprocess.\nLetusconsideramoregeneraldescription. Assumethatwehaveaqueryquery andakey\nkey that representssomething we intendto match withquery. Then, we define thefeature",
    "1.5 NLP Tasks as ML Tasks 65\nvectorsofquery andkey as\nq = Q(query) (1.104)\nk = K(key) (1.105)\nQ(·)andK(·)arefeatureextractors. Therelevancebetweenquery andkey isgivenbythe\nfunction:\nr = f(q,k) (1.106)\nf(·)couldbeononehandsimplyadistancemeasureifqandkareinthesamevectorspace,\nandontheotherhandamorecomplexmodelthatperformssomenon-lineartransformations.\nInfact,thewayofdefiningrelevancecanbeadoptedinseveraldifferentscenarios. Sometimes,\nrelevanceisalsotermedassimilarityorcorrelation. Ageneralexampleishowsimilartwo\nobjectsare. Letxandy betwosamples(saytwowords). Thesimilarityofxandy isgivenby\nr = f(g(x),g(y)) (1.107)\nwhere g(·) is a feature extractor, and f(·) is a similarity function. Learning both g(·) and\nf(·)iscalledsimilaritylearning. Inonesetupofsimilaritylearning,wefixf(·)andlearn\ng(·)inawaythatsimilarsamplesexhibitsimilaroutputsofg(·). Thelearningofthefeature\nextractorisnotevenrequiredtoworkwiththesimilarityfunction. Forexample,forobtaining",
    "thesimilaritybetweenwords,wecanlearng(·)inalanguagemodelanduseittogetherwith\nvarioussimilarityfunctions. Thisputstheprobleminalargertopicofmachinelearning: the\nlearningofasub-modelisindependentoftheproblemwhereweuseit. Suchanideaiswidely\nadoptedinpre-training,advancingtherecentstate-of-the-artonmanyNLPtasks.\nInanothersetupofsimilaritylearning,wecanlearnf(·)directly. Thiscanbeperformedby\neitherjointlylearningf(·)andg(·),orlearningf(·)ontopoffixedg(·). Theproblemisalso\nrelatedtometriclearning. Typically,metriclearningisframedasasupervisedproblem[Kulis,\n2013]. A desired similarity function could be learned with the supervision regarding some\ngold-standardsimilarity. However,inpracticethereisusuallynosuchsupervisedinformation\ninNLP.Inthiscase,onecouldtakerelativedistanceassomesupervision. Forexample,the\nsimilarityfunctioncanbelearnedbyoptimizingacontrastiveloss(seeSection1.3.4).\nMeasuringthesimilaritybetweenobjectsplaysanimportantroleinmanymachinelearning",
    "methods, such as clustering and nearest neighbor classification. On the side of NLP, it is\nusefulforexploringtherelationshipbetweenwords,phrases,sentences,anddocuments,e.g.,\nsimilarity is a way to examine how word vectors correspond to our understanding of word\nmeanings[Mikolovetal.,2013c;Penningtonetal.,2014].\n1.5.7 Linguistic Alignment\nLinguisticalignmentisasetofproblemswhereweestablishsomecorrespondencebetween\ntwosetsoflinguisticunits. InNLP,thesequence-to-sequenceandsequence-to-treeproblems\naretypicallylinguisticalignmentproblems,astheybothconnecttwolinguisticunits. However,",
    "66 Chapter 1. Foundations of Machine Learning\nbyconvention,thetermalignmentisreferredtoasaligningmultipleobjectssimultaneously22.\nAs an example, consider the well-known word alignment task: we align the words of\nasentencetothewordsofanothersentence. WereusethenotationinSection1.5.4asboth\nthesequence-to-sequenceandwordalignmenttasksperformonapairofsequences. Givena\nsource-sidewordsequences=s ...s andatarget-sidewordsequencet=t ...t ,theword\n1 m 1 n\nalignmentbetweenthetwosequencesisdenotedasanm×nmatrixA. A(i,j)=1ifthereis\nanalignmentlinkbetweens andt ,andA(i,j)=0otherwise. Theoptimalalignmentcan\ni j\nbedefinedas:\nAˆ = argmaxPr(A|s,t) (1.108)\nA\nwherePr(A|s,t)isthewordalignmentprobability. Likeinothermachinelearningproblems,\nwecanmodelPr(A|s,t)ineitheragenerativeordiscriminativemanner(seeSection1.2.4).\nForexample,inBrownetal.[1993]’swork,thewordalignmentmodelisfactoredintoseveral\ngenerativesteps,eachaccountingforsomeassumptionsabouttheproblem23.",
    "A0-1alignmentmatrixindicatesahardwayofwordalignment. Aproblemhereisthat\nthehardmodelmaynotdescribewellthehighlyambiguouswordalignments. Wetherefore\ncan represent A as a real-valued matrix (call it a soft word alignment matrix or a word\nalignmentweightmatrix). Assumethatthesource-sidewordsarerepresentedasasequence\n(cid:104) (cid:105)\nof feature vectors x= x ... x . Likewise, the target-side words are represented as\n1 m\n(cid:104) (cid:105)\ny= y ... y . Asoftwordalignmentmodelisgivenby:\n1 n\nA = a(s,t) (1.109)\nwherea(·)isanwordalignmentfunctionthatcomputesthealignmentweightA(i,j)foreach\npair of x and y . In fact, all the methods discussed in Section 1.5.6 are applicable to the\ni j\ndesign of a(·). This somehow links the modeling of word alignment with the modeling of\nsimilarity,andmakesitpossibletoaddressdifferentNLPproblemsbyusingthesamemachine\nlearningapproach.\nEq. (1.109) offers a very general way to discover the underlying connection over pairs",
    "of variables. In addition to aligning words in sequences, it is useful for aligning unordered\nobjects. Forexample,inbilingualdictionaryinduction,wecanlearnsuchaweightmatrixto\nestimatehowstrongawordinonelanguagecorrespondstoawordinanotherlanguage.\nHere is another note on linguistic alignment models. While linguistic alignment could\nbe thought of as an independent NLP task, it is commonly used in designing sub-models\nof some downstream systems. Many systems that model word-level relationships involve\nimplicit representation of linguistic alignment. As a consequence, linguistic alignment is\ntreatedassomelatentstates,andisaby-productofthesesystems. Forexample,intheearly\n22Theconceptofalignmentiswide-ranging.Weusethetermlinguisticalignmentheretodifferentiateitfrom\nthealignmentoflargelanguagemodelsdiscussedinsubsequentchapters.\n23Moreprecisely,Brownetal.[1993]modelPr(A,s|t)whichisasurrogateofPr(A|s,t),asPr(A|s,t)=\nP P r( r A (s , | s t | ) t) = (cid:80) A P ′ r P (A r( , A s| ′ t , ) s|t)",
    "1.5 NLP Tasks as ML Tasks 67\nageofstatisticalmachinetranslation,wordalignmentisahiddenvariableusedinmodeling\nthe mapping between sequences. The word alignment result can be easily induced from a\nmachine translation model. More recently, neural sequence-to-sequence models — most\nnotablyattentionalmodels[Bahdanauetal.,2014]—haveattemptedtodosomethingsimilar\ntowordalignmentbycomputingattentionweightsamongwords.\n1.5.8 Extraction\nInNLP,extractionisnotakindoftaskbutakindofbehaviorthatasystemexhibits. Informally,\nit denotes a process of gathering, distilling structured information from some information\nsources. So,thetermextractiongenerallyappearstogetherwithothertermstoformaspecific\ntask,suchaskeywordextraction,eventextraction,andrelationextraction. Manyofthese\ntaskscanbecategorizedintoanarea—informationextraction. Informationextractionis\nperhapsthebroadesttopicinNLP.Thereisevennoexhaustivelistofinformationextraction",
    "tasks. AccordingtoJurafskyandMartin[2008]’sbook,itincludesbutisnotlimitedtonamed\nentityrecognition,referenceresolution,relationextraction,eventextraction,templatefilling,\nandsoon.\nHowever, since information extraction is a “miscellany” of many different problems,\nit cannot be formulated as a single machine learning problem. Fortunately, most of these\nproblems can be framed as standard machine learning problems, such as classification and\nsequencelabeling,andcanbesolvedbyusingtheoff-the-shelftools. Insomecases,itmay\nrequireaslightupdateofexistingmodelsforadaptationtoanewtask. Forexample,extracting\na specific segment from text may require the system to produce a span that indicates the\nbeginningandendingpositionsoftheextractedsegment(callitspanprediction).\nOnthepracticalside,machinelearningisnotalwaysnecessaryinextractinginformation\nfromtext. Manyproblemscanbesolvedbyusinghand-craftedrules. Anexampleisusing",
    "regularexpressionstoidentifylocationsanddatesintext. Inpracticereal-worldsystemsare\nusuallycombinationsofheuristicmethodsandautomaticmachinelearningmethods.\n1.5.9 Others\nFigure 1.11 shows illustrative examples of the above NLP tasks. Note that many of the\ndiscussionsherearestillpreliminaryandincomplete. Forexample,weonlytalkedaboutNLP\nproblems in the supervised learning paradigm. Many unsupervised tasks are important for\nNLPresearchaswell. Forexample,itiscommontoclusterunlabeledwordsordocumentsto\neasetheprocessingindownstreamsystems. Severalmethodsaredirectlyapplicabletothis\ntask[Murphy,2012]. ArecenttrendinNLPisthatitisnotnecessarytosetastrictboundary\nbetweentheuseofsupervisedlearningandtheuseofunsupervisedlearning. Inmanycases,\nunsupervised methods help supervised tasks, and vice versa. A notable example is that we\nlearnapre-trainedfeatureextractoronunlabeleddataandbuildasupervisedclassifierontop\nofit. Thisleadstoanothertrendrunningtowardsimprovingrepresentationmodels(i.e.,feature",
    "extractors)withouttheneedofaccessingdownstreamsupervisedtasks.",
    "68 Chapter 1. Foundations of Machine Learning\n1.6 Summary\nThischapterhasgiventhebasicideasofmachinelearninganditsapplicationstoNLPproblems.\nInparticular,wehavepresentedasimpletextclassificationproblemtogetstartedwithmachine\nlearning. Also,wehavediscussedseveralgeneralproblemsonmachinelearning,e.g.,typesof\nmachinelearningmethods,inductivebiases,lossfunctions,overfittingandsoon. Theyare\nfollowedbyadiscussiononmodelselectionandassessment. Inaddition,wehavedescribed\nhowmodelNLPproblemsareframedasmachinelearningproblems.\nHowever,machinelearningisahugeresearchfield. Thereareseveralinterestingtopicswe\nleftout. Onetopicthatwesaidlittleaboutisreinforcementlearning. Ingeneral,reinforcement\nlearningisverypowerful. Itshouldbeandhasbeenconsideredasanapproachtoaddressing\nNLPproblems,e.g.,trainingasequence-to-sequencebyusingarisk-basedlossfunction. A\nreinforcementlearningtextbookwillofferthegeneralideasofreinforcementlearning[Sutton",
    "andBarto,2018]. AnothertopicwemissedhereisBayesianlearning[Gelmanetal.,2020;\nMcElreath,2020;Downey,2021]. Itopensupanotablestrandofresearchinstatisticallearning,\nandhasbeensuccessfullyusedinNLPtasks. Moreover,therearemanyothertopicsthatare\nspecializedincertainaspectsofmachinelearningandareofinteresttoNLPresearchersand\nengineers. Someofthemareefficientmachinelearning[Tayetal.,2020b],multi-tasklearning\n[ZhangandYang,2021],few-shot/zero-shotlearning[Wangetal.,2019b;2020c],andsoon.\nAfinalpointtowrapupthischapter. Weskipthedetaileddiscussiononcertainmachine\nlearning models and algorithms, such as classification and regression models, because the\nreader interested in them can find several excellent, comprehensive introductions [Bishop,\n2006; Hastie et al., 2009; Murphy, 2012; Mohri et al., 2018]. In the next chapter we will\ndiscussabitmoreaboutartificialneuralnetworkswhicharethebasisofdeeplearningand\nrecentstate-of-the-artNLPmodels.",
    "1.6 Summary 69\nclasscˆ\nˆl1=B ˆl2=I ˆl3=O ˆl4=B ˆl5=B\nSequenceLabeler\nClassifierf(x) (cid:2) (cid:3)\nf( x ... x )\n1 n\nSamplex(featurevector) x 1 x 2 x 3 x 4 x 5\n(a)Classification (b)SequenceLabeling\nw 5 t 3\nLanguageModel Encoder Decoder\nPr(w i |w 1 ,...,w i−1 ) Enc(s) Dec(Enc,t1...ti−1)\nw 1 w 2 w 3 w 4 s 1 s 2 s 3 t 1 t 2\n(c)LanguageModeling (d)SequenceGeneration(seq2seq)\n0.82\nS\nSimilarityFunc. f(q,k)\nParser NP VP\nPr(d|s)\nNNS VBD NN q=Q(query) k=K(key)\ns 1 s 2 s 3\ns1 s2 s3\nquery key\n(e)TreeGeneration(seq2tree) (f)RelevanceModeling\nt t t span=[2,4]\n1 2 3\nt1 t2 t3\n.8 s1\nAligner Extractor\n1 .6 s2\na(s,t) Extracteeverythingneeded,e.g.,asegment\n.7 s3\ns s s s s s s s\n1 2 3 1 2 3 4 5\n(g)LinguisticAlignment (h)Extraction\nFigure1.11: Naturallanguageprocessingtasksfromamachinelearningperspective.",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 2\nFoundations of Neural Networks\nArtificial neural networks (or neural networks, or neural nets for short) are powerful\nmachinelearningtoolsthathaveadvancedthepreviousstate-of-the-artinNLPinrecentyears.\nHowever,althoughthehistoryofneuralnetworkscanbetracedbacktothe1940s[McCulloch\nand Pitts, 1943], for quite a long time neural networks have not been found to consistently\noutperformothermachinelearningcounterparts. Thechangebeganaround2006when“new”\nideasweredevelopedtolearndeepneuralnetworks[Hintonetal.,2006;Hinton,2007]. Such\nmethodshavesincebeenknownasdeeplearning. Todate,deeplearninghasnodoubtbecome\noneofthemostactive,influentialareasinartificialintelligence,whileithasreceivedbenefits\nfromnotonly“deep”modelarchitecturesbutalsomany,manytechniqueswhichhelptolearn\nandusesuchmodels.\nIn this chapter, we will present the basic ideas of neural networks and deep learning.",
    "The chapter is not cutting-edge but covers several important concepts and techniques that\narewidelyusedinimplementingneuralsystems. Thisincludesbasicmodelarchitecturesof\nneural networks, training and regularization methods, unsupervised learning methods, and\nauto-encoders. Wewillalsopresentanexampleofusingneuralnetworkstosolvethelanguage\nmodelingproblem.\n2.1 Multi-layer Neural Networks\nTogetstarted,wegiveaquickintroductiontosingle-layerperceptrons,andextendthemtoa\nmoregeneralcasewheremultipleneuralnetworksarestackedtoformamorecomplexone.\n2.1.1 Single-layer Perceptrons\nSingle-layerperceptrons(orperceptronsforshort)maybethesimplestneuralnetworksthat\nhavebeendevelopedforpracticaluses[Rosenblatt,1957;MinskyandPapert,1969]. Often,it\nisthoughtofasabiologically-inspiredprogramthattransformssomeinputtosomeoutput.\nA perceptron comprises a number of neurons connecting with input and output variables.\nFigure2.1showsaperceptronwherethereisonlyoneneuron. Inthisexample,therearetwo",
    "real-valuedvariablesx andx forinputandabinaryvariabley foroutput. Theneuronreads\n1 2",
    "72 Chapter 2. Foundations of Neural Networks\ny\nneuron:\n(cid:40)\n1 x ·w +x ·w +b>0\n1 1 2 2\ny=\n0 otherwise\nw 1 w 2 b\nx 1 x 2 1\nFigure2.1: Aperceptronwithtwoinputvariables{x ,x }andanoutputvariabley. Thereare\n1 2\ntwoweights{w ,w }, eachcorrespondingtoaninputvariable. Theoutputdependsonthe\n1 2\nsumoftheweightedinputvariablesandthebiastermb,say,y=1ifx ·w +x ·w +b>0,\n1 1 2 2\nandy=0otherwise.\ntheinputvariablesanddetermineswhichoutputvalueischosen. Thisprocedureislikewhata\nbiologicalneurondoes—itreceiveselectrochemicalinputsfromotherneuronsanddetermines\niftheelectrochemicalsignalispassedalong.\nInamathematicalsense,aperceptroncanbedescribedasamappingfunction. Letxbea\nvectorofinputvariables(i.e.,afeaturevector). Anaffinetransformationofxisgivenby1:\nf(x) = x·w+b\n(cid:88)\n= x ·w +b (2.1)\ni i\ni\nwherewisaweightvectorandbisabiasterm. Then,astandardperceptroncanbedefinedto\nbe:\ny = ψ(f(x))\n(cid:40)\n1 f(x)>0\n= (2.2)\n0 otherwise",
    "whereψ(·)isabinarystepfunction. Anothernameforψ(·)isactivationfunction. Thislinks\ntheperceptrontotheclassificationmodelsdiscussedinSection1. Inotherwords,Eq. (2.2)\nisaclassifieritself: ψ(·)isadiscriminatefunctiondefinedoneachinputx,followedbyan\nactivationfunctionψ(·)usedforproducingadesirableoutput2.\nIncasetherearetwoormoreneurons,wecangrouptheseneuronsintoalayer. Asshown\ninFigure2.2,alltheneuronsinalayerreceivesignalsfromthesameinputfeaturevectorbut\nareweightedindifferentways. Theoutputofthelayerisanewfeaturevector,eachentryof\n1Inmathematics,alineartransformationmapseachvectorvinaspacetof(v)inanotherspace,satisfying\nforanyvectorsxandy,andscalarsαandβ,wehavef(αx+βy)=αf(x)+βf(y).Anaffinetransformationis\nalineartransformationfollowedbyatranslation,oftenwrittenintheformf(x)+b.\n2Sincethestepfunctionisalinearcombinationofindicatorfunctions,theperceptronisalinearclassifier.",
    "2.1 Multi-layer Neural Networks 73\ny y y y\n1 2 3 4\nw\n1\ny x w11 w12 w13 w14\n(cid:16)\ny1 y2 y3 y4 =ψ x1 x2 × w21 w22 w23 w24\nb\n1\n(cid:17)\n+ ×\n1 b1 b2 b3 b4\nx 1 x 2 1\nFigure 2.2: A single-layer perceptron involving four neurons. All these neurons receive\ninformationfromtheinputvariables{x ,x }. Theperceptrondescribesaprocessinthat1)\n1 2\nwefirsttransformtheinputvectorofvariablesbyanaffinetransformationf(x)=x·w+b;\n2)andthencomputetheoutputbyfeedingf(x)intotheactivationfunctionψ(·).\nwhichcorrespondstoaneuron. Moreformally,takingψ(·)andf(·)asvectorfunctions,the\nmathematicalformofthesingle-layerperceptronisgivenbytheequations:\ny = ψ(f(x)) (2.3)\nf(x) = x·w+b (2.4)\nwherex∈Rm,y∈Rn,w∈Rm×n andb∈Rn.\nAnothernoteontheactivationfunction. Thestepfunction,thoughextensivelyused,isnot\ntheonlyformoftheactivationfunction. Therearemanydifferentwaystoperformactivation.\nForexample,wecanusetheSoftmaxfunctionifwewantaprobabilitydistribution-likeoutput;",
    "wecanusetheSigmoidfunctionifwewantamonotonic,continuous,easy-to-optimizeoutput;\nwe can use the ReLU function if we want a ramp-shaped output. Table 2.1 shows several\ncommonly used activation functions. Note that, although a layer of neurons equipped with\nthese activations can be loosely called a single-layer perceptron, it can be categorized as a\nmoregeneralconcept,calledasingle-layerneuralnetwork. Ifnotspecifiedotherwise,we\nwillusethetermsingle-layerneuralnetworkthroughoutthisdocument.\n2.1.2 Stacking Multiple Layers\nAnextobviousstepistocreateaneuralnetworkcomprisingmultiplelayers. Todothis,allwe\nneedistostackmultiplesingle-layerneuralnetworkstoformamulti-layerneuralnetwork.\nSeeFigure2.3foranexample. Inthismulti-layerneuralnetwork,theoutputofeveryneuron\nofalayerisconnectedtoallneuronsofthefollowinglayer. Sothenetworkisfullyconnected.\nEssentially,amulti-layerneuralnetworkdescribesacompositionoffunctions. Forexample,",
    "wecanformulatetheneuralnetworkinFigure2.3asafunctionyieldedbycomposingafew\nsimplefunctions:\ny = Softmax(Sigmoid(ReLU(x·w )·w )·w +b ) (2.5)\n1 2 3 3",
    "74 Chapter 2. Foundations of Neural Networks\nName Formula(forentryiofavector)\nIdentity y =s\ni i\n(cid:40)\n1 s >0\ni\nBinaryStep y =\ni\n0 s ≤0\ni\nexp(s )−exp(−s )\ni i\nHyperbolicTangent y =\ni\nexp(s )+exp(−s )\ni i\n\n1 s >1\n i\n\nHardTangent y = s −1≤s ≤1\ni i i\n\n−1 s <−1\ni\n1\nSigmoid(Logistic) y =\ni\n1+exp(−s )\ni\n(cid:40)\ns s >0\ni i\nReLU(RectifiedLinearUnit) y =\ni\n0 s ≤0\ni\nSoftplus y =ln(1+exp(s ))\ni i\n(cid:16) 1 (s −µ )2(cid:17)\ni i\nGaussian y =exp − ·\ni 2 σ2\ni\nexp(s )\ni\nSoftmax y =\ni (cid:80)n\nexp(s )\ni′=1 i′\nMaxout y =max(s ,...,s )\ni 1 n\nTable2.1: Activationfunctions(y=ψ(s),wheres,y∈Rn). Allthesefunctionsarevector\nfunctions. We show formulas for entry i of the input and output vectors. µ and σ2 are the\ni i\nmeanandvariancerespectively.\nwherew ∈R3×4,w ∈R4×3,w ∈R3×3,andb ∈R3 areparameters.\n1 2 3 3\nUsually,thedepthofaneuralnetworkismeasuredintermsofthenumberoflayers. Itis\ncalledmodeldepthsometimes. Forexample,takingtheinputvectorasanadditionallayer,",
    "thedepthoftheexamplenetworkinFigure2.3is4. Arelatedconceptismodelwidth,which\nistypicallydefinedonalayer,ratherthanontheentirenetwork. Acommonmeasureforthe\nwidthofalayeristhenumberofneuronsinthelayer. Forexample,thewidthoftheoutput\nlayer in Figure 2.3 is 3. If all layers of a neural network are of the same width n, then we\ncansimplysaythatthemodelwidthisn. Bothmodeldepthandmodelwidthhaveimportant\nimplicationsforthepropertiesoftheresultingneuralnetwork. Forexample,ithasbeenproven\nthatevenaneuralnetworkwithtwolayersofneuronsandtheSigmoidactivationfunctioncan\ncomputeanyfunction[Cybenko,1989]. Forstrongersystems,promisingimprovementsare\ngenerallyfavorablewhendeepeningneuralnetworks.\nStackinglayersresultsinaverycommonkindofneuralnetwork—feed-forwardneural\nnetworks(FFNNs). Thesenetworksarecalled“feed-forward”becausetherearenocyclesin\nconnectionsbetweenlayersandallthedatamovesinonedirection. Wewillseeinthisbook\nthatmostoftoday’sneuralnetworksarefeed-forward. Afewexceptionswillbepresentedin",
    "Section2.3.",
    "2.1 Multi-layer Neural Networks 75\ny\ny y y\n1 2 3\nw\n3\n(cid:16)\n=Softmax ×\nLayer3(Output)\nb\n3 (cid:17)\n+ ×\n1\n(cid:16) (cid:17)\n1 =Sigmoid ×\nLayer2\nw\n2\n(cid:16) (cid:17)\n=ReLU ×\nLayer1\nw\n1\nx\nx x x\n1 2 3\nLayer0(Input)\nFigure 2.3: A multi-layer neural network. The input layer consists of three variables\n{x ,x ,x }. These variables are fully connected to all neurons of layer 1. The output\n1 2 3\noflayer1isanewvectorh =ReLU(x·w ). Itisthenfullyconnectedtolayer2,performing\n1 1\nthe mapping h =Sigmoid(h ·w ). Its output h is fed into layer 3, which generates the\n2 1 2 2\nfinaloutputy=Softmax(h ·w +b ). Theparametersofthisneuralnetworkarew ,w ,\n2 3 3 1 2\nw andb .\n3 3\n2.1.3 Computation Graphs\nComputation graphs are a common way of representing neural networks. As graphs in\nmathematics,acomputationgraphismadeupofnodesandedgesbetweennodes. Eachnode\nrepresentseitheramathematicaloperationoravariable,andeachedgerepresentsthedataflow",
    "fromonenodetoanother. Socomputationgraphsaredirected3. Consider,forexample,three\n3Whileanumberofmachinelearningmodelscanberepresentedasundirectedcomputationgraphs,theyare\nnotthefocusofthisdocument.",
    "76 Chapter 2. Foundations of Neural Networks\nfunctions:\ny = x+w (2.6)\ny = Softmax(x·w+b) (2.7)\ny = Sigmoid(x·w +b )−ReLU(x·w ) (2.8)\n1 1 2\nFigure2.4showsthecomputationgraphsofthesefunctions. Fromtheparsingpointofview,all\nneuralnetworkscanbeviewedasmathematicalexpressions. Acomputationgraphistherefore\ntherepresentationoftheresultwhenparsingamathematicalexpression. Inthisway,eachnode\nofthegraphyieldsasub-expression,andtherootnodeyieldsthewholeexpression.\nInacomputationgraph,anodecanbeconnectedtomultiplenodesbeneathitand/orabove\nit. Thisenablesthereuseofsub-graphsinrepresentingcomplexfunctions. Forexample,inEq.\n(2.8),thevariablexisusedtwiceandthecorrespondingnodehastwooutgoingedges. Infact,\norganizingneuralnetworksintocomputationgraphsresemblesthecompositionalnatureof\nneuralnetworks—typically,alargenetworkisbuiltbycomposingsmallnetworks. TakeEq.\n(2.8)asaninstance. Itcanberewrittenasasystemofthreeequations:\ny = h −h (2.9)\n1 2\nh = Sigmoid(x·w +b ) (2.10)\n1 1 1\nh = ReLU(x·w ) (2.11)\n2 2",
    "Inthecompositionoperation,thenodesofh andh inEq(2.9)arereplacedbythegraphsof\n1 2\nEqs. (2.10-2.11).\nThemainuseofcomputationgraphsisinexecutingthefunction. Thisisexactlythesame\nthing as predicting the output of a neural network. The method is quite simple. First, the\nnodes of the graph are topologically sorted such that they are placed in an order consistent\nwiththeinformationflow. Then,giventhevaluesthatarefedintotheinputnodes,thegraphis\ntraversedinawaythat wecomputetheoutputofeachnode andflushittoitsparentnodes.\nThefinalresultisgotoutoftheoutputnode. Thisprocedureistypicallycalledaforwardpass.\nAforwardpasscanbeefficient,aseverynodeonlyneedstobevisitedonceanditsoutputcan\nbereusedbymultiplenodeswithouttheneedofrecomputingtheresult. Moreover,aforward\npasscanbeoptimizedbyreconstructingthegraph. Thiscandevelopthereuseideaabitmore\nandavoidunnecessarycomputationandmemoryconsumption.\nAnother use of computation graphs is to compute gradients automatically. In training",
    "neural networks, it is in general required the partial derivatives of the loss function L with\nrespecttoeveryweightmatrix(w)andeverybiasterm(b),say ∂L and ∂L. Beforeseeinghow\n∂w ∂b\nthesepartialderivativesareusedinupdatingamodel(seeSection2.4.1),though,wefirstgive\nanideaofcomputingderivativesinacomputationgraph. Forexample,considerthefunction\nbelow:\ny = ψ(x·w +b )·w (2.12)\n1 1 2",
    "2.1 Multi-layer Neural Networks 77\ny\n−\ny\nSoft. Sigm. ReLU\n+ +\ny\n+ · b · b ·\n1\nx w x w x w x w\n1 2\n(a)y=x+w (b)y=Softmax(x·w+b) (c)y=Sigmoid(x·w +b )−\n1 1\nReLU(x·w )\n2\nFigure2.4: Computationgraphsofthreeexampleneuralnetworks. Theblackboxesrepresent\nthemathematicaloperations,andthecoloredboxesrepresentthevariables. Amathematical\noperationnodehasincomingedgesfromothernodes,andeachofthesenodescanbetreatedas\nanargumentoftheoperation. Forexample,insub-figure(a),theadditionnodehastwochild\nnodeslabeledwithxandwrespectively. Thisnodereadstheoutputofthenodesxandw,and\ngeneratestheoutputy=x+w. Thingsareabitinterestingforlargergraphs. Insub-graph(b),\ntheoutputofthedotnode(i.e.,x·w)ispassedalongtheedgetotheadditionnode. Then,the\nadditionnodecomputesthesumofx·wandbasitsoutput. Wecanrepeatthesameprocess\noverallthemathematicaloperationnodesinabottom-upmanner,andgetthefinalresultof\ncomputingthewholeexpressionoutofthetop-mostnode.",
    "To obtain ∂L , ∂L and ∂L , it is natural to use the chain rule of differentiation. For\n∂w1 ∂b1 ∂w2\nexample,foracompositefunctiony=p(q(x)),theformulaofthechainruleisgivenas:\n∂y ∂p ∂q\n= · (2.13)\n∂x ∂q ∂x\nButtheanalyticformulaofaderivativebasedonEq. (2.13)wouldmakealengthyequation.",
    "78 Chapter 2. Foundations of Neural Networks\nInstead,wecandecomposeacomplexfunctionintoseveralfunctions,eachstandingforsome\noperation. Then,Eq. (2.12)canberewrittenas:\ny = h ·w (2.14)\n1 2\nh = ψ(h ) (2.15)\n1 2\nh = h +b (2.16)\n2 3 1\nh = x·w (2.17)\n3 1\nAll these variables can be understood in a better way from a computation graph: each\nvariableisanodeofthegraph,andnodesareconnectedbyalgebraicoperationsandfunction\ncompositions. Taking Eq. (2.13) and some basic knowledge of calculus, we compute the\nderivativesofthevariables,likethese:\n∂L\nnode1:= = δ (2.18)\ny\n∂y\n∂L ∂L\nnode2:= = ·wT (2.19)\n∂h ∂y 2\n1\n∂L ∂L\nnode3:= = hT· (2.20)\n∂w 1 ∂y\n2\n∂L ∂L\nnode4:= = ⊙ψ′(h) (2.21)\n∂h ∂h\n2 1\n∂L ∂L\nnode5:= = (2.22)\n∂h ∂h\n3 2\n∂L ∂L\nnode6:= = (2.23)\n∂b ∂h\n1 2\n∂L ∂L\nnode7:= = ·wT (2.24)\n∂x ∂h 1\n3\n∂L ∂L\nnode8:= = xT· (2.25)\n∂w ∂h\n1 3\nwhere δ is the derivative of the loss with respect to the model output. δ depends on the\ny y\nchoiceofthelossfunction,e.g.,ifweusethesquaredlossL= 1(y−y )2,wherey is\n2 gold gold",
    "thebenchmark,thenδ =y−y . Theaboveprocessisessentiallyabackwardpass,asthe\ny gold\ngradientsarepassedinatop-downfashion. Anothernameforthisiserror-propagation. It\nhasbeenthedefactostandardfortrainingdeepneuralnetworks. Forabetterunderstandingof\nhowforwardandbackwardpasseswork,Figure2.5showstworunningexamples.\n2.2 Example: Neural Language Modeling\nLanguagemodelingisawell-knownNLPtaskthatestimatesaprobabilitydistributionover\nsequencesofwords. Givenasequenceofmwordsw ...w ,theprobabilityPr(w ,...,w )is\n1 m 1 m",
    "2.2 Example: Neural Language Modeling 79\n1 y(Output) 1 LossL\n∂L\n∂y\n8 8\n1 1\ny=h ·w · y=h ·w ·\n1 2 1 2\n∂L = ∂L·wT ∂L =hT·∂L\n∂h1 ∂y 2 ∂w2 1 ∂y\n2 3 2 3\nh 1 =ψ(h 2 ) ψ(·) w 2 h 1 =ψ(h 2 ) ψ(·) w 2\n∂L = ∂L ⊙ψ′(h)\n∂h2 ∂h1\n4 4\nh =h +b + h =h +b +\n2 3 1 2 3 1\n∂L = ∂L ∂L = ∂L\n∂h3 ∂h2 ∂b1 ∂h2\n5 6 5 6\nh =x·w · b h =x·w · b\n3 1 1 3 1 1\n∂L = ∂L ·wT ∂L =xT· ∂L\n∂x ∂h3 1 ∂w1 ∂h3\n7 8 7 8\nx w x w\n1 1\n(a)ForwardPass (b)BackwardPass\nFigure2.5: Theforwardpassandbackwardpassforanexamplecomputationgraph. Inthe\nforwardpass(left),thenodesarevisitedinanorderfromtheinputtotheoutput,say,fromnode\n8to1. Oneachnode,weexecutethecorrespondingfunction,suchasaddition,togeneratethe\noutput,whichisthenconsumedbythesubsequentnodes. Incontrast,inthebackwardpass\n(right),thenodesarevisitedinthereverseorder,say,fromnode1to8. Duringthisprocess,we\npassthegradientoftheloss(orerror)fromtheoutputtotheinput,thatis,foreachnode,we\ncomputethegradientattheinputpointofthenodebyusingthechainrule,giventhegradient\nattheoutputpointofthenode.",
    "givenbytheequation:\nm\n(cid:89)\nPr(w ,...,w ) = Pr(w |w ,...,w ) (2.26)\n1 m i 1 i−1\ni=1\nAs such, the language modeling problem is framed as predicting the next word given all\npreviouscontextwords. AsimplemethodofmodelingPr(w |w ,...,w )istoconditionthe\ni 1 i−1",
    "80 Chapter 2. Foundations of Neural Networks\npredictiononacontextwindowthatcoversatmostacertainnumberofwords,likethis:\nPr(w |w ,...,w ) ≈ Pr(w |w ,...,w ) (2.27)\ni 1 i−1 i i−n+1 i−1\nwhere n is the window size. One way to estimate the probability is the n-gram language\nmodeling approach: we compute the relative frequency for each n-gram w ...w , i.e.,\ni−n+1 i\nPr(w |w ,...,w )=\ncount(wi−n+1...wi)\n. While n-gram languagemodels have domi-\ni i−n+1 i−1 count(wi−n+1...wi−1)\nnatedtheNLPfieldforalongtime,theyusuallyrequirehugetablesforrecordingallthose\nn-gramprobabilities. Inconsequence,themodelswillbeverysparseifmoreandmoretexts\nareusedintrainingsuchmodels. Thisisalsoknownasakindofthecurseofdimensionality.\nHereweconsiderneuralnetworksinaddressingthelanguagemodelingproblem[Bengio\net al., 2000; 2003b]. Unlike n-gram language models, neural language models do not\ngeneralizeinadiscretespacethatrequiresanexponentiallylargenumberofdistinctfeature",
    "vectorsasmorewordsandalargecontextareinvolved,butinacontinuousspacethatencodes\nwordsviadense,low-dimensionalrealvectors. Inparticular,afeed-forwardnetworkisutilized\nheretopredicthowlikelyw occursgivenw ...w .\ni i−n+1 i−1\nFigure2.6presentsthearchitectureofthefeed-forwardneuralnetworkbasedlanguage\nmodel(FFNNLM).Theinputisthecontextwordsw ...w . Eachisadiscretevariable\ni−n+1 i−1\nchoosingvaluesfromavocabularyV. Sincetheneuralnetworkoperatesonvectors,allwords\narevectorizedasone-hotrepresentations. Inthiscase,thewordw=V isa|V|-dimensional\nk\nvectorinwhichentryk is1andotherentriesareall0. Forexample, consideravocabulary\nV ={“I”,“you”,“he”,“’she”,“they”}. Theone-hotrepresentationof“you”is\n(cid:104) (cid:105)\nw(“you”) = 0 1 0 0 0 (2.28)\nWhiletheone-hotvectorsmakewordrepresentationsdistinguishable,itmaynotappear\nthatwecangaintoomuchbythisbecausesuchrepresentationscannotdescribethecloseness\nbetweenwords,e.g.,similarwordsshouldtendtobecloseinthevectorspace. Ifwerelaxthe",
    "indicator-based representations to real-valued representations, then it turns out that we can\nobtainsomewordrelationshipbycomputingsimilaritiesbetweenthesevectors. Tothisend,\naneffectivetechniqueistotransformone-hotrepresentationstodistributedrepresentations.\nMoreformally,letxbeaone-hotvectorofawordw. Thedistributedrepresentationofthe\nwordisareal-valuedvector,givenby:\ne = x·C (2.29)\nwhere the representation e is a vector ∈ Rde, and d\ne\nis the number of dimensions of the\nrepresentation. Each dimension of e can be viewed as some countable aspect of the word,\nthoughitisnotrequiredtobeinterpretedbylinguistics. Cisa|V|×d matrix,ofwhichthe\ne\nk-th row corresponds to the vector for V . Hence, w·C is to “select” a row from C. For\nk",
    "2.2 Example: Neural Language Modeling 81\nPr(·|w ,w ,w )\n1 2 3\nOutputLayer\ny=Softmax(h ·W +B )\n1 2 2\n(Layer3)\nh 1 0 .9 .6 .6 −1\nHiddenLayer\nh =TanH(h ·W +B )\n1 0 1 1\n(Layer2)\nh 0 −1 .2 .8 0 7 .3 .2 8 1\ne 1 −1 .2 .8 e 2 0 7 .3 e 3 .2 8 1\nEmbeddingLayer e =x ·C e =x ·C e =x ·C\n1 1 2 2 3 3\n(Layer1)\n0 1 0 0 0 0 0 1 1 0 0 0\nx (w ) x (w ) x (w )\n1 1 2 2 3 3\nFigure2.6: Aneurallanguagemodel[Bengioetal.,2003b]. Blueboxesrepresentthelayers\nof the neural network. The input is three context words in their one-hot representations\n{x ,x ,x },andtheoutputistheprobabilitydistributionofthenextwordPr(w |w ,w ,w ).\n1 2 3 4 1 2 3\nFirst,anembeddinglayerisusedtomapeachwordintothedistributedrepresentation(i.e.,the\nwordembedding). Theembeddingsofthesewordsareconcatenatedtoformabiggervector\nh suchthattheconcatenatedvectorencodesallinputinformation. Then,h istakenasthe\n0 0\ninputtoanormallayerthatperformsthemappingh =TanH(h ·W +B ). Thefinallayer\n1 0 1 1",
    "readsh andproducesadistributionoverthevocabulary,i.e.,y=Softmax(h ·W +B )\n1 1 2 2\nwherey =Pr(V |w ,w ,w ).\nk k 1 2 3\nexample,givenC∈R5×3,thedistributedrepresentationof“you”isgivenby:\ne(“you”) = w(“you”)·C\n \n73 12 0.1\n12 0.5 18\n(cid:104) (cid:105)  \n= 0 1 0 0 0 ·37 0.7 28\n \n \n61 0.4 23\n \n62 11 0.4\n(cid:104) (cid:105)\n= 12 0.5 18 (2.30)",
    "82 Chapter 2. Foundations of Neural Networks\nEq. (2.29)impliesanideaoflearningtorepresentwords,leadingtoabigdevelopment\nof NLP. Typically, the vector e is called the word embedding, and the parameter matrix\nC is called the embedding matrix. A number of methods may be used for learning word\nembeddings,thoughwewilltendtonotfocusonsuchmethodsinthischapter. Thereadercan\nrefertoChapter3foramoredetaileddiscussiononthistopic.\nToencodethecontextwords{w ,...,w }(or{x ,...,x }),asimplemethodisto\ni−n+1 i i−n+1 i\nconcatenatethewordembeddings{e ,...,e }asanewvectorh :\ni−n+1 i−1 0\nh = [e ,...,e ]\n0 i−n+1 i−1\nThenextpartofthemodelisa2-layerfeed-forwardneuralnetwork. Thefirstlayer,called\nahiddenlayer,isastandardlayerofneurons,followedbythehyperbolictangentactivation\nfunction. Thelayerproducesad -dimensionalvector:\nh\nh = TanH(h ·W +B ) (2.31)\n1 0 1 1\nThesecondlayeristheoutputlayer. ItproducesadistributionoverV. Thiscanbeformulated\nas:\nPr(·|w ,...,w ) = Softmax(h ·W +B ) (2.32)\ni−n+1 i−1 1 2 2",
    "The parameters of the model are C∈R|V|×de, W\n1\n∈R(n−1)de×d h, B\n1\n∈Rd h, W\n2\n∈\nRd h ×|V|,andB 2 ∈R|V|. Apopularwaytooptimizetheseparametersistominimizethecross-\nentropylossviagradientdescent. Additionally,trainingcanbeimprovedviaregularization.\nThesemethodswillbediscussedinSections2.4and2.5.\nAfewremarksontheneurallanguagemodel. First,byusingdistributedfeaturevectors,\n“senses”canbesharedinpartbydifferentwords. Thisenableslearnablewordsensesbywhich\nthesimilaritybetweenwordsisimplicitlyconsidered. Anadvantageofsuchamodelisthat\na small change in word vectors would not lead to a big change in the result. For example,\nsupposewehaveseen“grapesarefruits”manytimesbuthaveneverseen“peachesarefruits”.\nIf “grapes” and “peaches” are close in the vector space, then we would say that n-grams\n“grapesarefruits”and“peachesarefruits”aresomethingsimilar. Thisdifferentiatesneural\nlanguagemodelsgreatlyfromn-gramlanguagemodelsinwhichdifferentsurfaceformsmean\ndifferentmeanings.",
    "Second,thedenserepresentationofwordsmakesasmallermodel. Forexample,acommon\nsetting of d and d is less than 1000, making the number of parameters under control. By\ne h\ncontrast,thesizeofann-gramlanguagemodelincreasesbyafactorof|V|asnincreases. For\nexample,therewillbeahugetableofprobabilitiesforacommonvocabularyifnislargerthan\n3.\nThird,theneurallanguagemodeliscomputationallyexpensivebecauseoftheheavyuseof\nvectorandmatrixoperations,suchasmatrixmultiplication. Thisisacommonproblemwith\nmostofdeepneuralnetwork-basedsystems. Acommonsolutionistobreakthecomputation\nproblemintoindependentsub-problemssothatthesesub-problemscanbehandledinparallel.",
    "2.3 Basic Model Architectures 83\nAt a lower level, one can use GPUs or other parallel computing devices to speed up linear\nalgebraoperators. Atahigherlevel,onecandistributepartsofthemodelorpartsofthedatato\nmultipledevicesformodel-levelortask-levelspeed-ups.\n2.3 Basic Model Architectures\nWenowdescribe,inmoredetail,severalbasicbuildingblocksforneuralnetworks. Theyare\nwidelyusedindevelopingstate-of-the-artneuralmodelsinNLP.\n2.3.1 Recurrent Units\nRecurrentneuralnetworks(RNNs)areaclassofneuralnetworksthatreadand/orproduce\nsequentialdataortimeseriesdata. Aswithafeed-forwardneuralnetwork,anRNNcomprises\nlayersofneuronsandconnectionsbetweenneurons[Hopfield,1984;Rumelhartetal.,1986;\nWilliamsandZipser,1989;Elman,1990]. Someoftheneuronsareusedasa“memory”that\nkeeps the state of the problem when the processing moves on along a sequence of signals.\nAsaresult,itisstraightforwardtouseRNNstodealwithvariablelengthproblems,suchas\nmachinetranslationandspeechrecognition.",
    "ThemainideabehindRNNsistorepeatedlyutilizearecurrentunit(orrecurrentcell)\nto compute the output at each position of an input sequence. To be more precise, given a\nsequenceofvectorsx ...x ,astandardrecurrentunitcanbedescribedasafunctionRNN(·)\n1 m\nthatconsumesaninputx andastates ateachtimeandgeneratesanewstates ,likethis:\ni i−1 i\ns = RNN(s ,x ) (2.33)\ni i−1 i\nThestates canbeviewedasa“memory”thatsummariesthepastdata,andwouldbeupdated\ni\nwhenthenewdatacomes. SeeFigure2.7(a)forvisualizationofEq. (2.33). Thecirclehere\nindicatesthereuseoftherecurrentunit. ThiscanbeunderstoodbyrewritingEq. (2.33)ina\nsequenceofcallsofthefunctionRNN(·):\nRNN(s ,x ) = RNN(RNN(s ,x ),x )\ni−1 i i−2 i−1 i\n= RNN(RNN(RNN(s ,x ),x ),x )\ni−3 i−2 i−1 i\n= ...\n= RNN(RNN(...(RNN(s ,x ),x )...x ),x ) (2.34)\n0 1 2 i−1 i\nFigure 2.7 (b) shows the structure of this network. This is sometimes referred to as an\nunrolled (or unfolded) structure of RNNs. Basically, Figures 2.7 (a) and (b) are the same",
    "thing. WhilearolledRNNhasasimpleandwell-explainedform,anunrolledRNNismore\nsuitableforvisualizingthedataflowthroughthenetwork. So,wewillusetheunrolledversion\nofRNNsthroughoutthisdocument. Moreover,itisworthnotingthatanunrolledRNNisin\nfactadeepfeed-forwardneuralnetwork. Forexample,eachuseoftherecurrentunitcreatesa\n“layer”thatreceivesinformationfromaprevious“layer”. Inthissense,anRNNisastackof",
    "84 Chapter 2. Foundations of Neural Networks\ns s s s\ni i−1 i i+1\nRNN RNN RNN RNN\nx x x x\ni i−1 i i+1\n(a)AnRNNunit (b)UnrollingtheRNN\nFigure2.7: ExampleofRNN(rolledvsunrolled). AnRNNunitreadstheinputateachtime\nstepiandtheoutputatthelasttimestepi−1,andproducesanewoutputs . Assuchwecan\ni\nreusethesameRNNunittomakepredictionsoverasequenceofinputs(seesub-figure(a)): for\neachi,thecurrentinputx andlastoutputs areconsumedandmappedtotheoutputthatis\ni i−1\nfedintothesameRNNunitfortheprocessingatthenexttimestep. Abetterwaytovisualize\ntheRNNistounrollitintoanetworkwithnocycles(seesub-figure(b)). TheunrolledRNN\ncanberegardedasadeepfeed-forwardneuralnetworkinthatallRNNunitssharethesame\nsetofparameters.\nlayers,say,stackinglayersfromlefttoright. AbenefitoftreatingRNNsasdeepfeed-forward\nneural networks is that one can use the same methods to train and deploy the two types of\nneural networks. An example is that both RNNs and feed-forward neural networks can be",
    "trainedbytheerror-propagationtoolprovidedwithinacommonoptimizer.\nThereareanumberofRNNvariants,differinginwaysofdefiningRNN(·). Thesimplest\noftheseistoformulateRNN(·)asasingle-layerneuralnetwork. Assumethats andx are\ni−1 i\ninRd h. TheformofRNN(·)isgivenby:\nRNN(s ,x ) = ψ(s ·U+x ·V) (2.35)\ni−1 i i−1 i\nwhereU∈Rd h ×d h andV∈Rd h ×d h areparameters. Thecommonchoicesfortheactivation\nfunctionψ areTanH(·),Sigmoid(·),ReLU(·),andamongothers. Eq. (2.35)isasingle-layer\nneuralnetworkbecauseithasthesameformasEqs. (2.3-2.4):\nψ(s ·U+x ·V) = ψ([s ,x ]·W) (2.36)\ni−1 i i−1 i\nwhere[s i−1 ,x i ]istheconcatenationofs i−1 andx i ,andW∈R2d h ×d h istheparametermatrix\n(cid:34) (cid:35)\nU\nthatisformedby .\nV\nRNNsoftenworkasapartofamodel. Forexample,theinputofarecurrentunitcouldbe\neitherarepresentationofrealdataoranoutputofanotherneuralnetwork. Also,wecanstack\notherneuralnetworksontopofarecurrentunit. Forexample,inmanyreal-worldsystems,an",
    "2.3 Basic Model Architectures 85\nadditionallayerisgenerallystackedons forprojectingittoadesirableoutput.\ni\n2.3.2 Convolutional Units\nConvolutional neural networks (CNN) are another well-known class of neural networks\n[Waibeletal.,1989;LeCunetal.,1989]. Inabiologicalsense,theyareinspiredbyhuman\nvision systems: neurons react to the stimulus in a certain vision region or patch (call it the\nreceptivefield)[HubelandWiesel,1959]. InCNNs,thereceptivefielddescribestheregion\nintheinputspacethatisinvolvedingeneratingtheoutputforaneuron. CNNsaretherefore\n“partiallyconnected”modelsinwhicheachneurononlyconsidersinputfeaturesinarestricted\nregion. This differentiates CNNs from fully connected feed-forward neural networks. In\ngeneral,CNNscanresemblethehierarchicalnatureoffeaturesdescribingdataandscalebetter\nincomplexity.\nWhileCNNshavemanyapplicationsinprocessing2Ddata,suchasimageclassification,\nwe discuss them here in a sequential data processing scenario for a consistent treatment of",
    "theprobleminthischapter. Typically, aCNNconsistsofaconvolutionallayer, apooling\nlayer,andotherlayersoptionally. Itbeginswiththeconvolutionallayerwherethereceptive\nfieldisdefinedbyasetofconvolutionkernelsorfilters. Afilterisalinearmappingfunction\nthatconvolvestheinputfeaturesinthereceptivefieldtoformanoutputfeature. Forexample,\nconsiderasequenceofnumbersx ...x . Thefilterrangingfrompositionitopositioni+r−1\n1 m\nisdefinedtobe:\nυ = Conv(x ,W)\n[i,i+r−1]\n= x ·W\n[i,i+r−1]\nr−1\n(cid:88)\n= x ·W (2.37)\ni+k k\nk=0\nwhere r is the size of the receptive field, x is the sub-sequence x ...x , and\n[i,i+r−1] i i+r−1\nW∈Rr istheparametersofthefilter. Then,asequenceofoutputfeaturescanbegenerated\nbymovingthefilteralongtheinputsequence. Letstridebethedistancebetweenconsecutive\nmoves. Theoutputformoveiisthendefinedtobe:\nυ = Conv(x ,W) (2.38)\ni [stride×i,stride×i+r−1]\nIn this way, the convolutional layer transforms the input sequence x ...x to the output\n1 n",
    "sequenceυ 1 ...υ ⌊ m ⌋ 4. AremarkhereisthattheparametersWaresharedacrosspositionsof\nstride\ntheinputsequence. Thismethodisknownasparametersharingorweightsharing. Parameter\nsharingmakesaCNNefficientbecauseitrequiresfewerparametersthanafeed-forwardneural\nnetworkgiventhesamenumberofneurons.\nAproblemwiththeaboveformulationisthattheuseofthefiltermaynotbetiledtofitthe\ninputsequence. Forexample,whenstride×i+r−1>m,theinputofthefilterisincomplete.\nAcommonlyusedsolutionispadding. Itsimplysetsthefeaturesoutsidetheinputsequence\n4⌊·⌋standsforthefloorfunction.",
    "86 Chapter 2. Foundations of Neural Networks\nv =Conv([x ,x ,x ]·W)\n2 1 2 3\n=x ·W +x ·W +x ·W\n1 1 2 2 3 3\nConvolutionalLayer v 1 v 2 v 3 v 4\nInput 0 x 1 x 2 x 3 x 4 0\nPadding ReceptiveField Padding\nFigure2.8: Convolutionoverasequenceofnumbers{x ,x ,x ,x }(r=3andstride=1).\n1 2 3 4\nThereceptivefielddefinestheregionintheinputthatistakenincomputingtheoutput. Here\nthereceptivefieldhasasizeof3,thatis,theconvolutionaloperationcoversthreeconsecutive\nnumbersintheinputsequence. Thefilter(orconvolutionalkernel)outputsaweightedsumof\nthesenumbers. Eachtimeweslidethereceptivefieldovertheinput,thefiltergeneratesanew\noutput. Assuch,theoutputoftheconvolutionallayerisavectorofnumbers. Also,apadding\nnumber(i.e. 0)isaddedtoeachendofthesequencesothattheconvolutionisfeasiblewhen\nthereceptivefieldisincomplete.\nto a constant. For example, we can attend dummy feature vectors (say 0) to each end of\nthe sequence so that all convolution operations are feasible. See Figure 2.8 for an example",
    "filtercomputedoverasequenceofnumbers. Notethatthereceptivefieldsofdifferentfilter\napplicationsmayoverlap. Thisisbeneficialsometimesbecauseitreducesinformationlossin\nfeaturerepresentationwhenalow-levelfeatureisusedinformingmultiplehigh-levelfeatures.\nInaddition,aconvolutionallayercaninvolveanactivationfunctionψ(·)toperformsome\nnon-linearmappingonthefilteroutput. Letm =⌊ m ⌋bethenumberoffilterapplications.\nk stride\nTheoutputofaconvolutionallayerisgivenby\n(cid:104) (cid:105) (cid:104) (cid:105)\nh ... h = ψ( υ ... υ ) (2.39)\n1 m 1 m\nk k\nIngeneral,aconvolutionallayermaynotberestrictedtoascalar-basedinputorasingle\nfilter. Often,wecantakeavectorastherepresentationofatokenintheinputsequence,and\ntakeasetoffiltersasfeatureextractors. Tothisend,wecanadoptthesameformulationasin\nEqs. (2.37-2.39),butreplacex ,υ andh bythevectorizedcounterparts.\ni i i\nAconvolutionallayeristypicallyfollowedbyapoolinglayer. Likeconvolution,pooling",
    "is a function that sweeps a filter on a sequence. But the pooling operation does not have\nanyparameters. Itcanbeinsteadthoughtofasanaggregationfunctionthatperformsdown-\nsampling on the input sequence. There are several ways to design a pooling function. One\nof the most common methods is max pooling which outputs the maximum value in the",
    "2.3 Basic Model Architectures 87\nPooling\nFilter2\nFilter1\n0 padding\n0\nx6\nx5\nFeatures\n0\nx1\nx2\nx\nIn\n3\nx\npu\n4\nt\nSequence\n0\nFigure2.9: ExampleofCNN.Therearetwofiltersfortheconvolutionallayer. Theinputis\nasequenceof6tokensrepresentedintheirfeaturevectors{x ,...,x }. Totilethefiltersto\n1 6\nfittheinputsequence,twopaddingvectorsareattachedtoeachendofthesequence. When\napplyingafilter,wemapthefeaturevectorsinthereceptivefieldtoanewfeaturevector. For\nexample, for filter 1, the receptive field is a 6×3 rectangle in the input, and the output is a\n2-dimensionalfeaturevector. Bysweepingthefilteronthesequence,weobtainasequenceof\nfeaturevectors,say,asequenceof8featurevectors,eachhaving2features. Thepoolinglayer\nfuzesfeaturesalongthesequence. Forexample,performingthepoolingontheoutputoffilter\n1resultsin2fuzedfeatures. ThefinaloutputoftheCNNistwo2-dimensionalfeaturevectors.\nreceptivefield. Anothermethodisaveragingpoolingwhichoutputstheaveragedvalueover",
    "thereceptivefield. ForacompletepictureofhowaCNNworks,Figure2.9depictsarunning\nexamplewhereconvolutionalandpoolingoperationsareperformedonasequenceoffeature\nvectorsvia2filters.\n2.3.3 Gate Units\nInneuralnetworks,agateisusedtodecidehowmuchinformationispassedalong[Hochreiter\nand Schmidhuber, 1997]. Consider a standard RNN as an example. At each time step i,\ninsteadofdirectlypassingthepreviousstates i−1 ∈Rd h totherecurrentunit,itmightbemore",
    "88 Chapter 2. Foundations of Neural Networks\ninteresting to see how much information in s is useful for a next-step decision. In this\ni−1\ncase,wewants tobemorelikearealmemory: asthetimegoesby,somethingshouldbe\ni−1\nmemorized,andsomethingshouldbeforgotten.\nA way to achieve this goal is to introduce a coefficient for controlling the scale of data\nflow. HerewereusethenotationinRNNs(seeSection2.3.1),butourdescriptionisgeneral\nandcouldbeappliedtoallthecasesthatneedsuchamethod. Letz∈[0,1]d h beacoefficient\nvector, where z =0 means that nothing is memorized for dimension i, and z =1 means\ni i\nthat everything is memorized for dimension i. We can set z as a gate on s . This can be\ni−1\nformulatedas:\nGate(z,s ) = z⊙s (2.40)\ni−1 i−1\nwhere⊙istheelement-wiseproductoftwovectorsormatrices. Gate(z,s )isanupdateof\ni−1\ns . Thus,zcanbecalledanupdategate,oraforgetgate,orsomethingsimilar. Alternatively,\ni−1\nwecandefinethegatingfunctioninanotherway:\nGate(z,s ) = (1−z)⊙s (2.41)\ni−1 i−1",
    "Eqs. (2.40)and(2.41)basicallytellthesamestorybuthavedifferentinterpretationsforzin\npractice.\nThekeyproblemhereishowtoobtainz. Ageneralmethodistodefinezastheoutputof\nanothernetwork. Forexample,forarecurrentunit,zcanbedefinedtobe:\nz = Sigmoid(s ·W +x ·W +B) (2.42)\ni−1 1 i 2\nTheuseoftheSigmoidactivationfunctionguaranteesthattheoutputfallsintotherangeof\n[0,1]. NotethatEq. (2.42)describesalearnablegate. Thisinturnmakesthegateapartofthe\nmodelandcanbetrainedtofitthedata. Thereareanumberofmethodstodesignagate,and\nwewillseeafewinChapter4.\n2.3.4 Normalization (Standardization) Units\nAneuralnetworkworksbytransformingfeaturevectorslayerbylayer. Whilethemulti-layer,\nmulti-dimensionalnatureofneuralnetworksenablesthemodelstocomputecomplexfunctions,\nitmightleadtoverydifferentdistributionsofoutputactivationsacrosslayersorfeatures. This\nisaproblemwithdeepneuralnetworksbecauseamodelofthiskindhastoadapttodifferent\ndistributionsoverdifferentlayersordifferentfeatures[IoffeandSzegedy,2015]. Sometimes,",
    "asmodelparametersareinitializedrandomlyinalllayersandinallfeaturedimensions,itis\nlikelyforsomefeaturestobelargevalues. Inthiscase,themodelwouldbebiasedtothose\nlargevaluefeatures.\nAwaytomitigatethisissueisnormalization,whichstandardizesann-dimensionalfeature\nvectorsas\ns−µ\nNormalize(s) = α⊙ +β (2.43)\nσ+ϵ",
    "2.3 Basic Model Architectures 89\nwhereµ∈Rn andσ∈Rn arethemeanandstandarddeviationofs,respectively. ϵisasmall\nnumberusedfornumericalstability[ChiangandCholak,2022]. α∈Rn andβ ∈Rn arethe\nparametersofthenormalizationunit. Asimplechoiceisα=1andβ =0, whereasamore\nsophisticatedmethodistolearnαandβ togetherwithotherparameters.\nWemayimplementEq. (2.43)inseveralwaysthatdifferinhowtodefineµandσ. Letus\nconsiderthisforonedimensionins,says,inageneralsetting. Supposethatsisdrawnfroma\nsetoffeaturevaluesΩ . ThemeanandthestandarddeviationonΩ arethendefinedtobe:\nk k\n1 (cid:88)\nµ = · s (2.44)\nk\n|Ω |\nk\ns∈Ω\nk\n(cid:115)\n1 (cid:88)\nσ = · (s−µ )2 (2.45)\nk k\n|Ω |\nk\ns∈Ω\nk\nSeveralmethodsareavailabletodefineΩ . Forexample,onecandefineΩ asfeaturesin\nk k\nthesamelayer[Baetal.,2016],orfeaturesalongthesamedimensionoverdifferentsamples\norinputpositions[IoffeandSzegedy,2015;Ulyanovetal.,2016],orsomethinginbetween\nthem[WuandHe,2018].\nAnadvantageofnormalizationistoputfeaturesonthesamescaleandmakethemcompa-",
    "rable. Thishasbeenfoundtobeveryhelpfulforstabilizingthetrainingprocessandmaking\nneuralnetworksbetterbehaved. Aswewillseeinsubsequentchapters,normalizationplaysan\nimportantroleinmanysuccessfulsystems.\nAsanaside,whilethetermnormalizationindeeplearningisusuallyreferredtoasaprocess\nofsubtractingthemeananddividingbythestandarddeviation,itisinfactastandardization\nprocess. Inotherareas,bycontrast,normalizationismoreoftenreferredtoasatechniquethat\nscales all entries of a vector to the interval [0,1]. Standardization has no such requirement.\nItinsteadtendstohavetheinputcenteredaround0. Inthissense,normalizationmightbea\nmisnomerindeeplearningsomehow. Nevertheless,normalizationandstandardizationareused\ninterchangeablyinthisbookwhenreferredtoprocesseslikeEq. (2.43).\n2.3.5 Residual Units\nThesuccessofdeepneuralnetworkshasbeenmostlyaccreditedtothemoreandmorelayers\nusedinformingmorecomplexfunctions. Althoughstackingalargenumberoflayersisthe",
    "simplestwaytoobtainadeepmodel,ithasbeenpointedoutthatsuchamodelisdifficulttotrain.\nThereareseveralreasonsforthis,e.g.,optimizationalgorithms,gradientvanishing/exploding\ninpassingthroughstackedlayers,parameterinitialization,andsoon. Even,afurthernotable\ndisadvantage comes with regard to feeding a single representation to upper-level feature\nextractors,asonemightwantdirectaccesstotheintermediaterepresentationsseverallayers\nahead.\nResidualneuralnetworksareoneofthemosteffectiveapproachestoaddressingthese\nissues [He et al., 2016a]. They are a special type of neural networks that add residual\nconnections(orskipconnections,orshortcutconnections)overlayersinalayerstack. Let\nF(x) be a neural network that maps x to some output. A residual neural network build on",
    "90 Chapter 2. Foundations of Neural Networks\nResidualConnection ResidualConnection\nF(x(l)) + F(x(l+1)) +\nx\nl xl+1 xl+2\nLayerl Layerl+1\nxl+1=F(xl)+xl\nFigure 2.10: A 2-layer residual neural network. For each layer, there is a skip or shortcut\nconnection(inredcolor)thatdirectlyaddstheinputtotheoutput.\nF(x),givenbysummingthemappingF(x)andtheidentitymappingx:\ny = F(x)+x (2.46)\nAmorecommonuseofresidualconnectionsisinaneuralnetworkconsistingofanumber\nofidenticallayers. Letxl andyl betheinputandoutputoflayerl inaresidualmulti-layer\nneuralnetwork. Theoutputoflayerlcanbedefinedas:\nxl+1 = F(xl)+xl (2.47)\nor\nyl = F(yl−1)+yl−1 (2.48)\nFigure 2.10 shows the architecture of a 2-layer residual neural network. Clearly, the\nresidualconnectionsaddtheoutputsofcurrentlayersdirectlytotheoutputsofthenextlayers.\nThe added identity mapping is generally thought of as one of the most effective ways to\nsimplifythenetworkandeasetheinformationflowinadeepmodel.\n2.4 Training Neural Networks",
    "In this section, we turn to the training problem, which is fundamental in developing neural\nnetwork-basedsystems. Mostofthediscussionhereisfocusedonmethodsinasupervised\nlearningsetting. WewilldiscussunsupervisedmethodsinSection2.6.\n2.4.1 Gradient Descent\nThegradientmethodhasbeenproventobeoneofthemostsuccessfulmethodsfortraining\nneuralnetworks. Thebasicideaistoiterativelyupdateparameterssothatwecanminimizea\ndifferentiablelossfunction. Inanupdate,thevaluesoftheparametersareadjustedinaway\nthatthelossdegradesthefastest. Inamathematicalsense,itrequirestheupdatetobeinthe",
    "2.4 Training Neural Networks 91\nminimum\nFigure2.11: Gradientdescentina2Dspace(bluelinesstandforlevelsets). Thegoalistofind\ntheparameters(i.e.,valuesalongthetwodimensions)thatminimizethevalueofagivenloss\nfunction. Gradientdescentdoesthisbystartingatarandompointandsteppingtotheminimum\ninanumberofupdatesoftheparameters. Ineachupdate,itadjuststheparametersθ inthe\nt\ndirectionthatmakesthelosslower. Theideahereisthattheupdatechoosesthedirectionof\nthesteepestascent,thatis,themodelmovesastepinthedirectionofthenegativegradientof\ntheloss(i.e.,−\n∂L(θt)\n). Thesizeofthemoveiscontrolledbyahyper-parameterlr,calledthe\n∂θt\nlearningrate. Thus,theamountofthechangetotheparametersis−lr·\n∂L(θt)\n. Byaddingthis\n∂θt\ntoθ ,weobtainthenewparametersθ . Thisprocessrepeatsforanumberofupdatesuntil\nt t+1\nthevalueofthelossfunctionisclosetotheminimum.\noppositedirectionofthegradientoftheloss. Thisisknownasgradientdescentorsteepest\ndescent. Let θ be the parameters at step t (call it an update step or a training step), and",
    "t\nL(θ )bethelosscomputedbythemodelparameterizedbyθ . Theupdaterule(ordeltarule)\nt t\nofgradientdescentisgivenbytheequation:\n∂L(θ )\nt\nθ = θ −lr· (2.49)\nt+1 t\n∂θ\nt\nwhere\n∂L(θt)\nisthegradientofthelosswithrespecttotheparametersatstept. Itcanobtained\n∂θt\nbyrunningtheerror-propagationalgorithmpresentedinSection2.1.3. Sinceθ isusuallyof\nt\nmultipledimensions,\n∂L(θt)\ncouldbeavectorormatrixthathasthesameshapeasθ . lr isthe\n∂θt t\nlearningratethatcontrolshowbigastepwetakeinthedirectionoftheminimum. Whilelr\ncanbesimplysettoaconstantvalueduringtraining,itismorecommontoadjustitsvalueas\nthetrainingproceeds(seeSection2.4.4foradiscussion). SeeFigure2.11foranillustrationof\ngradientdescent.\nEq. (2.49) gives a very basic definition of gradient descent. There are a number of\nimprovementstotheformofEq. (2.49). Someofthemare:",
    "92 Chapter 2. Foundations of Neural Networks\n• GradientDescentwithMomentum. Inphysics,momentumisavectorquantitythat\ndescribesthemassofmotion. Ifwethinkofupdatingparametersasmovinganobjectin\naspace,thenweneedtoconsiderthemomentumoftheobjectatapositiontodetermine\nthedirectionofthenextmove. Thisideacanbeimplementedbyre-definingtheupdate\nruleas:\nθ = θ +v (2.50)\nt+1 t t\nwhere v is the velocity vector of the momentum. In the classic momentum method\nt\n[Polyak,1964],v isdefinedtobe:\nt\n∂L(θ )\nt\nv = λ·v −lr· (2.51)\nt t−1\n∂θ\nt\nv retainssomeofthepreviousmomentum(i.e.,v ),followedbyacorrectionbased\nt t−1\nonthegradient(i.e.,\n∂L(θt)\n). λisascalarforweightingv inanupdate. Awell-known\n∂θt t\nimprovementtoEq. (2.51)istotakeintoaccountthemomentuminthegradient,avoiding\natoolargevelocitywhenapproachingtheminimum[Nesterov,1983],likethis\n(cid:2) (cid:3)\n∂ L(θ )+λ·v\nt t−1\nv = λ·v −lr· (2.52)\nt t−1\n∂θ\nt\nAmoredetaileddiscussiononthedifferencebetweenEq. (2.51)andEq. (2.52)canbe\nfoundinSutskeveretal.[2013]’spaper.",
    "• AdaptiveGradientDescent. Inadaptivemethodsforgradientdescent,theupdaterule\nisadaptedtoeveryparameter,ratherthanthewholemodel. AdaGradisamethodof\nthiskind[Duchietal.,2011]. Itscalesupthelearningrateforparametersthathavenot\nbeenupdatedtoomuch,andscalesdownthelearningrateforparametersthathavebeen\nmuchupdated. Assumethatθ and\n∂L(θt)\narebothd-dimensionalvectors. Wecandefine\nt ∂θt\nanewvariableG∈Rd×d asthesumoftheouterproductofthegradientoverthepastt\nsteps5:\nt\n(cid:88)(cid:2)∂L(θ\ni\n)(cid:3)T ∂L(θ\ni\n)\nG = · (2.54)\nt\n∂θ ∂θ\ni i\ni=1\n5Giventwovectorsa= (cid:2) a 1 ··· a d (cid:3) andb= (cid:2) b 1 ··· b d (cid:3) ,theouterproductofaandbis:\na⊗b = aT·b\na \n1\n=   . . .   · (cid:2) b 1 ··· b d (cid:3)\na\nd\na b ··· a b \n1 1 1 d\n= \n\n. .\n.\n... . .\n.\n\n\n(2.53)\na b ··· a b\nd 1 d d",
    "2.4 Training Neural Networks 93\nIn general, (G t ) 1 2 ∈Rd can be viewed as an indicator that describes to what extent a\n1\nparameterhasbeenupdatedsofar. However,computing(G\nt\n)2 isextremelyexpensive.\nSo it is more common to use the diagonal of G instead. Then, the update rule of\nt\nAdaGradisgivenby:\nlr ∂L(θ )\nt\nθ = θ − ⊙ (2.55)\nt+1 t (cid:112)\ndiag(G t )+ϵ ∂θ t\nwhere diag(G ) is the diagonal of G , i.e., diag(G )(k)=G (k,k). ϵ is a smoothing\nt t t t\nfactorfornumericalstability. Insteadofsummingoverthosesquaredgradientsinan\nunweightedmanner,anotherwayistoreducetheimpactof“old”gradientsandmake\n“recent”gradientsmoreimportant. AdaDeltaconsidersthisbyaccumulatingsquared\ngradientswithadecayfactor[Zeiler,2012]:\n(cid:18) (cid:19)\n∂L(θ ) ∂L(θ )\ng2 = σ·g2 +(1−σ)· t ⊙ t (2.56)\nt t−1 ∂θ ∂θ\nt t\nwhereσ isthedecayfactorofavalue<1. LikeEq. (2.55),theupdateruleforAdaDelta\ncanbegivenbyreplacingdiag(G )withg2:\nt t\nlr ∂L(θ )\nt\nθ = θ − ⊙ (2.57)\nt+1 t (cid:112)\ng2+ϵ ∂θ\nt\nt\n(cid:112)",
    "Since g2+ϵcanbeseenastherootmeansquare(RMS)ofthegradient,Eqs. (2.56-\nt\n2.57)arealsoknownastheRMSPropmethod[Hinton,2018].\n• Adam(AdaptiveMomentEstimation). TheAdamoptimizercombinesthemeritsof\nboththeadaptivegradientdescentandmomentummethods[KingmaandBa,2014]. It\ndefinesan estimate ofthemean ofthegradient(the firstmoment)andan estimateof\nthe variance of the gradient (the second moment). Let m and v be the two moment\nt t\nestimates. Theyaregivenbytheequations:\n∂L(θ )\nt\nm = β ·m +(1−β )· (2.58)\nt 1 t−1 1\n∂θ\nt\n(cid:18) (cid:19)\n∂L(θ ) ∂L(θ )\nt t\nv = β ·v +(1−β )· ⊙ (2.59)\nt 2 t−1 2\n∂θ ∂θ\nt t\nwhereβ ,β ∈[0,1]arehyper-parametersforatrade-offbetweenthepreviousestimate\n1 2\nandthegradient(orsquaredgradient)atthecurrentstep. β andβ arealsotreatedas\n1 2\nthedecayfactorsoftheseaverages. Forexample,commonchoicesforβ andβ are0.9\n1 2\nand0.999. Astheinitialmomentsaresetto0,theseestimatesarebiasedto0vectorsat\ntheverybeginningofthetrainingprocess. Toaddressthisissue, biascorrectionsare",
    "94 Chapter 2. Foundations of Neural Networks\nusedinAdam,leadingtobias-correctedestimates:\nm\nt\nmˆ = (2.60)\nt 1−βt\n1\nv\nt\nvˆ = (2.61)\nt 1−βt\n2\nSinceβ ,β <1,thecorrectionswouldbesufficientlysmallifalargernumberofupdates\n1 2\nareperformed. Theupdateruleisfinallydefinedtobe:\nmˆ\nt\nθ = θ −lr·√ (2.62)\nt+1 t\nvˆ +ϵ\nt\nEq. (2.62)resemblesthegeneralformofgradientdescent,butmakesuseofboththe\nmomentum method (i.e., the moving average of the past gradients) and the adaptive\nmethod(i.e.,themovingaverageofthepastsquaredgradients). Inpractice,Adamhas\nbecomeapopularoptimizerfortrainingneuralnetworks.\nImprovinggradientdescentisanactivesub-fieldofdeeplearning, butafulldiscussion\nof all those techniques is beyond the scope of this document. A few related issues will be\ndiscussedintheremainderofthissection.\nOn a last note of this subsection, a practical issue that one should consider in utilizing\niterativetrainingmethodsiswhentostoptraining. Stoppingcriterionisageneraltopicin",
    "optimization. Forgradientdescentanditsvariants,itiscommonpracticetosetamaximum\nnumber of training steps or training epochs6, say 20,000 steps, or 100 epochs. As an\nalternative, we can perform training until convergence. For example, we can say that the\ntrainingcoveragesifthelosstendstobestableforanumberoftrainingsteps. Whenthereis\nsomedataforvalidatingthemodel,abettermethodmaybetocheckthestatesofthemodel\nonvalidationdata. Forexample,wecanstopthetrainingwhenthepredictionerrorincreases\non the validation data. This method, known as early stopping, is often used as a means of\nregularization. InSection2.5.3,wewillseemoredetailsabouthowtoearlystopthetraining\nbyusingavalidationdataset. Onthealgorithmicside,therehasbeenmuchinterestandwork\nin studying the convergence and error bounds for machine learning methods. We refer the\ninterestedreadertoafewtextbooksforfurtherdiscussions[Mohrietal.,2018;Kochenderfer\nandWheeler,2019].\n2.4.2 Batching",
    "Thelossfunctionisanessentialaspectofthetrainingofneuralnetworks. Whileanumber\nofmathematicalformsareavailabletodefinethelossfunction(seeSection1),westillneed\ntodecideinwhatscaleofsamplesweusethatlossfunction. Perhapsthesimplestmethodis\nstochasticgradientdescent(SGD).Ineachupdateofparameters,SGDcomputestheloss\nfunctiononasinglesamplethatisrandomlyselectedfromthetrainingdataset. LetD beaset\noftrainingsamples,and(x(i),y (i) )bearandomlyselectedsamplefromD. Givenaneural\ngold\n6Atrainingepochmeansthatthetrainergoesoverthewholetrainingdatasetforonetime.",
    "2.4 Training Neural Networks 95\nnetwork\n(cid:16) (cid:17)\ny (i) =F x(i) (2.63)\nθ θ\nthelossofSGDisdefinedtobe:\n(cid:16) (cid:17)\n(i) (i)\nL(θ) = L y ,y (2.64)\nθ gold\n(i) (i) (i)\nwhereL(y ,y )isasample-levellossfunctionthatcountserrorsinthemodeloutputy\nθ gold θ\n(i)\nwithrespectthebenchmarky .\ngold\nSGDhasbeenoneofthemostimportantoptimizationmethodsinmachinelearningdue\nto its simplicity. However, SGD converges slowly because it is just an analog of the actual\ngradientontheentiretrainingset. Toestimatethegradientinamorepreciseway,wecantake\nintoaccountasetofsamples(callitabatch)incomputingtheloss. Thismethodisknownas\nbatching. LetS beasetofsamplesfromD. ThelossfunctionisthendefinedonS,asfollows\n1 (cid:88)\nL(θ) = · L(y ,y ) (2.65)\nθ gold\n|S|\n(y ,y )∈S\nθ gold\nIfS =D,thenwehavethebatchgradientdescent(BGD)method,i.e.,thegradientis\nestimated on the entire set of training samples. In general, batch gradient descent is what",
    "wewouldordinarilycallgradientdescent. However, calculatingthelossonallthetraining\nsamples simultaneously is time consuming. In practice, it is more common to use a batch\nmuchsmallerthanD. Thisisknownasmini-batchgradientdescent. Itisadoptedinlearning\nreal-worldsystemsforitsgoodefficiencyandstrongperformance.\nAsanother“bonus”,batchingisgenerallyusedasawaytomakedensecomputationonma-\ntricesforsystemspeedup. AssumethatSconsistsofmsamples{(x(i1),y (i1) ),...,(x(im),y (im) )}.\ngold gold\nWecanbatchallinputvectorsandbenchmarkvectorsasmatrices:\n \nx(i1)\nX =   . . .   (2.66)\n \nx\n(im)\ngold\n \ny\n(i1)\ngold\n . \nY gold =  . .  (2.67)\n \ny\n(im)\ngold\nThen,wecanruntheneuralnetworkonthebatchedinputandoutput,likethis:\nY =F (X) (2.68)\nθ θ",
    "96 Chapter 2. Foundations of Neural Networks\nLikewise,wecancomputethebatchedloss\n1\nL(θ)= ·L(Y ,Y ) (2.69)\nθ gold\nm\n(cid:16) (cid:17)\nwhere L(Y ,Y ) vectorizes the computing of\n(cid:80)m\nL y\n(i\nk\n)\n,y\n(i\nk\n)\n. Eqs. (2.68-2.69)\nθ gold k=1 θ gold\npreventtherepetitivecallsoftheforwardandbackwardpassesonindividualsamples. They\ninstead pack everything in a single pass through the network. This makes better use of\nmaximum available compute on modern GPUs which are the majority of the devices for\nrunningdeeplearningsystems.\n2.4.3 Parameter Initialization\nGradientdescentrequiresthatthetrainingprocessstartsfromsomeinitialparameters. Since\nthetrainingobjectiveinapracticalsystemisoftenanon-convexfunctionwithmanylocalmini-\nmums,theperformanceoftheresultingmodelishighlysensitivetotheparameterinitialization\nstep. Herewedescribesomeofthemostcommonmethodsofparameterinitialization.\n• ConstantInitialization. Thefirstmethodcouldassignthesamevaluetoallparameters",
    "(orallparametersofaparametermatrix). Thismethod,thoughquitesimple,resultsin\nthatalloutputentriesofamodelmakenodifference,renderingthemodelmeaningless.\nItperformspoorlyinmostcasesifnorandomnessisintroducedintotraining.\n• InitializationwithPredefinedDistributions. Ausefulwayistorandomlyinitialize\nparametersbysomedistributions. Thesimplestofthiskindistoassignaparametera\nvaluedrawnfromauniformorGaussiandistribution,e.g.,arandomvalueintheinterval\n[−0.1,0.1]. Interestingly,thismethodissatisfactoryinmostcasesinpractice.\n• Layer-sensitiveInitialization. Anextensiontorandominitializationistousetailored\ndistributions for different layers of a neural network. Xavier initialization is a well-\nknownmethodofthiskind[GlorotandBengio,2010]. Givenalayery=ψ(x·W+B),\nlet d and d be the numbers of the input and output dimensions (i.e., the row and\nin out\ncolumnnumbersofW). ThestandardXavierinitializationmethod,alsoknownasthe\nLeCun initialization method [LeCun et al., 2012], gives a random number to every",
    "parameterofW:\n(cid:18) (cid:19)\n1 1\nW∈Rdin×dout ∼ U −√ , √ (2.70)\nd d\nin in\nwhereU(−a,a)meansauniformdistributionovertheinterval[−a,a]. Likewise,we\ncaninitializethebiasterminasimilarway. Asanimprovement,thenormalizedXavier\ninitializationmethodconsidersbothd andd indefiningthedistribution,likethis:\nin out\n(cid:18) (cid:114) (cid:114) (cid:19)\n6 6\nW∈Rdin×dout ∼ U − , (2.71)\nd +d d +d\nin out in out\nMoredetailscanbefoundintheoriginalpaper. Notethattheuniformdistributionscan",
    "2.4 Training Neural Networks 97\nbereplacedbythenormaldistributionswithmean=0andvariance= 1 or 6 .\ndin din+dout\nManyparameterinitializationmethodsaredesignedforcertaintypesofneuralnetworks.\nForexample,XavierinitializationisassumedtoworkwiththeSigmoidandhyperbolictangent\nactivationfunctions. ForReLU,onecanrefertoHeetal.[2015]’swork. Anotherexample\nisinitializationfordeepneuralnetworks. Ithasbeenfoundthatappropriateinitializationis\ncriticaltothesuccessofextremelydeepmodelsinNLP.Consideringthemodeldepthasan\nadditionalfactorininitialization,wecanmodifyEq. (2.71)tobe:\n(cid:18) (cid:114) (cid:114) (cid:19)\nα 6 α 6\nW∈Rdin×dout ∼ U − s · , s · (2.72)\nl d +d l d +d\nin out in out\nwherelisthedepthforalayer,andα isahyper-parameter. Apartfromthis,severalmethods\ns\nare proposed to address the initialization of deep neural networks, including the Lipschitz\ninitialization [Xu et al., 2020], the T-Fixup initialization [Huang et al., 2020a], the Admin\ninitialization[Liuetal.,2020c],andsoon.",
    "Note that in practice we do not have to restrict training to a single starting point. It is\ncommontotryafewstartingpointsbyusingdifferentinitializationmethodsorrandomseeds,\nandtochoosethebestperformingonefromthesetries. Itgenerallyhelpswhenlocalminimums\nabound.\n2.4.4 Learning Rate Scheduling\nToachievedesirableresults,itisessentialtocarefullyconfigurethelearningratethroughout\nthelearningprocess. Whilesomeoftheupdaterules,asnotedabove,haveconsideredscaling\nthegradientfordifferentparameters,learningrateschedulingisconventionallyfocusedmore\nondesigningheuristicstoadjustlrovertrainingsteps. Inapracticalsense,atoolargelearning\nrateusuallyleadstoovershootingaroundtheminimum,whileatoosmalllearningrateusually\nleadstoslowconvergence(seeFigure2.12). Acommonideaistolearnfastatthebeginning\n(i.e.,alargelearningrate)andlearnslowlywhenthelossisclosetotheminimum(i.e.,asmall\nlearningrate). Herewepresentsomeofthepopularmethodsforlearningratescheduling.",
    "• FixedLearningRates. Fixingthelearningrateisgenerallyabadstrategy,butcouldbe\nusedinprototypingsystems,e.g.,aquicktestofanewmethodbytrainingitforonlya\nfewepochs.\n• LearningRateDecay. Decayisacommonly-usedtechniqueforlearningrateschedul-\ning. Therearemanyapproachestothisidea. Forexample,onecanhalvethelearning\nrateaftereachtrainingepoch. Hereweusen todenotethenumberoftrainingsteps,\nt\nandτ behowoftenwechangethelearningrate(e.g.,100steps). Table2.2shows\ndecay\nseveraldecayfunctionsforlearningratescheduling.\n• Warmup and Decay. As noted in Section 2.4.3, it is common to initially set model\nparameterstorandomvalueswhenaneuralnetworkisbeingtrained. However,learning\nfromscratchwithalargelearningrateisusuallynotagoodchoicebecausethegradient\nattheearlystageofthetrainingisnotmuchpreciseandthestateofthemodelisunstable.",
    "98 Chapter 2. Foundations of Neural Networks\n(a)Asmalllearningrate (b)Alargelearningrate (c)Adesirablelearningrate\nFigure 2.12: Learning with different learning rates. Small learning rates (left) help us step\ntotheminimuminapreciseway, butrequiremuchadditionaltimeforconvergence. Large\nlearningrates(middle),ontheotherhand,leadtofastlearning,whichisverybeneficialwhen\nwe are far away from the minimum. However, as we get closer to the minimum too large\nlearningratescauseovershooting. Amoredesirablestrategy(right)maybetolearnthemodel\ninareasonablyfastwaywhenthereisalongwaytogo,andtolearnthemodelslowerwhen\nweareclosetotheminimum.\nThus,itismorereasonabletostartwithasmalllearningrateandgraduallyincreaseit.\nThen,whenthemodelistrainedforsometime,thelearningratebeginstodecayasusual.\nSuchathoughtmotivatesthewarmupanddecaymethodforlearningratescheduling.\nApopularformofthismethodinrecentstudiesisproposedinVaswanietal.[2017]’s\nwork,asfollows:\n(cid:18) (cid:19)",
    "lr = lr ·min (cid:0) n t (cid:1)−0.5 , n t ·(n )−1.5 (2.73)\nnt 0\nn n\nwarmup\ndecay decay\nwhere lr is the initial learning rate, and n is a hyper-parameter that specifies\n0 warmup\nforhowmanystepsweexecutethewarmupprocess. Figure2.13plotsthecurveofEq.\n(2.73)wheren ,n ,andlr aresetto4,000,1and1. Weseethatthelearning\nwarmup decay 0\nrate increases linearly in the first n steps and then decays as an inverse square\nwarmup\nrootfunction.\nChoosinganappropriatelearningrateschedulingstrategyisahighlyempiricalproblem,\nand there are no universally good choices. The problem is even harder if we consider the\ncorrelationbetweenthelearningrateandotheraspectsofthetraining,thoughlearningrate\nschedulingistypicallytakentobeanindividualtask. Forexample,whenalargerbatchisused\nintraining, alargerlearningrateisdesiredforagoodresult[Ottetal.,2018b;Smithetal.,\n2018]. So,makinggoodlearningratechoicesisstilldifficultandtime-consuminginneural",
    "networkapplications. Occasionallyoneneedsalargenumberoftrial-and-testrunstofinda\ndesirablelearningratesetupfortheparticularproblemathand.",
    "2.5 Regularization Methods 99\nEntry Formula Hyper-parameters\nPiecewiseConstantDecay lr =β values{β ,...,β }\nnt i 1 m\nif γ ≤ nt <γ thresholds{γ ,...,γ }\ni n i+1 1 m\ndecay\nnt\nExponentialDecay lr\nnt\n=lr\n0\n·λndecay decayrateλ,init. lr. lr\n0\n⌊ nt ⌋\n(Drop)ExponentialDecay lr\nnt\n=lr\n0\n·λ ndecay decayrateλ,init. lr. lr\n0\nNaturalExponentialDecay lr =lr ·exp(−λ· nt ) decayrateλ,init. lr. lr\nnt 0 n\ndecay\n0\nInverseTimeDecay lr =lr · 1 decayrateλ,init. lr. lr\nnt 0 1+λ· nt 0\nndecay\n(Drop)InverseTimeDecay lr =lr · 1 decayrateλ\nnt 0 1+λ·⌊ nt ⌋\nndecay\n(cid:0) (cid:1)\nCosineDecay lr =lr · (1−α)·c +α coefficientα\nnt 0 decay\nc = 1· (cid:0) 1+cos(π· nt ) (cid:1) init. lr. lr\ndecay 2 n 0\ndecay\nTable 2.2: Decay functions. λ=decayrate, lr =initiallearningrate, and {β }, {γ } and\n0 i i\nα=otherhyper-parameters.\n0.015\n0.010\n0.005\n0.000\n0 4,000 10,000 20,000 30,000\nnumberofupdatesteps(n )\nt\n)\nrl(etargninrael\ntn\nFigure2.13: Learningratescheduling: warmupandthendecay(n =4,000,n =1,\nwarmup decay",
    "andlr =1). Thelearningrateincreaseslinearlywithn forthefirst4,000steps. Then,the\n0 t\nlearning rate follows an inverse square root function and decays as the learning continues.\nThe change of the rate learning will be small if n is sufficiently large, indicating the fine\nt\nadjustmentoftheparameterswhenweareapproachingtheminimumoftheloss.\n2.5 Regularization Methods\nWenowdiscusstheregularizationmethodsforpreventingoverfitting. Whileregularizationisa\nwide-rangingtopicinmachinelearning,wepresentsomeofthosethatarecommonlyadopted\nintrainingneuralnetworks.",
    "100 Chapter 2. Foundations of Neural Networks\n2.5.1 Norm-based Penalties\nOne of the most popular methods involves a regularization term based on the l norm. A\np\ngeneralformoftheregularizedobjectivecanbedefinedas:\nθˆ = argminL(θ)+α·R(θ) (2.74)\nθ\nwhereR(θ)istheregularizationtermweightedbyacoefficientα. Ingeneral,R(θ)servesas\nanadditionallossthatpenalizescomplexmodels. Thisismotivatedbythefactthatcomplex\nmodelsaremorelikelytooverfitthedata(seeSection1). Toimposeapenaltyonthemodel\ncomplexity,asimplewayistodefineR(θ)asthel normontheparametersθ. Letustreatθ\n1\nasavectorofparameters. Thel norm-basedregularizationtermisgivenby\n1\n(cid:88)\nR(θ) = |θ | (2.75)\ni\ni\nEq. (2.75)penalizesmodelshavinglargevalueparameters. Thiscanbeunderstoodinaway\nfromapolynomialfunction: largecoefficientsofvariablesinapolynomialfunctionleadtoa\ncomplexcurve. Typically,regularizationwiththel normisreferredtoasthel regularization\n1 1\northeLassoregularization. Suchamethoddoesnotrequireupdatesofthetrainer,andcanbe",
    "implementedbystandardgradientdescent. Moreinterestingly,thel regularizationtypically\n1\nprovidessparsesolutionstotheoriginaltrainingobjective. Itbiasesthemodeltothosehaving\nsmallvalues(orevenzerovalues)formostoftheparametersandlargevaluesforonlyvery\nfewparameters. Thisalsoimpliesaninherentabilityoffeatureselectionbecauseparameters\nareforcedtobeclosetozerofornot-so-importantfeatures.\nAnalternativetothel regularizationisthel regularizationortheRidgeregularization.\n1 2\nInthel regularization,theregularizationtermisgivenby\n2\n(cid:115)\n(cid:88)\nR(θ) = |θ |2 (2.76)\ni\ni\nLikethel norm,thel normpenalizesthecasesthatdeviatethemodelparametersfaraway\n1 2\nfrom the origin. However, it slightly differs from the l norm in that the l norm enforces\n1 2\nall parameters to have small values (but not necessarily to be zeros) and there are no large\nvalue parameters. In this sense, the use of the l norm does not introduce sparsity into the\n2",
    "solutionbutperforms“smoothing”ontheunderlyingdistributionsoffeatures. Notethatthe\nl regularization has a relatively bigger effect of regularization. So, it is sometimes called\n2\nweightdecaytoemphasizeitsabilitytopreventthemodelfromlearningparametersoftoo\nlargevalues.\nInabroadersenseofmachinelearning,Eq. (2.75)offersageneralmethodtointroduce\npriorknowledgeintothetrainingofaneuralnetwork. Thereareanumberofwaystodesign\ntheregularizationterm,andaddressingoverfittingisjustonepurposeofthesedesigns. Wecan\nseemanyapplicationsofthisapproachinNLP,andwillseeafewexamplesintheremaining\nchaptersofthisdocument.",
    "2.5 Regularization Methods 101\n2.5.2 Dropout\nInareal-worldneuralnetwork,alayertypicallyinvolveshundredsorthousandsofneuronsand\nproducesafeaturevectoraccordingly. Whileeachofthesefeaturesiscomputedbyasingle\nneuron,theyworktogethertoformtheinputtoeachneuronofthefollowinglayer. Asaresult,\nafeatureisforcedtocooperatewithotherfeatures. Itislikeagroupofpeoplesittingtogether\nandmakingacollectivedecision. Althoughamembercouldhaveopinionsindependently,he\norsheoccasionallytriestocorrecttheerrorwhenallothermembershavehadtheirdecisions.\nInthiscase, everygroup memberisco-adaptedtoothersin thegroup[Hintonetal.,2012].\nFromafeatureengineeringstandpoint,theco-adaptationofneuronshelpswhenmodeling\ncomplex problems, as it implicitly makes some sort of higher order features. Beyond this,\nthestrongsupervisioninformation(e.g.,propagatingerrorsthroughlayers)couldstrengthen\ntheco-adaptationintraining. Thisexplainsmoreorlesswhyaneuralnetworkwithalarge",
    "numberofneuronscanfitcomplexcurves. Attesttime,however,theco-adaptationprevents\ngeneralization. Since all neurons of a layer are learned to collaborate well on the training\ndata,asmallchangeintheinputcouldaffectalltheseneuronsandleadtoabigchangeinthe\nbehavioroftheneuralnetwork.\nAwaytomitigateoreliminatecomplexco-adaptationsistolearnforeachneurontopredict\nin the absence of other neurons. To this end, one can simply drop some of the neurons in\ntraining. Thismethodisknownasdropout[Srivastavaetal.,2014]. Letnbethenumberof\nneuronsofalayer. Givenaprobabilityρ(call1−ρthedropoutrate), wecangeneratean\nn-dimensionalmaskvectorM whereeveryentryissetto1withapossibilityofρ,andset\ndrop\nto0withapossibilityof1−ρ. Then,adropoutlayercanbedefinedas\ny = M ⊙ψ(x·W+B) (2.77)\ndrop\nwhere ψ(x·W+B) is a usual single-layer neural network. Eq. (2.77) only activates the\nneurons whose masks are 1. For dropped neurons, all connections from/to these neurons",
    "are blocked (see Figure 2.14 (a)). During training, M is randomly generated in a call\ndrop\nof the forward and backward passes. A neuron therefore can learn to work with different\nneuronseachtimeandwouldnotadapttothesamegroupof“co-workers”. Anotherwayto\nunderstand dropout is to view it as learning sub-models of a “big” model. The use of Eq.\n(2.77)isessentiallyasamplingprocessthatextractsasub-networkfromtheoriginalnetwork.\nSo,trainingwithdropoutisdoingsomethingliketraininganexponentiallylargenumberof\nsub-networks7. Ontheotherhand,thetrainingisefficientbecausethesesub-networkssharethe\nsameparametersforthesameneuronandtheupdateofaparametercanbenefitexponentially\nmanysub-networks.\nAt test time, all these sub-networks are combined for prediction. In this case, we do\nnot need to drop any neuron but use the original network as usual. This makes it simple to\nimplement dropout: a neuron is present with some probability on the training data, and all",
    "neuronsarepresentandworktogetheronthetestdata. Sincetheconnectionsbetweenneurons\nareinvolvedwithaprobabilityofρintraining,thelearnedweightsarescaleddownwithρin\n7Forasingle-layernetworkhavingnneurons,thereare2npossiblesub-networks.",
    "102 Chapter 2. Foundations of Neural Networks\nactiveneuron allneurons\n(presentwithρ) areactive\ndroppedneuron\nweightW weightρW\n32 32\n(a)Trainingthedropoutnetwork (b)Testingwiththedropoutnetwork\nFigure2.14: Dropoutforamulti-layerneuralnetwork(trainingvstest). Attrainingtime,every\nneuronisrandomlydroppedwithaprobabilityof1−ρ,resultinginaslimmednetwork. Inthis\nsense,dropouttrainingisessentiallyaprocessoflearninganexponentiallylargenumberof\nsub-networks. Attesttime,thefullnetworkisusedasusual,whichistheresultofcombining\nallthosesub-networksforprediction. Sinceallconnectionsbetweenneuronsareactivatedwith\ntheprobabilityρduringtraining,theweightsofthepredictingnetworkarescaleddownwithρ.\nthepredictingnetwork,i.e.,alayerhasaform:\ny=ψ(x·ρW+ρB) (2.78)\nSeeFigure2.14foracomparisonoftrainingandapplyingadropoutnetwork. Eq. (2.78)\nrequiresanupdateofthepredictingsystem. Analternativeistotakeintoaccountthescaling\nissueonlyinthetrainingprocessandleavethepredictingsystemasitis. Forexample,wecan",
    "scaleupalltheparameterswith 1 indropouttraining,likethis\nρ\n1 1\ny = M ⊙ψ(x· W+ B) (2.79)\ndrop\nρ ρ\nSincemultiplying 1WwithρyieldsW(thisalsoholdsforthebiastermB),wecanuse\nρ\nW(andB)astheparametersofthepredictingsystem.\n2.5.3 Early Stopping\nInChapter1wehavediscussedabitofhowtostopthetrainingbymonitoringtheperformance\nonthevalidationdata. Itcanbetreatedasawayofmodelselectionthatseeksanappropriate\nstate between underfitting and overfitting. Note that early stopping is not just an empirical\nmethod. Itisalsowellexplainedfromtheperspectiveofstatisticallearningtheory. Forexample,\nresearchershavefoundthat,undersomeconditions,earlystoppinghasasimilareffectasthe\nl regularization and restricts the learning to the region of small value parameters [Bishop,\n2\n1995a;Goodfellowetal.,2016]. Also,otherresearchshowsthatsomeearlystoppingrules",
    "2.5 Regularization Methods 103\nhaveatightrelationshipwiththebias-variancetrade-offandcouldguaranteenicepropertiesof\nconvergence[Yaoetal.,2007].\nOntheotherhand,earlystoppingrequiresseveralheuristicstomakeitpracticalanduseful.\nThe first problem is the condition of stopping. Ideally, one might imagine that there is a\nperfectU-shapederrorcurveonthevalidationdata,andthetrainingcanbehaltedimmediately\nwhentheerrorstartstoincrease. Thetruth,however,isthattheerrorcurvecannotbesimply\ndescribedasastrictlyconvexfunctionofthetrainingtime. Afterdropsintheerrorinacertain\nnumber of training steps, the performance of the model tends to fluctuate, leading to many\nlocalminimums. Theproblemwouldbemoreinterestingifonewantstosavetimeandstopthe\ntrainingasearlyaspossible. However,weneverknowwhetherthecurrentchoiceordecision\nisthebestonebecausewehavenoideaofwhathappensnext. Acommonly-usedmethodis\ntodecidewhetherthetrainingshouldstopbycheckingthemodelstatesforanumberofpast",
    "updatesteps(orepochs)[Prechelt,1998]. Someearlystoppingconditionsare:\n• The change in the performance is below a threshold for a given number of steps (or\nepochs).\n• Thechangeinthemodelparametersisbelowathresholdforagivennumberofsteps(or\nepochs).\n• Theaverageperformanceoveragivennumberofsteps(orepochs)startstodecrease.\n• Themaximumperformanceoveragivennumberofsteps(orepochs)startstodecrease.\nHowever,usingthemodelatthepointthatwestopthetrainingisnotalwaysagoodchoice.\nInpractice,amodeloftenhasalargevarianceingenerationerroraroundthatpoint,making\nmodelselectionmoredifficult. Insteadof“selecting”amodel,analternativewayistocombine\nmultiple models. For example, we can save the model for every run of a given number of\ntraining steps (call each copy of the model a checkpoint). The final model is induced by\naveraging the parameters of the last few checkpoints. For better results, one may use more\nsophisticatedensemblemethods(seeSection1).\n2.5.4 Smoothing Output Probabilities",
    "Instatistics,smoothingreferstotheprocessofreducingthevalueofnoisydatapoints(probably\nof high values) and increasing the value of normal data points. It is typically used when a\ndistributionisestimatedonsmalldataandtheprobabilitiesofrareeventsarenotwellestimated.\nForexample,considerthelanguagemodelingproblemdescribedinSection2.2. Alanguage\nmodel is trained in a way that enforces the model to output a one-hot distribution, that is,\nthe total probability of 1 is occupied by only one word, leaving other words assigned zero\nprobabilities. Itmaybemoredesirabletodistributetheprobabilitytoallwords,eventhough\nmanyofthemarenotobservedtobetheanswergiventhepreviouswords. Inthisway, the\nmodellearnstomakeasoftpredictionofwordprobabilitiessothatitcangeneralizebetteron\nunseendata.\n(cid:104) (cid:105)\nGivenadistributionp= p ... p ,itisthepurposeofsmoothingthatweobtainthe\n1 n\nnewestimatebetweenpandauniformdistribution 1. Acommonapproachtothisideaisto\nn",
    "104 Chapter 2. Foundations of Neural Networks\nuseashrinkageestimatortoimprovepbymakingitcloserto 1. Forexample,theaddictive\nn\nsmoothingmentionedinSection1isasimpletypeofshrinkageestimator. Hereweconsider,\nforexample,smoothingamultinomialdistribution. Letp denotetheprobabilityofeventk\nk\nands denoteaquantitythatdescribessomeobserved“count”oftheevent. Theprobability\nk\np isgivenby\nk\ns\nk\np = (2.80)\nk (cid:80)n\ns\nk=1 k\nThen,thesmoothedversionofp isdefinedas\nk\ns +α\nk\npˆ = (2.81)\nk (cid:80)n\n(s +α)\nk=1 k\nItsimplyaddsaquantityαtoeachs . Thevalueofαcontrolsthesmoothnessoftheresulting\nk\nestimate. Forexample,pˆ =p ifα=0,andpˆ ≈ 1 ifαchoosesanextremelylargevalue.\nk k k n\nApartfromaddictivesmoothing,wecansmoothadistributioninaSoftmaxmanner,as\nfollows\nexp(s /β)\nk\npˆ = (2.82)\nk (cid:80)n\nexp(s /β)\nk=1 k\nThisformisknownasaninstanceoftheBoltzmanndistribution[Uffink,2017],wheres\nk\nis viewed as the negative energy of a state, and β is viewed as the temperature indicating",
    "the degree of smoothing. Note that s can be interpreted in many ways. For example, in a\nk\nneuralnetwork,s istypicallydefinedasthestateofaneuron. Sometimes,s canevenbea\nk k\nprobability. ThismeansthatwecandirectlyapplyEqs. (2.81-2.82)toanypevenifthereisno\npriorknowledgeabouthowpisestimated. Then,wecanrewriteEqs. (2.81-2.82)byreplacing\ns withp :\nk k\np +α\nk\npˆ = (2.83)\nk (cid:80)n\n(p +α)\nk=1 k\nexp(p /β)\nk\npˆ = (2.84)\nk (cid:80)n\nexp(p /β)\nk=1 k\nAnothermethodofsmoothingistointerpolatepwiththeuniformdistribution. Aformof\ntheinterpolationisgivenby\n1\npˆ = (1−ϵ)·p +ϵ· (2.85)\nk k\nn\nwhereϵisahyper-parameterindicatingtowhatextentwerelyontheuniformdistributionin\ncomputingpˆ . ToillustratehowEq. (2.85)works,letussupposethatpisaone-hotvector,\nk\nsay, p =1 if k =z and p =0 otherwise. By using Eq. (2.85), we subtract an amount of\nk k\nprobability (i.e. ϵ) from p . The subtracted amount of probability is then redistributed to\nz",
    "alldimensionsevenly,makingtheresultingdistributionmoreflat-toppedandsmoother. See\nFigure2.15foranillustration.",
    "2.5 Regularization Methods 105\n1.\n0. 0 0. 0. 0. 0. 0. 0. 0.\np\nk\nϵ ×n\nn\n.82 ϵ→ ...\n.02 .02 .02 .02 .02 .02 .02 .02 .02\npˆ (ϵ=0.2)\nk\nFigure2.15: Smoothingadistributionbyinterpolatingitwiththeuniformdistribution: pˆ =\nk\n(1−ϵ)·p +ϵ· 1. Foreachdimensionk,itsubtractsanamountofϵfromtheprobabilityp\nk n k\nandredistributesthisamountofprobabilityevenlytoallthevariables,thatis,everyvariable\ngetsaprobabilityofϵ·p k.\nn\nInNLP,sincemanysystemsmakeprobability-likepredictions,acommonapplicationof\nsmoothing is to smooth a system’s output. There are two ways. First, we can smooth the\nbenchmarkprobabilitysuchthatthemodelisguidedbythegeneralizederrorratherthanthe\nerrormadebyharddecisions. Forexample,thelabelsmoothingtechniqueadoptsthesame\nformasEq. (2.85)andimprovesthebenchmarkrepresentationoncategoricaldata[Szegedy\netal.,2016]. Second,wecanreducethesteepnessandincreasethetailednessofapredicted\ndistribution8. This method is often used when the posterior probability of the prediction",
    "is required, such as minimum Bayesian risk decoding/training [Bickel and Doksum, 2015;\nGoodman,1996a;KumarandByrne,2004a].\n2.5.5 Training with Noise\nAbove,wehaveshownthataddingsomeamounttoeachobservedcountofeventsinpredicting\naprobabilitycanimprovegeneralization. Fromarobuststatisticspointofview[Olive,2022],\nthisisequivalenttoimprovingtherobustnessofanestimatorwhereaskeweddistributionoften\nleadstoabiasedmodel. Theadditionofasmallperturbationtotheestimatecanpreventlarge\nbiasescausedbyoutliersandunexpectedobservationsofrareevents. Inthissense,smoothing\ncan be regarded as a way of introducing noise into training, that is, we impose a prior of\nuniformdistributionontheestimatethoughthecorrectestimatemaynotbeuniform.\nNoisytrainingworkswithanideathatamodelislearnedtoworkinnon-idealconditions\nandavoidoverfittingdatapointsofextremevalues. Herethetermnoisehasawidemeaning,\nand there are a few different ways to regularize training with noise. One of the simplest",
    "methodsistousenoise-sensitivetrainingobjectives. Forexample,smoothingthebenchmark\n8IngeneralonemaywantadistributiontobeaMesokurticcurve.",
    "106 Chapter 2. Foundations of Neural Networks\ndistribution(e.g.,theone-hotrepresentationofthecorrectprediction)canbeseenasawayof\nmakingnoisyannotations. Alternatively,wecanaddrandomnoisetotheinput,output,and\nintermediatestateofaneuralnetwork. AcommonchoiceistheGaussiannoise. Supposewe\nhaveavectorx∈Rn. TheadditionoftheGaussiannoisedefinesanewvector,asfollows\nx = x+g (2.86)\nnoise\nwhereg∈Rn isavectorofnoise. ItfollowsaGaussiandistribution:\ng ∼ Gaussian(µ,σ2) (2.87)\nForentryk ofg,itdefinestheprobabilityPr(g )tobe:\nk\n1\n(cid:18)\n(g −µ\n)2(cid:19)\nPr(g ) = √ ·exp − k k (2.88)\nk σ 2π 2σ2\nk k\nwhereµ isthemeanofthedistribution,andσ isitsstandarddeviation. Often,µ issetto\nk k k\n0. σ is a hyper-parameter that is used to control the amount of noise we want to add. For\nk\nexample,alargeσ meansthattherandomnoisespreadsoutinalargeregioncenteredaround\nk\nµ ,anditismorelikelytogeneratelargenoise.\nk\nEq. (2.86)isgenericandcanbeappliedtoalmosteverywhereinaneuralnetwork. Givena",
    "layery=ψ(x·W+B),thenoise(sayg )canbeaddedtotheinput,likethis\ninput\ny=ψ((x+g )·W+B) (2.89)\ninput\nLikewise,thenoise(sayg )canbeaddedtotheactivation(oroutput):\noutput\ny=ψ(x·W+B)+g (2.90)\noutput\nForexample,onecansimplymakenoisyinputs(oroutputs)foramodelandrunallhidden\nlayersasusual,orcanaddrandomnoisetoallactivationsthroughouttheneuralnetwork. While\nitiscommontoaddrandomnoisetothelayerinputsand/oractivationsinaneuralnetwork\n[Plautetal.,1986;HolmströmandKoistinen,1992;Bishop,1995b],anotherapproachtonoisy\ntrainingistoaddrandomnoisedirectlytomodelparametersorgradients[Gravesetal.,2013b;\nNeelakantanetal.,2015]. Forexample,theadditionofnoisetothetransformationmatrixhas\nthefollowingform:\ny = ψ(x·(W+g )+B) (2.91)\nw\nwhereg isthematrixofnoiseandhasthesameshapeasW. Also,wecanaddnoise(say\nw\ng )tothegradientoflossforW. Letsdenotex·W+B. Thenoisygradientcanbe\ngradient",
    "2.5 Regularization Methods 107\nwrittenas:\n∂L ∂L\n= xT· +g\ngradient\n∂W ∂s\n(cid:18) (cid:19)\n∂L ∂y\n= xT· ⊙ +g\ngradient\n∂y ∂s\n(cid:18) (cid:19)\n∂L\n= xT· ⊙ψ′(s) +g (2.92)\ngradient\n∂y\nTheuseofnoisygradientshasbeenfoundtonotonlybehelpfulforrobusttrainingbutalsoto\neasethegradientflowinthenetwork[Gulcehreetal.,2016].\nItshouldbenotedthatnoiseisonlypresentduringtrainingandthemodelworkswithout\nthe addition of noise when making predictions on new data. In this sense, many of the\nregularizationmethodscouldfallunderthenoisytrainingframeworkthatisusedtoprevent\nfittingthetrainingdatapreciselyandenablethepredictingsystemtogeneralizewellonthe\ntestdata. Forexample,dropoutrandomlyinactivatessomeoftheactivationsofalayersothat\neveryneuronislearnedtoworkinanoisyenvironment. Whenrunningonthetestdata,allthe\nneuronsworktogetherasinausualneuralnetwork.\nThereisanadditionaladvantagewithnoisytraininginthattheuseofrandomnoisemakes",
    "“new” training samples. Even for the same sample, different noise could lead to different\ntrainingresults. Inotherwords,weessentiallytrainthemodelonaninfinitenumberofsamples.\nThis idea is also linked to another line of research on training with synthetic data, called\ndataaugmentation. Insimpleterms,dataaugmentationisasetofmethodstogeneratenew\nsamples from existing samples. An example is back-translation [Sennrich et al., 2016a].\nWhendevelopingamachinetranslationsystemfromlanguageAtolanguageB,wecanfirst\ntrainareversetranslationsystem(saytheB→Asystem)onthebilingualdata. Then,weusethe\nB→Asystemtotranslatesomeadditionaltarget-languagedatatosource-languagedata. This\nresultsinnewbilingualdatawherethetarget-languagedataisrealandthesource-language\ndata is synthetic. This new data can be used together with other bilingual data to train the\nA→Bsystem. Inadditiontoback-translation,therearemanydataaugmentationmethodsin",
    "NLP,includingreplacingwordswithsynonyms,swappingtwowords,deleting/insertingwords,\nandsoon. Moreover,wecandosimilarthingsonfeaturevectors,suchasreplacingaword\nembeddingwithasimilarembedding. Sincedataaugmentationcoversawidevarietyoftopics,\nwereferthereadertoafewsurveypapersformoreinformation[Fengetal.,2021;Shortenand\nKhoshgoftaar,2019].\nOne last note on data augmentation. Synthetic data can be made for some purpose. A\npopularideaisadversarialmachinelearning. Itgeneratesadversarialsamplesonthata\nmodelwouldmakemistakes(callsuchprocessesattacks)[Szegedyetal.,2014a;Goodfellow\netal.,2015]. Themodelislearnedtomakecorrectpredictionsonthesesamples,i.e.,itdefends\nthe attacks. For example, in some cases, the output of a machine translation system would\nbe completely wrong if we change the gender of the subject of the input sentence. For a\nmorerobustsystem,onemaytrainthetranslationmodelbyusingmoregender-balanceddata,",
    "gathered either manually or automatically. But it is not easy to craft samples that look like",
    "108 Chapter 2. Foundations of Neural Networks\nnormalsentencesbutcanfoolthemodel[Zhangetal.,2020b]. Thisinturnmakesitinteresting\nyet challenging to generate adversarial samples in NLP, since a small change in a sentence\n(such as word replacement) could lead to something with a very different meaning9. The\nchallengealsomotivatesathreadofresearchoninvestigatingadversarialsamplesinNLP[Jia\nandLiang,2017;BelinkovandBisk,2018;Ebrahimietal.,2018;Alzantotetal.,2018].\n2.6 Unsupervised Methods and Auto-encoders\nUnsupervised learning is concerned with discovering the underlying patterns in a set of\nunlabeleddatapoints. Anumberofproblemscanbeviewedasclassicalunsupervisedlearning\nproblems,thoughwewillnotdiscussthemindetailthroughoutthischapter. Forexample,data\nclusteringistofindgroupingsinacollectionofdataobjects,givennosupervisedsignalson\nwhatthecorrectgroupingis. Anotherwell-knownexampleisassociationrulemining. Itis\noftenframedasaprocessofestablishingtherelationshipamongsetsofdataobjects. While",
    "theseproblemsareindeedcoveredbyunsupervisedlearning,wewillfocusonproblemsof\nunsupervisedrepresentationlearningorfeaturelearning,thatis,amodelislearnedtomapan\nobjectfromaninputspacetoalow-dimensionalfeaturevectorspace10.\nLearning low-dimensional representations has been extensively studied in the context\nof finding a linear transformation from the original space to the new space. For example,\nprincipalcomponentsanalysis(PCA)anditsvariantstrytofindalinearmappingfunctionso\nthata(high-dimensional)dataobjectcanberepresentedasitscoordinatesalongthedirections\nof the greatest variance [Pearson, 1901; Wold et al., 1987]. Here we extend the mapping\nfunction to its natural non-linear generation and use neural networks as a solution to the\nmappingproblem.\nAswithothermachinelearningmodels,aneuralnetworkistypicallylearnedbyoptimizing\nmodelparameterswithrespecttosomelossfunction. Aconsiderablechallengewithunsuper-\nvisedlearningisthatthereisnobenchmarktosignalthelearning. Asolutiontothisissueisto",
    "resorttonon-parametricmethodsorheuristics(seeChapter1). However,suchmethodsthem-\nselvesarenotdesignedtoaddressthelearningissueoflarge-scaleneuralnetworks,particularly\nwhenaneuralnetworkisbuiltupofahugenumberofparameters. Inunsupervisedlearning\nofaneuralnetwork,therefore,itismorecommontousethe“supervision”informationfrom\nthe input data itself. While there are several ways to do this [Hopfield, 1982; Ackley et al.,\n1985;Dayanetal.,1995;HintonandSalakhutdinov,2006],wefocusonauto-encodersinthis\nsection. Wechooseauto-encodersfordiscussionbecausetheyresemblethegeneralformof\nsupervisedmodelsandcanbetrainedviaback-propagation.\nAnauto-encoderisatypeofneuralnetworksthattriestoreconstructtheinputdatafrom\nitsrepresentation. Itisinspiredbytheideaofdimensionalityreduction:\n9Bycontrast,incomputervision,itismucheasiertocreateadversarialsamplesbymakingasmallchangein\ntheinput(e.g.,pixels),sincetheinputspaceiscontinuousandasmallinputperturbationhasverylittleeffecton\nthewholeimage.",
    "10Inadditiontolearningtorepresentdataobjects,thissectionalsocoverssometopicsonthegenerationofdata\nobjects.WewillseetheminSection2.6.3.",
    "2.6 Unsupervised Methods and Auto-encoders 109\nHigh-dimensional data can be converted to low-dimensional codes\nby training a multilayer neural network with a small central layer to\nreconstructhigh-dimensionalinputvectors.\n–HintonandSalakhutdinov[2006]\nThisalsodevelopstheideaofrepresentationlearninginthattheinformationofanobject\ncanbesufficientlyrepresentedbyalow-dimensionalreal-valuedvector. Typically,anauto-\nencoderinvolvesa(probablynon-linear)dimensionalityreductionfunction(callitanencoder)\ntomaptheinputobjecttoitslow-dimensionalfeaturevectorrepresentation(callitacode).\nAlso,itinvolvesareversefunction(callitadecoder)thatmapsthecodebacktotheobject.\nSo,althoughanauto-encoderiscalledan“encoder”,itisnotjustanencoderbutacombination\nofanencoderandadecoder. Moreformally,letxbetheinputvectorofthemodel,suchasa\nhigh-dimensionalrepresentationofaword. Theencoderspitsoutavectordescribingthecode\norlow-dimensionalrepresentationofx,asfollows\nh = Enc(x) (2.93)",
    "where Enc(·) is the encoding network. Enc(·) is typically a multi-layer neural network\nand works as a plugged-in for other systems. Thus, Enc(·) is a general-purpose model. In\nsubsequentchapters,wewillseemanyexampleswhereencodersaretrainedandappliedas\npartsof“bigger”systems.\nOnceweobtainthecode,weusethedecodertomapitbacktotheinput:\nx = Dec(h) (2.94)\n(cid:101)\nwhere x is the reconstruction of the input, and Dec(·) is the decoding network. Given the\n(cid:101)\noriginalinputxandthereconstructedinputx,theobjectiveoftheauto-encoderistominimize\n(cid:101)\nthediscrepancybetweenxandx. Supposethattheencoderandthedecoderareparameterized\n(cid:101)\nby θ and ω, denoted as Enc (·) and Dec (·). The training objective over a set of samples\nθ ω\n{x(1),...,x(m)}isdefinedas\nm\n(cid:88) (cid:16) (cid:17)\n(θˆ,ωˆ) = argmin L x(i),x(i)\n(cid:101)\n(θ,ω)\ni=1\nm\n(cid:88) (cid:16) (cid:17)\n= argmin L x(i),Dec (h(i))\nω\n(θ,ω)\ni=1\nm\n(cid:88) (cid:16) (cid:17)\n= argmin L x(i),Dec (Enc (x(i))) (2.95)\nω θ\n(θ,ω)\ni=1",
    "whereL(·)isthelossfunctionthatcomputesthediscrepancybetweenxandx. Itissometimes\n(cid:101)\ncalledthereconstructionloss. Popularlossfunctionsforreconstructionincludemeansquared",
    "110 Chapter 2. Foundations of Neural Networks\nTrainingObjective:\nm\n(θˆ,ωˆ)=argmin (cid:80) L( x(i) , x(i) )\n(cid:101)\n(θ,ω) i=1\nInputx Reconstructionx\nCodeh (cid:101)\n(Bottleneck)\n7 7\n3 0 3\n.1 Encoder 5 Decoder .1\n2 h=Enc (x) -1 x=Dec (h) 0\nθ (cid:101) ω\n0 .8 1\n.1 .5\nFigure 2.16: An undercomplete auto-encoder with an encoder, a decoder and a bottleneck\nlayersandwichedbetweenthem. Aninputx(left)istransformedintoacodeh(middle)and\nthenareconstructionx(right). Theparametersofboththeencoderanddecoderareoptimized\n(cid:101)\nbyminimizingthediscrepancybetweentheinputxandthereconstructionxonanumberof\n(cid:101)\nunlabeled samples {x ,...,x }. On new samples, we throw away the decoder, and use the\n1 m\nencodertogeneratenewcodesorrepresentations.\nerrorloss,crossentropyloss,etc.\nPuttingtogethertheencoderandthedecoder,itistemptingtothinkofanetworkinwhich\nwe feed something into the input layer and get back the same thing out of the output layer.",
    "Thechallengehereisthatthelow-dimensionalvectorhservesasabottleneckininformation\nflow. Thereisariskofinformationlossintransformationeitherfromxtohorfromhtox,\n(cid:101)\nmakingitdifficultto“copy”theinputtotheoutput. Rather,weneedto“squeeze”anobject\nfromahigh-dimensionalspacetoadense,low-dimensionalspace,andthen“unsqueeze”it\nfromthenewspacetotheoriginalhigh-dimensionalspace. Aconsequenceofthissqueeze-and-\nunsqueezeprocessisthattheencoderisforcedtocompressthedatabutretaintheinformation\nasmuchaspossible. So,theauto-encoderdiscussedhereisalsocalledtheundercomplete\nauto-encoder,becausehhasasmallersizethanxandx. Figure2.16showsanillustrationof\n(cid:101)\ntheundercompleteauto-encoderstructure.\nGiventhelossfunctionL(·),theencoderEnc (·)andthedecoderDec (·),theparameters\nθ ω\nθˆ andωˆ canbeoptimizedbyusingthegradientdescentmethodasinsupervisedlearning(see\nSection2.4.1). Whenapplyingtheauto-encoder,onecansimplydropthedecoderandusethe",
    "encoderasafeatureextractor,thatis,givenanewinputx ,wegenerateanewrepresentation\nnew\nhˆ = Enc (x ) (2.96)\nnew θˆ new\nNotethattheencoderisnotastandalonesystembuttypicallyworkswithothermodelsfor",
    "2.6 Unsupervised Methods and Auto-encoders 111\nacompleteworkingsystem. Forexample,wecantrainanauto-encoderonsomesentences\nand place a Softmax layer on the output of the encoder to build a sentence classifier. The\nclassifier can be further trained on some task-specific data to solve a new problem, such as\ntaggingasentencewithitssentimentpolarity. Thisalsomakestheapplicationofauto-encoder\nfallunderthegeneralparadigmofpre-training: asub-model(i.e.,anencoder)isfirsttrained\non large-scale, task-irrelevant data, and then used as a component of a bigger model on a\ndownstreamtask.\n2.6.1 Auto-encoders with Explicit Regularizers\nAs more complex neural networks are involved, an auto-encoder tends to learn an identity\ntransformationalthoughthebottleneckmakesitabithardertopassthroughwithoutinformation\nloss. Thisiswhatwewouldordinarilyexpect: wecouldmakehasurrogateofxanddecode\nhtosomethingverysimilartox. Ontheotherhand,learninganexactidentitytransformation",
    "requires a highly complicated model and is prone to overfitting. Fortunately, as with other\nmachinelearningmodels,wecanregularizethetrainingbyusingthemethodspresentedin\nSection2.5. Oneofthemostpopularmethodsisaddinganexplicitregularizationtermtothe\nlossfunction. TakingtogetherEq. (2.74)andEq. (2.95),wecandefinethetrainingobjective\ntobe\nm\n(cid:88) (cid:16) (cid:17)\n(θˆ,ωˆ) = argmin L x(i),Dec (Enc (x(i))) +α·R (2.97)\nω θ\n(θ,ω)\ni=1\nwhereRistheregularizationtermaccountingforsomepriorknowledgewewanttoimpose\nontraining,andαisitscoefficient. AcommonchoiceforRinauto-encodersisthesparsity\npenalty(alsoknownassparseauto-encoders). Thesimplestwaytoimplementsuchapenalty\nistoapplythel orl normonthecode,asfollows\n1 2\nm\nR =\n(cid:88)(cid:88)(cid:12)\n(cid:12)h (i)\n(cid:12)\n(cid:12) (2.98)\nl1 (cid:12) k (cid:12)\ni=1 k\nm (cid:115)\n(cid:88) (cid:88)(cid:0) (i)(cid:1)2\nR = h (2.99)\nl2 k\ni=1 k\nIt is worth noting that, unlike those penalties on model parameters (see Section 2.5.1),",
    "the sparsity penalty regularizes the code h (or the output of the encoder) to be sparse. The\nideaofencouragingsparsenessinrepresentationsstemsfromsparsecoding[Olshausenand\nField,1997]. Itstatesthattheinformationofanobjectisembeddedincomplexdependencies\namongtheoriginalattributes(orfeatures)oftheobject. Adesirablerepresentationlearning\nsystemshouldextractsuchdependenciesandreformthemtobeasetofindependentfeatures.\nAndthereshouldbeasmallnumberoftheseindependentfeaturesthatareactive,whilethe\nactivefeaturesvarywhenweswitchtoanewobject. Notealsothat,fromaBayesianlearning\npointofview,otherpenaltiesinregularizedtrainingcouldbeinterpretedaspriorsovermodels.\nThesparsitypenalty,however,isnotapriorbecauseitdoesnotdependonmodels(ormodel",
    "112 Chapter 2. Foundations of Neural Networks\nparameters) but on the training data [Goodfellow et al., 2016]. In this view, the sparsity\npenaltyshouldnotbetreatedasa“regularization”term,butsimplysomedistributionoverthe\nmodel’sintermediatestates. Ontheotherhand,thesparsenessofthecode,thoughnotwell\nexplainedbyconventionaluseofregularizationterms,isindeedhelpfulinmanyapplications\nofauto-encoders,becauseitdirectlymodelsthewayofrepresentingtheinputandimposing\n“priors” on outcomes of encoders. When considered from an empirical point of view, the\nsparsitypenaltyisstillthoughtofasaregularizerthatbiasesthetrainingtocertainmodels.\nThereareotherchoicesfordefiningtheregularizationtermRinadditiontoEqs. (2.98-\n2.99). Forexample,awayofforcingsparsityistopenalizethecaseswheretheaveragevalue\nofeachentryh isfarawayfromapredefinedvalue[NairandHinton,2009]. Incasethath\nk k\nchooses values from [0,1], the regularization can be implemented by defining R as the KL",
    "divergencebetweentheaveragecodeoveranumberofsamplesandtheexpectedcode11. Let\nh¯ denote the average code over {x ,...,x }, where the value of h¯ is the mean of the k-th\n1 m k\nvariableofthecode:\nm\n1 (cid:88)\nh¯ = · Enc(x(i))(k) (2.100)\nk\nm\ni=1\nAlso,letqbetheexpectedcode,whereq =τ foranyk. Ifeachentryoftheaveragecode\nk\nisviewedasaBernoullirandomvariablewithmeanh¯\n,andeachentryoftheexpectedcodeis\nk\nviewedasanotherBernoullirandomvariablewithmeanτ,thentheregularizationtermcanbe\ndefinedasthesumoftheKLdivergencebetweenh¯ andq overallentries:\nk k\n(cid:88)\nR = KL(h¯ ,q )\nk k\nk\n(cid:88) τ 1−τ\n= τ·log +(1−τ)·log (2.101)\nh¯ 1−h¯\nk k\nk\nInthisform,Rpenalizesthemodelwhenh¯ deviatesfromq.\nAsanotherauto-encodervariant,thecontractiveauto-encoder(CAE)triestoimprove\ntherobustnessofrepresentationbyintroducinganewregularizationtermintotraining[Rifai\netal.,2011]:\n(cid:88) m (cid:13) (cid:13)∂h(i) (cid:13) (cid:13) 2\nR = (cid:13) (cid:13)\n(cid:13)∂x(i)(cid:13)\n(cid:13) (cid:13)\ni=1 F",
    "(cid:88) m (cid:13) (cid:13)∂Enc(x(i)) (cid:13) (cid:13) 2\n= (cid:13) (cid:13) (2.102)\n(cid:13) ∂x(i) (cid:13)\n(cid:13) (cid:13)\ni=1 F\nwhere ∂Enc(x(i)) (or ∂h(i) ) is the Jacobian matrix of the representation 12, and ∥·∥ is\n∂x(i) ∂x(i) F\n11Ingeneral,wecansetallentriesoftheexpectedcodetoτ ∈[0,1].Sparsecodeswillbepreferredifτ isclose\nto0,asfeaturesare“inactive”inmostcases.Bycontrast,densecodeswillbepreferredifτ iscloseto1.\n12SupposethattheencoderisafunctionEnc(·):x∈Rdx →h∈Rdh.TheJacobianmatrixofh=Enc(x)is",
    "2.6 Unsupervised Methods and Auto-encoders 113\nthe Frobenius norm of a matrix 13. The contractive penalty helps resist the influence of\nsmallperturbationstotheinput. Inthegeometricsense,itencouragesthattheneighborhood\nrelationshipholdsforoutputdatapointsiftheinputdatapointsareneighborhoods,inother\nwords, it forces Enc(·) to behave more like a contraction mapping14, hence the name of\ncontractiveauto-encoder.\n2.6.2 Denoising Auto-encoders\nAnothersourceofinspirationforimprovingtherobustnessofamodelarisesfromthedenoising\nidea: weaddnoisetotheinputandthenremoveittorecovertheoriginalinput. Denoisingauto-\nencoders(DAEs)aresuchatypeofneuralnetworksthatmarriestheideaofauto-encoding\nwiththeideaofdenoising. First,noiseisaddedtotheinputvectorinastochasticmanner. This\ncanbedescribedasaprocessofgeneratinganoisyinputx giventheoriginalinputx:\nnoise\nx ∼ Pr (x |x) (2.106)\nnoise noise noise\nwherePr (·)isadistributionforsamplingx . Forexample,wecanfollowthemethod\nnoise noise",
    "presentedinSection2.5.5andtakethenoisyinputasamultivariateGaussianvariable:\nx ∼ Gaussian(x,σ2) (2.107)\nnoise\nwhereGaussian(µ,σ2)generatesx viaaGaussiandistributionwiththemeanµandthe\nnoise\nvariance σ2. Eq. (2.109) introduces an additive noise to the input. Subtracting x from the\nmean,wehave\nx = x+g (2.108)\nnoise\ng ∼ Gaussian(0,σ2) (2.109)\nad\nh\n×dxmatrix:\n∂h\nJacobian =\n∂x\n(cid:104) (cid:105)\n= ∂h ... ∂h\n∂x1 ∂xdx\n ∂h1 ··· ∂h1 \n∂x1 ∂xdx\n=   . . . ... . . .   (2.103)\n \n∂hdh\n···\n∂hdh\n∂x1 ∂xdx\n13ForamatrixA∈Rdh×dx,theFrobeniusnormisgivenbytheequation:\n(cid:115)\n(cid:88)\n∥A∥ = A2 (2.104)\ni,j\ni,j\n14LetXbeametricspacewithametricd.Givenafunctionf(·)fromXtoX,f(·)isacontractionmappingif\nandonlyifthereisanumberϵsuchthatforanyx\n1\n,x\n2\n∈X:\nd(f(x 1 ),f(x 2 ))≤ϵ·d(x 1 ,x 2 ) (2.105)",
    "114 Chapter 2. Foundations of Neural Networks\nTrainingObjective:\nm\n(θˆ,ωˆ)=argmin (cid:80) L( x(i) , x(i) )\n(cid:101)\n(θ,ω) i=1\nCorruptedInput\nInputx Noiseg x noise =x+g Codeh Reconstructionx (cid:101)\n(Bottleneck)\n7 0 7 7\n3 0 3 0 3\n.1 + .2 = .3 Encoder 5 Decoder .1\n2 0 2 h= -1 x=Dec (h) 0\n(cid:101) ω\n0 .5 .5 Enc (x ) .8 1\nθ noise\n.1 .1 .2 .5\nFigure2.17: Thestructureofadenoisingauto-encoder. Aninputxisfirstcorruptedintoa\nnoisyorcorruptedinputx . Then,itispassedthroughanencodertoformacodeh. Then,\nnoise\nthecodeispassedthroughadecodertoformareconstructedinputx. Thetrainingisperformed\n(cid:101)\nbyminimizingthelossbetweenxandx. Thisprocessistermed“denoising”becauseittriesto\n(cid:101)\nremovethenoisefromx andrecovertheoriginalinputx.\nnoise\nSometimes, this process is called the corruption of the input, and x is called the\nnoise\ncorruptedinput. AsidefromadditiveGaussiannoise,thereareafewdifferentwaystocorrupt",
    "the input [Vincent et al., 2010]. One of the popular methods is to zero some of the entries\nof x. For example, we can set each entry to 0 with a pre-defined probability. This is also\ncalledmaskingnoise. Anothermethodistousesalt-and-peppernoiseorimpulsenoisefor\ncorruption. Itrandomlychoosessomeoftheentries,andsetseachofthemtoaminimumor\nmaximumvaluewithapre-definedprobability. Differenttypesofnoiseareappliedtodifferent\napplicationsofauto-encoders. Forexample,themaskingnoiseispopularintraininglanguage\nmodels,andthesalt-and-peppernoiseismorecommonlyusedinimageprocessing.\nThen,thecorruptedinputx isfedintoanencoder-decodernetwork,andthenetwork\nnoise\nproduces a reconstructed input x=Dec(Enc(x )). The training process is regular. We\n(cid:101) noise\nreuseEq. (2.95)tominimizethelossofreplacingxwithx. Thus,wecanrewriteEq. (2.95)to\n(cid:101)\nadapttheobjectivetothedenoisingcase:\nm\n(θˆ,ωˆ) = argmin (cid:88) L (cid:16) x(i),Dec (Enc (x (i) )) (cid:17) (2.110)\nω θ noise\n(θ,ω)\ni=1",
    "Eq. (2.110)differsfromEq. (2.95)onlyinthattheinputoftheauto-encoderisx instead\nnoise\nofx. Inotherwords,wedenoisethecorruptedinputtorecovertheoriginalinput. SeeFigure\n2.17forthestructureofdenoisingauto-encoders.\nNotethatbothcontractive/sparseauto-encodersanddenoisingauto-encoderscanbethought",
    "2.6 Unsupervised Methods and Auto-encoders 115\nof as ways to improve the robustness of auto-encoders. Their difference lies in that they\nregularize the training at different points of the model. Contractive auto-encoders aim at\nimproving the robustness of encoding, that is, the representation is learned to be not so\nsensitive to small perturbations to the input. Denoising auto-encoders, on the other hand,\naim at improving the robustness of reconstruction. It affects both encoders and decoders\nsimultaneously. In some sense, denoising auto-encoders are direct applications of noisy\ntraining to auto-encoders (see Section 2.5.5). It is of course difficult to say which models\narebetter. Forexample,contractiveauto-encodershavemoredirectguidanceonlearningthe\nrepresentation, which is what we are concerned the most about. The training of denoising\nauto-encoders,thoughhasanindirecteffectonencoding,receivesadditionaldenoisingsignals",
    "fromthedecoder. Thisoffersanewviewofrobusttraining: arobustrepresentationcanbe\nlearned in both where it is generated (the denoising encoder) and where it is applied (the\ndenoisingdecoder).\n2.6.3 Variational Auto-encoders\nvariationalauto-encoders(VAEs)werenotinitiallyproposedtomodeltheencodingproblem,\nalthoughitistermedan“auto-encoder”. Theyaretypicallyusedtogeneratenewdatasimilar\ntoobserveddata,hencehavingverydifferentformulationsfromtheclassicalauto-encoderswe\nmentionedabove. Instatisticsandmachinelearning,variationalauto-encodersaremoreoften\nviewedasinstancesofvariationalBayesianmethodsandusedtoperformefficientstatistical\ninferenceoverlatentvariableswhentheposteriorprobabilitiesofthesevariablesareintractable\n[KingmaandWelling,2014;2019]. Ontheotherside,variationalauto-encoders,implicitly\norexplicitly,dealwithwhatwedoininducingtheunderlyingrepresentationofanobserved\nobject. Wethereforeinvolveitinthissectionforarelativelycompletediscussion.",
    "Webeginwithagenerativestorydescribinghoweachdatapointisgenerated. Suppose\nthat, for an observed sample x in our dataset, there is an unobserved latent variable h that\ndescribesx. Nowweintendtodevelopaprobabilisticmodeltomodelthegenerationprocess\nofx,say,estimatingtheprobabilityPr(x). Thiscanbeobtainedbycomputingthemarginal\ndistribution:\n(cid:90)\nPr(x) = Pr(x,h)dh (2.111)\nwhereweexplicitlyintroducethelatentvariablehintotheinferenceofx. TosolveEq. (2.111),\nweuseamodelp (x,h)toapproximatePr(x,h)(i.e.,p (x,h)≈Pr(x,h)),andwehave\nω ω\n(cid:90)\np (x) = p (x,h)dh (2.112)\nω ω\nwherep (x,h)isaprobabilitydensityfunctionparameterizedbyω. Wereplacetheleft-hand\nω\nsideofEq. (2.113)withp (x)toemphasizethattheprobabilityisdeterminedbythemodel\nω\np (·). Therearegenerallymanywaystodefinep (x,h). Herewecansimplythinkofitasa\nω ω\nneuralnetwork.",
    "116 Chapter 2. Foundations of Neural Networks\nThen,wecanrewriteEq. (2.113)byusingthechainrule:\n(cid:90)\np (x) = p (h)·p (x|h)dh (2.113)\nω ω ω\nwherep (h)istheprioroverh,e.g.,aGaussianprior. Theconditionalprobabilityp (x|h)\nω ω\ndescribes how likely x is observed given the latent variable h. To model this generation\nprocess,p (x|h)isoftenassumedtobeaGaussiandistributionthatisparameterizedwithits\nω\nmeanµ andvarianceσ :\np p\np (x|h) = Gaussian(µ ,σ ) (2.114)\nω p p\nwhereµ andσ aredeterminedbyadecodingnetworkDec (·)(wewillexplainlateronwhy\np p ω\nitiscalled“decoding”):\n(µ ,σ ) = Dec (h) (2.115)\np p ω\nHowever,Eq. (2.113)isstillintractableeventhoughp (h)andp (x|h)arebothtractable,\nω ω\nbecause it is impossible to summing over all possible h’s. This also leads to an intractable\nposterior:\np (h)·p (x|h)\nω ω\np (h|x) = (2.116)\nω\np (x)\nω\nItlookslikewearestuckwithp (x)andp (h|x)! Variationalauto-encodersaddressthis\nω ω\nissuebyapproximatingp (h|x)withatractableposteriorq (h|x):\nω θ",
    "q (h|x) ≈ p (h|x) (2.117)\nθ ω\nwhere θ is the parameter of the new model. Like Eqs. (2.114-2.115), q (h|x) is defined as\nθ\nanotherGaussiandistribution:\nq (h|x) = Gaussian(µ ,σ ) (2.118)\nθ q q\n(µ ,σ ) = Enc (x) (2.119)\nq q θ\nwhereEnc (·)istheencodingnetworkthatreadsxandgeneratesthemeanandvarianceof\nθ\nthedistributionq (h|x). Thisisinteresting! WenowhaveafeasiblepathtocomputePr(x):\nθ\nwe first sample a latent variable h via q (h|x), and then compute p (x) via the product of\nθ ω",
    "2.6 Unsupervised Methods and Auto-encoders 117\np (h)andp (x|h). Inthiscase,thelog-scaleprobabilityoftheobservationisdefinedtobe\nω ω\nlogPr(x) ≡ E logp (x)\nh∼q (h|x) ω\nθ\np (h)·p (x|h)\n= E log ω ω\nh∼q θ (h|x) p (h|x)\nω\np (h)·p (x|h) q (h|x)\n= E log ω ω · θ\nh∼q θ (h|x) p (h|x) q (h|x)\nω θ\nq (h|x) p (h)·p (x|h)\n= E log θ +E log ω ω (2.120)\nh∼q θ (h|x) p (h|x) h∼q θ (h|x) q (h|x)\nω θ\nThefirsttermoftheright-handsideofEq. (2.120)istheKLdivergence(relativeentropy)\nbetweenq (h|x)andp (h|x),i.e.,\nθ ω\nq (h|x)\nD(q (h|x)||p (h|x)) = E log θ (2.121)\nθ ω h∼q θ (h|x) p (h|x)\nω\nThus,givenD(q (h|x)||p (h|x))≥0,wehave15\nθ ω\np (h)·p (x|h)\nlogPr(x) ≥ E log ω ω\nh∼q θ (h|x) q (h|x)\nθ\n(cid:20) (cid:21)\np (h)\n= E logp (x|h)+log ω\nh∼q θ (h|x) ω q (h|x)\nθ\n= E logp (x|h)+D(p (h)||q (h|x)) (2.122)\nh∼q (h|x) ω ω θ\nθ\nThe right-hand side of Eq. (2.122) is a lower bound of the likelihood logPr(x). It\nis also known as the evidence lower bound (ELBO). The first term of the ELBO can be",
    "approximatelycomputedbysamplingdifferenth’s. Also,computingthesecondtermisnot\ndifficultbecausethereisananalyticalformforD(p (h)||q (h|x))iftheformsofp (h)and\nω θ ω\nq (h|x))aregiven. LetL(x,θ,ω)denotethenegativeELBO.Then,thetrainingprocessofa\nθ\nvariationalauto-encodercanbeframedasminimizingL(·)overanumberofobservedsamples\n{x(1),...,x(m)}:\nm\n(cid:88) (cid:16) (cid:17)\n(θˆ,ωˆ) = argmin L x(i),θ,ω (2.123)\n(θ,ω)\ni=1\nNotethat,becausesamplinghfromq (h|x)isanon-continuousoperation,E logp (x|h)\nθ h∼q (h|x) ω\nθ\nisnotstraightforwardlydifferentiated. Tofitthetrainingofauto-encodersinstandardback-\nprorogation,acommonwayistousetheso-calledreparameterizationtrick. Hereweskip\nthedetailsandreferthereadertoafewpapersformoreinformation[KingmaandWelling,\n2014;Doersch,2016].\nFigures2.18illustrateshowavariationalauto-encoderworks. Itpresentsuswithatwo-step\ngenerationprocess:\n15TheKLdivergencebetweenpandqiszeroonlyifp=q,andispositiveotherwise.",
    "118 Chapter 2. Foundations of Neural Networks\nm\nTrainingObjective: (θˆ,ωˆ)=argmin (cid:80) L(x(i),θ,ω)\n(θ,ω) i=1\nEncodingStep: q (h|x) DecodingStep: p (x|h)\nθ ω\nµ\np\nInputx 7 ρ2 Inputx\nµ p\nq 3\nCodeh\n.1\n7 .1 ρ2 .1 7\nq .2\n3 4 0 .2 3\n.1 .1\n-1 2\n.1 Encoder 5 Decoder .1\n.2 .1\n.9 4\n2 Enc θ (x) .1 -1 Dec ω (h) .3 2\n0 .3 .8 .1 0\n.1 .1\nsampling\nsampling\nGaussian( µ q , ρ2 q ) Gaussian( µ p , ρ2 p )\nFigure2.18: Thegenerativestoryofavariationalauto-encoder. Foraninputsamplex, we\ngenerate a latent variable h by using an encoder q (h|x). In the encoding step, a neural\nθ\nnetworkEnc (·)isfirstusedtoproducethemeanandvarianceofaGaussiandistribution,say,\nθ\nµ and σ2. The latent variable h is then drawn according to Gaussian(µ ,σ2). After that,\nq q q q\nweregeneratetheoriginalsamplexbyusingadecoderp (x|h). Inthedecodingstep,like\nω\nthegenerationprocessintheencoder,aneuralnetworkDec (·)isusedtogeneratethemean\nω\nµ andvarianceσ2 ofGaussian(µ ,σ2). Thesameinputxisspittedoutbysamplingfrom\np p p p\nGaussian(µ ,σ2).",
    "p p\n• Encoding. For an input sample x, we sample a latent variable h from q (h|x). This\nθ\ninvolvesanencodingnetworkEnc (x)thatgeneratesthemeanµ andvarianceσ2 of\nθ q q\nthe Gaussian distribution q (h|x). The latent variable is then generated by sampling\nθ\nfromGaussian(µ ,σ2).\nq q\n• Decoding. Forthelatentvariableh,wesampletheoriginalinputxfromp (x|h). It\nω\nfollows again a Gaussian sampling process: a decoding network Dec (h) is used to\nω\ndeterminethemeanµ andvarianceσ2 ofthedistribution. xisgeneratedbyfollowing\np p\nGaussian(µ ,σ2).\np p\nSometimes,q (h|x)andp (x|h)arecalledan“encoder”anda“decoder”,astheytryto\nθ ω\n“map”aninputtoarepresentationandthen“map”itbacktotheinput. However,q (h|x)and\nθ\np (x|h)themselvesimplysomenon-deterministicmodels,thatis,theyoutputtheprobability\nω\ndensityfunctionsofthevariablesratherthanpointestimates. Animportantconsequenceof",
    "2.7 Summary 119\nthis result is that variational auto-encoders do not tend to find the “best” representation for\nthe input. At first glance it sounds weird as every model we have talked about so far can\ngive a fixed value output. This, however, is the case of the Bayesian inference — we only\nlearn a distribution over possible values of a latent variable. On the empirical side, if you\nwanttoobtainsomethinglikeagoodrepresentation,itisfinetojustsampleavaluefromthat\ndistributionyoudeveloped. Itwouldbeahighprobabilitythatyougetanot-so-badoutcomeif\nyourmodelworkswell[Knight,2009].\nInpractice,themainuseofvariationalauto-encodersisingenerationbutnotrepresentation.\nAttesttime,providedtheoptimizedparametersθˆ andωˆ,theencoder(i.e.,q (h|x))isremoved,\nθˆ\nandthedecoder(i.e.,p (x|h))workswithrandomlygeneratedh’s. Moreprecisely,wesample\nωˆ\nalatentvariableh fromaGaussiandistribution,andinferasamplex byp (x|h )\nnew new ωˆ new\nasusual. WewillseeinthesubsequentchaptersthatmanyNLPproblemscanbecategorized",
    "as generation problems where sequential or hierarchical data objects are generated on the\nconditionofsomegivendataobjectsorlatentvariables.\n2.7 Summary\nInthischapterwehavetalkedaboutwhataneuralnetworkis,aswellasafewbasicarchitec-\ntures, whicharecommonlyusedasbuildingblocksinconstructingpowerfuldeeplearning\nsystems. Also, we have talked about how to train neural networks, how to regularize the\ntraining process, and how to apply neural networks to feature learning in an unsupervised\nmanner.\nButneuralnetworksanddeeplearningarewide-rangingtopicsandallofourdiscussionsare\nalittle“peek”intothem. Foramorecomprehensiveintroductiontothesetopics,Goodfellow\netal.[2016]’sbookmaybeagoodchoice. Italsocoversseveraladvancedtechniques,such\nasdeepstructuredmodelsandrandomizedmethods,fordevelopingstate-of-the-artsystems.\nHowever,asalways,thereisabigdifferencebetweenknowingwhatatechniqueisandbeing\nfluentwithusingitinsolvingreal-worldproblems. So,forpractitionerswhowanttoapply",
    "neuralnetworksanddeeplearninginevensimplesituations,thereareanumberofbookson\nimplementationdetailsofdeeplearningsystems[Géron,2019;Zhangetal.,2021;Chollet,\n2021],andopen-sourceprojectsthatprovidecode-basesforreference16.\nIn the following chapters, we will dig into how to use neural models to address NLP\nproblems. Alongtheway,wewillseehowtolearntherepresentationofwordsandsentences\nusingthemethodswehavediscussedsofar(Chapters3-4),andhowtomodeldifferentNLP\nproblemsbyusingseveralinterestingneuralnetwork-basedmethods,includingtheattention\nmechanismandTransformers(Chapters5-6),pre-training(Chapter7),largelanguagemodels\n(Chapters8-10),andsoon.\n16URLstoafewpopularonlinetutorials:https://pytorch.org/tutorials,https://keras.io/\nexamples/nlp,andhttps://www.tensorflow.org/tutorials",
    "II\nBasic Models\n3 Words and Word Vectors ................. 123\n3.1 Tokenization\n3.2 VectorRepresentationforWords\n3.3 Count-basedModels\n3.4 InducingWordEmbeddingsfromNLMs\n3.5 WordEmbeddingModels\n3.6 EvaluatingWordEmbeddings\n3.7 Summary\n4 Recurrent and Convolutional Sequence Models\n171\n4.1 ProblemStatement\n4.2 RecurrentModels\n4.3 Memory\n4.4 ConvolutionalModels\n4.5 Examples\n4.6 Summary\n5 Sequence-to-Sequence Models ........... 211\n5.1 Sequence-to-SequenceProblems\n5.2 TheEncoder-DecoderArchitecture\n5.3 TheAttentionMechanism\n5.4 Search\n5.5 Summary\n6 Transformers ............................ 269\n6.1 TheBasicModel\n6.2 Syntax-awareModels\n6.3 ImprovedArchitectures\n6.4 EfficientModels\n6.5 Applications\n6.6 Summary",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 3\nWords and Word Vectors\nWordsarebasicunitsoflanguage[Jackendoff,1992]. Mostlanguagesystemsthatpeopleuse\ntoexpresstheirfeelingsandcommunicatewithothersinvolvecreating,mixing,andcombining\nwordsinsomeway. Beforeunderstandinghowawordisusedinforminglargerlanguageunits,\nitisworthfirstunderstandingwhatawordis. Thisinvolvestwofundamentalquestions:\n• Whatisthesurfaceformofaword?\n• Whatisthemeaningofaword?\nButthesequestionsaredifficult,ofcourse,becausetherearenosimplerulestodescribe\nhowawordisformedandhowitsmeaningisdefinedorinduced. Whilethereareavarietyof\ntheoriestoanswerthesequestionsinlinguistics,NLPresearchersareconcernedmorewith\ntwopracticalissues:\n• Tokenization: givenastring,howtosegmentitintoasequenceofwords(alsocalled\ntokens)suchthatthesewordscanbeusedasbasicunitsindownstreamNLPtasks?\n• WordRepresentationLearning: givenacorpus,howtolearntorepresenteachword",
    "insomecountableform, andhowtoenableNLPmodelsto“compute”ontopofthis\nrepresentation?\nOne goal of this chapter is to show how a sentence is segmented in either a linguistic\nor statistical manner. Specifically, we describe several approaches to tokenizing a string of\ncharactersintowordsorsubwordsbyheuristicrulesorstatisticalmodelslearnedfromdata.\nThe other goal here is to show how words can be represented as real-valued vectors. In\nparticular,wepresentmodernapproachestolearningandevaluatingthesewordvectors. The\nvalue of this part is not on drilling on those formulas and models but on showing the core\nideaofwordvectorrepresentationwhichisthebasisofmanyNLPsystems. Inthenextfew\nchapters,wewillseeanaturalgenerationofthisideatomodelingmorecomplicatedproblems,\nsuchasrepresentingsequentialandtree-likedata.",
    "124 Chapter 3. Words and Word Vectors\nInput: 一直以来，完美的机器翻译是人类的梦想之一。\nChinese\nOutput: 一直/以来/，/完美/的/机器翻译/是/人类/的/梦想/之一/。\nInput: 西日本や海はく晴れて、汗ばむ暑さとなる。\nJapanese\nOutput: 西日本/や/海/は/く/晴れて/、/汗ばむ/暑さ/と/なる/。\nInput: Shesaid,“Deeplearningisnotthesolutiontoallworld’sproblems”.\nEnglish\nOutput: She/said/,/“/Deep/learning/is/not/the/solution/to/all/world/\n’s/problems/”/.\nFigure3.1: Tokenizationfordifferentlanguages(slash=wordboundary). ForChineseand\nJapanese where there are no delimiters between words, tokenization is often called word\nsegmentation.\n3.1 Tokenization\nIn computer science and related fields, the term token can be used in many different ways.\nHerewesimplythinkofatokenasawordinlinguistics,althoughitcanbesomethingdifferent\n(seeSection3.1.4). InNLP,tokenizationorsegmentationisataskrelatedtomorphological\nanalysis[AronoffandFudeman,2011]. Whilemorphologicalanalyzersorparsersaregenerally\nusedtostudytheinternalstructureofwords,tokenizationisconcernedwithhowsentencesare",
    "brokendownintowords. Itappearsthatweneedtoknowhowwordsarecomposedifwewant\ntoknowhowsentencesareformedbywords. Thingsareevenmoreinterestingbecausethe\nvarietyoflanguagesmakesitdifficulttofindageneralsystemtodescribethemorphologyof\neverylanguage. Forexample,analyticlanguages(suchasChinese)havelittleinflection,and\nrelyonwordordertoconveymeaning. Bycontrast,syntheticlanguages(suchasFrench)may\nhaverichinflectionandthemeaningofawordishighlyinfluencedbymorphology.\nOntheotherhand,dividingsentencesintosmallerlinguisticpiecesisimportantinmany\nNLPtasks,eventhoughmanyoftheworld’slanguageshavelittlemorphology. Forexample,\nChineseisamorphologicallysimplelanguagethathasnoexplicitwordboundaries. Whileit\nalsomakessensetotakecharactersasunitsinunderstandingwhataChinesetextistalking\nabout,itismoredesirableandreasonabletoconsiderlargerunitsinprocessingthetext. Note\nthat,evenforlanguageshavingdelimitersbetweenwords,suchasEnglish,westillhaveto",
    "tokenizesentencessuchthattheyarestandardizedwhenservingastheinputand/oroutputof\nanNLPsystem.\nInthissection,weskipthediscussiononwhatexactlyawordisinmorphologyandsyntax,\nbutsimplyviewtokenizationasataskofaddingwordortokenboundariestoagivenstring\n(seeFigure3.1). Wewillshowthatasentencecanbebrokendownintowordsortokensin\neitheraheuristicorstatisticalmanner. Notethatthisprocessisdesignedtoproducesomeunits\nthat can ease the processing of languages in NLP systems, not necessarily to make strictly\nlinguisticsense.",
    "3.1 Tokenization 125\n3.1.1 Tokenization via Rules and Heuristics\nA common and simple approach to tokenization is to identify every word in a sentence by\napplying a set of pre-defined rules. In general, these rules are linguistically motivated and\nreflectourpriorknowledgeofwhattheformofawordshouldbe. Forexample,considerthe\nEnglishexampleinFigure3.1. Wecandefinethefollowingrulesfortokenizingthesentence:\n• Words do not contain spaces. In this sense, we can split the sentence into “word\ncandidates”withspace.\n• EverywordcandidatethatismadeupofEnglishlettersonly(i.e.,a-zandA-Z)isaword.\n• Everypunctuationmark(i.e.,quote,comma,period,etc.) shouldbeisolatedtoforma\nword.\n• ’sisaword,indicatingnounpossessive.\nThis might be one of the smallest rule sets we can use in English tokenization. Surely,\nmorerulescanbeaddedtocovermorelinguisticphenomena,e.g.,wordswithdashes,words\ncontaining non-English letters, and so on. However, there are no standards to define such",
    "a set of rules. In practice, and particularly in NLP applications, we want a minimal set of\nrulestodealwithmostproblems,andthetokenizationisusuallyimplementedbyanumberof\nregularexpressions. Herewewillnotdiscusstheserulesandregularexpressionsindetail,\nbutreferthereadertoafewtextbooksformoredetails[Lawson,2003;Friedl,2006;Jurafsky\nandMartin,2008]1.\nAlso,itiscommontonormalizethetextbeforetokenizationsothattheinputofthetok-\nenizeriscanonical. Forexample,forEnglishandotheralphabeticlanguages,normalization\norcanonicalizationreferstoaprocessoflowercasingwords,normalizingcharacterrepresen-\ntation(e.g.,Unicodecharacters),andsoon. Inaddition,wecanmapdifferentformsofaword\ntothesameformforfurthergeneralizationofthetokenization. Asimplewaytodothisisto\nconflateallinflectedformsofawordintoitsbaseform. Inlinguistics,thebaseformofaword\niscalledlemma,andtheprocessofmappingwordstolemmasiscalledlemmatization. Here\naresomeexamplesoflemmatization.\nlearn → learn\nlearning → learn\nlearns → learn\nbest → good",
    "Therearewordsthatcorrespondtotwoormoredifferentlemmas(oftenwithdifferentpart-of-\nspeeches). Inthiscase,weshouldselectthecorrectlemmaaccordingtothecontext. Inother\nwords,lemmatizationiscontext-dependent.\n1Tokenization scripts can be found in many open-source projects, such as Moses [Koehn et al., 2007]\n(https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/\ntokenizer.perl) and the tokenizers in SacreBLEU (https://github.com/mjpost/sacrebleu/\ntree/master/sacrebleu/tokenizers).",
    "126 Chapter 3. Words and Word Vectors\nOriginal Shesaid,“Deeplearningisnotthesolutiontoallworld’sproblems”.\nNormalization shesaid,“deeplearningisnotthesolutiontoallworld’sproblems”.\nTokenization she/said/,/“/deep/learning/is/not/the/solution/to/all/world/’s/problems/”/.\nLemmatization she/say/,/“/deep/learning/be/not/the/solution/to/all/world/’s/problem/”/.\nStemming she/said/,/“/deep/learn/is/not/the/solut/to/all/world/’s/problem/”/.\nFigure3.2: Normalization,lemmatization,andstemmingofanEnglishsentence. Innormal-\nization,thewholesentenceislowercased. Inlemmatization,everywordislemmatizedand\nrewrittenasitslemma. Instemming,thesuffixesofsomewordsareremoved.\nCloselyrelatedtolemmatizationisstemming,whichrepresentsawordasitsstem. Like\nlemmas,astemissomebaseformofaword. However,unlikelemmas,astemisnotnecessarily\navalidword,althoughtherearemanywordswhoselemmasandstemsareidentical. Another\ndifferencefromlemmatizationisthatstemmingisperformedonindividualwords,withoutthe",
    "needofcontextfordisambiguation. So,stemmingiscontext-independent. Thereareseveral\nefficientalgorithmsforstemming. Apopularoneissuffixstripping[Porter,1980]. Itsimply\nremovesthesuffixesing,ed,ion,etc.,likethese\nremove → remov\nremoving → remov\nremoval → remov\nbest → best\nFormoreexamples,Figure3.2showsnormalization,lemmatization,andstemmingresultsfor\nanEnglishsentence.\nItisworthnotingthattheabovemethodsaretypicallyimplementedusingregularexpres-\nsions, dictionary lookups, and additional heuristics. While in our little exploration here it\nseems that tokenization is not so difficult, much more work is needed to make it practical.\nInparticular,ifwedealwithlanguageswithanon-alphabeticwritingsystem,orlanguages\nwithoutexplicitspacingbetweenwords,thentokenizationwouldbeahardproblem,andin\nthatcase,usingsimpleruleswouldnotbeagoodstrategy. Inthefollowingsubsections,we\nwillreframetokenizationasamachinelearningproblemwherethewaytotokenizeorsegment",
    "sentencesislearnedfromdata. Thesemethodsarelanguage-independentandcanbeappliedto\nawiderangeoftokenizationorsegmentation-likeproblems.\n3.1.2 Tokenization as Language Modeling\nNowletusmovetostatisticalmodelingofthetokenizationproblem. Foreaseofdiscussion,in\nthissubsectiononlylanguages(ormorepreciselywritingsystems)withoutwordboundaries\nareconsidered,butthemethodshouldbeunderstoodtocoverotherproblemswheredelimiters\nareusedtoindicatetheendorbeginningofaword. Letx=x ...x beastringofcharacters,\n1 l",
    "3.1 Tokenization 127\nandy=y ...y beasequenceofwordsortokens. Wewouldsaythatyisatokenizationresult\n1 m\nofxifydefinesasegmentationonx. ConsiderthefollowingChinesesentence:\nx = 机器翻译是人类的梦想之一。\nWecandefineasegmentationonthesentence,forexample2,\ny = 机器翻译/是/人类/的/梦想/之一/。\n(cid:104) (cid:105)\n= “机器翻译” “是” “人类” “的” “梦想” “之一” “。” (3.1)\nInthisway,tokenizationcanbeframedasaproblemofmappingxtoy. Givenaninputstring,\ntheoutputisthemostlikelysegmentation:\nyˆ = argmax Pr(y|x)\ny\n= argmax logPr(y|x) (3.2)\ny\nEq. (3.2)describesapredictionmodelwehavebeenreferencingseveraltimesinthisbook.\nHowever,theproblemwearedealingwithiseasierbecauseycontainstheinformationofx,\nandwecanremovetheconditionfromPr(y|x)intheargmaxoperation:\nyˆ = argmax logPr(y)\ny\n= argmax logPr(y ,...,y ) (3.3)\n1 m\ny\nItiseasytocheckthatEq. (3.3)infactdescribesalanguagemodelingproblem. There\nareafewdifferentwaystoestimatethejointprobabilityPr(y ,...,y ). Asimplemethodisto\n1 m\nrewritelogPr(y ,...,y )intoasumoflog-scaleconditionalprobabilities:\n1 m",
    "logPr(y ,...,y ) = logPr(y )+logPr(y |y )+...+logPr(y |y ,...,y ) (3.4)\n1 m 1 2 1 m 1 m−1\nEachconditionalprobabilityPr(y |y ,...,y )canbeapproximatedby\ni 1 i−1\nPr(y |y ,...,y ) = Pr(y |y ,...,y ) (3.5)\ni 1 i−1 i i−n+1 i−1\nthatis,thegenerationofy onlydependsonthen−1previouscontextwords. Tocompute\ni\nPr(y |y ,...,y ),wecaneitherusetherelativefrequencymethodsorneuralnetworks\ni i−n+1 i−1\n(seeChapter2).\nNowwecanthinkoftokenizationasasupervisedlearningproblem. Theprocessisoutlined\nhere:\n• Preparesomesentencesthatarecorrectlysegmented.\n2Followingthenotationusedpreviously,weusebothy=y 1 ...ymandy= (cid:2) y 1 ... ym (cid:3) todenoteasequence\nofvariables.",
    "128 Chapter 3. Words and Word Vectors\n• LearnalanguagemodelPr(y)ontheselabeledsentences.\n• Foranewsentence,findthe“best”tokenizationyˆ thatmaximizesPr(yˆ),asinEq. (3.3).\nWhilethisprocedurefollowsastandardpipelineofsupervisedlearning,thereareseveral\npractical issues we have to iron out. First, the language model requires a vocabulary from\nwhich y can choose a value, but new words are always around. To handle them, one way\ni\nis to segment an unknown substring into characters, that is, we treat characters as words if\nthesubstringyieldingthesecharactersisnotcontainedinthevocabulary. Analternativeisto\ntakeintoaccountallsubstringsthatarenotcoveredbythevocabulary,andreplacethemwith\nthe<unk>tag. The<unk>trickiswidelyadoptedinstate-of-the-artlanguagemodelsandis\nusuallyhelpful.\nSecond,thelanguagemodeldescribedabovehasabiastowardsshortsequencesbecause\nPr(y ,...,y )wouldbelargeifmisasmallnumber. Ageneralwaytomitigatethisbiasisto\n1 m\nintroducealengthreward(orlengthbonus)tothemodel,forexample,",
    "yˆ = argmax logPr(y)+λ·m (3.6)\ny\nor\nlogPr(y)\nyˆ = argmax (3.7)\nmλ\ny\nwhereλ·mandmλ rewardlongsequencesandλ>0isahyperparametercontrollinghow\nmuchwerelyontherewardinassessingthegoodnessofy. Interestingly,itisfoundthatthe\nlengthbiasisnotabigproblemwithtokenizationinpracticebecausethevarianceinlengthis\nsmallforthose“good”tokenizationresults. Forexample,usingaunigramlanguagemodel\n(i.e.,n=1)withoutanylengthrewardworkswellinmanyreal-worldapplications. Wewill\nseeafewexamplesinSection3.1.4.\nThird,performingargmaxisdifficultbecausethereareexponentiallymanytokenization\ncandidates. However, the use of language models here enables efficient search algorithms.\nConsider, for example, applying a unigram language model to tokenization. For the input\nstring x, we keep, at each position j of x , a state that describes the probability of the best\ntokenization on x ...x (denoted as p(j)) as well as the last word of this tokenization. At\n1 j",
    "positionj+1,wecreateanewstateandcomputetheprobabilityofthebesttokenizationon\nx ...x by\n1 j+1\np(j+1) = max p(i)·Pr(x ...x )\ni+1 j\n1≤i≤j\n= max p(i)·Pr(w ) (3.8)\n[i+1,j]\n1≤i≤j\nwherePr(w )istheprobabilityofthewordspanningx ...x . Onthealgorithmicside,\n[i+1,j] i+1 j\nEq. (3.8)describesadynamicprogrammingmethodthathasatimecomplexityofO(l2)for\naninputoflengthl. Forthefinaloutput,wecantracebackfromthefinalstateanddumpthe\nwordsequencealongthepathoftheoptimaltokenization.",
    "3.1 Tokenization 129\nNote that the methods here are generic and can be applied to tokenization for other\nlanguages. For example, when applying it to English, we only need a slight update on the\nformatoftheinput: theinputisnotacharactersequencebutasequenceofthesmallestpossible\npiecesseparatedoutbypunctuationandspaces. Forexample,forthesentenceIsthisTom’s\nlaptop?,wehave\n(cid:104) (cid:105)\nx = Is this Tom ’ s laptop ? (3.9)\nThen,thetokenizationprocesscanproceedasinEqs. (3.2-3.7).\n3.1.3 Tokenization as Sequence Labeling\nOneofthemajorwaysbywhichNLPresearchersgrouptogetherconsecutivelinguisticpieces\nisthroughtaggingthesequencewithagrouping-inspiredlabelset,oftenknownassequence\nlabeling. Althoughwelimitourselvesheretotheproblemofgroupingcharacterstowords,as\nwewillseeinthefollowingchapters,suchamethodisagoodsolutiontomanyNLPproblems,\nsuchaspart-of-speechtagging,namedentityrecognition,andsoon. Sincetheideaofsequence",
    "labelinghasbeendiscussedinChapter1,wepresentherehowitisadaptedtothetokenization\ntask.\nThelabelsetsusedintokenizationareregular. Thesimplestoftheseisthe“IB”set. The\n“I”labelindicatesalinguisticpieceinsideaword,andthe“B”labelindicatesthebeginning\nofaword. Thelabelsetcanbeenrichedbyaddingthe“E”label(i.e.,theendingofaword)\nand/orsplittingthe“B”labelintosub-labels(e.g.,B andB indicatethefirstandthesecond\n1 2\nlinguisticpiecesofaword)[Zhaoetal.,2006]. Givenaninputsequencexandatokenization\nresulty, transformingy tothelabelsequenceisfairlysimple. Consideragaintheexample\nusedintheprevioussubsection. Wecanlabelthesequenceindifferentformats:\nx: 机 器 翻 译 是 人 类 的 梦 想 之 一 。\ny: 机 器 翻 译 是 人 类 的 梦 想 之 一 。\n{I,B} B I I I B B I B B I B I B\n{I,B,E} B I I E B B E B B E B E B\n{I,B ,B ,E} B B I E B B B B B B B B B\n1 2 1 2 1 1 2 1 2 1 2 1\nSincethelabelsequencecanbetreatedasanotherformofthetokenization,wecanrestate\ntheproblemasfindingthebestlabelsequencegivenaninput:\ncˆ = argmax logPr(c|x) (3.10)\nc",
    "wherec=c ...c isalabelsequence. ManymethodshavebeenproposedtomodelPr(c|x). A\n1 l",
    "130 Chapter 3. Words and Word Vectors\nclassicwayisgivenbyrewritingPr(c|x)usingtheBayes’rule:\nPr(x|c)Pr(c)\ncˆ = argmax log\nPr(x)\nc\n= argmax logPr(x|c)+logPr(c) (3.11)\nc\nPr(x|c)Pr(c)\nInthismodel,Pr(x)isaconstantforallc’s,andthuscanberemovedfrom in\nPr(x)\nsearch. Pr(x|c)istheprobabilityofgeneratingtheinputx(i.e.,observations)giventhelabel\nsequencec(i.e.,latentvariables),andPr(c)isalanguagemodeldefinedonthelabelsequence.\nSimplifications are in general required for a tractable model. For example, we can make a\nMarkovassumptionthatthechoiceofc isdependentonlyonthechoiceofc . Thisleads\ni i−1\ntothehiddenMarkovmodel(HMM)whichiswidelyusedingenerativemodelingforNLP\nproblems.\nAn alternative method is discriminative modeling. A common idea is to treat sequence\nlabelingas aseries ofindependentclassification problems. Forexample, wecan develop a\nlocal classifier that conditions the prediction of c on a set of features around position i. In\ni",
    "more sophisticated models, such as conditional random fields (CRFs), the context of the\nentiresequencecanbeusedintheprediction. Whileitmaybeinterestingtogomoredeeply\nintothedetailsaboutthesesequencelabelingmodels,wesimplyskipthemtomakethetopic\ninthissectionmoreconcentrated. Instead,thereaderisreferredto[Kupiec,1992;McCallum\netal.,2000;Laffertyetal.,2001]forthoroughdiscussionsofhowthesemodelsaredeveloped\nandapplied. Inaddition,foracomparisonofgenerativemodelinganddiscriminativemodeling,\nwereferthereadertoChapter1.\n3.1.4 Learning Subwords\nItisacommonlyheldbeliefthatwordsarethebasicunitsinlanguageuse. Thisdoesnotmean\nthatwordsarethesmallestlinguisticunits. Rather,wordscanbebrokendownintosmaller\npieces that have meanings, such as morphemes. It is this which accounts for the important\nroleofwordsinthesyntactichierarchyofalanguage,e.g.,wordsaremadeupofmorphemes,\nandphrasesandsentencesaremadeupofwords. Itisthereforenaturaltothinkofwordsas",
    "distinctcomponentsoflanguagesthathavesomefunctioninformingthestructureormeaning\nofaphraseorasentence. InNLP,however,viewingsentencesassequencesofwordsisnotso\ndesirablesometimes. Aproblemisthatsomewordsarerare,makingitdifficulttoadequately\nlearnamodelbecauseofdatasparseness. Forexample,uncopyrightableisanEnglishword\nthatrarelyoccurs. AnNLPsystemmaysimplyrecognizeitasanunknownword(i.e.,anOOV\nword),althoughwecangetthemeaningofthiswordbydecomposingitintoparts: un,copy,\nright,andable. Anotherproblemisthatlinguistics-basedtokenizationstandardssomewhat\nlimit the use of computers for automatically learning the way to segment the sentence into\nunitsinamachinelearningsense. Inthiscase,itishelpfultoconsideridentifying“new”words\nthatarenotstrictlyconstrainedbylinguisticsbutarebettersuitedtoNLPsystems.",
    "3.1 Tokenization 131\n1. Byte Pair Encoding\nBytePairEncoding(BPE)isoneofthemostsuccessfulmethodstolearnsubwordunitsfrom\nasetofwordsequences[Sennrichetal.,2016b]. WhiletheBPEapproachstemsfromdata\ncompression[Gage,1994],itismoreoftenusedinNLPasasolutiontotheopenvocabulary\nproblem. ThebasicideaofBPEisthatwerepeatedlyreplacethemostfrequentpairofbytes\nin the data to form a new byte. As a result, common bytes are often involved in merging\nsubstringsofbytes,andrarebytesareoftenisolatedandconsidereduniqueunits. Theoutcome\nofBPEisabytevocabularythatcanbeusedtoencodenewdata.\nInNLP,abytecanroughlycorrespondtoacharacter. Andeachentryofthevocabularyis\nacharactersequence,calledasymbolorsubword. BPEbeginswithsplittingagiventextintoa\nsequenceofcharacters,forexample,wecanaddaspaceaftereachoccurrenceofanEnglish\nletterorapunctuationmark. Thisingeneralresultsinaverylongsequence. WhileBPEitself\nhasnorestrictionsoninputlength,amorecommonwayistopreventcross-wordsymbolsfor",
    "efficiencyconsiderations. Thus,wecanrepresentthetextasalistofspace-separatedwords,\neachbeingassociatedwiththefrequencyoftheword. Forexample,considerawordlist:\nf l o w # : 2\nb l o w # : 2\nf l a t # : 1\nf l a g # : 4\nwhere#isaspecialsymbolindicatingtheendofaword3. Fromthiswordlist,wecancollect\naninitialvocabulary:\nf : 7 b : 2\nl : 9 a : 5\no : 4 t : 1\nw : 4 g : 4\n# : 9\nThen,wecounttheoccurrencesofeachsymbolbigram:\n3Insteadoftaking#asaseparatesymbol,anotherwayistoconcatenate#withthelastcharacterineachword,\nlikethis\nf l o w# :2\nb l o w# :2\nf l a t# :1\nf l a g# :4\nwhere“w#”,“t#”,and“g#”representcharactersthatoccurattheendofaword.",
    "132 Chapter 3. Words and Word Vectors\nf l : 7 a g : 4\nl a : 5 g # : 4\nl o : 4 b l : 2\no w : 4 a t : 1\nw # : 4 t # : 1\nWemergethemostfrequentsymbolbigram“fl”toanewsymbol“fl”andreplaceinthe\nwordlisteachoccurrenceof“f l”with“fl”:\nfl o w # : 2\nb l o w # : 2\nfl a t # : 1\nfl a g # : 4\nAccordingly,thesymbol“fl”isaddedtothevocabulary:\nf : 7 b : 2\nl : 9 a : 5\no : 4 t : 1\nw : 4 g : 4\n# : 9 fl : 7\nThen,thisprocessisrepeatedagain. Thistime,wemergethesymbolbigram“fl a”and\ncreateanewsymbol“fla”. Assuch,wehaveanewwordlist:\nfl o w # : 2\nb l o w # : 2\nfla t # : 1\nfla g # : 4\nandanewvocabulary:\nf : 7 b : 2 fla : 5\nl : 9 a : 5\no : 4 t : 1\nw : 4 g : 4\n# : 9 fl : 7\nWecanrunthisprocessacertainnumberoftimes. Themoretimesweperformthemerge\nprocess, the larger the vocabulary is. The entries of the final vocabulary are reordered by\nsymbolfrequencies. Forexample,ifwesetthenumberofmergeoperationsto6,wewillhave\navocabulary,likethis:",
    "3.1 Tokenization 133\nl : 9 fla : 5 ow# : 4\n# : 9 o : 4 flag : 4\nf : 7 w : 4 flag# : 4\nfl : 7 g : 4 b : 2\na : 5 ow : 4 t : 1\nItcorrespondstothewordlist:\nfl ow# : 2\nb l ow# : 2\nfla t # : 1\nflag# : 4\nHaving obtained a vocabulary like above, we can apply it to tokenize new words. The\nsubwordtokenizationfollowsthesameprocedureofmergingsymbolbigramsasthatusedin\nbuildingthevocabulary. GivenaBPEvocabulary,wefirstsegmenttheinputtextintocharacter\nsymbols. Then,weexamineeachsymbolbigraminthesequence,andmergetheonethathas\nthe highest frequency in the vocabulary. We repeat this operation until there are no further\nmerges. Consider,forexample,thefollowingtext:\ntowaflag\nItisfirsttransformedintoacharactersequence:\nt o w # a # f l a g #\nByusingtheBPEvocabularywehaveobtained,wecandoBPEmergingonthissequence,\nlikethis\nt o w # a # f l a g #\nf l⇒fl\n−−−−−−−−−→ t o w # a # fl a g #\nfl a⇒fla\n−−−−−−−−−→ t o w # a # fla g #\no w⇒ow\n−−−−−−−−−→ t ow # a # fla g #\now #⇒ow#\n−−−−−−−−−→ t ow# a # fla g #\n... ...",
    "−−−−−−−−−→ t ow# a # flag#\nThissubwordsequencecanbeusedassomeinputand/oroutputofadownstreamNLP\ntask,suchasmachinetranslation. Sometimes,wewanttomapsubwordsbacktowords. Thisis\nsimple: wekeepthespaceaftereachoccurrenceofthe#symbol,andremoveallotherspaces\nand#. AlsonotethattheBPEmethodwedescribehererequiresword-segmentedinputs,that\nis,weneedapre-tokenizertoroughlytokenizetheinputsequenceintosomeunits. Thiscanbe\ndonebyusingthemethodspresentedinSections3.1.1-3.1.3.",
    "134 Chapter 3. Words and Word Vectors\n2. WordPiece\nTheWordPiecemethodisverysimilartotheBPEmethodinthatitfirstdividestheinputtext\nintothesmallestsymbolsandthenprogressivelymergespairsofconsecutivesymbolstoform\nlargersymbols[SchusterandNakajima,2012]. Thedifferencebetweenthemisonlyinthe\nway of selecting which symbol bigram to merge. In BPE, we merge each time the symbol\nbigramwiththehighestfrequency. Let(x ,x )beabigraminthesequencex. Themerge\ni i+1\nruleofBPEcanbedescribedas\n(x ,x ) = argmax count(x ,x ) (3.12)\nˆi ˆi+1 i i+1\ni∈[1,|x|−1]\nwhere the function count(x ,x ) returns the frequency of (x ,x ) in the corpus, and\ni i+1 i i+1\n(x ,x )isthebigramwiththehighestfrequency.\nˆi ˆi+1\nTheWordPiecemethod,instead,adoptsamaximumlikelihoodcriterionforbigramselec-\ntion. Moreprecisely,itmergesthebigramsothatthelikelihoodofthedataismaximized. This\ncanbeformalizedas:\nPr(x ,x )\ni i+1\n(x ,x ) = argmax log\nˆi ˆi+1\nPr(x )Pr(x )\ni∈[1,|x|−1] i i+1\n= argmax [logPr(x ,x )−log(Pr(x )Pr(x ))] (3.13)",
    "i i+1 i i+1\ni∈[1,|x|−1]\nlogPr(x ,x )−log(Pr(x )Pr(x )) describes the increase in log-likelihood of the\ni i+1 i i+1\ntext when we replace consecutive symbols (x ,x ) with a single symbol x x 4. Thus,\ni i+1 i i+1\napplicationsofsuchamergeruleproduceasequenceofcodingsteps,eachofwhichincreases\nthelikelihoodabitontopofthelaststep. Theoutcomeofthisprocessisacodebook(i.e.,a\nvocabulary)bywhichwecandefinethemostlikelycodesequenceforthegiventext.\n3. SentencePiece\nBoth the BPE and WordPiece methods require that the input text is pre-tokenized in some\nway. Thismakesitsomewhatcomplicatedtodevelopatokenizationsystem. Asanalternative,\nSentencePieceisamoregeneralmethodthatdealswithrawtextsandconsidersallcharacters\n(includingspaces)intokenization[KudoandRichardson,2018]. ThemainideaofSentence-\nPieceistoscaledownabigvocabularysothattheunigramprobabilityofthetextisminimized\natsomelevelofthevocabularysize5,calledtheunigrammethod[Kudo,2018].",
    "The unigram method frames subword segmentation as a unigram language modeling\nproblem,resemblingthegeneralformofEqs. (3.3-3.4). Letxbeasequenceofcharactersand\n4Instatistics, Pr(a,b) iscalledthepointwisemutualinformationofvariablesaandb.Seemoredetails\nPr(a)Pr(b)\ninSection3.3.1.Anothernameforthisisinformationgain.ItcanbeinterpretedbyusingtheKullback-Leibler\ndivergenceorothermeasuresininformationtheory(seeChapter1).\n5Thetermvocabularysizemayhavedifferentmeanings.Hereitreferstothenumberofentriesofthevocabulary.\nSometimes,ontheotherhand,itisthoughtofasthetotalnumberofbytesusedtostorethevocabulary.",
    "3.1 Tokenization 135\nybeasequenceofsymbolsorsubwordsyieldingx. Theprobabilityofyisgivenby:\n|y|\n(cid:89)\nPr(y) = Pr(y ) (3.14)\ni\ni=1\nThen,wecanwritethelikelihoodofxintermsofthejointprobabilityofxandy:\n(cid:88)\nPr(x) = Pr(x,y) (3.15)\ny∈Y(x)\nwhere the sum is over all possible tokenization results Y(x). Since y can be viewed as a\nsegmentation-annotated version of x, the model of Pr(x,y) provides no more information\nthanthemodelofPr(y)andwehavePr(x,y)=Pr(y). Thus,wecanrewriteEq. (3.15)as:\n(cid:88)\nPr(x) = Pr(y)\ny∈Y(x)\n|y|\n(cid:88) (cid:89)\n= Pr(y ) (3.16)\ni\ny∈Y(x)i=1\nTakingthisequation,thelog-likelihoodofasetofstringsX isgivenby\n|y|\n(cid:89) (cid:88) (cid:89)\nPr(X) = log Pr(y )\ni\nx∈Xy∈Y(x)i=1\n \n|y|\n(cid:88) (cid:88) (cid:89)\n= log Pr(y\ni\n) (3.17)\nx∈X y∈Y(x)i=1\nIfweconsider−Pr(X)asalossfunction,thenthetaskherecanbestatedasfindingthe\nbest estimate for each unigram probability Pr(y) so as to make Pr(X) as large as possible.",
    "At first glance this optimization problem looks complicated. Fortunately, there are several\npowerfultoolstosolveit. ApopularmethodistousetheExpectation-Maximization(EM)\nalgorithm[Dempsteretal.,1977],whichiscommonlyusedwhenonetriestofindastatistical\nmodelthatmaximizesthelikelihoodofthedata. NotethattheEM-basedsolutiontoEq. (3.17)\nis similar to those for other NLP problems, such as statistical machine translation, and has\nbeenwelldiscussedinthosecontexts. Sowereferthereaderto[Brownetal.,1993]fordetails\naboutthesemethods. InthischapterwejusttakeEMasanoff-the-shelftooltoestimatePr(y)\ngivenEq. (3.17). 6\n6InEM,wecanviewX asanobservation,andPr(X|θ)asastatisticalmodelthatdescribeshowlikelythe\nobservationoccurs.Hereθisthemodelparametersthatweintendtodetermine.EMisbasedonanobjectiveof\nmaximumlikelihoodestimation,thatis\nθˆ = argmaxPr(X|θ) (3.18)\nθ\nForthemodelhere,wecanview{Pr(y)}asmodelparameters. WeskipthederivationdetailsabouttheEM",
    "136 Chapter 3. Words and Word Vectors\nSentencePieceisessentiallya“pruning”methodthatremoveslowprobabilityentriesfrom\nthevocabulary. ItstartswithabiginitialvocabularyV. Forexample,wecancreatetheinitial\nvocabularybyenumeratingallstringswithalengthconstraint. Typically,cross-wordstrings\nareexcludedtoreducethevocabularysize. Then,werunthefollowingsteps:\n• Estimatetheprobabilityforeachentryy ofV byoptimizingEq. (3.17).\n• Computethelossforeachentryy ofV viatheremove-onestrategy,thatis,thelossis\nthereductioninthelikelihood(seeEq. (3.17))wheny isremovedfromthevocabulary.\n• RemoveacertainpercentageofentriesofV withlargelosses. Forexample,wekeep\n80%oftheentries,anddiscardtherest.\nTheoutcomeofthisprocessisanewvocabularyaswellastheprobabilityassignedtoeach\nsubword. Wecanrepeatthisprocessanumberoftimesuntilthevocabularysizeisreducedto\nadesirablelevel.\nSentencePiecediffersfromBPEandWordPieceinthatitconsidersallpossiblesubword\n(cid:80)",
    "sequencesforagivenstring(seethesum inEq. (3.15)). Fromthemachinelearning\ny∈Y(x)\npointofview, thiscanbeseenas awayof regularization, thatis, wecanreducethe riskof\noverestimatingtheparameterscorrespondingtothesingle-bestsubwordsequencethatmay\nhaveerrors. AnalternativewayistoonlyconsidersomeofthesubwordsequencesinY(x)for\nthesakeofefficiency. Forexample,wecansamplek subwordsequencesaccordingtoPr(y)\ntoformthecandidatesetY(x).\nNotethattheSentencePiecemethoddoesnotdependonword-separatedinputsequences.\nWhile the BPE and WordPiece methods can also deal with raw text if updated, the Senten-\ncePiecemethodexplicitlytakesthespaceandotherdelimitersaspartsofthesubwords. See\nFigure3.3forafewtokenizationresultsfortowaflag.\nGiven a learned vocabulary and the corresponding unigram probabilities, we can apply\nthemtodealwithanewtext. Thisisinfactasearchproblem: wefindthemostlikelysubword\nsequenceintermsoftheunigramprobability. Aslanguagemodelingisawell-studiedtopic",
    "in NLP, many search algorithms are directly applicable to the case here. For example, the\nestimateofPr(y)butdirectlypresenttheresult.TheEMalgorithminvolvestwosteps.\n• TheExpectationStep(ortheE-step): GiventhecurrentestimateofPr(y)(say,Prt(y)),wecompute\ntheposteriorPrt(y)foreachyaccordingtoEq.(3.14).Then,wecomputethefractionalcountofeach\nsubwordyinthevocabularyV,likethis\n \n|y|\n(cid:88) (cid:88) (cid:88)\nfcount(y) = Prt(y) δ(y,y i ) (3.19)\nx∈Xy∈Y(x) i=1\n(cid:80)|y|\nwhereδ(y,y i )returns1ify=y i,and0otherwise. i=1 δ(y,y i )countsthenumberoftimesyoccursin\nthesubwordsequencey.\n• TheMaximizationStep(ortheM-step):GiventhefractionalcountsobtainedintheE-step,were-estimate\ntheunigramprobabilitiesbytheequation:\nfcount(y)\nPr t+1 (y) = (cid:80) fcount(y′) (3.20)\ny′∈V\nThetwostepsareiteratedforanumberofroundsuntiltheparametersconvergetosomevalues.",
    "3.2 Vector Representation for Words 137\nsubwordsequence unigramprobabilities([subword]:probability)\nt/ow_/a/_flag [t]:0.030 [ow_]:0.002 [a]:0.041 [_flag]:0.001\nt/ow/_/a_/f/lag [t]:0.030 [ow]:0.005 [_]:0.113 [a_]:0.093 [f]:0.041 [lag]:0.002\nt/ow/_a_/fla/g [t]:0.030 [ow]:0.005 [_a_]:0.084 [fla]:0.003 [g]:0.027\ntow/_/a_/f/lag [tow]:0.001 [_]:0.113 [a_]:0.093 [f]:0.041 [lag]:0.002\nt/ow_/a_/flag [t]:0.030 [ow_]:0.002 [a_]:0.093 [flag]:0.001\nFigure3.3: Differenttokenizationresultsfortowaflag. Everysubwordisassignedaprobability\nthatisestimatedthroughaunigramlanguagemodel. Everywhitespaceisreplacedwith“_”\nforaclearpresentation.\nmethodspresentedinSection3.1.2arestraightforwardlyapplicablehere.\n3.2 Vector Representation for Words\nWordshavemeanings7. Inthebroadestsense,themeaningofawordisthewayinwhichit\ncanbeinterpreted. Thisissomethingbehindthesurfaceformofawordbutcanbeunderstood\nbylanguagespeakers. Forexample,considerthefollowinglinesoftextfromapoem[Knight,\n2018]:\nTherewasalittlesparrow",
    "Whosatonawheelbarrow,\nAndtweetedtoallherfriendsaround.\nAcatwithopenjaws\nAndverypointedclaws,\nSpiedherasheracedalongtheground.\nThese words are not merely strings of English letters and punctuation marks but have\nidentifiablemeaningsthatareknownbyEnglishspeakers. Forexample,“little”meanssmallin\nsize,“sparrow”meansakindofbird,and“friends”meanspeoplewhoyoulikeandtrust. From\nanNLPperspective,awordmeaning(orwordsense)isnotjustwhatthewordexpressesin\none’sbrainbutsomethingcomputer-readableandcomputable.\n7WhilewehavesofardiscussedseverallinguisticelementsusedinNLP,suchassubwords,westillusewords\nasthebasicunitsinourdiscussionhere.Themethodswewillpresentintheremainingpartofthischaptercouldbe\nunderstoodtocoverothertypesoflanguageunitsonemayuseindevelopingNLPsystems,includingcharacters,\nsubwords,andsoon.",
    "138 Chapter 3. Words and Word Vectors\n3.2.1 One-hot Representation\nAnaturalwaytorepresentwordmeaningsistouselanguagetodescribethem. Forexample,\nwecanfindinadictionarytheabovewordswiththeiridsandmeanings. Someofthemare8:\ncat 511 A small animal with fur, four legs, a tail, and claws,\nusuallykeptasapetorforcatchingmice\nher 5220 Used,usuallyastheobjectofaverborpreposition,to\nrefer to a woman, girl, or female animal that has just\nbeenmentionedorisjustabouttobementioned\njaws 6186 Themouth,includingtheteeth\nground 6402 Thesurfaceoftheearth\nsparrow 8331 Acommon,small,gray-brownbird\nwheelbarrow 9954 Alarge,opencontainerformovingthingsinwithawheel\natthefrontandtwohandlesattheback,usedespecially\ninthegarden\nTo represent a word, the simplest idea may be to replace it with the id number in the\ndictionary. Inthisway,eachwordrepresentationisauniquenumber. Anequivalentformto\nthisistheone-hotrepresentation. Itisavectorwhosedimensionalityisequaltothevocabulary",
    "size. In this vector, only the entry corresponding to the word has a value of 1 and all other\nentrieshave0values. Forexample,thewordsparrowcanberepresentedasaone-hotvector\nbasedonitsid(8331),likethis\n[ 0 0 ... 0 1 0 ... 0 0 ]\n↑\nid=8331\n3.2.2 Distributed Representation\nHowever,itappearsthattheone-hotrepresentationonlyprovidesthe“identity”oftheword\nbut not the “description” of what the word is. An obvious problem is that every word is\northogonaltootherwords. Thismakesitdifficultto“compute”therelationshipbetweenwords\nbecausethereisnoconnectionamongtheassociatedwordvectorseventhoughsomeofthe\nwordsarethoughttobesimilarinouruseoflanguage. Here,ourdesireisamodelinwhich\nwordsaredescribedascountableattributesandtheclosenessbetweendifferentwordsiswell\nexplained. Awaytodothisistoenrichtherepresentationwiththeworddescription. Consider\nagainthewordsparrowforexample. Inthedictionary,wehaveitsmeaningacommon,small,\ngray-brownbird. ByusingthetokenizationandnormalizationmethodsmentionedinSection",
    "3.1.1,thistextcanbetransformedintoasequenceofwords\n(cid:104) (cid:105)\na common , small , gray - brown bird\n8Allthesewordsandtheirmeaningsarefoundinhttps://dictionary.cambridge.org/.",
    "3.2 Vector Representation for Words 139\nThen,wevectorizethissequenceusingthebag-of-wordsmodel(seeChapter1),leadingto\nanewvectorofnumbers\n[ 0 0 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 1 ... 0 0 ]\n↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑\n, - a bird brown common gray small\nwherethevalueofanentryis1ifthecorrespondingwordispresent,and0otherwise. Thisway\nenablesthesharingofcontentamongwords. Wewouldsaythattwowordsaresimilarifthey\nhaveoverlapsintheirwordvectors. Consideranewwordcuckoo. Wecanfinditsmeaningin\nadictionary,e.g.,agreybirdwithatwo-notecallthatsoundssimilartoitsname. Itiseasy\nto know that sparrow and cuckoo are two words that share something similar because they\nbothmarkthe“bird”dimensionas1andthevectorsimilaritybetweenthetwowordvectorsis\ngreaterthan09.\nTreatingwordsasvectorsofnumbersoffersageneraltooltorepresentwordsinvarious\ndifferentways. Wedonotevenhavetoexplainawordvectorfromtheviewpointofsemantics.\nForexample,wecanintroduceanewdimensionintothevectortomarkifthewordbelongsto",
    "somesyntacticcategory. Inabroadsense,wecandefineanarbitraryfunctiononeachentry\nofthevectorandviewthefunction’soutputasafeaturedescribingtheword. Forexample,a\nsimpleimprovementtotheaboverepresentationistouseafunctioncountingtheoccurrences\nofawordinsteadofthebinary-valuedfunctionmarkingthepresenceoftheword. Morefeature\nfunctionscanbefoundinSection3.3.\nNotethatitisnotnecessarytoconstrainthefeaturefunctionstoformsthatmakelinguistic\nsensealthoughlinguisticallymotivateddesignsofthefeaturefunctionsareusuallyofinterest\nto NLP researchers. A more general form for word representation is simply a real-valued,\nmulti-dimensionalvector. Itisoftencalledthedistributedrepresentationofaword,orthe\nwordembedding. Forexample,thewordsparrowcanberepresentedasavectorlikethis\n(cid:104) (cid:105)\n1.9 −7 3 −1.2 ... 2.01 −2.05\nInthemachinelearningpointofview,thisvectorcandescribesomeunderlyingattributesofa\nword. Theseattributesmaynotbeexplainableinhumanunderstandingbutcanbelearnedfrom",
    "data. Oneofthechallengesinlearningsucharepresentationisthatonecanhardlymeasurethe\ngoodnessofavector. Ingeneral,itmakesnosensetoaskwhetherthedistributedrepresentation\nof a single word is good or not. Rather, we would like to know if the representations of a\ngroupofwordsarewellbehaved. Forexample,itisacommonbeliefthatsimilarwordsshould\nhavesimilarrepresentations. So,therelationshipbetweenwordsisoftenthoughtofassome\n“distance” between the word representations in a vector space. This leads to a number of\nmethodstovisualizeandevaluatewordrepresentations. InSection3.6,wewillgiveamore\ndetaileddiscussionabouttheseissues.\n9Thesimilarityoftwovectorscanbemeasuredbythecosineoftheanglebetweenthem.",
    "140 Chapter 3. Words and Word Vectors\nOn the other hand, word representations typically do not work alone in NLP systems\nbut are used as some intermediate states of a model. A standard approach in NLP, to learn\ndistributedrepresentationsofwords,istotakeitasaby-productoftraininga“big”system.\nThatis,therepresentationmodelworksasacomponentofasystem,andisoptimizedtogether\nwith other components when the system is trained in some way. This inspires a promising\nparadigm of representation learning: the representation model is learned as a sub-model in\nan easy-to-train system, and can be used as a plug-in for a completely different system. In\nneural language modeling, for instance, we can force the model to map each input one-hot\nwordvectorintoareal-valued,low-dimensionaldistributedrepresentation. Thesedistributed\nrepresentationsarefedintoaneuralnetworkthatpredictsaprobabilitydistributionoverthe\nvocabulary. Themappingfunctionorembeddingfunctionistrainedsoastominimizetheloss",
    "ofthelanguagemodelonsomedata(seeSection3.4). Whenapplyingthelearnedembedding\nfunction,wedropallotherpartsofthelanguagemodelandusethefunctiontogeneratethe\ndistributed representation for each word in downstream tasks. An alternative strategy is to\nspecifically tailor the model to the word representation learning problem. Systems of this\nkindaretypicallynotdesignedtodealwithstandardNLPproblems,butwithanemphasison\nspecificproblemsinwordrepresentationlearning,suchasexplicitlymodelingtherelationship\nbetweenwords(seeSection3.5).\n3.2.3 Compositionality and Contextuality\nWhilewerestrictourdiscussiontowordrepresentationlearninginNLP,studyingthemeanings\nofwordsisatraditionalsub-fieldoflinguistics. Inlexicalsemantics,forinstance,researchers\nareconcernedwithhowwordmeaningsaredefinedandused,andhowthesemeaningsform\nthesentencemeanings. Infact,thetaskoflearningtorepresentwordsdoesnotconcernitself\nwiththeissueofsemanticsinlinguistics. Instead,itprovidesmachinelearningapproaches",
    "totransforminglinguisticunitsintocomputer-friendlyforms. However,thesemanticsissue\niscriticalwhenoneunderstandsandusesalanguage. Itisthereforestillworthconsidering\nsemanticsandrelatedproblemsinthedesignofwordrepresentationmodels. Forexample,\n• Compositionality. Compositionality is a common concept in semantics, logic and\nrelatedfields. Itoftencomesoutwiththeprincipleofcompositionality:\nThe meaning of a complex expression is determined by its structure\nandthemeaningsofitsconstituents.\n–Szabó[2020]\nThis offers a useful tool to describe how the meaning of a big thing is built up from\nthemeaningsofitsparts. Theprincipleofcompositionalityisfundamentalandexists\neverywhereinthelanguageworld. Forexample,whenyouseethephrasewhitecat,itis\neasytoknowitsmeaningintermsofthemeaningsoftheconstituentwordswhiteandcat.\nAnotherexampleatahigherleveloflanguageuseiscompoundsentences. Acompound",
    "3.2 Vector Representation for Words 141\nsentence forms its meaning by simply connecting multiple independent clauses with\nconjunctions. Notethattheprincipleofcompositionalityisnotasimplerulebywhichwe\nusetodescribehowabigitemismadeupofsmallerones,althoughresearchershavetried\ntodefineitformally[Montague,1974]. Thereareevendisagreementsanddebateson\nhowthisprincipleisinterpretedandhowitisadequatelymodeledbysemanticaltheories.\nStill, if we focus on NLP problems and set aside the theoretical part of linguistics,\ncompositionalityisaveryusefulpropertythatonecanmakeuseofinsystemdesignand\nevaluation. Sometimes,ifonefindsthataproblemiscompositional,itimpliesthatthere\naremanygoodmethodstoaddressitbecauseacomplexthingcanbedividedintosmaller\nand easier things. For word representation learning, we may wish that the resulting\nwordrepresentationsexhibitsomecompositionality,inresponsetothecompositional\nnatureoflanguage. InSection3.6,wewillseeafewexamples,e.g.,therepresentations",
    "learnedbyneuralnetworksshowmeaningfulresultsunderlinearalgebraicoperations,\nthoughthemodelsarethemselvesnon-linear. However,ontheotherhand,theprinciple\nof compositionality is not the principle of everything. There are many situations in\nwhichcompositionalityisnotheld,suchascollocationsandidioms. Inthiscase,natural\nlanguagesarenon-compositional. ThisexplainswhytheNLPproblemissochallenging.\n• Contextuality. Contextualityissomesortofnon-compositionality. Itstatesthataword\nmayhavemultiplepossiblemeaningsandthe“true”meaningisdeterminedbylooking\natthecontextprecedingand/orfollowingthisword. Forexample,considerthefollowing\nsentences10\nTheysatroundthedinnertable,arguingaboutpolitics.\nCometothetableeverybody-supper’sready.\nHecameinwithfourshoppingbagsanddumpedthemonthetable.\nThetablecanhelpyouevaluatethepotentialrisksofinvestingintheFund.\nBuildingsocietiesdominatethebest-valuetablesformortgages.\nThistablerepresentsexportsales.\nIntheseexamplesentences,tableisapolysemywithtwomeanings:",
    "Sense1: aflatsurfaceusedforputtingthingson.\nSense2: anarrangementofitemsinrows,orcolumns,orblocks.\nInotherwords,tableisanambiguousword. Thisambiguitywouldbeeliminatedifwe\nconsiderthesurroundingwords. Forexample,whentablefollowsdinner,itiseasyto\nfigureoutthatitreferstosense1. Theambiguityalsoexistswhenawordstemsfroma\nfewdifferentformsorlexemes(callitahomonymy). Forexample,bearcanbeeithera\n10All these sentences are from https://dictionary.cambridge.org/dictionary/english/\ntable",
    "142 Chapter 3. Words and Word Vectors\nverboranoun. Disambiguatingawordforagivensetofwordsenseshasbeenstudied\nfordecadesinNLPandiscommonlyknownaswordsensedisambiguation(WSD)\n[Kelly and Stone, 1975]. However, the word representation problem discussed here\nismorechallengingbecauseweusuallydonothaveapre-definedsetofwordsenses\ninhand. Weinsteadwantacontextualrepresentationmodelthatcangenerateaword\nrepresentationdependentonitscontext. Thus,itisimportanttotaketheideathatthe\nmeaningofawordmaynotbeconstant. Thismakestheproblemsomewhatdifferent\nfromwhatwediscussedatthebeginningofthesection,aswenolongerhavealookup\ntableforwordrepresentations,butamodelthatproducesdifferentrepresentationsofa\nwordindifferentcontexts.\nIntheremainingsectionsofthischapter,wefocusonlearningvectorrepresentationsof\nwords from their distributions in language use. We leave the discussion on the contextual\nmodelsforlearningdensewordrepresentationstoChapters4-6.\n3.3 Count-based Models",
    "We have framed the induction of word meanings as a problem of learning word vectors.\nIn this section, we proceed by assuming that the meaning of a word is determined by the\nenvironmentwherethewordisused. Thisisusuallystatedasthedistributionalhypothesis—\nwordsaresemanticallysimilariftheyappearinsimilarcontexts[Harris,1954;Firth,1957].\nAwordrepresentationlearnedunderthishypothesisisalsocalledthedistributionalword\nrepresentation or distributional representation11. To ease the reading, however, we will\nstill use the terms word vector and word representation throughout this book. Next, we\nintroduceseveralmethodsformodelingthedistributionofwordsintexts,andthenoffersome\nrefinements.\n3.3.1 Co-occurrence Matrices\nIndistributionalsemantics,wordsarerepresentedwithsemanticmodelsthatconsidervarious\naspects of the context. These models differ in how the context of a word is modeled, for\nexample,howlargethecontextisconsidered,howeachoccurrenceofawordiscounted,how",
    "thedimensionalityofadistributionisdefined,andsoon. Inthissectionweassume,asinmost\nmodelsusedinNLP,thatwordrepresentationsarelearnedfromacollectionofdocuments.\nA way to view a document is as a very simple way of decomposing it into a set of\nunordered words. Then we can think of each occurrence of these words as an independent\ncontextindicator. Inthisway,thedistributionofawordinitscontextcanbedescribedasthe\nnumber of times the word co-occurs with the context words. We can do this by building a\n11Itshouldbenotedthatdistributionalrepresentationanddistributedrepresentationaretwodifferentconcepts.\nAdistributionalrepresentationreferstoarepresentationthatdescribesthedistributionoflanguageitemsinlanguage\nuse. Arelatedtermisnon-distributionalrepresentationwhichmeanssomethingthatisobtainedfromlexical\ndatabases,suchastheinterpretationofawordinadictionary.Ontheotherhand,adistributedrepresentationrefers",
    "toavectorofvariablescorrespondingtosomeunderlyingattributesofalanguageitem.Incontrasttodistributed\nrepresentation,aone-hotrepresentationjustdescribesthewordsymbol.",
    "3.3 Count-based Models 143\nco-occurrencematrixwhereacellcountsthenumberofco-occurrencesofarowitemanda\ncolumnitem. Consider,forexample,thefollowingdocuments12:\nDoc1 Aberryisasmall,pulpy,andoftenediblefruit.\nDoc2 In botanical terminology, a berry is a simple fruit with seeds and pulp\nproducedfromtheovaryofasingleflower.\nDoc3 The term \"banana\" is also used as the common name for the plants that\nproducethefruit.\nDoc4 Bananaseedsarelargeandhardandspikyandliabletocrackteeth.\nDoc5 Abananaisanelongated,ediblefruit-botanicallyaberry-producedby\nseveralkindsoflargeherbaceousfloweringplantsinthegenusMusa.\nFor each pair of words, we collect the total number of times they co-occur in these\ndocuments,leadingtoamatrix,calledtheword-wordco-occurrencematrixorterm-term\nco-occurrencematrix. Hereisasubsetofthematrixfortheabovedocuments.\nflowering fruit herbaceous ... often plants seeds\nberry 1 3 1 ... 1 1 1\nterminology 0 1 0 ... 0 0 1\ncommon 0 1 0 ... 0 1 0\nteeth 0 0 0 ... 0 0 1\nbanana 1 2 1 ... 0 2 1",
    "simple 0 1 0 ... 0 0 1\nand 0 2 0 ... 1 0 2\nInthematrix,eachrowwordisassociatedwithawordvectorof|V|entries. Thenumbers\nintheentriesdescribehowoftentherowwordco-occurswithdifferentcontextwords,that\nis,howagivenwordisdistributedindifferent“contexts”. Inageometricsense,iftwowords\nhavesimilardistributionsinco-occurringwiththesamegroupofcontextwords,thentheangle\nbetweenthewordvectorswouldbesmall13. Forexample,ifwethinkofthesewordsasvectors\ninavectorspace,berryisclosertobananathanteeth(seeFigure3.4). Thisgeometricintuition\nisthebasisofmanyrepresentationmodels. MoreexampleswillbegiveninChapters4and5.\nAproblemwiththismethodisthatthedistancebetweenwordsisnottakenintoaccount\nalthoughthecorrelationisnotthatstrongwhenthecontextwordisdistant. Asimplesolution\nis to constrain context words in a window, called the context window or window for short\n[LundandBurgess,1996]. Forexample,foreachwordinadocument,weonlycountthe-2\nand+2wordssurroundingit(i.e.,awindowofsize5).\n12ThetextsarefromWikipedia.",
    "13Theanglebetweentwovectorsdoesnothingwiththelengthsofthevectors.Ifthevectorsarenormalizedin\nsomeway(e.g.,byvectornorm),similarvectorsmeanthatmostentriesofthetwovectorshavesimilarvalues.",
    "144 Chapter 3. Words and Word Vectors\nz\nteeth\ndistance2\nberry\ndistance1\nbanana\ny\nx\nFigure3.4: Wordvectorsinavectorspacethatisbuiltfromthewordco-occurrencestatistics\nontheEnglishdatafromWMT2012. Allthevectorsarenormalizedandrepresentedasarrows.\nForvisualization,weprojectthesevectorsfromahigh-dimensionalspacetoa3-dimensional\nspaceviaprincipalcomponentanalysis. Asexpected,berryisclosertobananathantoteeth.\nNotethatthewordvectorslearnedbythebag-of-wordsmodelinSection3.2isaspecial\ninstance of the co-occurrence matrix. In that example, we only have one document from\nwhich we collect context words. For each entry of a word vector, an indicator function is\nused to mark the presence of the context word. In addition to the indicator and counting\nfunctions,thereareotherchoicesforcomputingwordvectorsbyexaminingtheco-occurrence\nofwords. Inpractice,thevalueofanentryofawordvectorcanbethoughtofasthedegree\nofthecorrespondencebetweenwords. Iftwowordsarecorrelatedwitheachotherinsome",
    "context,afeaturefunctionmayassignascorebetweentheminanymanner. Thisscoredoesnot\nnecessarilyhavetobeacount,butcanbeanarbitraryrealnumber. Assuch,theproblemcan\nbestatedasmeasuringtheassociationstrengthbetweenwords. Itiscommonpracticetodefine\nsuchameasureonthebasisofcorrelationmodels. Instatistics,correlationdescribestowhat\nextenttwovariablesareassociated,measuredbycorrelationcoefficients. Commoncorrelation\ncoefficients include the Pearson correlation coefficient (Pearson’s r), the Spearman’s rank\ncorrelation coefficient (Spearman’s ρ), and so on14. In NLP, a widely used measure is the\npointwisemutualinformation(PMI)[ChurchandHanks,1990]. Letaandbbetwowords.\nThemathematicalformofPMIisgivenby\nPr(a,b)\nPMI(a,b) = (3.21)\nPr(a)Pr(b)\n14Some of the correlation coefficients assume certain distributions of the data. For example, the Pearson\ncorrelationcoefficientiscalculatedbasedontwovariablesfollowingnormaldistributions.",
    "3.3 Count-based Models 145\nwhere Pr(a,b) is the joint probability of a and b co-occurring, and Pr(a) (or Pr(b)) is the\nprobabilityofa(orb)occurring. Theseprobabilitiescanbesimplyestimatedonthetextsbythe\nrelativefrequencymethod15. Givenawordaandavocabularyofcontextwords{b ,...,b },\n1 |V|\nthePMI-basedwordvectorofaiswrittenas\n(cid:104) (cid:105)\ne(a) = PMI(a,b ) ... PMI(a,b ) (3.22)\n1 |V|\nCorrelationcoefficientsaregenerallyusedtotestwhethertwovariablesare(linearly)related.\nSo,analternativemethodistodefineanentryofthewordvectorastheoutcomeofatest. For\nexample,theentry(a,b)choosesavalueof1,ifthecorrelationcoefficientbetweenwordsa\nandbislargerthanathreshold,orthecorrelationofwordsaandbissufficientlysupportedby\nhypothesistesting.\nHowever,modelingwordsasvectorsofcorrelationscoresbetweenwordssomewhatlimits\nthe scope of contextual information one may use. Another idea for word vectorization is\nto consider each document as a whole and establish the relationship between words and",
    "documents. We can do this by using the word-document co-occurrence matrix or term-\ndocumentco-occurrencematrix. Forexample,fortheabovementioneddocuments,wecan\nbuildamatrix,likethis\nDoc1 Doc2 Doc3 Doc4 Doc5\nberry 1 1 0 0 1\nterminology 0 1 0 0 0\ncommon 0 0 1 0 0\nteeth 0 0 0 1 0\nbanana 0 0 1 1 1\nsimple 0 1 0 0 0\nand 1 1 0 2 0\nIn the matrix, the value of entry (a,d) is defined to be the number of times the word a\noccurs in the document d, giving the strength of the relationship between a and d. This is\ncommonlycalledthetermfrequency(TF)ofaind(denotedbytf(a,d)). Also,wecanuse\na0-1indicatorfunctiontomarkthepresenceofthewordoccurrence(seeSection3.2). See\nTable3.1forafewvariationsoftheTFweightingfunction.\nAsaco-occurrencematrix,eachrowoftheabovematrixisthevectorrepresentationof\nthe row word. In addition, each column is a vector representation of a document. Recall\nthebag-of-wordsmodelusedinthetextclassificationproblemmentionedinChapter1. The",
    "word-documentco-occurrencematrixisbasicallythesamethingasthebag-of-wordsmodel\n15AproblemwithPMIisthatthemeasurebecomesunstablewhenthewordsarerare.Forexample,ifavery\nrarewordhappenstoappearinadocument,thePMIvalueofthiswordandanyotherwordinthisdocumentwould\nbeunreasonablylarge.",
    "146 Chapter 3. Words and Word Vectors\nEntry Mathematicalform\n(cid:40)\n1 aoccursind\nBinary tf(a,d)=\n0 otherwise\nCount tf(a,d)=count(a;d)\nExponentialCount tf(a,d)=count(a;d)α\nLog-scaleCount tf(a,d)=log(1+count(a;d))\ncount(a;d)\nNormalizedCount(orFrequency) tf(a,d)=\n(cid:80) a′count(a′;d)\nTable3.1: Functionsoftheterm-frequencyweightingscheme. count(a;d)countstheoccur-\nrencesofthewordainthedocumentd.\nwheretheorderingofwordsisignoredbutthewordcountsmatter. Hereweperformdocument\nvectorizationviathismodelonacollectionofdocuments.\n3.3.2 TF-IDF\nThemodelingofword-documentassociationsisknowntobeimportantformanyNLPtasks.\nAnimprovementonusingword-documentrelationshipstobuildwordvectorsanddocument\nvectorssimultaneouslyisthetermfrequency-inversedocumentfrequency(TF-IDF)method.\nGiven a set of documents D, the TF-IDF weighting scheme assigns a score to each word-\ndocumentpair(a,d)bytheequation\ntfidf(a,d,D) = tf(a,d)·idf(a,D) (3.23)\nwhere",
    "• tf(a,d)isthetermfrequency(seeTable3.1). Whentf(a,d)islarge,thewordaisagood\nindicator for thedocument d. In contrast, when tf(a,d) is small, the word-document\nassociationisnotthatstrong.\n• idf(a,D)istheinversedocumentfrequency(IDF).Itisdevelopedbasedonthefact\nthatcommonwordsacrossdocumentsarelessinformative. Forexample,foracollection\nof documents on sports, it is likely to see player and players in most documents. In\nthiscase,thewordsplayer andplayersarelessinterestingindiscriminatingdifferent\ndocumentsorcontexts. Letdf(a,D)bethenumberofdocumentsinD containingthe\nworda. Acommonformofidf(a,D)isgivenby\n|D|\nidf(a,D) = log (3.24)\ndf(a,D)\nEq. (3.25)wouldpenalizeawordifitmoreoftenappearsinthecollectionofdocuments.",
    "3.3 Count-based Models 147\nSimilarly,wecanhaveasmoothedversionofidf(a,D),likethis\n|D|\nidf(a,D) = log +1 (3.25)\ndf(a,D)+1\nHavingtheTF-IDFfeaturefunctioninhand,wecanbuildaword-documentco-occurrence\nmatrixforagivencollectionofdocuments,thatis,thevalueoftheentry(a,d)ofthematrix\nis tfidf(a,d,D). Then, as described in the last subsection, we can treat a row of the matrix\nas the vector representation of the row word. Note that, traditionally, the TF-IDF method\nandword-documentco-occurrencematricesareoftenusedindocumentrepresentation. For\nexample,onecanrepresentaqueryandanumberofdocumentsastheTF-IDF(column)vectors\ninaninformationretrievalsystem. Thisallowsustolookathowmuchthequerymatcheseach\nof these documents viavector similarity. However, the vector space models in information\nretrievalarebeyondthescopeofthischapter,butthereadercanrefertorelatedtextbooksfor\ngreatercoverageofthistopic[Manningetal.,2008;Buttcheretal.,2016].\n3.3.3 Low-Dimensional Models",
    "Co-occurrence matrices are often high dimensional. Suppose, for example, that there is a\nvocabulary of 20,000 unique words and a collection of 10,000,000 documents. Then, a\nword-documentco-occurrencematrixhas20,000×10,000,000=2×1011 entries. However,\nif we consider the computational burden of such a model, it would be hard to imagine that\na word is represented as a 10,000,000-dimensional vector and a document is represented\nas a 20,000-dimensional vector. Instead, we expect that the representation of a word (or a\ndocument)requiresonlyareasonablysmallnumberoffeatures. Inthissubsection,wediscuss\nsome standard approaches to transforming words (or documents) into lower-dimensional\nrepresentationsfromtheco-occurrencematrices. Mostoftheseapproacheshavebeenwell\nstudiedintheliteratureandhavebeensuccessfullyappliedinseveraldisciplines[Barber,2012;\nWrightandMa,2022]. Sowedonotdiveintothemathematicaldetailsbehindthem,butshow\nhowtoapplytheminthecontextoflearningword(ordocument)vectors.",
    "1. Latent Semantic Analysis\nInNLP,latentsemanticanalysis(LSA)isamethodofseekingthelatentsemanticstructure\nbehindtheword-documentassociations[Deerwesteretal.,1990;Landaueretal.,1998]16. It\nassumesthateitherwordsordocumentscanberepresentedaslow-dimensionalvectorsthatare\ndistilledfromtheco-occurrencematrix,preservingthepropertyoftheoriginalvectorspace\nmodel,e.g.,theanglebetweenvectorsissmallforsimilarwords.\nMorespecifically,LSAfactorizestheco-occurrencematrixintoamatrixforwordrepre-\nsentation,amatrixfordocumentrepresentation,andathirdmatrixconnectingthefirsttwo\nmatrices. Mathematically, this can be framed as a singular value decomposition (SVD)\nprocess[Stewart,1993]. LetM∈R|V|×|D| beaco-occurrencematrixoveravocabularyV\n16Latentsemanticanalysisisalsocalledlatentsemanticindexing(LSI).Thistermismoreoftenusedin\ninformationretrievalandrelatedfields.",
    "148 Chapter 3. Words and Word Vectors\nandadocumentsetD. TheSVDproducesafactorizationofM,likethis\nM = PΣQT (3.26)\nwhereP∈R|V|×r,Σ∈Rr×r andQT∈Rr×|D|. Inthisfactorization,therepresentationmodel\nisisolatedintotwotermsPandQTsothatbothofthemaresemi-unitary(orsemi-orthogonal\nin our case)17, that is, the columns of either P or Q are orthogonal vectors. Thus, these\ncolumnsformanorthogonalbasisofRr,wherer istherankofM. Thismeansthatweusea\n“minimum”numberofdimensionsofdatatorepresentM. Σisadiagonalmatrix:\n \nσ 0 ... 0\n1\n0 σ ... 0\n 2 \nΣ =   . . . . . . ... . . .   (3.27)\n \n0 0 ... σ\nr\nThediagonalentries{σ ,...,σ }areallnon-negativerealnumbers,andarecalledthesingular\n1 r\nvaluesofM. Typically,{σ ,...,σ }arearrangedindescendingorder(i.e.,σ ≥σ ≥...≥σ ).\n1 r 1 2 r\nThus,SVDisuniqueforthegivenmatrixM. IfwewritePasasequenceofcolumnvectors\n(callthemleft-singularvectors)\n(cid:104) (cid:105)\nP = p ,...,p (3.28)\n1 r\nandQT asasequenceofrowvectors(callthemright-singularvectors)\n qT\n1\n.",
    "QT =   . .   (3.29)\nqT\nr\nthenwecanwriteMas\nr\n(cid:88)\nM = σ p qT (3.30)\ni i i\ni\nForrepresentingwords,wecanthinkofp asthevaluesofafeaturefunctionoverallthe\nl\nentriesofthevocabularyV. Then,wedescribeaworda asanr-dimensionalfeaturevector\ni\ne inwhichthel-thfeatureisthei-thentryofp . Inotherwords,thevectorrepresentationof\ni l\na is\ni\n(cid:104) (cid:105)\ne = p (i) ... p (i) (3.31)\ni 1 r\n17Anon-squarematrixXissemi-orthogonalifandonlyifXXT=IorXTX=I.",
    "3.3 Count-based Models 149\nSimilarly,thevectorrepresentationofadocumentd canbewrittenas\nj\n(cid:104) (cid:105)\nh = q (j) ... q (j) (3.32)\nj 1 r\nInthisway,wehavetwoseparaterepresentationmodelsforwordsanddocuments: Pdeals\nwithwordrepresentationsandQdealswithdocumentrepresentations. Thus,wecantakeM\nasaproductoftheserepresentationmodels,likethis\nM = PΣQT\n= sdrow\n  \ne σ ... 0\n1 1\ndocuments   . . .     . . . ... . . .   (cid:104) hT ... hT (cid:105) (3.33)\n1 |D|\ne 0 ... σ\n|V| r\nIn practice, the rank r is usually much smaller than |V| and |D|. Thus, we have, for\neachword(oreachdocument),anewrepresentationwhosedimensionalityismuchsmaller\nthan the representation contained in the co-occurrence matrix. A further improvement can\nmakeuseofther∗ largestsingularvalues(i.e.,{σ ,...,σ })andthrowawaytherest. Asa\n1 r∗\nconsequence,weonlykeepthefirstr∗ left-singularvectorsandright-singularvectorsinPand\nQrespectively. Herer∗<r isahyperparameterspecifyingthenumberofvectorsinPandQ,",
    "i.e.,thenumberoffeaturesusedtodescribeawordoradocument. Inthisway,wehaveanew\nfactorizationofMas\nr∗\n(cid:88)\nM ≈ σ p qT (3.34)\ni i i\ni\nThe right hand side of Eq. (3.34) is also known as a low-rank approximation of M. By\nspecifyingr∗,itcanapproximateMwithamatrixhavinganarbitraryrank<r.\nThereareanumberofalgorithmsforimplementingtheSVD[ClineandDhillon,2014]. In\nfact,mostofthemodernimplementationsoftheSVDareefficientandscalable. Onecanuse\nthemasoff-the-shelftoolkitsinNLPapplications.\n2. Principal Component Analysis\nIndataanalysis,principalcomponentanalysis(PCA)isawidely-usedtechniquefordimen-\nsionreduction. Givenasetofdatapoints,PCAfindsasequenceoforthogonaldirectionsin\nthecoordinatespacesothatthevarianceofthedatapointsalongthesedirectionsismaximized.\nThesedirectionsaretypicallyrepresentedasunitvectors,calledprincipalcomponentload-\ningsorprincipalcomponentcoefficients. Asaresult,theyformanewcoordinatespaceto\nwhichwecanmapthegivendatapointsbyanorthogonallineartransformation.",
    "Consider a word-document co-occurrence matrix M∈R|V|×|D|, where each row is a\n|D|-dimensionalwordvectororfeaturevector. ThePCAdefinesalinearmappingfromR|D|\ntoRp,thatis,wetransformeach|D|-dimensionalwordvectortoap-dimensionalwordvector.",
    "150 Chapter 3. Words and Word Vectors\nThisisgivenby\nN = MC (3.35)\nwhereN∈R|V|×p isthemappedwordvectorsoverthevocabularyV,andC∈R|D|×p isthe\nmatrixofthelinearmapping. Then,wecanwriteCasasequenceofcolumnvectors\n(cid:104) (cid:105)\nC = c ... c (3.36)\n1 p\n \nc (1)\ni\n.\nEachcolumnvectorc i =  . .   isagroupofprincipalcomponentcoefficients,indi-\nc (|D|)\ni\ncatingalinearfunctionthatcombinestheinputfeaturesintoanewfeature. Forexample,ifwe\nviewMasthevaluesofabunchoffeaturefunctions(say,columnvectors{m ,...,m }),\n1 |D|\nwecanmapMtoanewfeaturespaceintermsofc :\ni\n \nc (1)\ni\n(cid:104) (cid:105) .\nMc i = m 1 ... m |D|   . .  \nc (|D|)\ni\n|D|\n(cid:88)\n= c (k)m (3.37)\ni k\nk=1\nMc (i.e.,thei-thcolumnofN)isacolumnvectorwhereeachentryisthenewfeature\ni\nforawordinV. InPCA,wegenerate{c ,...,c }insequencesuchthattheymaximizethe\n1 p\nvarianceofthelinearmappinginEq. (3.37). Thus,foreachi∈[1,p],theoptimalprincipal\ncomponentcoefficientsaredefinedtobe\ncˆ = argmaxVar(Mc )\ni i\nci\n= argmaxcTSc (3.38)\ni i\nci",
    "where Var(Mc ) is the variance of Mc , and S is the covariance matrix of M. For a well-\ni i\ndefinedsolutiontoEq. (3.38),itiscommontoimposeanadditionalconstraintthatc isaunit\ni\nvector,i.e.,cTc =1. Then,theproblemcanbeframedas\ni i\ncˆ = argmaxcTSc −λ (cTc −1) (3.39)\ni i i i i i\nci\nwhereλ istheLagrangemultiplier. SolvingEq. (3.39)undersuchaconstraintrequirescˆ\ni i\nto be an eigenvector of S and λ to be the corresponding eigenvalue [Jolliffe, 2002]. Since\ni\nSisap×psymmetricmatrix, ithasexactlypeigenvectorsandeigenvalues. Then, wecan\nordertheseeigenvectorsbytheassociatedeigenvalues,andtaketheorderedeigenvectorsas\n{cˆ ,...,cˆ }. Inotherwords,cˆ istheeigenvectorofSwiththelargesteigenvalue,cˆ isthe\n1 p 1 2",
    "3.3 Count-based Models 151\n14\nDirection1\n12\n10\n8\nDirection2\n6\nProj.\n4\n2\n0\n0 2 4 6 8 10 12\nFigure 3.5: Transforming 2-dimensional data to 1-dimensional data via PCA. There are a\nnumber of data points (represented by black circles) on a Euclidean plane. By using PCA,\nwe find a direction (represented by an arrow) such that the variance of the projected data\n(represented by colored circles) in this direction is maximized. Such a direction can be\nrepresented by a unit vector, called principal component coefficients. In this example, the\nprincipalcomponentcoefficientsdescribea1-dimensionalcoordinatespace. Wecanmapthe\ndatafromthe2-dimensionalcoordinatespacetothe1-dimensionalcoordinatespacevialinear\ntransformation. Themappeddataiscalledtheprincipalcomponentoftheoriginaldatapoints.\neigenvectorofSwiththesecondlargesteigenvalue,andsoon. Typically,Mcˆ iscalledthe\ni\ni-thprincipalcomponentofM.\nAnintuitivewaytothinkaboutPCAistomapdatapointsinaEuclideanspacefromone",
    "coordinatesystemtoanother. ForadatasetM,wecanvieweachrowinMasthecoordinates\nof a data point in a |V|-dimensional coordinate system A. In PCA, we want to represent\nthesedatapointsinanewp-dimensionalcoordinatesystemB. Thei-thdimensionofthenew\ncoordinatesystemissimplyadirectionrepresentedbyaunitvectorc . Forthei-thcoordinate\ni\nof each data point in B, we project the data point in A onto the c line. The optimal c is\ni i\nchosenintermsofhowtheseprojecteddatapointsarespreadalongc . Inotherwords,weseek\ni\nalinealongwhichwecanbestseparatethedatapoints. Inthisway,wegenerateasequenceof\nprincipalcomponentcoefficients, successivelysolvingEq. (3.38). Weillustratetheideaof\nPCAusinganexampleprojecting2-dimensionaldatato1-dimensionaldatainFigure3.5.\nIn real-world applications, p is commonly set to a number much smaller than |D|, and\nPCAcansignificantlyreducethenumberofdimensionsusedinrepresentingwords. Notethat\nPCAisaverygeneralmethodandisfoundtobeusefulinmanydisciplines. Inpractice,M",
    "canbeextendedtorepresentobservationsonasetofvariables. ByapplyingPCA,onecan\ntransformtheseobservationsintodatavaluesoffewernewvariables.",
    "152 Chapter 3. Words and Word Vectors\n3. Others\nInmachinelearning,learninglow-dimensionalmodelsisafundamentalproblem,andhasbeen\ngeneralizedinseveraldirections. Forexample,theneuralwordembeddingmodelsdescribed\ninSections3.4and3.5themselvestendtolearnlow-dimensional,real-valuedwordvectors\nfromtexts. Herewepresentsomeofthedimensionreductionmethodsonemaycomeacrossin\ntheNLPandmachinelearningliterature.\n• Topicmodels. Technically,topicmodelsarenotwaysofdimensionreduction,buttools\nfordescribinghowdocumentsandwordsaregeneratedbasedondistributionsovertopics\n[Blei,2012]. Forexample,latentDirichletallocation(LDA)modelsthegenerationof\nadocumentbyusingdocument-topicandtopic-worddistributions[Bleietal.,2003]. As\naby-product,weobtainadistributionoverwordsforeachtopic,indicatinghowlikelya\nwordoccursgivenatopic. Ifwewriteallthesetopic-worddistributionsasamatrix,say\na|V|×K matrixwhere|V|isthenumberofwordsandK isthenumberoftopics,then",
    "wewillhavesomesortofwordrepresentationsthatareverysimilartothosedescribedin\nprevioussections. K iscommonlysettoa“small”number(e.g.,200). Inthiscase,we\nhavealow-dimensionalmodelforrepresentingwords. AlthoughLDAisnotsopopular\ninlearningwordrepresentationsinNLPapplications,itoffersawaytorepresentwords\nasdistributionsoverlatentthematicstructures.\n• Auto-encoders. Undercompleteauto-encodersareatypeofneuralmodelthatencodes\nfeaturesintolow-dimensionalcodessuchthattheinputfeaturescanbereconstructed\nfromthecodes. Anadvantageofauto-encodersisthattheydonotmakeassumptions\non the hidden structures of the features. Thus, auto-encoders can be used to learn to\ntransformanytypeofdataintolow-dimensionalrepresentations. Forexample,inChapter\n7wewillseeexamplesofapplyingauto-encoderstolearnsentencerepresentations. For\nmoredetailsaboutauto-encodersthereadercanrefertoChapter2.\n• Superviseddimensionreduction. Traditionally,dimensionreductionmethods(suchas",
    "PCA)areassumedtoworkinanunsupervisedmanner. Whenthebenchmarkdataofthe\ntargettaskisaccessible,itisnaturaltomakeuseofthisinformation. Acommonexample\nissuperviseddimensionreductionforclassification. Forexample,intheFisher’slinear\ndiscriminant and linear discriminant analysis methods, we find a mapping from\nhigh-dimensionaldatatosingle-dimensionaldatasothattheseparationoftheclasses\nassociatedwiththedataismaximized. Thisideacanbegeneralizedtomulti-dimensional\ndataintheCanonicalVariatesmethod[Barber,2012].\n• Feature selection. Feature selection refers to a process of selecting a subset of the\nfeatures used in representing an object and thus reducing the dimensionality of the\nrepresentation. Feature selection is a wide-ranging topic in machine learning, and\nmanymethodscanbeseenasinstancesoffeatureselection[GuyonandElisseeff,2003;\nLiu and Motoda, 2012]. The simplest is to frame it as a search problem: we search",
    "in the space of feature subsets so that the selected features maximize (or minimize)\nsomeobjective. Ingeneral,thedesignoftheobjectivedependsonthetaskwherewe\napply the features. This makes feature selection somewhat difficult because one has",
    "3.4 Inducing Word Embeddings from NLMs 153\nto consider many factors in such a process, such as the performance measure of the\ntarget task, the search efficiency, and the representation of each feature subset. Note\nthatfeatureselectionisgenerallydiscussedinsupervisedlearningthatrequireslabeled\ndatatocomputelossforoptimization. ThereaderisreferredtoSolorio-Fernándezetal.\n[2020]’sreviewpaperforunsupervisedfeatureselectionmethods.\nIn statistics, many methods can fall under the dimension reduction framework and are\nrelatedtowhatwediscussedinthissection. Forexample,factoranalysisisamethodsimilar\ntoPCAbecausetheybothseekalinearmappingfromtheinputvariablestoasmallernumber\nofnewvariables. Thedifferencebetweenthemisthatfactoranalysisfocusesonmodelingthe\ncommonvarianceofvariables,whilePCAfocusesonmaximizingthevarianceoftheprojected\ndata. Anotherexampleisindependentcomponentanalysis(ICA).UnlikePCA,thegoalof\nICAistofindindependentcomponentsthatareadditivelyseparable. Moreexamplescanbe",
    "foundinmachinelearningandstatisticstextbooks[McClaveandSincich,2006;Freedman\netal.,2007;Barber,2012].\n3.4 Inducing Word Embeddings from NLMs\nCountingword-wordorword-documentoccurrencesisasimplewaytorepresentwordsby\nusingtheirdistributionsintexts. Whilethismethodiseffectiveinmanyapplications,itimposes\naconstraintonwordrepresentations: theentriesofawordvectorshouldbeabletobeexplained\nassome“evidence”onhowtheworddistributesindifferentcontexts. Ideally,wewouldliketo\nrepresentwordsinamoregeneralform,say,areal-valuedvector(callitthewordembedding)\nwithoutconstraintsorassumptionsonhowthemeaningofeachentryofthevectorisdefined.\nLearningwordvectorswithnoconstraintscomesatacost. Unlikethecount-basedmethods\npresentedinSection3.3,wedonotuseheuristicsorpriorknowledgetoestimatethevalueof\na word vector but wish to induce meaningful word representations directly from data. One\nofthedifficultieshereisthatthereisnogoldstandardtoguidethelearningprocessbecause",
    "it is simply impossible to manually annotate a real-valued word vector. Thus, we are often\ninterested in treating the learning of word vectors as a part of a well-defined task (call it a\nbackground task). The learned word vectors are then a by-product of the learning on the\nbackgroundtask.\nAcommonexampleistheinductionofwordvectorsfromneurallanguagemodels(NLMs).\nRecall the NLM described in Chapter 2. Its goal is to build a neural network that predicts\nthe probability of a word given its preceding words [Bengio et al., 2003a]. More formally,\nletw bethewordwewanttopredict,and{w ,...,w }bethecontextwordswehave\ni i−n+1 i−1\nseen. First, the words {w ,...,w } are transformed to d -dimensional word vectors\ni−n+1 i−1 e\n{e ,...,e }throughanembeddinglayer. Assumingw istheone-hotrepresentationof\ni−n+1 i−1 j\nwordj (arowvectorofsize|V|),thewordvectore isgivenby\nj\ne = w C (3.40)\nj j\nwhereC∈R|V|×de istheparameteroftheembeddinglayer. Cisoftenknownastheword",
    "154 Chapter 3. Words and Word Vectors\nembeddingtableinwhichthek-throwistherepresentationofthek-thwordinV.\nThen,weuseafeed-forwardneuralnetworktocomputetheprobabilitydistributionofthe\nwordatpositioni. Thisisgivenby\nPr(·|w ,...,w ) = F (e ,...,e ) (3.41)\ni−n+1 i−1 θ i−n+1 i−1\nwhereF (·)isafeed-forwardneuralnetworkparameterizedbyθ. Typically,theembedding\nθ\nlayer can be seen as a component of the NLM. Here we use slightly different notation to\nemphasizethattheNLMisafunctionofbothθ andC,likethis\nPr (·|w ,...,w ) = F (w ,...,w ) (3.42)\nθ,C i−n+1 i−1 θ,C i−n+1 i−1\nFortraining,weoptimizebothθ andCtominimizealossfunction. Apopularmethodis\nmaximumlikelihoodtrainingwhichmaximizesthesumoflog-likelihoodoveralln-gramsin\nthedata. Givenasequenceofwordsw ...w ,theobjectiveofthetrainingisdefinedtobe18\n1 m\nm\n(cid:88)\n(θˆ,C(cid:98)) = argmax logPr\nθ,C\n(w\ni\n|w\ni−n+1\n,...,w\ni−1\n) (3.43)\nθ,C\ni=n\nHaving obtained the optimized parameters θˆ and C(cid:98), we can apply F\nθˆ,C(cid:98)\n(·) to deal with",
    "newn-grams. Moreimportantly,wehavesomewell-trainedwordvectors(i.e.,C(cid:98))thatcanbe\nusedinsystemsotherthanNLMs. Thisisalsoknownasthepre-trainingofwordvectors. In\npre-training,wecandefineF (·)asanysystemthatmakesuseofthewordvectorsC. Thus,\nθ,C\nthetaskoflearningCistransformedtothetaskofoptimizingF (·)onthebackgroundtask\nθ,C\n(seeFigure3.6foranillustration). Themainadvantageofthismethodisthatwecanreuse\nexistingNLPtaskstotrainthewordvectors. Ariskhereisthatthe“best”wordvectorsfound\nintrainingF (·)mightnotbewellsuitedforthesystemwherethewordvectorsareinactual\nθ,C\nuse. Interestingly,inmanysituations,wordvectorsthatarepre-trainedbyNLMsareofgood\nquality for downstream tasks, or at least provide a good starting point for further tuning of\nthesewordvectorsinthetargetsystem.\n3.5 Word Embedding Models\nInprinciplewordvectorscanbelearnedinanymanner. Treatingwordvectorsascomponents\nofexistingNLPsystemsisoneoption,buttypicallylackstask-specificconsiderations. Another",
    "optionistodevelopmethodsspecificallytailoredtotheproblem. Thetrainingofsuchsystems,\ntherefore,doesnotneedtosatisfytheconstraintsofstandardNLPtasks,makingiteasierto\nlearnwordvectors.\n18Thiscanbegeneralizedtoadatasetconsistingofmultiplesequences.",
    "3.5 Word Embedding Models 155\nOptimizingCandθ UsingoptimizedC(cid:98)\nvialanguagemodeling onthenewtask\nG (·)\nF (·)\nθ,C π,C(cid:98)\n(cid:20) (cid:21) (cid:20) (cid:21)\n• • optimizedwordvectorsC(cid:98) • •\nC= • • C(cid:98) = • •\n• • • •\nTrainingWordVectors ApplyingWordVectors\nFigure3.6: Illustrationofpre-trainingwordvectorsinanNLM.TheNLMcanbedenotedas\na function F (·) of the word embedding table (i.e., C) and other parameters of the NLM\nθ,C\n(i.e., θ). The pre-training of C is essentially a process of training F (·) on a background\nθ,C\ntask. TheoutcomeistheoptimizedwordvectorsC(cid:98) whicharethenappliedtoanewsystem\nG (·)thatmightbedifferentfromtheNLM.Inthenewsystem,C(cid:98) isthewordembedding\nπ,C(cid:98)\ntablelearnedfromtheNLMandπ istheparametersspecializedtoG(·).\n3.5.1 Word2Vec\nWord2Vec is a short name for the models proposed in [Mikolov et al., 2013a;c]. As with\nneural language models, the Word2Vec models are based on neural networks. Rather than",
    "resortingtothegenerativemodelingofn-grams,theWord2Vecmodelsdescribethelearning\nofwordvectorsinalog-linearfashion. Inconsequence,thearchitecturesofthesemodelsare\ndifferentfromthoseusedinlanguagemodeling. TherearetwotypesofmodelsinWord2Vec:\n• Thecontinuousbag-of-wordsmodel(ortheCBOWmodel). TheCBOWmodelisa\nwordpredictionmodel. Itisusedtopredicthowlikelyawordatpositionioccursgiven\nthe−nand+nwordwindowsaroundit. ThestructureoftheCBOWmodelissimilarto\nthatoftheneurallanguagemodelintroducedinChapter2(seeFigure3.7(a)). First,we\nuseanembeddinglayertotransformthecontextwordsw ...w andw ...w to\ni−n i−1 i+1 i+n\ncorrespondingwordvectors. Thisisperformedbymultiplyingtheone-hotrepresentation\nofeachinputwordw\nj\nwiththeembeddingtableC∈R|V|×de,asshowninEq. (3.40).\nThesewordvectorsarethenaveragedtoproduceasinglerepresentationfortheinput\nwords,givingus\n \ni−1 i+n\n1 (cid:88) (cid:88)\nh =  w j C+ w j C (3.44)\n2n\nj=i−n j=i+1\nNotethattheabovedefinesamodelthatcompletelyignorestheorderofinputwords",
    "becauseoftheuseofthesumoperation. ThisexplainswhytheCBOWmodeliscalled\nbag-of-words. TheoutputlayeroftheCBOWmodelisastandardSoftmaxlayerthat",
    "156 Chapter 3. Words and Word Vectors\nprojectshtoaprobabilitydistributionoverthevocabulary\ny = Softmax(hU+b) (3.45)\nwhereU∈Rde×|V|istheparametermatrixofthelinearmappingandb∈R|V|isthebias\nterm. yisadistributionoverthevocabulary,andPr(w |w ,...,w ,w ,...,w )=\ni i−n i−1 i+1 i+n\ny(w ). Eqs. (3.44-3.45)describeaverysimpleneuralnetwork. Anadvantageisthatthe\ni\nresultingmodelissmallandefficientascomparedtoNLMs. ThetrainingoftheCBOW\nmodelisregular. Wecanframeitasfindingthemaximumlikelihoodestimationofthe\nparametersofthemodel. Forsimplicity,letθ denotetheparametersotherthanC(i.e,\nθ={U,b}). Wehave\nm−n−1\n(cid:88)\n(θˆ,C(cid:98)) = argmax logPr\nθ,C\n(w\ni\n|w\ni−n\n,...,w\ni−1\n,w\ni+1\n,...,w\ni+n\n) (3.46)\nθ,C\ni=n+1\nwheremisthelengthofthewordsequence. Aftertraining,wecansimplydropθˆ and\nuseC(cid:98) asawordvectorlook-uptable.\n• Thecontinuousskip-grammodel(ortheskip-grammodel). Theskip-grammodel\nis another word prediction model. It models the reverse of the task described in Eqs.",
    "(3.44-3.45). Tobemoreprecise,ourobjectiveistopredicteachofthe±ncontextwords\ngivenw . Thisisgenerallyframedasestimatingtheprobabilityofw occurringgiven\ni j\nw (i−n≤j ≤i−1 or i+1≤j ≤i+n). Figure 3.7 (b) shows the structure of the\ni\nskip-grammodel. Theembeddinglayerdealswithw asusual. Therepresentationofw\ni i\nisgivenby\nh = w C (3.47)\ni\nItisthenpassedtoaSoftmaxlayertopredicttheprobabilityforeachcontextwordw\nj\n(assumingj =i+k)19\ny = Softmax(hV +b ) (3.48)\nk k k\nwhereV andb aretheparametersofthemodel(−n≤k≤−1and1≤k≤n). We\nk k\nhave\nPr(w |w ) = Pr(w |w )\nj i i+k i\n= y (w ) (3.49)\nk i+k\nLetθ beashortrepresentationof{V }and{b }. Thetrainingproblemcanbedefined\nk k\n19Whenk>0,w j isawordintherightcontextwindowofw i;whenk<0,w j isawordintheleftcontext\nwindow.",
    "3.5 Word Embedding Models 157\nas\nm−n−1\n(cid:88) (cid:88)\n(θˆ,C(cid:98)) = argmax logPr\nθ,C\n(w\ni+k\n|w\ni\n) (3.50)\nθ,C\ni=n+1 −n≤k≤−1,\n1≤k≤n\nBothoftheabovemodelsmakeananalogytoclozetestsbyconsideringonlythepairwise\ndependencybetweenwords. Adangeristhatifcomplexrelationshipsamongwordsandword\norderinformationarerequired,theresultingprobabilitydistributionswillbenotthatprecise\ncomparedtolanguagemodels. Note,however,thatthegoalofthesemodelsisnottoprecisely\npredictmissingwordsgiventheircontexts,buttolearnwordrepresentationsfromsometask\nthatcapturesword-wordrelationships. Itisthereforenotsoimportanttocareabouttheword\npredictionperformanceofthelearnedmodel.\nAnothermeritofthesemodelsisthattheyhaveverysimple,easy-to-trainarchitectures.\nForexample,inbothmodelstherearenohiddenlayersandtheembeddinglayerisdirectly\nconnectedtotheoutputlayer. Thesemodelstructurescanbeseenasinstancesoflog-linear\nmodelinginmachinelearning: theinputvariablesarelinearlytransformedtoafeaturevector",
    "(e.g.,Eq. (3.44)),followedbyalog-linearfunction(e.g.,Eq. (3.45)).\n3.5.2 GloVe\nGlobalvectors,alsoknownasGloVe,arewordvectorsthatarelearnedbyusingbothglobal\nstatisticsoverthecorpusandlocalmodelsofwordprediction[Penningtonetal.,2014]. The\nGloVemethodstartswithaword-wordco-occurrencematrix(seeSection3.3),andthenforms\naneuralmodelbymakingaseriesofassumptions.\nGiven a word-word co-occurrence matrix M, where each cell M(a,b) = count(a,b)\nrepresents the number of co-occurrences of words a ∈ V and b ∈ V, we can obtain the\nconditionalprobabilityPr(b|a)byusingtheequation\ncount(a,b)\nPr(b|a) =\n(cid:80) count(a,b′)\nb′\ncount(a,b)\n= (3.51)\ncount(a)\nwherecount(a)isthenumberoftimesthewordaoccursinthecorpus.\nLet us now see a motivating example of GloVe. Suppose that we want to distinguish\nbetweenwordsairandwater. Itiseasytoobtainhowlikelyoneofthesewordsoccursgivena\ncontextwordinthecorpusviaEq. (3.51). Seethefollowingtableforasmallfractionofthe\nPr(b|a)matrixfrom3.8M-sentenceEnglishdatainWMT14.",
    "Entry w=fly w=drink w=breath w=live w=flow\nPr(air|w) 1.5×10−4 6.2×10−5 2.2×10−4 1.6×10−4 3.6×10−4\nPr(water|w) 1.3×10−5 4.1×10−4 1.8×10−5 1.4×10−4 3.0×10−4\nPr(air|w)/Pr(water|w) 11.54 0.15 12.2 1.14 1.2",
    "158 Chapter 3. Words and Word Vectors\nPr(w |w ,w ,w ,w )=y(w )\ni i−2 i−1 i+1 i+2 i\ny=Softmax(hU+b)\n(cid:16) (cid:17)\nh= 1 (cid:80)i−1 e + (cid:80)i+2 e\n4 j=i−2 j j=i+1 j\nEmbedding\ne =w C e =w C e =w C e =w C\ni−2 i−2 i−1 i−1 i+1 i+1 i+2 i+2\nw w w w\ni−2 i−1 i+1 i+2\n(a)CBOW\nPr(w |w ) Pr(w |w ) Pr(w |w ) Pr(w |w )\ni−2 i i−1 i i+1 i i+2 i\n=y (w ) =y (w ) =y (w ) =y (w )\n−2 i−2 −1 i−1 1 i+1 2 i+2\ny =Softmax( y =Softmax( y =Softmax( y =Softmax(\n−2 −1 1 2\nhV +b ) hV +b ) hV +b ) hV +b )\n−2 −2 −1 −1 1 1 2 2\nEmbedding h=w C\ni\nw\ni\n(b)Skip-gram\nFigure 3.7: The CBOW and skip-gram architectures. The CBOW model computes\nthe probability Pr(w |w ,w ,w ,w ) where w is a word in a sequence and\ni i−2 i−1 i+1 i+2 i\n{w ,w ,w ,w } are words in the ±2 context windows. The context representa-\ni−2 i−1 i+1 i+2\ntionhisthemeanofthewordvectorsthatareproducedthroughanembeddinglayer. histhen\nfedintoaSoftmaxlayertooutputadistributionoverthevocabulary(i.e.,y). Theprediction",
    "probability of w is Pr(w |w ,w ,w ,w )=y(w ). The skip-gram model is also\ni i i−2 i−1 i+1 i+2 i\nbasedontheembedding+Softmaxstructure. Itmodelstheprobabilityofeachcontextword\nw giventhewordw . ThisisachievedbysimplycomputingtheoutputofastandardSoftmax\nj i\nlayerthattakesthevectorrepresentationofw asinput. BoththeCBOWandskip-grammodels\ni\nare trained in a maximum likelihood fashion. The resulting lookup table of the embedding\nlayeristhewordvectors(orembeddings)forthewordsinthevocabulary.\nInthistable,Pr(air|w)andPr(water|w)indicatehowwellairandwatercorrelatewith\ndifferent w. We also compute the probability ratio Pr(air|w)/Pr(water|w) in the last line",
    "3.5 Word Embedding Models 159\nof the table. Interestingly, it is found that w can be viewed as a probe word by which\nPr(air|w)/Pr(water|w) models the relevance between words. When w is more relevant\nto air but less relevant to water (e.g., w = fly or w = breath), Pr(air|w)/Pr(water|w) is\nlarge. Incontrast,whenw islessrelevanttoairbutmorerelevanttowater(e.g.,w=drink),\nPr(air|w)/Pr(water|w)issmall. Whenwisrelevanttobothwords,orirrelevanttothem(e.g.,\nw=liveorw=flow),Pr(air|w)/Pr(water|w)isaround1.\nAninsightthatwecangainfromtheaboveexamplesisthatthewordvectorsshouldbe\nabletointerpretPr(air|w)/Pr(water|w). Asimpleideaistodevelopamodeltoapproximate\nthisprobabilityratio,say,\nPr(a|w)\nF(e ,e ,e˜ ) = (3.52)\na b w\nPr(b|w)\nwheree\na\n,e\nb\n∈Rde arethevectorrepresentationsofthewordsaandb,ande˜\nw\n∈Rde isthe\nvectorrepresentationofthecontextwordw. Notethatthenotationhasdifferentmeaningsfore\nande˜. TheformerisawordvectorfromanembeddingtableC,andthelatterisawordvector",
    "fromanotherembeddingtableC(cid:101). Theuseoftwoembeddingtableshasseveraladvantages.\nThemainadvantageisthatcombiningmultiplesetsofparameterscouldmitigatetheoverfitting\nofthemodel. Thefinalwordembeddingtabletakestheform C+C(cid:101).\n2\nTherearemanywaystodefinethefunctionF(·). HerewesimplytreatF(·)asaneural\nnetworkparameterizedbyC,C(cid:101) andsomeotherparameters. Consideringthesubtractionnature\nPr(a|w)\nincomparingaandbin ,wecanassumethatF(·)dependsone −e . Furthermore,\nPr(b|w) a b\nwecantakee eT ∈R(ore eT ∈R)tomodeltherelationshipbetweentheworda(orb)and\na w b w\nthecontextwordw. Theseleadtoanewformofthefunction\nF\n(cid:0)\n(e −e\n)e˜T(cid:1)\n=\nPr(a|w)\n(3.53)\na b w Pr(b|w)\nwhere (e −e )e˜T ∈R is the difference in representing words a and b when taking w as a\na b w\nprobeword.\nThere are still many solutions to Eq. (3.53), though the input of the function is greatly\nsimplified. For a feasible form of F(·), we further assume that Eq. (3.53) holds when we",
    "either exchange the embedding tables C and C(cid:101) (i.e., exchange e and e˜ for a, b and w), or\ntransposetheword-wordco-occurrencematrix(i.e., useMinsteadofM(cid:102)). Tomakeuseof\ntheseassumptions,onewayistoletF(·)beahomomorphismbetweentwosidesofEq. (3.53).\nThatis\nF (cid:0) (e −e )e˜T(cid:1) = F(e a e˜T w ) (3.54)\na b w F(e e˜T)\nb w",
    "160 Chapter 3. Words and Word Vectors\nThesolutiontoEq. (3.54)requiresthatF(·)=exp(·),andwehave\nF(e e˜T) = exp(e e˜T)\na w a w\n= P(a|w)\ncount(a,w)\n= (3.55)\ncount(a)\nRewritingthisequation,wehave\ne e˜T+logcount(a)−logcount(a,w) = 0 (3.56)\na w\nAproblemwithEq. (3.56)isthatthetermlogcount(a)makesthesolutionnon-exchangeable\nfor M and M(cid:102). To address this, a method is to absorb logcount(a) in some terms that are\nsymmetricforaandw,likethis\ne e˜T+β +β˜ −logcount(a,w) = 0 (3.57)\na w a w\nwhere β and β˜ are bias terms that depend on a and w, respectively. The quantity on the\na w\nleft-handsideofEq. (3.57)describeshowwelle e˜T+β +β˜ fitstheco-occurrencematrix.\na w a w\nWe wish to find some word vectors to enforce this quantity to be close to 1. Then, we can\ndefinethesquaredloss,asfollows\n(cid:16) (cid:17)2\nL = e e˜T+β +β˜ −logcount(a,w) (3.58)\na,w a w a w\nThelossoverallpairsofaandw isgivenby\n(cid:88)\nL = γ(count(a,w))·L (3.59)\nGloVe a,w\na,w∈V",
    "whereγ(count(a,w))isascalarforL . InPenningtonetal.[2014]’spaper,\na,w\n(cid:16) (cid:17)σ\ncount(a,w)\n count(a,w)<count\nγ(count(a,w)) = countmax max (3.60)\n1 otherwise\nwherecount andσ arehyper-parameters. Typically,σ issettoanumbersmallerthan1.\nmax\nAssuch,γ(count(a,w))willpenalizetheword-pair(a,w)ifcount(a,w)<count ,that\nmax\nis,thelossfunctionwillassignsmallerweightstorareword-pairs.\nEqs. (3.58-3.59)provideaverysimplewaytolearnwordvectorsandcanbeimplemented\nbyusingstandardneuralnetworkbuildingblocks(e.g.,vectordotproductandsummation).\nAnimportantpropertyofGloVeisthatthemodele e˜T+β +β˜ −logcount(a,w)isitself\na w a w\nlinear. Thetrainingisevenachievedwithouttheneedofcross-entropyloss. Thisdifferentiates\nGloVe greatly from NLM and word2vec in which expensive normalization of the output is\nrequired. Theintuitionhereisthattherelationbetweentwowordscanbemodeledinways\notherthanprobability-baseddivergence. Infact,Eq. (3.58)looksmorelikearegressionmodel",
    "3.5 Word Embedding Models 161\nthatfitsthedataoflogcount(a,w),thatis,wetendtolearntopredictlogcount(a,w)forany\npairof(a,w).\nAnothernoteabouttheuseofglobaldatabearsrepeating. Theco-occurrencematrixis\nasourceofinformationthatdescribestheentirecorpus. Animportantconsequenceofusing\nsuchinformationisthatthelearningtaskisframedasfindingwordvectorsthatareglobally\noptimized. Ofcourse,thisdoesnotmakeGloVeuniquebecausethelearningofmanymodels\nlikeNLMandWord2Vecitselfadmitsasimpleformulationasaglobaloptimizationproblem,\ne.g.,maximizingthelikelihoodovertheentireinputspace. However,theobjectivesinthose\nproblems are complex, and most of them are in practice trained via online learning, e.g.,\nupdatingthemodelparametersonabatchofsampleseachtime. Giventhis,GloVeactually\ndefinesamoreefficientglobalmodelascomparedwithNLMandWord2Vec.\n3.5.3 Remarks\nWehaveseenintheprevioussectionshowwordvectorsarelearnedbyusingseveraldifferent\nmethods. Wenowturntodiscussionsofissuesthatonemightbeinterestedinwhentraining",
    "and/orapplyingwordvectors.\n• Count-basedvsNeuralNetwork-based. Thesimplicityandinterpretabilityofcount-\nbased methods have long been appreciated. The use of the distributional hypothesis\ngreatlysimplifiestheproblem,butmakesastrongassumptionontheinformationsource\nthe word vectors can be learned from, and generally leads to data sparsity due to the\ncurseofdimensionality. Attheotherendofthespectrumislearningwithnoassumptions.\nIn these methods, we remove the constraints on the meaning of each dimension, but\ntreat word vectors as low-dimensional intermediate states of a neural network that is\ndevelopedtoaccomplishsomeNLPtask. Thisenablesthelearningoffeaturesthatare\nhardtodescribeinrepresentingaword. Thecomparisonofthetwotypesofmethods\nherecanfallunderthecomparisonoftwowell-knownlearningparadigms,say,feature\nengineeringvs. end-to-endlearning. Herewedonotwanttogetboggeddownbythis\ntopic. Itis,however,worthpointingoutthatitdoesnotnecessarilyrestrictwordvectors",
    "tocertainforms. Ingeneral,thechoiceofthetypesofwordvectorsdependsoninwhat\napplicationweapplythemandwhatinterpretationweplaceonthem. Forexample,if\nwewishtohavesomeinterpretable,easy-to-learnwordrepresentation,inducingword\nvectorsfromco-occurrencematricesmightbeagoodchoice;ifwewishtohavesome\nreal-valued,low-dimensionalwordvectorsthatwillbeintegratedintoabiggerneural\nnetwork,deeplearningmethodsmightbeworthatry. Notethat,learningcontinuous\nword vectors has become more and more common recently, given that the past few\nyearshavesignificantprogresstowardneuralmodelsofNLP.Also,therehasbeenmuch\ninterestincomparingcount-basedandneuralnetwork-basedmethods,andinexploring\nrelationshipsbetweenthem[LevyandGoldberg,2014b;Baronietal.,2014;Levyand\nGoldberg,2014c;Schnabeletal.,2015a;Levyetal.,2015;Gladkovaetal.,2016].\n• Shallow Models vs Deep Models. While it has become popular to solve the word\nvectorlearningproblemusingneuralnetworks,themodelstructuresweintroducedin",
    "162 Chapter 3. Words and Word Vectors\nthis chapter are simple. Technically, they all have one or two layers of neurons and\nare often thought of as instances of shallow models. A similar example is the vLBL\nword embedding model [Mnih and Kavukcuoglu, 2013]. It models the interaction\namong words using a two-layer neural network. This model, which does not even\ninvolve a Softmax function, is one of the simplest word embedding models subject\nto our knowledge. Such a simple model, however, still works well in many cases. A\nbenefit of shallow models is that they are efficient and scalable to a large amount of\ndata. Thismakesiteasiertousethemtodealwithmore“difficult”NLPproblems. A\ngoodexampleisthefastTextsystemfortextclassification[Joulinetal.,2017]. Ithasa\nsimilararchitecturetotheCBOWmodel(seeSection3.5.1). InfastText,theinputtextis\nrepresentedasabagofwordvectorsthatareaveragedtoformahiddenrepresentation\nofthetext. Thisisfollowedbyanoutputlayerthatmapsthehiddenrepresentationto",
    "a distribution over predefined classes. In this way, the classification model and word\nvectorsaretrainedjointly. Althoughshallowmodelsareremarkablyeffectiveforword\nvectorlearning,therearedeepermodelsthatonemaybeinterestedinformoremodeling\npower. As with most multi-layer neural networks, learning word vectors with deep\nneuralnetworkshasacoupleofbenefits[Telgarsky,2016]. First,byusingadeepmodel,\nwecanexploitpotentiallybetterhypothesesinalargehypothesisspace. Second,deep\nmodelsintroducemorenon-linearityintomodeling,andthusincreasetheabilityofthe\nmodeltodescribecomplexproblems. Therearemanyexamplesoflearningwordvectors\nin deep models. The simplest of these might be to simply stack more layers on the\nwordembeddinglayerinthosesystems. Thestackedlayerscanbefeed-forwardlayers,\nrecurrent layers, convolutional layers, or some combination of them. More recently,\nword vectors have been employed and/or trained by very deep and complex systems,",
    "achievingstate-of-the-artperformanceonmanyNLPtasks[Radfordetal.,2018;Devlin\netal.,2019]. However,strongermodelscomewithaddedcomputationalandtraining\nchallenges. Sothereareseverallinesofresearchonmeetingthesechallenges[Pascanu\net al., 2013; Bapna et al., 2018; Wang et al., 2019a; Zhang et al., 2019a; Pham et al.,\n2019;Lietal.,2020b]. InChapters4-6,wewillseeseveralsuccessfulNLPsystemsthat\narebasedonverydeepneuralnetworks.\n• Training Objectives. The idea of taking word vector representations as parameters\nofamodelfitswellwiththelatent-variablemodeling: amodelisparameterizedwith\nlearnablewordvectors,andthevaluesofthesewordvectorsareinferredbymaximizing\norminimizingsomeobjectivefunctionoftheentiremodel. Whilesuchalearningprocess\nisregularinmostsituations,thetrainingobjectivevariessomewhat. Adifficultywith\nthisisthatthereisnoobviousobjectivefordirectlysignalingthetrainingofwordvectors.\nAsimplesolutiontothisdifficultyistoresorttowell-definedNLPtasks. Forexample,",
    "we can use word vectors to represent the input of an NLP model (such as language\nmodelingandtextclassificationsystems). Hencethewordvectorscanserveasstandard\nparametersofthemodelandbeoptimizedasusual. Anothersolutionistodevelop“new”\ntrainingtasks. Asingeneralmachinelearningproblems,however,thisisawide-ranging\ntopicandtherearesomanychoicestodesignatrainingobjective. Soageneralmethod",
    "3.6 Evaluating Word Embeddings 163\nis to slightly update existing tasks. For example, the training objective of CBOW is\nessentiallybasedonthegeneralwordpredictionproblem,andhasasimilarformasthat\nusedinlanguagemodeling. Wewillalsoseeseveralnewtasksthatstemfromlanguage\nmodelinginChapter7. Yetinanothersensethesetrainingtasksdonotdirectlyconcern\nthemselveswiththeissueoflearningwordvectors,butgenerallyofferawaytoinjectit\nintoawell-designed,efficienttrainingprocedure. Notethat,inwordvectorapplications,\nwe may not assume a supervised learning scenario: the learned word vectors can be\nusedinvarioussystemsthatwehavenoideaoftheseapplicationsystemsinthetraining\nstage. This makes the problem more like an unsupervised learning problem because\nthereisnosupervisioninformationfromthetaskwherethewordvectorsareinactual\nuse. Sometimes,whenthetargetapplicationisaccessible,andthereissomelabeleddata,\nwecanhavefurthertrainingonthosewordvectorsthathavebeentrainedsomewhere.\n3.6 Evaluating Word Embeddings",
    "Having obtained the vector representation of words, we need to assess the quality of these\nvectors. Ideally, we wish to evaluate the word vectors against a gold standard. However,\nunfortunately,thereisingeneralnosuchgoldstandarddatasincenoonecanannotateavector\nofnumbersfordescribingaword. Asimplesolutioninthiscaseistoresorttotheresultof\nsomeworkingsysteminwhichthesewordvectorsareinvolved. Typically,therearetwotypes\nofevaluationapproaches[Schnabeletal.,2015b].\n• ExtrinsicEvaluation(orend-to-endtesting). Wedirectlyincorporatethewordvectors\ninto an NLP system which is easy to evaluate, and see how the performance of the\nsystemisinfluencedbythewordvectors.\n• IntrinsicEvaluation. Wetesttheabilityofthewordvectorstomodelthegivenaspects\nofmorphological,syntactic,andsemanticproblems.\nWewillbrieflydescribebelowhowtheseapproachesareappliedtowordvectorevaluation.\n3.6.1 Extrinsic Evaluation\nThis approach is often taken in practice since it allows researchers and engineers to glean",
    "aquickunderstandingofhowareal-worldsystembehaveswhenchangingpartofit. Since\nmanyNLPsystemsusewordsasinputs,itiscommontoreplacethesymbolicrepresentation\nof words in these systems with the word vectors. So far, we have seen several systems of\nthiskind,commonlywithanembeddinglayertransformingtheone-hotrepresentationtothe\nreal-valued vector representation of each input word, see for example the neural language\nmodelinChapter2.\nGivensuchasystemandasetoflearnedwordvectors,wecanuseitsperformanceasa\nmeasureofthequalityofthewordvectors. Consideringthewayweusethewordvectors,there\naretwowaystotrainthesystem:\n• WordVectorsasFixedParameters. Wefixthewordvectors,andtrainotherparameters",
    "164 Chapter 3. Words and Word Vectors\nofthesystemasusual.\n• WordVectorsasInitialParameters. Wetrainalltheparametersinthesamemanner.\nIn this way, the provided word vectors can be seen as initial values of some of the\nparameters,andwouldbeupdatedduringtraining.\nBoth methods fall under the area of pre-training, and could be extended to cover many\nproblemswherepartofamodeliswelltrainedbeforeseeingthedownstreamtask. Byfixing\nwordvectors,wesimplifythetrainingprocess,leadingtoaquickevaluationofthewordvectors.\nIncontrast,treatingthewordvectorsaslearnableparametersmayincreasethedifficultyof\ntraining,butcouldlearn“new”wordvectorsthatarebettersuitedfortheworkingsystem.\nNotethatalthoughextrinsicevaluationisofinteresttopractitioners,theresultsfromthis\nevaluationarehighlydependentonthesysteminwhichweapplythewordvectors. Because\ndevelopingadesiredNLPsystemofteninvolvessophisticatedtrainingandtuningprocedures\notherthanwordrepresentation,theconclusiondrawnbyexperimentingwithsuchacomplex",
    "systemisgreatlyinfluencedbythewaywebuildandusethesystem. Thisisalsothecasefor\nmanyotherNLPproblems. Forexample,atokenizationmethodthatishelpfulforamachine\ntranslationsystemmightnotbeagoodchoiceforaninformationretrievalsystem. Therefore,\ntotestthegeneralizabilityofthegivenwordvectors,awidely-usedapproachistocarryout\nexperimentsonavarietyofNLPsystems.\n3.6.2 Intrinsic Evaluation\nAlthoughmuchofwordrepresentationresearchinvolvesend-to-endtestsinNLPapplications,\nitalsoinvolvesexaminingtheabilityoftherepresentationtodealwithcertainproblems,such\nasinterpretingtherelationshipbetweentwowords. Therearemanywaystodesignintrinsic\nevaluation,eachaddressingaspecificproblem. Inthefollowingwedescribesomeofthese\nmethods. Formorecomprehensivedescriptionsaboutintrinsicevaluation,thereadercanrefer\ntopapersonthissubject[Baronietal.,2014;Bakarov,2018;Rogersetal.,2018].\n1. Semantic Relatedness\nModelingtherelatednessbetweenwordsisperhapsthemostpopularmethodtoevaluatethe",
    "qualityofwordvectorsinNLP[ReisingerandMooney,2010;Huangetal.,2012;Baronietal.,\n2014]. Itisfundamentallyaboutcomputingsomedistancebetweenwords(callittheword\nsemanticdistanceorworddistanceforshort). Themotivationisthattheworddistancein\na word vector space should agree with the judgments on the word relatedness in our mind\n[Rubenstein and Goodenough, 1965]. For example, we wish that dog is close to wolf, and\npeachisfarfromtelevision. Mathematically,therearealotofwaystocalculatethedistance(or\nangle)betweentwovectors. AsimpleandcommonlyuseddistancemeasureistheEuclidean\ndistance. Also,wecancomputethecosinesimilarityoftwovectorstoobtainascoreinthe\ninterval[−1,1]20.\nIn evaluation, we are given a set of word pairs, each of which is assigned an expected\n20Itisoftentousetheabsolutevalueofthecosinescoresothat0indicatestwovectorsinthesamedirectionand\n1indicatestwoorthogonalvectors.",
    "3.6 Evaluating Word Embeddings 165\ndistance by humans. Then, given a pair of words, we compare the expected distance with\nthe distance in the word vector space. The quality of the word vectors is reflected in the\ndifferencebetweenthetwodistances. However,adifficultyhereisthatthereis,inpractice,\nno gold-standard distance between words. Even for humans, it is still very difficult to give\nan exact number to describe how close a word is to another. An alternative method in this\ncaseistocategorizethedistanceintoafewcategoriesorratingscores,suchasanintegerin\n[1,5] [Reisinger and Mooney, 2010]. This greatly reduces the difficulty in data annotation.\nAnotherwaytoreducethedifficultyistoletthemodelfindthemostsimilarwordinasmall\nset of candidates to a given word. Such a method prevents us from predicting an absolute\ndistancebetweenwords. Insteadweonlyneedsomemechanismtoobtaintherelativedistance\norsimilaritybetweenwords[Baronietal.,2014].",
    "Judging the relationship between words, however, may result in a highly ambiguous\ntaskbecauseoftheambiguousnatureoflanguageuseandunderstanding. Ingeneral,many\nfactors may affect one’s thoughts on how words are related [Faruqui et al., 2016]. For\nexample,cornandcorneaaresimilarifweconsiderstringoverlapsinthesuffix,buttheyare\nsemanticallydissimilarbecausetheyrefertodifferentmeanings. Theambiguityalsocomes\nfromthedefinitionofrelatedness. Sometimes,relatednessandsimilarityaretwotermsused\ninterchangeablybuttheymayrefertodifferentconcepts. Forexample,carisrelatedtoroad,\nbutinanothersensecar issimilartovan. Anotherproblemisthatthemeaningofawordis\noften context-dependent. This makes it more difficult to establish the relationship between\nwordswithmultipledifferentmeanings(i.e.,polysemy). Broadlyspeaking,thisisaninherent\nproblemwithstatisticwordvectormodelswhereeverywordisassumedtobemappedtoa\nsinglevector. Forcontextualizedmodelingofwordvectors,wewilldescribeinthefollowing",
    "chaptersseveralmethodsthatconsiderawordtobedifferentinrepresentationgivendifferent\ncontexts.\n2. Word Analogy\nWordanalogyisconcernedwithmodelinganalogicalrelationsbetweenpairsofwords. The\nassumption here is that the relation between words can be captured by performing simple\nalgebraicoperationsonthecorrespondingwordvectors. Awell-knownexampleistheone\npresentedinMikolovetal.[2013d]’spaper,whereitisfoundthatthewayawordisrelatedto\nanotherwordcanbedescribedbyvectorsubtraction. Thisleadstoaninterestingresult: ifwe\nsubtractman’swordvectorfromking’swordvector,andaddwoman’swordvectortoit,then\nwewillobtainawordvectorclosetoqueen’s. Thatis\ne −e +e ≈ e (3.61)\nking man woman queen\nFormally, word analogy is a task of comparing two word pairs (a,a∗) and (b,b∗). An\nanalogycanbemadeifthewayaisrelatedtoa∗ issimilartothewaybisrelatedtob∗. This\nessentiallyreflectssomesortoflinguisticregularityinwordvectors,whichcanbeexpressed",
    "166 Chapter 3. Words and Word Vectors\nbyusingvectorsubtraction:\ne −e ≈ e −e (3.62)\na∗ a b∗ b\nThe word analogy can be framed as an analogical reasoning task: we try to predict e\nb∗\nusinge ,e ande . Morespecifically,wewishe −e +e tobeclosetoe if(a,a∗)and\na a∗ b a∗ a b b∗\n(b,b∗) hold similar relations. Also, improvements can be made on such a formulation. For\nexample, we can consider the angle between vectors e −e and e −e , rather than the\na∗ a b∗ b\ndifferenceine −e +e ande [LevyandGoldberg,2014b].\na∗ a b b∗\nWordanalogyprovidesasimplewaytoexaminethelinearitypropertyofawordvector\nmodelwhichisnottypicallyinvolvedinclassicmethods. Aninterestingpointhereisthatthe\nrecentwordvectormodelsexhibitgoodlinearbehavior,althoughwedonotconsiderthisin\nmodelingand/ortraining. Italsogivesresearchersusefulinsightsintothemodelslearnedby\nthosemethodsandintopotentialwaysofapplyingthesemodels[LevyandGoldberg,2014b;\nLinzen,2016;AllenandHospedales,2019]. Ontheotherhand,wordanalogyisnotageneral-",
    "purposemethod. Inmanycases,itdoesnotcorrelatewellwiththeperformanceofdownstream\nsystems,andistherebyusedasawaytostudycertainissuesofwordrepresentation.\n3. Word Categorization (or Clustering)\nAnother way to see how well the word vectors correlate with our understanding of word\nmeaning is to see how well these vectors can be categorized into meaningful groups. This\nis often achieved by performing clustering algorithms on the word vectors. We wish that\nsimilarwordsaregroupedintothesamecluster,anddissimilarwordsaregroupedintodifferent\nclusters. Forexample, apple, grape, peach, andorangebelongtothesamegroupofwords\nbecause they are all fruits. An advantage of this kind of evaluation is that many clustering\nalgorithmsandwordclusteringbenchmarkshavebeendevelopedandarestraightforwardly\napplicablehere. Ontheotherhand,asinmostclusteringtasks,therearepracticalissuesthat\nwehavetodealwith,suchasdeterminingthenumberofclusters.",
    "In machine learning, most clustering methods require computing the distance between\ndatapoints. Inthissense,wordclusteringisessentiallybasedonthesameideaofmodeling\nthewordrelatedness,thoughwedonotneedtojudgethequalityofthedistanceinthiscase.\nThisshowssomeintrinsicconnectionsamongdifferentevaluationmethods. However, asa\nside-effect,wordclusteringinheritsthesameproblemwithrelatedmethods(suchassemantic\nrelatedness). AsdiscussedinSection3.6.2,itisdifficulttodesignagold-standardcriterionto\nmeasurehowwellthewordsareclustered,sincewecangroupwordsintoclustersinsomany\ndifferentways.\n4. Subconscious Evaluation\nThe general idea of subconscious evaluation is to examine the correlation between the use\nofwordvectorsandsubconsciousbehaviorsorbrainfunctionswhenonereadstext. Awide\nvarietyofpsycholinguisticphenomenacanbeusedasthetest[MitchellandLapata,2010]. A\nwell-knownmethodisprimingwhichstudieshowapersonrespondstostimuli[Schacterand",
    "3.6 Evaluating Word Embeddings 167\nBuckner,1998;TulvingandSchacter,1990;WiggsandMartin,1998]. Forexample,wecan\ndesignanexperimenttotestthespeedwithwhichapersonreadsagivenword(callitthetarget\nword)whenitfollowsanotherword(callittheprimeword)[MeyerandSchvaneveldt,1971;\nLund,1995;McNamara,2005]. Ifthetargetwordtisreadmorequicklywhenfollowingaword\nathanwhenfollowinganotherwordb,thenwewouldsaythattcorrelatesmorewithathanb.\nThen,wecanusesuchapsychologicalmeasuretojudgethedistanceorsimilaritybetween\nwordvectors. Toobtainthetimetheparticipanttakesinreading,apopularmethodistoframeit\nasaself-pacedreadingtask21. Anothermethodistouseeye-trackingtoautomaticallyrecord\ntheinformationoftheeyemovementandposition. Byusingthesetechniques,severalmethods\nanddatasetshavebeenusedforstudyingavarietyofpsycholinguisticissues[Mitchelland\nLapata,2010;Hutchisonetal.,2013;LapesaandEvert,2014;Klerkeetal.,2015;Søgaard,\n2016;Augusteetal.,2017].",
    "Inadditiontotrackinghumanbehaviorinreading,wecanmonitorbrainactivitybyusing\nneurological tests, such as functional magnetic resonance imaging (fMRI) and electroen-\ncephalography (EEG) [Devereux et al., 2010; Søgaard, 2016; Bhattasali et al., 2020]. For\nexample, it is often hypothesized that, when a person reads and understands words, some\nactivationsoccurinhisorherbrain. Thereforewecanlinkthemeaningofwordswithbrain\nfunctions. Ontheotherhand,anobjectionisthattheknowledgeaboutthemechanismbehind\ntheseprocessesisstilllimited,makingitdifficulttocorrelatetheresultsofthesestudieswith\nreal-worldNLPsystems[Baronietal.,2014;Bakarov,2018].\n5. Linguistically Motivated Evaluation\nLinguistically motivated evaluation is based on an assumption that word vectors learned\nfromdatashouldexplainlinguisticresources. Oneinterestingapproachtoperformingsuch\nevaluationistoalignthewordvectorswithsomerepresentationsoftheentriesofadictionary",
    "[Tsvetkovetal.,2015;AcsandKornai,2016]. Thequalityofthewordvectorsismeasured\nin terms of the correlation between these word vectors and the linguistic representations22.\nApartfromstandarddictionaries,wecancomparethewordvectorsagainstasemanticnetwork,\nsuchasWordNet. Inthisway,theevaluationwouldbeimprovedifweconsidergraph-based\nalgorithmsonresourcesofthistype[Agirreetal.,2009].\n3.6.3 Visualization\nTaking word vectors as data points, we can adopt general approaches to visualizing multi-\ndimensionaldatatolocatedatapointsina2or3-dimensionalmap. Inthisway,wecananalyze\npatternsencodedinthesewordvectorsandinterpolatetherelationshipbetweenwords. Sincea\nwordvectorgenerallyhashundredsofdimensionsinpracticalapplications,weneeddimension\nreductiontechniquestomapitto2or3-dimensionaldataforvisualization. OnemethodisPCA\nwhichseeksalinearmappingfromahigh-dimensionalspacetoalow-dimensionalspace(see\n21Inself-pacedreading,thetextissegmentedintowords(orphrases),andtheparticipantisaskedtopressa",
    "buttontorequestthedisplayofasegment.\n22Alinguisticrepresentationcanbeseenasafeaturevectorthatismanuallybuiltonalinguisticresource(such\nasadictionary).",
    "168 Chapter 3. Words and Word Vectors\nSection3.3.3). Anotherwell-knownmethodist-distributedstochasticneighborembedding\n(t-SNE)[HintonandRoweis,2002;VanderMaatenandHinton,2008]. t-SNEisanon-linear\ndimensionreductionmethod,andhasbeenwidelyusedinvisualizinghigh-dimensionaldata.\nApartfromthese, onecanconsiderthemethodspresentedinSection3.3.3aswellasthose\ntailoredforvisualizingwordvectors[Zhangetal.,2019b;Liuetal.,2017].\n3.7 Summary\nIn this chapter we discussed two interesting problems in NLP: tokenization and word (or\ntoken)representation. First,weintroducedmodelsfordividingasentenceintounitsthatare\nmeaningfuland/orwellsuitedfordownstreamtasks. Second,weintroducedtheideaofword\nvectormodelswithparticularattentiontolearningbothcount-basedhigh-dimensionalmodels\nand real-valued low-dimensional models. While most of these models are simple, they are\noftenusedincomplexNLPsystemsandformthebasisofmanyadvancedmodels,aswillbe\nshowninthefollowingchapters.",
    "Tokenization(orsegmentation)isanimportant“operation”inNLP,commonlyasapre-\nprocessingstepformanyapplications[WebsterandKit,1992]. However,theuseoftheterm\ntokenization is somewhat misleading because it originally refers to a process of dividing a\nstring into substrings and is more often used as a general computer science term. In NLP,\ntokenizationcandrawonconceptsandresultsfromseveralsub-fields. Onthelinguisticsside,\ntokenizationishighlyrelatedtotwofundamentalquestions: howwordsarecomposedandhow\nwordsformsentences. Itisthereforenaturaltousetheoriesandmethodsofmorphologyand\nsyntaxtodefinethebasicunitsofalanguage,leadingtomanyrule-basedtokenizationsystems\ncoveringavarietyoflanguages. Onthemachinelearningside,tokenizationhaslongbeencast\nasaproblemoflearningtokenboundariesfromdataineitherasupervisedorunsupervised\nmanner[Mielkeetal.,2021]. Acommonapproachistofirstannotatesometokenizedtextwith\nhumanknowledgeaboutwhatbasiclanguageunitsshouldbe,andthenlearntotokenizeon",
    "thisannotateddata(seeSection3.1.3). Morerecently,learningtokenizerswithoutlinguistic\nconstraintshasbeenfoundtobepromising(seeSection3.1.4). Sincenaturallanguagesare\nthemselvessetsofcharactersorbytesequences,itisalsopossibletosegmentasentenceinto\ncharacters or bytes [Ling et al., 2015; Lee et al., 2017]. The tokenization-free method in\ngeneralmayhelpwhenonewantsalanguage-independenttokenizerandasimplerpipelinefor\nprocessingthetext.\nFromamoremathematicalperspective,tokenizationcanbethoughtofasamappingfrom\nthe input data to a sequence of variables. In this way, the concept of tokenization can be\ngeneralizedbyrelaxingtheassumptionthatboththeinputandoutputvariablesareconstrained\ntodiscretevalues. Inrecentimageandspeechprocessingsystems,forexample,researchers\ntrytotransformcontinuousinputdata(suchaspixelsandacousticsignals)intoasequence\nofvector-based“tokens”[Schneideretal.,2019;Dosovitskiyetal.,2021]. Someinteresting",
    "extensionsoftheseideasareeventotransformimageandspeechdatatoasequenceofindices,\nleadingtoapproachesbearingacloserrelationtoNLP[Oordetal.,2017;Baevskietal.,2020;\nHsuetal.,2021].",
    "3.7 Summary 169\nGiventhattheinputtextisdividedintosmallerpieces,anaturalnextstepistorepresent\nthesepiecesinsomewaythatcapturestheirunderlyingfeatures. Whilerepresentinglanguage\nunitsasvectorsofnumbershasbeenthedefactostandardforthedevelopmentofrecentNLP\nsystems,theworkonvectorrepresentationdatesbacktotheveryearlydaysofcomputational\nlinguistics. Accordingtomanypopulartextbooksandpapers[ManningandSchütze,1999;\nJurafskyandMartin,2008],theideaofusingadistributiontorepresentwordmeaning,also\nknownasdistributionalsemantics,startedinthe1950swiththeriseofempiricism. Atthe\ntime,mostoftheworkwasinfluencedbyHarris’sdistributionalism[Harris,1954]andrelated\nwork [Firth, 1957; Wittgenstein, 1953]. In parallel, Osgood [1952] proposed to define the\nmeaningofaconceptasapointinamultidimensionalspaceinapsychologicalmanner. All\ntheseideasgreatlyinfluencedthewaylinguisticsandNLPpeoplethinkofwordmeaningin\nthefollowingdecades.\nModernapproachestodistributionalsemanticsappearedinthe1990s,mainlyasaresultof",
    "therevivalofempiricisminartificialintelligence[Church,2011]. Mostoftheseweredrivenby\nthedistributionalhypothesis: wordshavingsimilarmeaningsaremorelikelytooccurinsimilar\ncontexts. Inresponse,anumberofmethodsweredeveloped,differinginthewaythecontexts\nare modeled. For example, a context can be the words in a context-window, or the words\nwitharelationtothegivenwordinasyntaxtree. ApartfromthosementionedinSection3.3,\nmethodsthatarenotcoveredinthischapterincludehyperspaceanalogueoflanguage(HAL)\n[LundandBurgess,1996],distributionalmemory[BaroniandLenci,2010],dependency-based\nsemanticspacemodels[PadóandLapata,2007],andsoon. Forcomprehensivedescriptionsof\ndistributionalsemanticsmodels,thereadercanrefertopapersthatsurveythistopic[Lenci,\n2018; Mitchell and Lapata, 2010]. Note that most of the above-mentioned work can be\nthoughtofasinstancesofthevectorspacemodelwhichcandealwithproblemsbeyondlexical\nsemantics. Forexample,incompositionaldistributionalsemantics,themeaningofaphraseor",
    "asentencecanberepresentedasavectorobtainedbyperformingsimplealgebraicoperations\nonthewordvectors[Clarketal.,2008;MitchellandLapata,2010;BlacoeandLapata,2012].\nWhiledistributionalmodelshaveattractedattentionintheNLPcommunityformanyyears,\nwordembeddingmodelsthatlearnlow-dimensional,real-valuedwordvectorsdirectlyfrom\ntextshavebeenapredominantapproachrecently. AsdescribedinSections3.4-3.5,models\nofthistypedonotdependonstrongassumptionslikethedistributionalhypothesis,butlearn\nto represent a word as a vector of hidden attributes (or features) describing the word. The\nresulting model is an extension of the feature-based semantic model [Markman, 2013]. A\nrecognizeddifferencewithtraditionalfeature-basedmethodsisthatwedonotneedtomanually\ndefinethefeatures. Weinsteadtakethesefeaturesasparametersofthemodel,andtrainthem\ninthewayasincommon(supervised)machinelearningsystems.\nFormulatingwordrepresentationasanend-to-endlearningproblembringswithitseveral",
    "benefits. One of the benefits is that new features can be found because no constraints are\nplaced on how these features are learned and interpreted. On the other hand, as shown in\nSection3.6.2,thewordvectorsobtainedinthiswayindeedshowsomelinguisticproperties,\nthough the word embedding models are not trained to achieve this. Another benefit is that\nthe word embedding models also fall in the vector space models in NLP, enabling the easy",
    "170 Chapter 3. Words and Word Vectors\nuseofwordvectorsinvariousapplications. Therearealsomanyexamplesofmethodsthat\nattempttoimprovestandardwordembeddingsystems. Forexample,researchershavetriedto\nincorporateadditionallinguisticinformationintowordvectors[LevyandGoldberg,2014a;\nCotterellandSchütze,2015;Tissieretal.,2017],andtolearnuniversalwordvectorsacross\nmultiplelanguages[Klementievetal.,2012;Mikolovetal.,2013b;Ammaretal.,2020;Smith\netal.,2017;Artetxeetal.,2017].\nWidelyassociatedwithneuralmodelsinNLP,theideaofdistributedrepresentationhas\nbeensuccessfullyappliedtoproblemsbeyondwordrepresentation,e.g.,sentencerepresentation\n[LeandMikolov,2014;Kalchbrenneretal.,2014;Kirosetal.,2015;Hilletal.,2016;Arora\netal.,2017;Linetal.,2017;Conneauetal.,2017b],tree/graph-structurerepresentation[Socher\netal.,2011;Perozzietal.,2014;Taietal.,2015;GroverandLeskovec,2016],andsoon. In\nparticular,contextualizedrepresentationsofwords,thoughnotdiscussedinthischapter,are",
    "generallyappreciatedformodelingsequentialdata[McCannetal.,2017;Petersetal.,2018;\nDevlinetal.,2019].",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 4\nRecurrent and Convolutional Sequence Models\nThewholeismorethanthesumofitsparts.\n–Aristotle,384-322BC[Ross,1924]\nAristotlemightormightnotthinkoflinguisticphenomenawhenhavingthisthought,butitis\nindeedsomethingwewanttoexpressinthischapter: thereissomethingdifferentinasentence\norphrasebesideswords. Ofcourse,wordshavemeanings,alone. However,whentheycome\ntogethertoformasentenceorphrase,themeaningofthewholecouldbemuchmorecomplex\nanddiverse. Thisleadstothemostbeautifulaspectoflanguagethathumanbeingscanexpress\nanymeaningusingafinitesetofelements(e.g.,wordsorcharacters).\nTheinfiniteandnon-compositionalnatureoflanguagemakesitmoredifficulttomodela\nsequenceofwordsthantomodelindividualwords. Adifficultyisthatawordmayrepeatedly\nalteritsmeaningindifferentcontexts. Takingtheideaofwordembeddingthatawordcanbe\nrepresentedasalow-dimensional,real-valuedvector,the“meaning”ofalanguageunitcould",
    "becontinuous. Itisthereforepossibletoextendmethodsofdistributedrepresentationfrom\nwordstosequencesofwords. Thisleadsustoexploremodelsinwhichtheprocessofdealing\nwithvariable-lengthwordsequencesisfundamentallycontinuous.\nHereweconsiderthegeneralapproachtolearningthedistributedrepresentationofword\nsequences. Inparticular,weconsiderrecurrentandconvolutionalneuralnetworkswhichhave\nbeenextensivelyusedinmanyfieldsrangingfromspeechprocessingtocomputervision. For\nnaturallanguageinputs,theresultofapplyingthesemodelsisasequence-levelrepresentation\noftheinput. Therepresentationcouldbeeitherasinglereal-valuedvector,orasequenceof\nsuchvectors,eachcorrespondingtoacontextualizedrepresentationforaninputwordofthe\ninput sequence. Such a model of representation, that can broadly be called an encoder, is\ngenerallyusedwithavarietyofsystemswhoseinputissequentialdata. Wewillseeseveral\nexamplesofitinthischapter.",
    "172 Chapter 4. Recurrent and Convolutional Sequence Models\n4.1 Problem Statement\nFormanyNLPapplications,ourobjectiveistomakepredictionsbasedonaninputsequence.\nLet us consider again the text classification problem mentioned in Chapter 1. If we obtain\na text that may talk about food or not, we want to assign one of the two classes to it (say\nFoodorNot-food). Todothis,acommonmethodofclassificationistorepresentthetextasa\nbagoffeatures,denotedasH. Then,aprobabilityisassignedtoeachoftheclassesusinga\nprobabilisticmodelPr(y|H). Thepredictedclassistheonethathasthemaximumprobability\nyˆ=argmax Pr(y|H).\ny\nWhilethisisastandardprocedureforclassification, theunderlyingideacanbeusedto\ndescribeageneralproblem. Formally,letw=w ...w beasequenceofwords1. Asequence-\n1 m\nlevelNLPsystemcanbeformulatedasafunctionthatmapsthesequencewtosomeoutput\ny. This can be divided into two steps, called the representation (or encoding) step and the\npredictionstep.",
    "• Representation(orEncoding). Ittransformstheinputsequencewtosome“features”\nHbyusinganencoderEnc(·):\nH = Enc(w) (4.1)\n• Prediction. ApredictorPredict(·)takesHandgeneratesanoutput:\ny = Predict(H) (4.2)\nAsimpleformofHisafeaturevector. Forexample,Hcouldbeasetofhuman-designed\nindicatorfeaturesextractedfromw(asahigh-dimensionalsparserepresentation),orasetof\nrealnumbersindicatingsomelatentfeatures(asalow-dimensionaldenserepresentation). In\nNLP,anothercommonformofHisasequenceofvectorsinwhicheachvectorh corresponds\ni\ntoaninputwordw (seeFigure4.1). Inthiscase,h canbeviewedasa“new”representation\ni i\nofbothw anditscontextinw2. Thecorrespondencebetweenh andw enablestherepresen-\ni i i\ntationtomakedistinctionsamongdifferentpositionsofthesequence,andmoreimportantly,to\nvaryitsmodelingpowerforvariable-lengthinputs.\nThe form of y is dependent on the problem we intend to deal with. For example, for\nclassificationproblems,yistheindexofaclass(oradistributionofclasses);forregression",
    "problems,y isarealnumber;fortranslationproblems,y isasequenceofwordsinanother\nlanguage, and so on. Note that, in the above model, representation and prediction can be\nregardedastwoseparateproblems. Agreatadvantageofisolatingrepresentationandprediction\nisthatwecanusethesameencoderinmanyapplicationswithdifferentpredictors. Thisalso\nmotivatesapromisinglineofresearchinwhichageneral-purposeencoderistrainedonlarge-\nscaledataandthenusedascomponentsindifferentdownstreamsystems[Petersetal.,2018;\n1Although we restrict ourselves to word sequences for discussion, the methods can be used to deal with\nsequencesofanylanguageunits,e.g.,sub-words,characters,etc.\n2Thisarchitecturecanbeextendedtoencodersinwhichtheinputandoutputhavedifferentlengths,say,the\ninputisw\n1\n...wmandtheoutputish\n1\n...hn(m̸=n).",
    "4.2 Recurrent Models 173\nH (asinglevector) h 1 h 2 ··· h m−1 h m =H\nEncoder Encoder\nw w ··· w w w w ··· w w\n1 2 m−1 m 1 2 m−1 m\n(a)Encodingthesequenceasavector (b)Encodingthesequenceasavectorsequence\nFigure4.1: Representingawordsequenceas(a)avectoror(b)asequenceofvectors.\nDevlinetal.,2019].\nThere are many possible forms for Enc(·) and Predict(·). For text classification, for\nexample, one way is to define Enc(·) as a function computing a feature vector using a set\nofhand-craftedfeaturetemplates,anddefinePredict(·)asastatisticalclassificationmodel\n(suchasSVMsandmaximumentropy-basedmodels). AnotherwayistodefineEnc(·)asa\nmulti-layerneuralnetworkthatoutputsareal-valuedvector,anddefinePredict(·)asasimple\nneuralnetworkthatinvolvesonlyoneSoftmaxlayer. Inthischapterwewillfocusonneural\nnetwork-based encoders. We will show that such a type of encoder could be applied to a\nnumberofNLPtasksinSection4.5.\n4.2 Recurrent Models",
    "A study of various sequence models is not easy work. It is convenient, however, to first\nintroduce one of the most common and practical neural models, called recurrent neural\nnetworks(RNNs). WewillseelaterthatRNNsareextensivelyusedinsequencemodeling,\nandthetechniquespresentedherearegenericandapplicabletomanysystems.\n4.2.1 An RNN-based Language Model\nPerhapsthemostpopularuseofsequencemodelsinNLPisestimatingtheprobabilityofa\nwordsequence,alsoknownaslanguagemodeling. Mathematically,languagemodelingisan\ninstanceofawell-knownprobleminthefieldofstochasticprocesses(orrandomprocesses):\ntheproblemofmodelingtimeseriesdata[Hamilton,1994;Chatfield,2003;Fuller,2009]. As\natimeseries,asequenceofwordscanbetreatedasasequenceofdatapointsattimeintervals\nthat are equally spaced. In this sense, the methods we present here are somewhat general,\nalthoughthediscussiononabroaderrangeoftimeseriesproblemsisbeyondthescopeofthis\nbook.\nGiven a sequence of words w ...w , the goal of language modeling is to compute",
    "1 m\nPr(w ,...,w ). Thisjointprobabilityistypicallywrittenasaproductofconditionalprobabili-\n1 m",
    "174 Chapter 4. Recurrent and Convolutional Sequence Models\ntiesusingthechainrule:\nPr(w ,...,w ) = Pr(w )·Pr(w |w )···Pr(w |w ,...,w ) (4.3)\n1 m 1 2 1 m 1 m−1\nInotherwords,theproblemofgeneratingw ...w isthesameastheproblemofgenerating\n1 m\na word w at a time based on the previous words w ...w . RNN-based language models\ni+1 1 i\nrepresentw ...w viaarecurrentunitRNN(·)[Mikolovetal.,2010],likethis\n1 i\nh = RNN(h ,x ) (4.4)\ni i−1 i\nwherex\ni\n∈Rde isthewordvector(orwordembedding)forw\ni\n. LetV bethevocabularyfrom\nwhichwecanchooseaword. Ifw ∈R|V| isaone-hotwordrepresentation3,x isgivenby\ni i\nmultiplyingw\ni\nwiththewordembeddingtableC∈R|V|×de:\nx = Embed(w )\ni i\n= w C (4.5)\ni\nAs shown in Chapter 1, the use of C transforms a |V|-dimensional (and probably high-\ndimensional)vectortoad -dimensional(andprobablylow-dimensional)vector. NotethatC\ne\nisessentiallyalookuptable,withadistincttableentry(i.e.,arow)foreachwordinV. So,the\nright-handsideofEq. (4.5)isinpracticeafunctionthatselectsarowfromCwiththeword",
    "index.\nNow we go back to Eq. (4.4). The equation is not difficult to understand: the state of\nthe context we have seen so far (i.e., h ) is some representation of the combination of the\ni\ncurrentinput(i.e.,x )andthestateoftheearliercontext(h ). Putanotherway,itcanbe\ni i−1\nthoughtofasaprocessofrepeatedlyaddinginformationofanewwordtoacacheof“history”.\nAnelegantaspectofthisprocessisthatitcanbeeasilyimplementedbyrunningEq. (4.4)a\nnumberoftimesuntiltheendofthesequence.\nRNN(·)canbeanyfunctionthattakesh andx ,andproducesanewvectorh . The\ni−1 i i\nvanillaRNNhasaform\nRNN(h ,x ) = ψ(h U+x V) (4.6)\ni−1 i i−1 i\nwhere ψ(·) is an activation function, such as TanH(·) and Sigmoid(·). Together with Eqs.\n(4.4)and(4.5),wecandefineh asafunctionofh andw\ni i−1 i\nh = ψ(h U+w CV) (4.7)\ni i−1 i\nwhereU∈Rd h ×d h,V∈Rde×d h,andC∈R|V|×de arelearnableparametersofthemodel,and\nd isahyper-parameterindicatingthenumberofdimensionsofh andh .\nh i i−1",
    "We now have an encoder that represents the word sequence w ...w as a sequence of\n1 m\n3Theone-hotrepresentationw iisa|V|-dimensionalvectorinwhichonlyoneentryis1andallotherentries\narezeros.Followingthenotationusedthroughoutthisbook,avectorisingeneralrepresentedasavariableinbold\ntext.Herewetreatw iasawordindexandinterchangeablyuseitwiththeone-hotrepresentation.",
    "4.2 Recurrent Models 175\nRNN’s outputs H={h ,...,h }. Given that each h encodes the sub-sequence spanning\n1 m i\nfromw tow ,wecanplaceaSoftmaxlayeronh toobtainadistributionofwords:\n1 i i\ny = Softmax(h O+b) (4.8)\ni+1 i\nwhereO∈Rd h ×|V| andb∈R|V|. Takingthewordindexw i+1 ,wehave\nPr(w |w ,...,w ) = y (w ) (4.9)\ni+1 1 i i+1 i+1\nThus,wehavedevelopedalanguagemodelthatproducesaprobabilityPr(w |w ,...,w )\ni+1 1 i\nateachstep. Figure4.2showsanillustrationoftheRNN-basedlanguagemodelforanexample\nsequence. Torunthismodelonawordsequence,wesurelywishtostartwithpredictingw but\n1\nthisrequiresaprecedingwordw thatistakenastheinput. Asimpleandwidelyapplicable\n0\nmethodforgivinganappropriatestartingstatetoRNNsistoaddabeginningsymbol⟨SOS⟩\ntothesequencesothatallsequencesstartwiththesame“word”. Likewise,wecanattachan\nendsymbol⟨EOS⟩tothesequencetomodelthecompletenessofthesequence. Thisleadstoa\nnewformoftheprobabilityofthesequence\nPr(⟨SOS⟩,w ,...,w ,⟨EOS⟩) = Pr(⟨SOS⟩)·\n1 m\nPr(w |⟨SOS⟩)·\n1\nPr(w |⟨SOS⟩,w )·\n2 1\n···",
    "Pr(w |⟨SOS⟩,w ,...,w )·\nm 1 m−1\nPr(⟨EOS⟩|⟨SOS⟩,w ,...,w ) (4.10)\n1 m\nWecansimplyassumePr(⟨SOS⟩)=1. ToobtainPr(⟨SOS⟩,w ,...,w ,⟨EOS⟩),wetake\n1 m\n⟨SOS⟩w ...w asaninputsequenceandw ...w ⟨EOS⟩astheoutputsequence.\n1 m 1 m\n4.2.2 Training\nAs a neural network, the RNN-based language model can be trained in a regular way. The\ntrainingproblemhasbeenwelldiscussedinChapter2. So,wedonotgiveafulldescriptionin\nthischapter,butalittlebitaboutitsbasicideaaswellassomerefinements.\nRNN-basedlanguagemodelingcanbeframedasanext-step-predictionproblem. Suppose\nwearegivenacollectionofwordsequencesS. Foreachsequencew=w ...w inS, we\n1 |w|\nhaveasequenceofpairsofaninputwordandthecorrespondinggold-standardanswer,like\nthis4\n{(w ,w ),(w ,w ),...,(w ,w )}\n1 2 2 3 |w|−1 |w|\nThe language model takes the input sequence w ...w and returns a sequence of\n1 |w|−1\n4Whilethe⟨SOS⟩and⟨EOS⟩tricksaregenerallyconsideredinreal-worldsystems,wedropthe⟨SOS⟩and\n⟨EOS⟩symbolsfromnowonforsimplification.",
    "176 Chapter 4. Recurrent and Convolutional Sequence Models\nPr(w1|w0) Pr(⟨EOS⟩|w0,···,wm)\nPr(·|w\n0\n)\n···\nOutputLayer\nSoftmax Softmax ··· Softmax Softmax\nSoftmax(hiO+b)\nHiddenLayer\nh1 h2 hm−1 hm\nRNN RNN ··· RNN RNN\nψ(hi−1U+xi−1V)\nEmbeddingLayer\nEmbedding Embedding ··· Embedding Embedding\nwi−1C\n···\nw 0 (⟨SOS⟩) w 1 w m−1 w m\nFigure 4.2: Illustration of using an RNN-based language model to calculate\nPr(⟨SOS⟩w ...w ⟨EOS⟩). The input is ⟨SOS⟩w ...w , and the output is the proba-\n1 m 1 m\nbility Pr(w |⟨SOS⟩)Pr(w |⟨SOS⟩ w )...Pr(⟨EOS⟩|⟨SOS⟩ w ...w ). As Pr(⟨SOS⟩) =\n1 2 1 1 m\n1, the probability of generating the sequence is simply Pr(⟨SOS⟩w ...w ⟨EOS⟩) =\n1 m\nPr(⟨SOS⟩)Pr(w |⟨SOS⟩)Pr(w |⟨SOS⟩ w )...Pr(⟨EOS⟩|⟨SOS⟩w ...w ). For each input\n1 2 1 1 m\nw ,wefirstrepresentitasawordvectorx viatheembeddinglayer,resultinginasequence\ni i\nof word vectors x ...x . The RNN layer maps x ...x to a sequence of hidden states\n0 m 0 m\nh ...h . In this process, we repeat the same thing: an RNN unit takes both h and x",
    "1 m+1 i−1 i\nand produces a new state h . On top of that, we use the output layer (Softmax) to obtain\ni\nPr(w |⟨SOS⟩w ...w ).\ni+1 1 i\ndistributionsy ...y . Seethefollowingtableforanillustrationoftheinputsandoutputsof\n2 |w|\nthemodel.\nStep History Input Output Gold-\n(One-hot) (Distribution) Standard\n1 w y w\n1 2 2\n2 w w y w\n1 2 3 3\n3 w ,w w y w\n1 2 3 4 4\n... ... ... ... ...\n|w|−2 w ,w ,...,w w y w\n1 2 |w|−3 |w|−2 |w|−1 |w|−1\n|w|−1 w ,w ,...,w ,w w y w\n1 2 |w|−3 |w|−2 |w|−1 |w| |w|\nAlossfunctionL(y ,w )isdefinedtomeasurehowmany“errors”wewillmakeifweuse\ni i\ny insteadoftheone-hotrepresentationw . Acommonchoiceisthecross-entropylosswhich\ni i\ncomputesthedivergenceofadistributionfromanother[Mitchell,1997;Bishop,2006].",
    "4.2 Recurrent Models 177\nThen,thelossovertheentiresetisdefinedtobe\n|w|\n(cid:88)(cid:88)\nL = L(y ,w ) (4.11)\ni i\nw∈Si=2\nOnceweknowtheloss,thetrainingoftheRNN-basedlanguagemodelcanbeachievedby\nusinggradientdescent. Asimpleformofthismethodisthedeltarule\n∂L\nθ = θ −lr· (4.12)\nnew old\n∂θ\nwhere θ stands for the parameters. For the model described in Section 4.2.1, θ includes C,\nU,V,Oandb. ∂L isthederivativeofthelosswithrespecttotheparameters,callederror\n∂θ\ngradient.\nEq. (4.12)canbeunderstoodasaprocessofmovingthecurrentparametersasmallstep\ninthesteepestdownhilldirection(i.e.,thedirectionof−∂L). Herelr standsforhowfarwe\n∂θ\nmoveineachstepofgoingdownhill,alsocalledthelearningrate. Obtaining ∂L oftenrequires\n∂θ\na back-propagation process that flushes the error gradient from the output to the input. In\nmodernimplementationsofdeeplearningsystems,inwhichneuralnetworksarerepresentedas\ncomputationgraphs,back-propagationissimplesinceitisjustaby-productofgraphtraversal",
    "and there are many automatic differentiation toolkits to do this. Similar algorithms, called\nback-propagationthroughtime(BPTT),werealsousedinearliersystems[Werbos,1990].\nForfurtherinformationabouttrainingneuralnetworks,seeChapter2and/ortextbooksonthis\nsubject[Goodfellowetal.,2016;Zhangetal.,2021].\nIf the input is a long sequence, the application of RNNs would result in a deep neural\nnetwork. Inthiscase,theuseofthechainruleoforderedderivativesmakeslargeorsmallloss\nderivativesaccumulate,andtheupdatetotheparametersinEq. (4.12)isconsequentlyvery\nlargeorsmall. Thesearetypicallyknownastheexplodingandvanishinggradientproblems.\nThereareseveralmethodstomitigatetheseproblemsforRNNs[Sutskever,2013]. Someof\nthemare\n• Regularization. Introducing regularization terms (such as the l and l norms on\n1 2\nparametermatrices)intotrainingcanavoidmodelsinwhichmostoftheparametershave\nlargevalues,andthushelptoavoidexplodinggradients. Similarly,onecanpenalizethe",
    "casesinwhichthenormsofthegradientsaretoosmall[Pascanuetal.,2013].\n• GradientClipping. Whenthenormofthegradientsistoolarge,itisnaturaltodirectly\nscaledowntheirmagnitudes. Asimplemethodistoclipthegradientnormintermsof\na threshold τ. If the norm ||∂L|| is larger than τ, we can rescale ∂L accordingly, say,\n∂θ ∂θ\n∂L = τ ·∂L.5\n∂θ ||∂L|| ∂θ\n∂θ\n5Itisusuallyformulatedasanequation\n∂L τ ∂L\n= · (4.13)\n∂θ max(τ,||∂L||) ∂θ\n∂θ",
    "178 Chapter 4. Recurrent and Convolutional Sequence Models\n• TruncatedBack-propagation. Anotherideaistobreakalongsequenceofinput-output\npairsintoshorterpieces,andtrainRNNsontheseseparatesub-sequences[Williamsand\nPeng, 1990; Elman, 1990]. This reduces both the cost of training and the risk of too\nlargeorsmallvaluesinaccumulatingerrorgradients.\n• ImprovedArchitectures. Itisalsopossibletoredesignthemodeltooverwhelmthe\nlimits of standard RNNs, usually using the memory mechanism. In Section 4.3, we\nwillseeafewexamplesofredesigningtherecurrentunitforaddressingthevanishing\ngradientproblem.\n• Initialization and Constraints of Parameters. Initializing the model parameters to\na desirable region is generally helpful for optimization, and, sometimes, helpful for\npreventingverysmallgradients. Analternativemethodistorandomlysetthemodeland\nonlylearntheparametersoftheoutputlayers[JaegerandHaas,2004].\n• Non-saturating Activations. Many common activation functions have a compact",
    "rangeofoutputs,e.g.,theSigmoidfunctionhasarangeof[0,1]. Theyarealsocalled\nsaturatingactivationfunctions6. Theuseofsaturatingactivationfunctionsoftenleads\ntothedecayofgradientsoverlayers,i.e.,thevanishinggradient. Itisthereforepromising\ntousenon-saturatingactivationfunctionsinstead,e.g.,theReLUfunction.\n• NormalizationofActivations. Saturatingactivationsmayalsoresultingettingstuck\nin a saturated region of outputs, and we need a large learning rate to escape from\nlocaloptimums[IoffeandSzegedy,2015]. Thus,thetrainingwouldbeunstable,and\nsubtlechangesininputsand/ormodelparameterswouldleadtoabigvarianceinmodel\nbehavior. A possible solution is to normalize the activations to reduce the variance,\ne.g., subtractingthemeanoftheactivationsinagroupofsamples(e.g., samplesina\nmini-batchoftraining),anddividingbytheirstandardderivation.\n4.2.3 Layer Stacking\nIfwethinkoftheapplicationofarecurrentunitasafunctionmappingavariablesequencetoa",
    "newvariablesequenceofthesamelength,itisnaturaltocomposethisfunctionwithanother\nfunctionofthesametype,orevenwithitself. ThismakesitveryeasytoextendRNNstodeep\nneuralnetworks: allyouneedistostackRNNs.\nLethl betheoutputofthel-threcurrentunitinthestackatpositioni. Wecanapplyanew\ni\nrecurrentunittohl,resultinginanewoutputatlevell+1\ni\nhl+1 = RNN(hl+1,hl) (4.14)\ni i−1 i\nwherehl+1 istheoutputofthepreviousstepatlevell+1. TomakeEq. (4.14)well-formed,\ni−1\nwetypicallydefineh0=x . Inotherwords,thestackstartsoffwiththewordvectorx ,thena\ni i i\nseriesofRNNoutputs(i.e.,h1,h2,h3,etc).\ni i i\nTo illustrate, Figure 4.3 (a) shows a stacked RNN for language modeling. We see that\n6Anactivationfunctionf(x)isnon-saturatingifandonlyifwhenx→∞(or−∞),f(x)→∞.Anactivation\nfunctionissaturatingifitisnotanon-saturatingactivationfunction.",
    "4.2 Recurrent Models 179\n··· ··· ··· ···\nLayerl+2 Layerl+2\nRNN RNN RNN RNN\nhl+1 hl+1\nLayerl+1 i Layerl+1 i\nhl+1 hl+1 hl+1 hl+1\ni−1 i i−1 i\nRNN RNN RNN RNN\nhl hl\ni i\nLayerl Layerl\nRNN RNN RNN RNN\n··· ··· ··· ···\npositioni−1 positioni positioni−1 positioni\n(a)A3-layerRNN. (b)A3-layerRNNwithresidualconnections.\nFigure 4.3: 3-layer RNNs (with and without residual connections). To stack RNN layers,\nwe feed the output of layer l to layer l+1. Thus the output of layer l+1 is given by\nhl+1=RNN(hl+1,hl). Linesinredcolorstandfortheresidualconnectionswhichdirectly\ni i−1 i\naddtheinputofalayertoitsoutput,resultinginhl+1=RNN(hl+1,hl)+hl.\ni i−1 i i\napplyingastackofrecurrentunitsisequivalenttocreatingmultiplelayersofRNNssimultane-\nously. However,therewouldbeariskofconfusionifwecallanunrolledrecurrentnetwork\na layer, as the term layer typically refers to a set of neurons receiving the same inputs in a\nfeed-forwardneuralnetwork. Hereweextendthetermlayertocoveramoregeneralconcept:",
    "agroupofneuronsthataretopologicallyplacedonthesamelevel. So,wesaythatthelanguage\nmodelinFigure4.3has3RNNlayers.\nStackingmultiplelayersofRNNs, webuildamodelwhichisdeeperbutmoredifficult\ntotrain. Thisdifficultyarisesinpartfromthebarriersofpassinginformationthroughmany-\nlayered RNNs. To make the training easier, a widely-used approach is to introduce skip\nconnections or residual connections into a multi-layer neural network [He et al., 2016a].\nThese connections are intended to leverage an additional path to allow information to skip\nlayers. AsdescribedinChapter2,theformofaresidualneuralnetworkisgivenby\nyl+1 = F(yl)+yl (4.15)\nwhereyl istheoutputoflayerl. ExtendingthisformulationtoEq. (4.14)leadstomulti-layer\nRNNswithresidualconnections,givenby\nhl+1 = RNN(hl+1,hl)+hl (4.16)\ni i−1 i i",
    "180 Chapter 4. Recurrent and Convolutional Sequence Models\nThe only difference from Eq. (4.14) is that we introduce the identity map of hl to the\ni\nright-handsideofEq. (4.16). Thus,theinputhl isdirectlyaccessiblefromlayerl+1. This\ni\ngreatlysimplifiesthewaythattheinformationflowsthroughtheneuralnetwork,andallows\nthesystemto“skip”layersinpropagatingerrors. Figure4.3(b)showsa3-layerRNNwith\nresidualconnections.\n4.2.4 Bi-directional Models\nThe use of RNNs enables us to formulate the problem of encoding a word sequence as a\nproblem of left-to-right generation of words. One advantage of this approach is that the\nmodelingofcontextwordsarisesnaturally: theoutputofanRNNunitinsomewaydescribes\nthe history words up to that point. This feature makes it very straightforward to model the\nprobabilitydistributionPr(w |w ,...,w ),aswecanuseh asarepresentationofthecontext\ni+1 1 i i\nw ...w ,thatis,Pr(w |w ,...,w )=Pr(w |h ).\n1 i i+1 1 i i+1 i",
    "The left-to-right generation is widely used in sequence generation, such as machine\ntranslation. Itcanbeviewedasaninstanceofautoregressiveprocesses(ARprocesses)in\nwhichthestateofavariableisdependentonthestateofthepreviousvariables[Chatfield,2003;\nBoxetal.,2015]7. However, suchamethodisnottheonlychoiceformodelingsequences.\nWedo noteven necessarilyrestrict ourselves tolanguage modelingfor traininga sequence\nencoder. Thisgivesrisetoaninterestingquestion: howcanwedevelopanencoderofword\nsequenceswithoutassumptionsregardingthepredictor? Answeringthequestionleadsusto\nisolatethelearningofthetextencoderfromaspecificNLPtask,andtoregarditasaseparate\ntaskwhoseresultcanbeappliedtomanyothersystems. Amoredetaileddiscussionisnotthe\nfocushereandweleaveittosubsequentchapters.\nWe now present a simple extension of the left-to-right sequence model by returning to\nRNNs. Notethatinsequencemodelingourdesireissomerepresentationoftheentiresequence.",
    "A problem with usual RNNs is that they are uni-directional models in which the context\nwordsfollowingw areabsent. Toconsiderboththeleftandrightcontextsofagivenword,\ni\nwecaninsteadusebi-directionalmodels. Figure4.4showsanexampleofthebi-directional\nRNN.Therearetwosub-models: aleft-to-rightRNNandaright-to-leftRNN.Theyhavethe\n7Asastochasticprocess,anautoregressiveprocessexpressesavariableattimetbyrelatingittothepastvalues\noftheprocessandthecurrentvalueofanerrorprocess[Chatfield,2003]. Formally,atimeseries{z 1 ,...,z T }\ndescribesanautoregressiveprocessoforderpifforanyt∈{p+1,...,T}\np\n(cid:88)\nzt = α i z t−i +ϵt (4.17)\ni=1\nwhere{α\n1\n,...,αp}aretheparametersoftheprocess,andϵtistheerrorattimet.Thisprocessiscalledregressive\nbecauseithasthesameformasthemultiplelinearregressionmodel.Theprefixauto-comesfromthewaywe\nregresszt: ztisdependentonitspastvaluesinsteadofadditionalindependentvariables. Onewaytointerpret\nlanguagemodelinginanautoregressiveprocessperspectiveistosimplytreat{z\n1\n,...,z\nT",
    "}asrepresentationsofa\nsequenceofwords{w\n1\n,...,w\nT\n}.Thus,wecangainsomeideaofpredictingwtusingpreviouswords{w\n1\n,...,wt}\nbyconsideringtheautoregressivepropertyoftheproblem.However,itshouldbenotedthatmostofthesequence\ngenerationmodelsusedinNLParenotmathematicallyequivalenttoEq. (4.17),althoughtheyareoftencalled\nregressivemodels.Forexample,theRNN-basedlanguagemodeldiscussedhereisnotalinearmodel.Rather,it\ntakeslayersofnon-linearitytodescribethecomplexrelationshipsamongwords.",
    "4.3 Memory 181\n−→ ←− −→ ←− −→ ←−\nh =[h ,h ] h =[h ,h ] h =[h ,h ]\ni−1 i−1 i−1 i i i i+1 i+1 i+1\n←−\n→−\nhi\nhi\nRNN RNN RNN\n··· ···\n(right-to-left) (right-to-left) (right-to-left)\nRNN RNN RNN\n··· ···\n(left-to-right) (left-to-right) (left-to-right)\nEmbedding Embedding Embedding\nw w w\ni−1 i i+1\nFigure4.4: Abi-directionalRNNmodel. Givenawordsequence,werunanRNNfromleftto\nrightandanotherRNNfromrighttoleft. Therefore,ateachpositionweobtainaleft-to-right\nrepresentationandaright-to-leftrepresentation. Theoutputistheconcatenationofthetwo\nrepresentationssothatitinvolvesboththeleftandrightcontexts.\nsamearchitecturebutworkinoppositedirections. Foreachinputwordw ,theleft-to-right\ni\n→−\nRNN outputs a vector representing the context {w ,...,w } (denoted by h ), and the right-\n1 i i\n←−\nto-leftRNNoutputsavectorrepresentingthecontext{w ,...,w }(denotedby h ). Wecan\ni m i\n→− ←−\nconcatenate h and h toobtainabi-directionalrepresentation\ni i\n→− ←−\nh = [h , h ] (4.18)\ni i i",
    "Thus,thebi-directionalRNNhasthesameformofoutputasthatoftheuni-directionalRNN,\nthatis,asequenceofvectors{h ,..,h }. Unliketheuni-directionalRNN,therepresentation\n1 m\nh heredescribesthecontextonbothsides.\ni\nFor a stronger model, the bi-directional RNN can be extended to a neural network of\nmultipleRNNlayers. Forexample,wecanrundeepRNNsintwodirectionsandcombinetheir\nresultsasinEq. (4.18). Suchmodelarchitectureshavebeenextensivelyusedinlanguageand\nspeechprocessingtasks,includingmachinetranslation[Wuetal.,2016],sentimentanalysis\n[Tang et al., 2015], POS tagging [Huang et al., 2015], speech recognition [Graves et al.,\n2013a;b],andsoon.\n4.3 Memory\nRNNscanbeappropriateforsequencelearninginwhichwesummarizeateachstepthepast\ninputsandthenmakesomepredictiononthissummaryofthe“history”. AbenefitofRNNsis\nthatwecanrepresentahistoryofarbitrarylengthasafixed-sizevector,andupdateitwhen",
    "182 Chapter 4. Recurrent and Convolutional Sequence Models\nnewinformationarrives. Inotherwords,wehaveamemory,thoughnotexplicitlydefined,to\nstoretheinformation. Nextweshowthatsuchamemorymechanismisgeneralandcanbe\nusedtoimprovesequencemodels.\n4.3.1 Memory as A System\nIn psychology, memory is the ability of the mind to retain and recall information. There\nare many cognitive models of psychology. A well-known model is the multi-store model\n[AtkinsonandShiffrin,1968]. Itdefinesmemoryasasystemconsistingofthreecomponents:\nshort-livedsensorymemory,short-termmemory,and long-termmemory. Thesensory\nmemoryretainsthesensoryinformationthatisveryquicklyceased,suchasimmediatedata\nfromthesensesofsightandsmell. Theshort-termmemorystoresinformationforalonger\ntimebutisnotpermanent. Anexampleoftheshort-termmemoryisthatwetrytomemorizea\nsequenceofdigits(e.g.,aphonenumber)butmayforgetitafterashortwhile. Thelong-term\nmemory is permanent. This also means that the information is retained indefinitely. For",
    "example,adultscanrememberdetailsoftheeventsthatoccurredintheirchildhood.\nGiventhiscategorization,thereappeartobeinterestingconnectionsbetweentheabove\nmodelofmemoryandtheneuralnetworkswediscusshere. Forexample,thestateofarecurrent\nunitcanbesimplythoughtofasashort-termmemory. Itmaintainsinformationuntilweget\ntothe endofa sequenceandwouldbereset ifweswitch toanewsequence8. Ontheother\nhand, the entire language model and associated parameters perform more like a long-term\nmemory: the language model is intended to learn and memorize some useful information\nabout probabilistic word prediction from the text, so that it can be used whenever we want\nto. Moreover,thereareotherconceptsthatmaystemfrompsychologybutareusedinseveral\ndifferentfields. Forexample,codingorencodingisreferredtoashowtheinformationisstored\ninamemory,durationisreferredtoashowlongtheinformationisstoredinamemory,and\ncapacityisreferredtoashowmuchinformationisstoredinamemory.",
    "InmachinelearningandNLP,wecangainanunderstandingofmemorybyconsideringit\nfromaninformationprocessingpointofview. Broadly,memorycanbeviewedasasystem\nthatwritesinformationtoa“storage”andreadsitwhenqueried. Ithasthefollowingfunctions.\n• Encoding. Theinputofthesystemisencodedinaformthatiseasytoprocess. Fortext\ninputs,thiscanbesimplythoughtofasthesameencodingprocessaswediscussinboth\nChapter3andthischapter: awordorasequenceofwordsisrepresentedasafeature\nvectororasequenceoffeaturevectors.\n• Update. Giventheencodedinformation,westoreitinthememory. Thisoperationis\ngenerallydependentontheorganizationofthememory. Forexample,onecantreata\ngroup of encoded items as a datastore with an indexing system. In this case, storing\nanitemrequiresfindingtherightplacetokeepit. Alternatively,onecanrepresentthe\nmemoryasasinglevectorofnumbers.\n• Retrieval. Thestoredinformationcanberetrieved. Thistypicallyinvolvesmatching",
    "8Anotherexplanationisthatthestateofarecurrentunitatstepimaycontainlittleinformationaboutveryearly\nsteps.",
    "4.3 Memory 183\neach item in the memory against an input query. If the memory is represented in a\nsimplerform,suchasavector,itmaynotbeexplicitlyretrieved,andwereturntheentire\nmemorywhenrequired.\nThese functions can be designed in many different ways, leading to a variety of NLP\nsystems. One simple example is information retrieval [Manning et al., 2008]. A typical\ninformationretrievalsystemindexesalargenumberofdocuments(orotherresources)and\nallowsuserstosearchforinterestedinformationinthiscollectionofdocuments. Toenable\nsearch,documentsarerepresentedinformsthatareconvenienttouse,forexample,wemay\nusethebag-of-wordsmodeltocomputethematchingscorebetweenadocumentandaquery,\nand may use the inverted index to make an efficient mapping of a document to its location\ninthestorage. Systemsofthistypecoverawiderangeofapplications,includingtranslation\nmemory,dialogue,summarization,documentclassification,andsoon.\nAnotherdesignchoicemadeformemorysystemsistoconsider,eitherpartiallyorfully,",
    "acontinuousformfortheabovecomponents. Onemethodistoencodeeachinputitemasa\nreal-valuedvector(e.g.,awordembedding)butusethesamemodulesofupdateandretrieval\nasinusualinformationretrieval-likesystems[Westonetal.,2015;Khandelwaletal.,2019].\nAn alternative method is to adopt differentiable functions for all the steps in building and\naccessingthememory. Thesemodelsaretypicallyimplementedusingneuralnetworksand\ntrainedusinggradientdescent[Sukhbaataretal.,2015;Gravesetal.,2014;Kumaretal.,2016;\nGravesetal.,2016;Milleretal.,2016]. Thisideamotivatesworkonexploringapproachesto\ncouplingneuralnetworkswithmemories,suchasend-to-endmemorynetworksandneural\nTuringmachines. Notethattheabovemodelsaresometimescalledexternalmemories,as\ntheyareusedasseparatemodulesworkingwithothersystems.\nMemorycanalsoworkasaninternalorhiddencomponentofasystem. Inthiscase,the\nmemoryistypicallyrebuiltforeachinputsample,andsoitcanberegardedasaninstanceofthe",
    "short-termmemory. Therearevariouswaysofusingthistypeofmemorytoimprovesequence\nmodels. In the remainder of the section, we will focus on using the memory mechanism\nin RNNs. In Chapter 5, we will see how the idea of memory is extended to model the\ncorrespondencebetweentokensoftwosequences.\n4.3.2 Long Short-Term Memory\nInthevanillaRNNpresentedinSection4.2.1,thesummarizationofthecontextwordswas\ngivenbytheoutputofarecurrentunit. Itimplicitlydefinesamemory,andthusenablesthe\npredictionbasedonpastinformationforanarbitraryduration. Thememorysimplycombines\ntherepresentationsoftheearlierhistoryw ...w andtheinputatthecurrentstepw ,butdoes\n1 i−1 i\nnotconsiderhowmuchinformationfromdifferentstepsshouldbesqueezedintoafixed-length\nrepresentation. Aproblemwiththismodelisthat,iflong-termdependenciesarerequiredfor\nprediction,memorymayprovidelittleinformationaboutit,anditmaybehardtolearnthese\ndependenciesthroughback-propagation[Bengioetal.,1994;Pascanuetal.,2013]. Amore",
    "powerfulapproach,therefore,istocomputewhatshouldberetainedateachstep,andtoletthe\nmodellearntodecidewhethertomemorizeorforget.",
    "184 Chapter 4. Recurrent and Convolutional Sequence Models\nLongshort-termmemory(LSTM)isperhapsthebest-knownvariantofRNNstoaccom-\nplishtheabovegoal[HochreiterandSchmidhuber,1997]. ThebasicideaofLSTMisthata\nrecurrentunitcanlearntomemorizeusefulthingsandforgetunusefulthingsbymaintainingan\nexplicitmemory[Gersetal.,2000]. Tothisend,thevanillarecurrentunitisreplacedwithan\nLSTMunitthatismadeupofanoutputvector(callitarecurrentcell),amemoryvector(call\nitamemorycell),andthreegatestocontroltheinformationflowinsidetheLSTMunit. Asan\nextensiontoRNNs,anLSTMnetworkdealswithaninputsequenceasusual: itstartswith\nsomeinitialstates,andthenrepeatedlytakesaninputandoutputsavector. Akeydifference\nbetweenLSTMnetworksandRNNsisthattheLSTMunitofstepitakesboththerecurrent\ncellandthememorycellofitspreviousstep. TheformofanLSTMunitisgivenby\n(h ,c ) = LSTM(h ,c ,x ) (4.19)\ni i i−1 i−1 i\nwhere h i ∈Rd h is the recurrent cell of step i, c i ∈Rd h is the memory cell of step i, and\nx\ni",
    "∈Rde istheinputofstepi. GivenLSTM(·),applyingtheLSTMmodelisstraightforward.\nWe simply repeat the call of LSTM(·) for the inputs {x ,...,x } and obtain the outputs\n1 m\n{h ,...,h }. This resembles the way we use vanilla RNNs, making it very easy to extend\n1 m\nLSTMtomulti-layermodels(seeSection4.2.3)andbi-directionalmodels(seeSection4.2.4).\nWecandivideLSTM(·)intothreesteps.\n• Step1: Forget. Assumingthatc containstheinformationthatthemodelmemorizes\ni−1\natstepi−1,weneedtodeterminehowmuchinformationinc isdiscardedinbuilding\ni−1\nc . Todothis,agateisusedtocontroltowhatextentweforgetforeachdimensionof\ni\nc . Theforgetgateisdefinedtobe:\ni−1\nf = Sigmoid(h U +x V +b ) (4.20)\ni i−1 f i f f\nwheref i ∈[0,1]d h isavectorwiththesamenumberofdimensionsasc i−1 . TheSigmoid\nfunctionmapstheinputdatatotherange[0,1]. Thus,anentryoff indicateshowmuch\ni\nis preserved for the same entry of c . Taking this further, f ⊙c describes the\ni−1 i i−1",
    "memorythatisleftoutafterpassingthroughtheforgetgate. SeeFigure4.5(a)foran\nillustrationoftheforgetgateintheLSTMunit.\n• Step2: Update. Nextweupdatethememorybyconsideringboththepreviousstate\nofthememory(i.e.,c )andtheinputoftheLSTMunit(i.e.,x andh ). Wefirst\ni−1 i i−1\ncombinex andh usingasimpleneuralnetwork,likethis\ni i−1\ncˆ = TanH(h U +x V +b ) (4.21)\ni i−1 c i c c\ncˆ canbetreatedasthenewinformationweintendtoaddtothememoryatstepi. Again,\ni\nweneedawaytocontroltheamountofinformationcomingintothememory. Hencewe\ndefineaninputgateas\ng = Sigmoid(h U +x V +b ) (4.22)\ni i−1 g i g g",
    "4.3 Memory 185\nThis equation is similar to Eq. (4.20) but with different parameters. We then define\ng ⊙cˆ tobetheactualnewinformationthatweareinterestedin. Takingbothf ⊙c\ni i i i−1\nandg ⊙cˆ ,thememorycellatstepiisgivenby\ni i\nc = f ⊙c +g ⊙cˆ (4.23)\ni i i−1 i i\nIn other words, we forget something old in c and memorize something new in cˆ .\ni−1 i\nSeeFigure4.5(b)foranillustrationoftheupdatestep.\n• Step 3: Output. In the last step we generate the output h based on the memory c .\ni i\nInsteadofcopyingc toh ,wefeedc toahyperbolicfunctionandmultiplyitsresult\ni i i\nwiththeoutputgate. LikeEqs. (4.20)and(4.22),theoutputgateisgivenby\no = Sigmoid(h U +x V +b ) (4.24)\ni i−1 o i o o\nThen,theoutputoftheLSTMunitisdefinedtobe\nh = o ⊙TanH(c ) (4.25)\ni i i\nSeeFigure4.5(c)foranillustrationoftheoutputstep.\nThe LSTM model is parameterized by U f ,U c ,U g ,U o ∈ Rd h ×d h, V f ,V c ,V g ,V o ∈\nRde×d h, and b\nf\n,b\nc\n,b\ng\n,b\no\n∈Rd h. Compared with vanilla RNNs, additional parameters are",
    "introducedherebecauseoftheuseofthreegates. Inpracticeonecanimplementtheminmany\ndifferentways,e.g.,usingactivationfunctionsotherthanSigmoid(·)andTanH(·),removing\nthe bias terms b , b , b , and b , and so on. Training LSTM models follows the standard\nf c g o\nparadigmoftrainingRNN-basedmodels. Forexample,wecanbuildanLSTM-basedlanguage\nmodelandtrainitbyusingthemethodspresentedinSection4.2.2.\n4.3.3 Gated Recurrent Units\nAbove,wesawtheimportantroleplayedbythegateunitsandthememorycell. Ingeneralthe\nuseoftheseneuralnetworksmakesthemodelcomputationallymoreexpensive. Analternative\nto LSTM in a cheap case, namely gated recurrent units (GRUs), uses a simplified model\nstructurewithfewergatefunctions[Choetal.,2014;Chungetal.,2014]. UnlikeLSTM,a\nGRUdoesnothaveamemorycellso,asanRNNunit,ittakesboththepreviousstatevector\nh andthecurrentinputvectorx ,andproducesthecurrentstatevectorh .\ni−1 i i\nInGRUs,therearetwogateunits: theresetgateandtheupdategate. Theresetgate,as",
    "thenamesuggests,isusedtoreset(orrescale)thestateoftheGRU(i.e.,h ). Followingthe\ni−1\ngatefunctionsusedinLSTM,theresetgateisdefinedtobe\nr = Sigmoid(h U +x V +b ) (4.26)\ni i−1 r i r r\nwherer i ∈[0,1]d h isavectorofscalars,eachdimensiondescribinghowmuchinformationin\nthecorrespondingdimensionofh isretained. Thus,wehavearepresentationofretained\ni−1",
    "186 Chapter 4. Recurrent and Convolutional Sequence Models\nc\ni−1 × f ⊙c +\ni i−1\nGate♯1 ForgetGate\n× TanH\n+\nf\ni\nGate♯1 Gate♯2 FFN Gate♯3\nh\ni−1\nx\ni\n(a)Theforgetgate.\nc\ni−1 × + c i\nGate♯1 ForgetGate\n× g i ⊙cˆ i TanH\n+\nGate♯2 InputGate\ng\ni\ncˆ\ni\nGate♯1 Gate♯2 FFN Gate♯3\nh\ni−1\nx\ni\n(b)Theinputgate.\nh\ni\nc c\ni−1 × + i\nGate♯1 ForgetGate\n× TanH\nTan\n⊙\nH(ci)\n×\nGate♯2 InputGate\noi\nGate♯1 Gate♯2 FFN Gate♯3 Gate♯3 OutputGate\nh\nh i\ni−1\nx\ni\n(c)Theoutputgate.\nFigure4.5: ThearchitectureoftheLSTMunit. Atstepi,ittakestheinputx ,andthenupdates\ni\nboththememorycell(c →c )andtherecurrentcell(h →h ). Thisprocessinvolves\ni−1 i i−1 i\nthreegates: theforgetgatecontrols how muchinformationinc isretained atstepi, the\ni−1\ninputgatecontrolshowmuchinformationinc andx isretainedatstepi,andtheoutput\ni−1 i\ngatecontrolshowmuchinformationinc isusedtoformh .\ni i\ninformation\nυ = r ⊙h (4.27)\ni−1 i i−1\nTakingboththeretainedinformationυ andthecurrentinputx ,anewstatevectorisdefined\ni−1 i",
    "4.4 Convolutional Models 187\ntobe\nhˆ = TanH(υ U +x V +b ) (4.28)\ni i−1 h i h h\nTheupdategateisthengivenby\nu = Sigmoid(h U +x V +b ) (4.29)\ni i−1 u i u u\nu can be thought of as a coefficient vector which could be used to control the trade-off in\ni\nchoosingthenewstatevectorhˆ ortheoldstatevectorh . Finally,theoutputoftheGRUis\ni i−1\ndefinedasalinearinterpolationofhˆ andh\ni i−1\nh = u ⊙hˆ +(1−u )⊙h (4.30)\ni i i i i−1\nFigure 4.6 shows how the information flows in a GRU unit. The parameters here are\nU r ,U h ,U u ∈ Rd h ×d h, V r ,V h ,V u ∈ Rde×d h, and b r ,b h ,b u ∈ Rd h. Therefore, the GRU\nmodel is smaller than the LSTM model because of the use of fewer gate units. Note that\nremoving the memory cell makes GRUs more efficient. In this case, the role of memory is\nimplicitlyplayedbyGRU’soutputh ,andwemaintainitbymemorizingmore“important”\ni\ninformation.\n4.4 Convolutional Models\nInthissectionwedescribeanothertypeofmodelforsequencemodeling,calledconvolutional",
    "neural networks (CNNs). Our description is mostly standard, but not a full introduction to\nthenumerousvariantsofCNNsandcutting-edgetechniques. Inparticular,wefocusonusing\nCNNstodealwithsequentialdataandpresentingsomerefinements.\n4.4.1 Convolution\nCNNsfeaturetheirshared-weightarchitecturesbywhichakernelorfilterslidesovertheinput\ndataandproducesamapoffeatures. Theideaisthatthefilteronlyreceivessignalsfroma\nrestrictedregionofdataatatime(callitthereceptivefield),andcomputestheweightedsum\nof these input signals. To illustrate this, we follow the convention that a filter in CNNs is\ngenerallyusedtodealwith2Ddata. Considera3×3datamatrix\n \n1 9 7\nA = 3 1 2  (4.31)\n \n0 1 −1\nanda2×2filterwithaweightmatrix\n(cid:34) (cid:35)\n2 0\nW = (4.32)\n2 2",
    "188 Chapter 4. Recurrent and Convolutional Sequence Models\n× FFN hˆ\nυ i\ni−1\nGate♯1 : ResetGate\nr\ni\nGate♯1\nh\ni−1\nx\ni\n(a)Theresetgate.\nui⊙hˆ i h i\n× FFN × +\n1−ui Gate♯1 : ResetGate\n1− ×\n(1−ui)\nu\n⊙hi−1\ni\nGate♯1 Gate♯2\nGate♯2 : UpdateGate\nh\ni−1\nx\ni\n(b)Theupdategate.\nFigure4.6: ThearchitectureoftheGRU.UnliketheLSTMunit,theGRUdoesnotinvolve\na memory cell, and thus follows the same input and output forms of a standard RNN unit.\nThere are two gates in the GRU. The reset gate controls how much information in h is\ni−1\nretainedatstepi. Theretainedinformationisthentakentofuzewiththeinputx ,generating\ni\nthecandidateoutputhˆ . Theupdategateseeksabalancebetweenhˆ andh incomputing\ni i i−1\nthefinaloutputoftheGRU.\nWecanapplythefiltertoevery2×2sub-matrixofA(therearefour2×2sub-matriceshere),\nand compute the sum of the 2×2 entries weighted by W. For example, consider the 2×2\nsub-matrixintheupperleftcornerofA. Theoutputofthefilterisgivenby\n(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)\n1 9 1 9 2 0",
    "Conv( ,W) = Conv( , )\n3 1 3 1 2 2\n= 1×2+9×0+3×2+1×2\n= 10 (4.33)\nConv(·)definesaconvolutionoperationthatsumstheentriesoftheelement-wiseproduct\nofthetwomatrices. Theconvolutionoperationcanbeextendedtocovertheentireinputmatrix",
    "4.4 Convolutional Models 189\nbyslidingthefilteroverit,asfollows\n \n1 9 7\nConv(A,W) = Conv(3 1 2 ,W)\n \n0 1 −1\n (cid:34) (cid:35) (cid:34) (cid:35) \n1 9 9 7\nConv( ,W) Conv( ,W)\n 3 1 1 2 \n \n=  (cid:34) (cid:35) (cid:34) (cid:35) \n 3 1 1 2 \nConv( ,W) Conv( ,W)\n0 1 1 −1\n(cid:34) (cid:35)\n10 24\n= (4.34)\n8 2\n(cid:34) (cid:35)\n10 24\nTheoutput isalsocalledthefeaturemapforthefilterWonA. Sometimes,\n8 2\ntheconvolutionoperationConv(A,W)iswrittenasA∗W wherethesymbol∗standsfor\ntheconvolutionproduct.9\nNowletusconsideramoregeneraldescriptionofconvolutioninCNNs. SupposethatA\nisamulti-dimensionaldataarray. Afilterdefinesawindow(orreceptivefield)onA. Wecan\nmovethewindowonAindifferentdirections. Thisresultsinasetofdataarrays,denotedby\nΩ. Eachdataarraya ∈ΩisformedbytheelementsfromthecorrespondingregionofA. For\np\n(cid:34) (cid:35) (cid:34) (cid:35) (cid:34) (cid:35)\n1 9 9 7 3 1\nexample,therearefoursub-matricesinEq. (4.34)): a = ,a = ,a = ,\n1 2 3\n3 1 1 2 0 1\n(cid:34) (cid:35)\n1 2",
    "anda = . Also,wesupposethefilterisparameterizedbyaweightarrayWwiththe\n4\n1 −1\nsamesizeofa,i.e.,|a |=|W|. TheresultofapplyingthefiltertoAisanarrayoffeatures\np\n(cid:104) (cid:105)\nConv(A,W) = v ... v (4.37)\n1 |Ω|\n9Inmathematicalanalysis,giventwointegrablefunctionsf(·)andg(·),convolutiondefinesanewintegrable\nfunctionf∗g(·)todescribetheintegraloff(·)weightedbyreflected,shiftedg(·).Moreformally,theconvolution\nforcontinuousfunctionsisdefinedas\n(cid:90)\nf∗g(x) = f(y)g(x−y)dy (4.35)\nR\nwheref(y)isthefunctionthatweareconcernedwith,andg(x−y)istheweightfunctionwhichistranslatedby\nreflectingg(y)alongthey-axisandthenshiftingitbyx.Aspecialcaseisthatxandyarebothintegers.Inthis\ncase,wecandefinef∗g(·)as\n(cid:88)\nf∗g(x) = f(y)g(x−y) (4.36)\ny\nwhichisthebasicformofEq.(4.33).InCNNs,x,yandx−ycanbeseenasindicesofitemsindataarrays.f(y)\nisadataitemintheinputarray,andg(x−y)isthecorrespondingweightinthefilter. ByusingEq. (4.36),we\ncalculatethevalueoftheitemindexedbyxintheoutputarrayf∗g(x)(i.e.,thefeaturemap).",
    "190 Chapter 4. Recurrent and Convolutional Sequence Models\nEachfeaturev isgivenby\np\nv = Conv(a ,W)\np p\n= a ·W\np\n|W|\n(cid:88)\n= a (k)·W(k) (4.38)\np\nk=1\nwherea (k)andW(k)arethek-thelementsofa andW,respectively. Notethatthearray\np p\n(cid:104) (cid:105)\nv ... v canbeorganizedintodifferentshapes,suchasamatrixora3Dtensor,though\n1 |Ω|\ntheyareessentiallythesamethingfromthedatastorageviewpoint. Forexample,for2Dinput\ndataanda2Dfilter,thefeaturemapisamatrixlikeEq. (4.34).\nFurthermore,weneedtoconsidertwothingstomakethemodelpractical. First,weneed\ntospecifythestrideofeachmoveofthefilteroverA. Intheaboveexample,wesimplyuse\nstride=1. Bychoosingalargerstride,wecancompressAintoasmallernumberoffeatures.\nSecond, in some situations, to ensure that the feature map has a desired size, we can add\ndummyelements(orpaddings)aroundtheinputdata. Acommonmethodofpaddingistoset\nzerostotheelementsoutsidetheinputregion. Forexample,considera2×2datamatrix.\n(cid:34) (cid:35)\n1 9\nA = (4.39)\n7 3",
    "Wecanaddzero-valuedentriesaroundittoobtaina4×4matrix,likethis\n \n0 0 0 0\n0 1 9 0\nA =   (4.40)\npadding 0 7 3 0\n \n0 0 0 0\nUsingthesamefilterasinEq. (4.33)withstride=1,wehavea3×3featuremap\n \n2 20 18\nConv (A ,W) = 14 22 24 (4.41)\nstride=1 padding  \n0 14 6\nIfstride=2,thenwewouldhaveafeaturemapwiththesamesizeoftheinputdata\n(cid:34) (cid:35)\n2 18\nConv (A ,W) = (4.42)\nstride=2 padding\n0 6\n4.4.2 CNNs for Sequence Modeling\nFollowingtheformulationintheprevioussections,weassumethattheinputofasequence\nmodelisavectorsequencex ...x andtheoutputisanothervectorsequenceh ...h . For\n1 m 1 m\nexample,wecanthinkofx\n1\n...x\nm\nasamatrixX∈Rm×de inwhichthei-throwvectoristhe",
    "4.4 Convolutional Models 191\nh6\nh5\nh4\nh3\nh2\nh1\nFilter1 Filter2\n0 padding\n0\nx6\nx5\nFeatures\n0\nx1\nx2\nx\nIn\n3\nx\npu\n4\nt\nSequence\n0\nFigure 4.7: Two filters applied to a sequence of word vectors. The inputinvolves ten word\nvectors(wordsx ...x andtwopaddingwordsoneachofthetwoendsofthesequence). Each\n1 6\nword vector has 6 dimensions, and so, the input is a 10×6 matrix. Filter 1 has a receptive\nfieldofsize3×6. Byslidingitovertheinputmatrix,weobtainasequenceofoutputs,each\ncorrespondingtoaposition(i.e.,asequenceof6outputs). Similarly,weapplyfilter2tothe\ninputsequenceandobtainanothersequenceofoutputs. Thetwooutputsequencesarethen\norganizedasa2×6matrixinwhichthei-throwvectorish .\ni\nrepresentationofthei-thwordofthesequence.\nIt is straightforward to perform convolution on X. Since x is just a set of unordered\ni\nfeatures,itisnotnecessarytoslideafilteroverdifferentfeatures. Hencewecanuseareceptive\nfield of size r×d , and consider all the dimensions of x in the convolution operation. In\ne i",
    "practicalapplications,theremightbemultiplefiltersforrepresentingtheinputsindifferent\naspects. Forexample,onecanuseafilterwithalargereceptivefieldtoinvolvemorecontexts\ninmodeling,anduseafilterwithasmallreceptivefieldtoconcentratemoreonlocalfeatures.\nSeeFigure4.7fortwofiltersthatareusedtodealwithasequence.\nTo distribute features to {h ,...,h }, we can associate each application of a filter to a\n1 m\npositionofthesequence. Toensuretheinputandoutputsequencesareofthesamelength,a\npaddingvectorisaddedtoeachendofthesequence. Thefollowingshowstheinputandoutput",
    "192 Chapter 4. Recurrent and Convolutional Sequence Models\nofaCNNforanexamplesequence.\nPosition Input ReceptiveField Output\n0 x (=0) N/A N/A\n0\n1 x {x ,x ,x } h\n1 0 1 2 1\n2 x {x ,x ,x } h\n2 1 2 3 2\n3 x {x ,x ,x } h\n3 2 3 4 3\n4 x {x ,x ,x } h\n4 3 4 5 4\n5 x (=0) N/A N/A\n5\nAnactivationfunctionistypicallyusedtointroducesomenon-linearitytothefinaloutput.\nIn this way, we build a standard convolutional layer which can be viewed as a sequence of\nfullyconnectedneural networks, each taking inputsfroma fixed-sizewindow. Forthei-th\nposition,theoutputoftheconvolutionallayerisgivenby\nv = ψ(Conv(a ,W)) (4.43)\ni i\nwherea istheinputsinthereceptivefield10,andWistheparametersofthefilter. Insituations\ni\ninvolving multiple filters (say d h filters), we have a set of parameters {W(1),...,W(d h )}, a\nset of activation functions {ψ(1),...,ψ(d h )}, and a set of inputs {a (1) ,...,a (d h ) }. Each tuple\ni i\n(W(k),ψ(k),a (k) )givesanoutputby\ni\nv (k) = ψ(k)(Conv(a (k) ,W(k))) (4.44)\ni i\n(k)",
    "Notethatv issimplyanentryofh . Thus,h canbewrittenas\ni i i\n(cid:104) (cid:105)\nh = v (1) ... v (d h ) (4.45)\ni i i\nManyCNN-basedsystemsofpracticalinterestcomprisetwoormoreconvolutionallayers.\nThesimplestwaytoachievethisislayerstacking,asinmulti-layerRNNs(seeSection4.2.3).\nThatis,wetreattheoutputofaconvolutionallayerastheinputofthefollowinglayer. See\nFigure4.8foranexampleofaCNNinvolvingthreeconvolutionallayers. Oneofthebenefits\nofmulti-layerCNNsisalargerscopeforrepresentation. AsseenfromFigure4.8,aneuronin\nlayer1connectsthreeinputvectors,whileaneuroninlayer3connects,thoughnotdirectly,\nseven input vectors. Since neurons of the higher-level layers receive and process signals\nfromalargerspanofthesequence,theyareexpectedtoproduceahigher-levelrepresentation\nof the sequence and to be able to deal with more difficult problems, such as long-distance\ndependencies.\nInsomeapplications,weneedafixed-length,low-dimensionalrepresentationoftheentire",
    "sequence. Acommonwayistoaddapoolinglayertomerge{h ,...,h }intoasinglevector.\n1 m\nForexample,wecanselectthemaximumvalue(i.e.,max-pooling)oraveragethevalues(i.e.,\n10Foranr×dereceptivefield,a iisdefinedtobe{x ⌈i−r⌉ ,...,x ⌈i+r−1⌉ }or{x ⌊i−r⌋ ,...,x ⌊i+r−1⌋ }.\n2 2 2 2",
    "4.4 Convolutional Models 193\nScopeforRepresentation\nLayer3 CNN CNN CNN CNN CNN\nLayer2 0 CNN CNN CNN CNN CNN 0\nLayer1 0 CNN CNN CNN CNN CNN 0\nInput 0 x 1 x 2 x 3 x 4 x 5 0\nPadding Padding\nFigure 4.8: A CNN with 3 convolutional layers (stride=1 and r=3). Each layer takes a\nsequenceofvectorsandproducesanothersequenceofvectors. Inthisprocessafiltermoves\novertheinputandperformstheconvolutionoperationineachmove. Inlayer1,thereceptive\nfieldofthefilterisaregionofthreeinputitems(seegreenshadows). Thehigheralayeris,\nthelargerreceptivefieldafilterhas. Forexample,inlayer3,anapplicationofthefiltercanat\nmostcovertheentireinputsequence(seeorangeshadows).\nmax-pooling) along each dimension. This is a generic method in machine learning and is\napplicabletomostofthesequencemodelsdiscussedinthisbook.\n4.4.3 Handling Positional Information\nOneinterestingpropertyofCNNsistheirabilitytobalancecomplexityandefficiency. This\nisachievedbyrestrictingfullconnectivitytoonlyasmallregionoftheinputdata. Thisalso",
    "leads us to describe a convolution layer using the same mathematical form of a layer in a\nfully-connectedneuralnetwork: theoutputofaneuronissometransformationoftheweighted\nsumoftheinputnumbers. Despitethesimplicityinherentinmodeling,aproblemwithsuch\nmodels is that the order of inputs is completely ignored. An interesting point, however, is\nthat,ifwerestrictourselvestosequencemodeling,thisshouldnotbeaproblembecausethe\noutputofthemodelisitselfasequence. Itseemsreasonabletoassumethattheoutputsequence\npreservestheorderinginformationoftheinputsequence. Ontheotherhand,applyingCNNs\ntosequentialdatadoesnotguaranteeaone-to-onemappingbetweentheinputandoutputitems.\nTechnically, h is not simply a representation of x . It instead encodes a window of inputs\ni i\ncenteredatx . This,inturn,makestheproblemverycomplicated,sinceitisdifficulttowork\ni\noutfromh howthoseinputsareordered.\ni",
    "194 Chapter 4. Recurrent and Convolutional Sequence Models\nExplicitly modeling word orders is very important in NLP, and has been extensively\nstudied in tasks like machine translation [Lopez, 2008; Koehn, 2010]. For neural network-\nbasedmodels,onemayaddresstheproblembyresortingtoorder-sensitivemodelarchitectures\nlikeRNNs. Amorepopularapproachinrecentsystemsistodevelopapositionalencoding\nsub-modelandincorporateitintoexistingsequencemodels[Gehringetal.,2017b;Vaswani\netal.,2017;Shawetal.,2018;Dufteretal.,2022]. Formally,wesaythattheinputatpositioni\nisacombinationoftheoriginalinputx andthepositionalencodingofi(denotedbyPE(·)):\ni\nxp = Merge(x ,PE(i)) (4.46)\ni i\nwhere Merge(·) combines x and PE(i) in some way. The use of positional encoding is\ni\nstraightforward: all you need is to replace {x ,...,x } with {xp ,...,xp } in a sequence\n1 m 1 m\nmodel. So,thisapproachismodel-free.\nInthissubsection,wepresentseveralversionsofPE(·)andwaystocombinethemwith",
    "x . NotethatthefollowingdiscussionisnotspecifictoCNNs. Weconsideritherebecause\ni\npositionalencodingisusefulformodelsthatareinsensitivetotheorderofinputs,andCNNs\nareagoodexampletoseehowitisused[Gehringetal.,2017a;b]. InChapter6,wewillseean\napplicationofpositionalencodinginTransformerwhichisastate-of-the-artneuralmodelin\nmanyareas.\n1. Offset-based Positional Encoding\nThesimplestwaytodescribeapositioniistojustleaveitasitis. Thiscanbeformulatedas\nthe“distance”fromareferencepoint\nPE(i) = i−i (4.47)\n0\nwherei isanintegerindicatingwherewestartcounting. Ifi =0,PE(i)=igivesthenormal\n0 0\nwaytodefineaposition. NotethatPE(i)couldbeanegativenumberifi >i. Inthissense,\n0\nPE(i)isnotarealdistancebutitisfinewithconsideringitasafeatureinamachinelearning\nsystem. Todesignanon-negativemeasure,theright-handsideofEq. (4.47)canbedefinedas\nanabsolutevalue\nPE(i) = |i−i | (4.48)\n0\nTreating positions as simple integers leads to unbounded, discrete positional encoding.",
    "A more desirable method might be to use a continuous representation in a range of values,\nbecauseitallowsthesystemtoworkwithinasamplespacethatissmoothandeasytooptimize.\nA simple way to do this is normalization. For example, dividing i−i by some maximum\n0\nvalue,weobtainanormalizedversionoftheoffset-basedencoding\ni−i\n0\nPE(i) = (4.49)\ni −i\nmax 0\nForexample,wecanseti =themaximumpossiblelengthofthesequenceandi =0,\nmax 0",
    "4.4 Convolutional Models 195\nsothatPE(i)choosesitsvaluein[0,1]. Anothercommonchoiceistoseti =m(i.e.,the\nmax\nlengthoftheinputsequence)anddefinePE(i)asaratiowhosevaluevariesasmchanges.\nTo make use of these scalar positions, it is straightforward to enrich the original input\nvectorsbyaddingnewdimensions,providedtheycanbeviewedasnewfeatures. Thus,xp is\ni\ngivenby\nxp = [x ,PE(i)] (4.50)\ni i\nwhere[·]standsfortheconcatenationoperation.\n2. Sinusoidal Positional Encoding\nThenextobviousstepistorepresentpositionsasvectorsinsteadofscalars. Althoughvector-\nizingtherepresentationsofpositionssoundscomplicated,asimpleideaistouseacarrying\nsystemwhichdescribeshowanaturalnumberisexpressedbyapolynomialwithrespecttoa\nbase[Kernes,2021]. Forexample,icanbewrittenas\nk (cid:88)max\ni = a(i,k)bk (4.51)\nk=0\nwherea(i,k)isthek-thdigit,k +1isthemaximumnumberofdigits,andbisthebaseof\nmax\nthesystem. Thecarryingoccurswhena(i,k)reachesb: weincreasea(i,k+1)by1androll",
    "backa(i,k)to0. Inthiswaywecanchangea(i,k)withaperiodofbk,thatis,a(i,0)changes\nwithaperiodofb0,a(i,1)changeswithaperiodofb1,a(i,2)changeswithaperiodofb2,and\nsoon.\nUsingthissystem,icanberepresentedasavector\n(cid:104) (cid:105)\nPE(i) = a(i,0) a(i,1) ... a(i,k ) (4.52)\nmax\n(cid:104) (cid:105)\nFor example, when b=2, PE(11)= 1 1 0 1 . However, in Eq. (4.52), PE(i) is still\na discrete function. As discussed throughout this book, we may want a continuous vector\nrepresentation that can describe intermediate states between discrete events. Considering\na(i,k) as a periodic function, a common choice is the sine function. Thus a(i,k) can be\nre-defined,asfollows\na(i,k) = sin(i·ω ) (4.53)\nk\nThisfunctionhasanamplitudeof1andaperiodof 2π. Usingananalogousformofperiodsto\nω\nk\nthatusedinEq. (4.51),wedefineω as\nk\n1\nω = (4.54)\nk (b model )k/d model\nwhere b > 0 and d > 0 are hyper-parameters of the model. Obviously, we have\nmodel model\n2π < 2π <...< 2π .\nω0 ω1 ω\nkmax",
    "196 Chapter 4. Recurrent and Convolutional Sequence Models\nFigure4.9: AheatmapofthepositionalembeddingmodelofEqs. (6.14)and(6.15)(b =\nmodel\n10,000andd =512). Considerapositioni(i.e.,thei-throw),thenmoveanotherposition\nmodel\nj fromiupwardsordownwards. Intuitively,wheniandj arecloser,thecorrespondingrow\nvectorsaremoresimilar. Bycontrast, whenj movesawayfromi, thesimilarityisnotthat\nobvious. This property helps explain the idea behind the positional embedding model: the\n“distance”betweentwopositionsisimplicitlymodeledbycomparingtheirmulti-dimensional\nrepresentations.\nSimilarly,wecandefinea(i,k)viathecosinefunction\na(i,k) = cos(i·ω ) (4.55)\nk\nTakingbothEqs. (4.53)and(4.55),wecreateanewrepresentationofi,asfollows\n(cid:104) (cid:105)\nPE(i) = sin(i·ω ) cos(i·ω ) ... sin(i·ω ) cos(i·ω ) (4.56)\n0 0 kmax kmax\nVaswanietal.[2017]instantiatedtheaboveformbysettingb =10,000. LetPE(i,k)\nmodel\nbe the k-th dimension of PE(i). Vaswani et al. [2017]’s version of positional encoding is\nwrittenas\n1",
    "PE(i,2k) = sin(i· ) (4.57)\n100002k/d\nmodel\n1\nPE(i,2k+1) = cos(i· ) (4.58)\n100002k/d\nmodel\nChoosing b =10,000 is empirical. One can adjust it for specific tasks. Figure 4.9\nmodel\nplotsthepositionalencodingfordifferentpositions. Weseethat,whenk becomeslarger,the\nchangeofthecolorfollowsalargerperiod.\nNote that Eqs. (6.14) and (6.15) have a useful property that PE(i+µ) can be easily",
    "4.4 Convolutional Models 197\nexpressedbyalinearfunctionofPE(i)foragivenoffsetµ11\nPE(i+µ,2k) = PE(i,2k)·PE(µ,2k+1)+\nPE(i,2k+1)·PE(µ,2k) (4.61)\nPE(i+µ,2k+1) = PE(i,2k+1)·PE(µ,2k+1)+\nPE(i,2k)·PE(µ,2k) (4.62)\nTheresultingbenefitisthattheencodingcansomewhatmodelrelativepositions. Thatis,the\nstateatpositioni+µcanbedescribedbystartingwithiandthenappendingitwiththeoffset\nµ.\nWhen applying the sinusoidal positional encoding, one way is to use Eq. (4.50) to\nconcatenatex andPE(i). InVaswanietal.[2017]’swork,theyinsteadassumePE(i)tobea\ni\nvectorofthesamesizeasx (i.e.,|PE(i)|=|x |=d ),andaddPE(i)tox ,likethis\ni i e i\nxp = x +PE(i) (4.63)\ni i\nThis sinusoidal additive model has been the basis of many positional encoding approaches\n[Dehghanietal.,2018;Likhomanenkoetal.,2021;Suetal.,2021].\n3. Learnable Positional Encoding\nTheresultofsinusoidalpositionalencodingisalookuptableC\nPE\n∈Rmmax×de (wherem\nmax\nisthemaximumsequencelengthwecanchoose)\n \nPE(1)\nC =  ...  (4.64)\nPE  \nPE(m )\nmax",
    "Inthistable,eachrowvectorPE(i)correspondstotheembeddingofapositioni. These\nvectors,asdescribedabove,arecomputedbasedonsomeassumptionsandheuristicalgorithms.\nAnalternativeapproachistotreatvectorsofpositionsasparametersofthemodelandlearn\nthemasusual. Inthiscase,bothwordembeddingsandpositionembeddingscanbetrainedin\nthesamemanner. SeeChapters2and3formoreinformationaboutlearningwordembeddings\ninneurallanguagemodels.\nOne last note on positional encoding. What we have shown in this section can broadly\nbecharacterizedasanabsolutepositionalencodingparadigm: apositionisdescribedbyits\nlocationinacoordinatesystem. Anotherconceptthatisworthexploringisrelativepositional\nencoding[Shawetal.,2018]. Forexample,wecanextendEq. (4.48)todefinethedistance\n11Onecanderivethesebytaking\nsin(α+β) = sin(α)·cos(β)+cos(α)·sin(β) (4.59)\ncos(α+β) = cos(α)·cos(β)−sin(α)·sin(β) (4.60)",
    "198 Chapter 4. Recurrent and Convolutional Sequence Models\nbetweentwopositionsiandj\nPE(i,j) = |i−j| (4.65)\nIn this case, the positional encoding is no longer an attribute of the i-th input but some\nrepresentationofthedistancerelativetoareferencepositionj. Infact,mostofthemethods\nforrelativepositionalencodingarevariantsonathemeinwhichpositionsaredescribedby\ntheirpair-wiserelationships. Thisformsthebasisofseveralmodelsofthistypeaswewillsee\ninChapter6.\n4.5 Examples\nBothrecurrentandconvolutionalmodelshavebeensuccessfullyusedinnumerousapplications.\nHerewediscussafewoftheinterestingexamples. Whilethesemodelsaremostlybasic,they\nformthefoundationsofmanystate-of-the-artsystems.\n4.5.1 Text Classification\nTo illustrate how sequence models could be used, we first consider the text classification\nprobleminwhichweassignoneofsomepre-definedclassestoatext. Itcanbeextendedto\ncoverabroadrangeofproblemsinNLP,includingclassifyingnewstexts,flaggingsentiment",
    "sentences,identifyingspamemails,detectingfakecomments,andsoon.\nIntextclassificationweareinterestedinselectingthebestclassfromasetC,givenaword\nsequencew ...w :\n1 m\ncˆ = argmax Score(c,w ...w ) (4.66)\n1 m\nc∈C\nHere Score(c,w ...w ) measures how well a class c is predicted for the input sequence\n1 m\nw ...w . Here we map the sequence of words to the sequence of word vectors (or word\n1 m\nembeddings),thatis,w ...w →x ...x . AssumingScore(·)isaprobabilisticfunctionthat\n1 m 1 m\ndescribesthedistributionoftheclasses,wecanreformulatetheproblemas\ncˆ = argmax Pr(c|w ...w )\n1 m\nc∈C\n= argmax Pr(c|x ...x ) (4.67)\n1 m\nc∈C\nThe central issue here is the modeling of Pr(c|x ...x ). We define Pr(c|x ...x ) by\n1 m 1 m\nfollowingthegeneralencoder+predictorframework,asfollows\n• Theinputx 1 ...x m isrepresentedasafeaturevectorH∈Rd h byusinganencoder\nH = Encoder(x ...x ) (4.68)\n1 m",
    "4.5 Examples 199\nPr(c|x ...x )\n1 m\nPredictor Softmax(H·U +b )\nc c\nFeaturevectorH ···\nPooling Pooling Pooling\nFilter1 Filter2 Filter3 ···\nx\n1\nx\n2\nx\n3\nWordvectors\nx\n4\nx\n5\nx\n6\nFigure 4.10: A CNN-based text classifier [Kim, 2014]. The input is a sequence of word\nvectors. Aconvolutionallayerinvolvingmultiplefiltersisusedtoextractfeaturesindifferent\ndimensions. Apoolinglayerisusedtoreducethenumberoffeaturesforrepresentingtheinput\ntext,leadingtoalow-dimensionalfeaturevectorH. ThepredictionconditionsonHandis\nmadebyusingastandardSoftmaxlayer.\n• HisfedtoastandardSoftmaxlayertopredicttheclassdistribution\nPr(·|H) = Softmax(H·U +b ) (4.69)\nc c\nwhereU c ∈Rd h ×|C| andb c ∈R|C| aremodelparameters.\nEncoder(·)isexactlythesamethingwediscussedintheprecedingsections. Thereare,\ntherefore,manyencodingmodelsthatareapplicablehere. Forexample,considertheCNN-\nbased encoder presented in Kim [2014]’s work. Kim [2014]’s model is based on a single",
    "convolution layer involving d filters. The application of a filter produces a set of features,\nh\neach being associated with a position of the sequence. Since we want a single vector for\nrepresentingtheentiresequence,apoolinglayerisaddedsothatthenumberoffeaturesfor\neachfilterisreducedtoone. Then,foranyc,theprobabilityPr(c|H)canbecomputedtrivially\naccordingtoEq. (4.69). SeeFigure4.10foranillustrationofthisclassifier.\nTotrainsuchamodel,wejustneedtooptimizeitonsomeloss,and,asmentionedseveral",
    "200 Chapter 4. Recurrent and Convolutional Sequence Models\ntimesinthisbook, oneofthemostcommonmethodsistominimizethecross-entropyloss\nusinggradientdescent. Also,wecanuseregularizationtoimprovethetrainingofCNNs. More\ndetailsaboutthesetechniquescanbefoundinChapter2.\nNotethatwhilethemodeldescribedhereisquite“simple”,itisamongthemosteffective\nmodels known for text classification. There are, of course, improvements to this kind of\nclassifier. ExamplesofsuchsystemsincludedeepCNNs[Conneauetal.,2017c],character-\nbasedCNNs[SantosandGatti,2014;Zhangetal.,2015],recurrentCNNs[Laietal.,2015],\nandsoon.\n4.5.2 End-to-End Speech Recognition\nSpeech recognition is a task of taking a sequence of acoustic signals and mapping it to a\nsequenceofwordsorcharacters(callitatranscription)[Reddy,1976;RabinerandJuang,\n1993]. Sincetheoriginalinputisanacousticwaveformoverthetimedomain,itiscommonto\ntransformitintoasequenceofwaveformfragments(callthemframes). Typically,aframe",
    "is represented as a feature vector, denoted by x . This is achieved by using either feature\ni\nfunctionsinsignalprocessing[DavisandMermelstein,1980;Picone,1993;Campbell,1997]\norlearnableembeddings[Chorowskietal.,2019;Schneideretal.,2019]. Regardingtheoutput,\nspeechrecognitionsystemsgenerallydonotproducewords. Instead,theyproducesequences\noftranscriptionunits(ortranscriptionlabels),e.g.,phonemes,characters,sub-words,etc.\nInthissubsectionweassumethattheoutputofaspeechrecognitionsystemisasequenceof\nEnglishletters,denotedbyy ...y ∈Vn. ThealphabetV consistsofnormalEnglishletters\n1 n y y\n(a−z),numbers(0−9),spaces(⟨sp⟩),periods(⟨pe⟩),andotherpunctuationmarks. Aswith\nmostmodernspeechrecognitionsystems,weaddablanksymbolϵtothealphabetinorderto\nindicatethenulloutput.\nThegoalhereistofindastringyˆ ...yˆ foragiveninputsequencex ...x ,sothat\n1 n 1 m\nyˆ ...yˆ = argmaxPr(y ...y |x ...x ) (4.70)\n1 n 1 n 1 m\ny1...yn\nThis model is relatively difficult compared to the classification model described in the",
    "previoussubsection,astheoutputcanbeanarbitrarystring,ratherthanaclassinapredefined\nclass set. The string generation problem leads to two difficulties. First, we need some\nmechanismtomodelPr(y ...y |x ...x )foranexponentiallylargenumberofpairsofinput\n1 n 1 m\nand output sequences. Second, in practice y ...y is often much shorter than x ...x (i.e.,\n1 n 1 m\nn<m),andsoweneedsomemechanismtoalignalongsequencetoashortone. However,\nwedonotneedtoconsiderthesedifficultiesinthestageofrepresentingtheinputsequence,\nandcanstillencodetheinputsequencex ...x inthesamewayasothersequencemodels.\n1 m\nSpecifically,werepresentx ...x inthefollowingform\n1 m\nh ...h = Encoder(x ...x ) (4.71)\n1 m 1 m\nTheencodercanbeRNNs,CNNs,ormoreadvancedmodels(suchasTransformer). Here\nwe consider the encoder architecture used in Graves et al. [2013b] and Graves and Jaitly",
    "4.5 Examples 201\n[2014]’swork. Itisamulti-layerbi-directionalLSTMmodel. Weskipthedetailsofthismodel\nwithoutlossofcontinuity,asthereadermaybealreadyfamiliarwithitinSections4.2.3,4.2.4\nand4.3.2.\nHavingobtainedthesequencerepresentationH=h ...h ,asoftmaxlayerisusedtomap\n1 m\neachh i ∈Rd h toadistributionoftranscriptionlabels,givenby\nPr(·|h ) = Softmax(h ·U +b )\ni i s s\nwhereU s ∈Rd h ×|Vy| andb s ∈R|Vy| aremodelparameters. Pr(·|h i )∈R|Vy| isaprobability\ndistributionoverV ,andtheprobabilityoftranscriptionlabell atpositioniissimplyPr(l |h ).\ny i i i\nWecanthenwritetheprobabilityofalabelsequenceintheform\nm\n(cid:89)\nPr(l ...l |H) = Pr(l |h ) (4.72)\n1 m k i\nk=1\nThisformulationlookssimple. Wecanappealtotheargmaxoperationtofindthemost\nprobable label sequence as usual. However, l ...l cannot be straightforwardly used as a\n1 m\nsystemoutput,becauseitoftencontainsmanyduplicateandblanksymbols. To“post-process”\nl ...l ,wefirstmergethesub-sequenceoflabelstoasinglelabelwhentheyarethesame,and\n1 m",
    "thenremovetheblanksymbols. Forexample,consideralabelsequence\ns s ϵ e e ϵ ϵ e ⟨sp⟩ y ϵ o o u\nBymerging“ss”,“ee”,and“oo”,wehave\ns ϵ e ϵ ϵ e ⟨sp⟩ y ϵ o u\nThen,weremoveallϵandobtain\ns e e ⟨sp⟩ y o u\nTheabovesequenceiswhatwewouldcallatranscription. Obviously,differentlabelse-\nquencescancorrespondtothesametranscription. LetB(y ...y )bethesetoflabelsequences\n1 n\ncorrespondingtoy ...y 12. Wenowturntothefollowingformofthetranscriptionprobability\n1 n\n(seeFigure4.11foranillustration)\n(cid:88)\nPr(y ...y |x ...x ) = Pr(l ...l |H) (4.73)\n1 n 1 m 1 m\nl1...lm∈B(y1...yn)\nA problem with this model is that the number of the sequences in B(y ...y ) grows\n1 n\nexponentially with n (and m). Fortunately, there are very efficient methods for comput-\n(cid:80)\ning Pr(l ...l |H). See [Graves et al., 2006] for a dynamic programming\nl1...lm∈B(y1...yn) 1 m\n12B(y 1 ...yn)maycontainlabelsequencesofarbitrarylengths. However,ifwerestrictinputtothesequence\nx\n1\n...xm,theneachsequenceinB(y\n1\n...yn)isoflengthm.",
    "202 Chapter 4. Recurrent and Convolutional Sequence Models\nGold-standard s e e ⟨sp⟩ y o u\nTranscription s e e ⟨sp⟩ y o u\nLabelSequence s s ε e e ε ε e ⟨sp⟩ y ε o o u\nSoftmax\nHiddenStates h 1 h 2 h 3 h 4 h 5 h 6 h 7 h 8 h 9 h 10 h 11 h 12 h 13 h 14\nBi-directionalLSTMEncoder\nSpectrum\nx x x x x x x x x x x x x x\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nFeatures\nSpeech\nFigure 4.11: An end-to-end speech recognition architecture. The input of the system is\na sequence of acoustic signals that are represented as a sequence of feature vectors (i.e..,\nx ...x ). These feature vectors are taken by a bi-directional LSTM encoder. The output\n1 14\noftheencoderisasequenceofcontextualizedrepresentations(i.e., h ...h )whichisthen\n1 14\nfedintoasoftmaxlayerforgeneratingadistributionoflabelsateachposition. Wecanthen\ndrawasequenceoflabelsfromthesedistributions. Wemapeachlabelsequencetoaformof\nfinal output by eliminating duplicate symbols and blank symbols. An output of the system",
    "correspondstoanumberoflabelsequences,andtheprobabilityoftheoutputisthesumofthe\nprobabilitiesoftheselabelsequences.\nalgorithmforsolvingthisproblem.\nNote that Eq. (4.73) is also known as a form of connectionist temporal classification\n(CTC)[Gravesetal.,2006]. Itisoneofthemostwidelyusedmethodsfortrainingend-to-end\nspeechrecognitionandspeechtranslationsystems. OneofthemeritsofCTCisthatitallows\nustoalignanylabelsequencetoatranscriptioninaverysimpleandefficientway. Itiseasyto\nmakeuseofCTCintrainingaspeechrecognitionsystem. Supposethereisasetofpairsof\ninputsequenceandtranscription,denotedbyS. Acommontrainingobjectiveistomaximize\nthelikelihoodofthesetranscriptionsgiventhecorrespondinginputs,writtenas\n(cid:88)\nθˆ = argmax logPr(y ...y |x ...x ;θ) (4.74)\n1 n 1 m\nθ\n(y1...yn,x1...xm)∈S\nwherePr(y ...y |x ...x ;θ)istheprobabilitycomputedviaEq. (4.73),andθistheparameters\n1 n 1 m",
    "4.5 Examples 203\nofthemodel.\nWhen testing on new data, we search for an optimal transcription as in Eq. (4.70).\nThisprocess,alsoknownasdecoding,generallyinvolvesoptimizedsearchalgorithmsand\npruningtechniques. Forexample,wecanusethe1-bestlabelsequenceinsteadofallpossible\nlabel sequences to obtain an approximation to Eq. (4.73), that is, Pr(y ...y |x ...x ) =\n1 n 1 m\nmaxPr(l ...l |H). This leads to an efficient decoding method, called Viterbi decoding,\n1 m\nwhichhasbeenextensivelyusedinspeechrecognitionandmachinetranslation[Lopez,2008].\nFormoredetailsaboutthedecodingofsequencegeneration,wereferthereadertoChapter5.\n4.5.3 Sequence Labeling with LSTM and Graphical Models\nSequencelabelingisaconceptuallystraightforwardapproachtoclassifyingdatainsequence.\nInNLP,ithaspenetratedmanysub-areaslikewordsegmentation,part-of-speechtagging,and\nchunking. LearninginthesemodelsconsistsofsimplypredictingalabelinalabelsetV at\ny\neachpositionofasequence. Ideally,wewishtoperformasequenceoflabelingactionsbased",
    "ontheentireinput,givenby\nyˆ ...yˆ = argmaxPr(y ...y |x ...x ) (4.75)\n1 m 1 m 1 m\ny1...ym\nwhere x ...x is an input sequence (such as a sequence of word vectors), and y ...y is a\n1 m 1 m\nlabelsequenceinwhicheachlabely correspondstoaninputitemx .\ni i\nAswehaveseeninthischapter,Eq. (4.75)perfectlyfitstheformofthesequencemodeling\nproblem. Asafirststepweuseanencodertomapx ...x toasequenceofcontextualized\n1 m\nrepresentations,asfollows\nh ...h = Encoder(x ...x ) (4.76)\n1 m 1 m\nWe define Encoder(·) as a bi-directional LSTM model because it involves a memory\nmechanismformodelinglong-rangedependenciesinbothleftandrightcontexts. Hence,the\narchitectureoftheencoderisthesameasthatusedintheprecedingsubsection.\nh ...h canthenbetakentobetheinputofausualsequencelabelingsystem(seeFigure\n1 m\n4.12). ThesequencelabelingproblemhasbeendiscussedinChapter1,andmanymodelsare\napplicabletoit. Thesimplestistheonethatinvolvesaclassifier(suchasmaximumentropy",
    "andSVM-basedmodels)forpredictingalabeldistributionforeachh . Aproblemwiththese\ni\nmodelsisthatthepredictionsaremadeindependently. Amorepowerfulapproachistouse\ngraphical models to consider dependencies among predicted labels. For example, hidden\nMarkovmodels(HMMs)describehowasequenceofobservations(i.e.,x ...x )isgenerated\n1 m\ngivenasequenceofvariables(i.e.,y ...y ). ThekeyideaistorewritePr(y ...y |x ...x )\n1 m 1 m 1 m\nusing the Bayes’ rule and approximate Pr(y ...y |x ...x ) by a product of simple factors.\n1 m 1 m\nHowever, these models require probability density functions of continuous variables (e.g.,\nPr(x |y )) which are difficult to estimate. This differentiates the use of HMMs in neural\ni i\nmodelsgreatlyfromthatinconventionalmodelswhereallstatesandobservationsarediscrete",
    "204 Chapter 4. Recurrent and Convolutional Sequence Models\n...\ny\ny3 m\n−\nLabelSequence y 1\n1 y2 y m\n...\nCRFNetwork\nHiddenStates h 1 h 2 h 3 ... h m−1 h m\nBi-directionalLSTMEncoder\nInputSequence x 1 x 2 x 3 ... x m−1 x m\nFigure4.12: TheBiLSTM+graphicalmodelarchitectureforsequencelabeling. Theencoder\nis a standard bi-directional LSTM model. Given a sequence of input feature vectors (i.e.,\nx ...x ),itproducesanewsequenceoffeaturevectorsformappingtheinputtocontextualized\n1 m\nrepresentations(i.e.,h ...h ). ACRFnetworkisplacedonh ...h topredictadistribution\n1 m 1 m\noflabelsequences. Theoptimallabelsequenceistheonethathasthemaximumprobability.\nvariables13.\nHMMsandtheirdescendantscanbeviewedasinstancesofgenerativemodels. Another\ntypeofmodelthathasbeencommonlyusedtosolvesequencelabelingproblemsisdiscrimina-\ntivemodels. Onesuchmodelisconditionalrandomfields(CRFs)[Laffertyetal.,2001]. The\nCRFmodelfeaturesitsabilitytodirectlymodelthejointprobabilityoftheentireinputand",
    "13InHMMs,asequenceofvariablescanbeviewedasapathoftransitingoversomestateswhosevaluesare\nchosenfromapre-definedset. Ineachtransitionfromonestatetoanother, somethingisobserved(callitan\nobservation).Bymakingsomeassumptions,wecanapproximatePr(y\n1\n...ym|x\n1\n...xm)inthefollowingfashion\nPr(y\n1\n...ym|x\n1\n...xm) =\nPr(y\n1\n...ym)·Pr(x\n1\n...xm|y\n1\n...ym)\nPr(x\n1\n...xm)\n(cid:81)m\nPr(y |y )·\n(cid:81)m\nPr(x |y )\n≈ i=1 i i−1 i=1 i i\nPr(x\n1\n...xm)\n(cid:81)m\nPr(y |y )Pr(x |y )\n= i=1 i i−1 i i (4.77)\nPr(x\n1\n...xm)\nwherePr(y\ni\n|y\ni−1\n)isthetransitionprobabilityofmovingfromy i−1toy i(wheni=1,wedefinePr(y\ni\n|y\ni−1\n)=\nPr(y 1 |y 0 )=Pr(y 1 )),andPr(x i |y i )istheemissionprobabilityofobservingx igiveny i.Asthedenominator\nPr(x\n1\n...xm)isaconstantnumberfordifferenty\n1\n...ym,itcanbedroppedintheargmaxoperationofEq.(4.75),\nasfollows\nm\n(cid:89)\nyˆ 1 ...yˆm = argmax Pr(y i |y i−1 )Pr(x i |y i ) (4.78)\ny1...ym\ni=1",
    "ToestimatePr(x i |y i ),apossiblesolutionistotakePr(x i |y i )= Pr(yi P |x r( i y ) i P ) r(xi) ,anduseaneuralnetworkto\ncomputePr(y\ni\n|x\ni\n).",
    "4.5 Examples 205\nlabelsequences,andtoallowustomakeuseofavarietyoffeaturestodothis. Consider,for\nexample,thelinear-chainCRF[SuttonandMcCallum,2012]. ItdefinesPr(y ...y |h ...h )\n1 m 1 m\ninthefollowingform\nPr(y ...y ,h ...h )\n1 m 1 m\nPr(y ...y |h ...h ) =\n1 m 1 m\nPr(h ...h )\n1 m\nexp(Score(y ...y ,h ...h ))\n1 m 1 m\n= (4.79)\nZ(h ...h )\n1 m\nwhereZ(h ...h )isanormalizationfactor,andhastheform\n1 m\n(cid:88) ′ ′\nZ(h ...h ) = exp(Score(y ...y ,h ...h )) (4.80)\n1 m 1 m 1 m\ny′...y′\n1 m\nScore(·) is a score for weighting the sequence pair (y ...y ,h ...h ). It is given by\n1 m 1 m\nsummingoverthevaluesofasetoffeaturefunctions{f (·),...,f (·)},likethis\n1 J\nm J\n(cid:88)(cid:88)\nScore(y ...y ,h ...h ) = f (y ,y ,h ) (4.81)\n1 m 1 m j i i−1 i\ni=1j=1\nThe outer loop of the summation corresponds to a visit to each position i. Given i, each\nfunctionf (·)takesthecurrentlabely ,thepreviouslabely andthecurrentinputvectorh ,\nj i i−1 i\nandthenreturnsthevalueofafeature.",
    "Thismodeliscalledlinear-chainbecauseitrepresentsy ...y asachainstructurewhere\n1 m\neachnodey ,alongwithanobservedvariableh ,onlyconnectstoitsprecedingnodey and\ni i i−1\nitsfollowingnodey 14,likethis\ni+1\ny — ··· — y — y — y — ··· — y\n1 i−1 i i+1 m\n| | | | |\nh ··· h h h ··· h\n1 i−1 i i+1 m\nInCRFs,itisassumedthatthevariablesinthegraphisonlydependentonitsneighboring\nvariables. Therefore,f (y ,y ,h )canbedefinedaccordingtohowy isconnected. There\nj i i−1 i i\naregenerallytwotypesoffeatures.\n• Transition-likeFeatures. Thistypeoffeaturesmodelstheconnectionbetweenconsec-\nutivelabels(y ,y ),givenby\ni−1 i\nf (y ,y ,h ) = u(y ,y ) (4.82)\n1 i i−1 i i−1 i\nwhereu(y ,y )isanentryofaweightmatrixu,indexedby(y ,y ).\ni−1 i i−1 i\n• Emission-likeFeatures. Thesecondtypeoffeaturesmodelstheconnectionbetweena\n14CRFscanbroadlybecategorizedasatypeofundirectedgraphicalmodels.Theydefineagraphoveraset\nofobservedvariablesandasetofunobservedvariables.Thesevariablesareconnectedinsomewaythatformsa\ngraph.",
    "206 Chapter 4. Recurrent and Convolutional Sequence Models\nlabely andtheassociatedinputx ,givenby\ni i\nf (y ,y ,h ) = g (y ) (4.83)\n2 i i−1 i i i\nwhereg\ni\n(y\ni\n)istheentryy\ni\nofavectorg\ni\n∈R|Vy|. Thevectorg\ni\nrepresentstheweights\nofassociatingh witheachlabelintheform\ni\ng = h ·v (4.84)\ni i\nwherev∈Rd h ×|Vy| isaweightmatrix.\nTosimplifynotation,weusey (ory )todenotetheone-hotrepresentationforalabel15.\ni i−1\nThen,substitutingtheabovefeaturefunctionsintoEq. (4.81)allowsthescoringfunctiontobe\nwrittenintheform\nm\n(cid:88)\nScore(y ...y ,h ...h ) = u(y ,y )+g (y )\n1 m 1 m i−1 i i i\ni=1\nm\n(cid:88)\n= y ·u·yT+h ·v·yT\ni−1 i i i\ni=1\nm\n(cid:88)\n= (y ·u+h ·v)·yT (4.85)\ni−1 i i\ni=1\nTheright-handsideoftheequationonlyinvolvessimplealgebraicoperationsonvectors\nand matrices, allowing viewing this model as a normal neural network. In this way, it is\nconvenienttoimplementthesequencelabelingsystemwithvariousneuralnetworktoolkits.",
    "We just need to stack a CRF network on an encoder network and learn the entire network\nasusual. Forexample, onecantrainthissystembymaximumlikelihood, andoptimizethe\nlossfunctionbygradientdescent. Notethat,aswithotherchainorlattice-basedmodels,the\nCRF network can be efficient because there are dynamic programming algorithms, called\ntheforward-backwardmethods,forcomputingbothScore(y ...y ,h ...h )andZ(h ...h ).\n1 m 1 m 1 m\nWerefertheinterestedreadertorelatedpapersformoredetaileddiscussions[Laffertyetal.,\n2001;SuttonandMcCallum,2012].\nOneadvantageofmarryingtheworldsofdistributedrepresentationandsequencelabeling\nisthatwedonotneedtospecifyanyfeaturetemplatesasinconventionalapproaches. Instead,\nthemodelisfreetolearnfeaturesthatdescribewhateverinputsequencesaremosteffectiveat\noptimizingsomeobjectiveforsequencelabeling. Suchanarchitecturehasbeenusedasthe\nbackbonemodelforseveralstate-of-the-artsystems[Huangetal.,2015;Lampleetal.,2016;\nMaandHovy,2016;Lietal.,2020c].\n15Inthiscase,y\ni",
    "∈R|Vy|,althoughitisoriginallyusedasascalar.",
    "4.6 Summary 207\n4.5.4 Hybrid Models for Language Modeling\nAswehavealreadynoted,manysequencemodelingproblemscanbedealtwithbyeitherRNN-\nbasedorCNN-basedmodels. Eachofthesetwotypesofmodelshasitsowncharacteristics:\nRNNsareoriginallydesignedfordealingwithvariable-lengthtemporaldata,andCNNsare\nmoreeffectiveininterpretinglocalinformationinrestrictedregionsofinput. Hereweconsider\nahybridapproachtolanguagemodelingforobtainingthebenefitsofboth.\nRecallfromSection4.2.1thataneurallanguagemodelislearnedtopredictaprobability\ndistributionoveravocabulary,givensomerepresentationofthehistorywords. Letw ...w\n1 m\nbeawordsequencetowhichwewanttoassignaprobability. First,werepresenteachword\nw asawordvectorx . Then,anRNNmodeltakesawordvectoratatimeandoutputsthe\ni i\nprobability\nPr(w |w ...w ) = Pr(w |x ...x )\ni+1 1 i i+1 1 i\n= Pr(w |h ) (4.86)\ni+1 i\nwhereh isthestateoftherecurrentunitatstepi.\ni\nThe process of converting words from symbols to continuous representations plays an",
    "importantroleinthismodel. Whileitiscommonforpractitionerstoobtainx fromaword\ni\nembedding table, this approach treats each word as a whole and simply ignores its internal\nstructure. In consequence, it might be difficult to learn distinct vectors for rare words in\nlanguageswithlargevocabularies[Bojanowskietal.,2017].\nHereweconsideradifferentwayofrepresentingwords. Theideaissimple: anadditional\nneuralnetworkisusedtoembedwords[Lingetal.,2015;Kimetal.,2016]. Supposeevery\nword w can be expressed as a sequence of characters. We represent these characters as\ni\nreal-valuedvectorse ...e viaacharacterembeddingtable. FollowingKimetal.[2016]’s\ni,1 i,leni\nwork,wecanuseaCNNtorepresente ...e asawordvectorinthefollowingform\ni,1 i,leni\nx = CNN(e ...e ,W)\ni i,1 i,leni\n= Pooling(TanH(Conv(e ...e ,W))) (4.87)\ni,1 i,leni\nwhere Conv(·,W) is a convolutional layer with parameters W, TanH(·) is a hyperbolic\ntangentfunction,andPooling(·)isapoolinglayer.",
    "Figure4.13showsthearchitectureofthemodel. Weseethatthereisahierarchicalstructure\nbehindthismodel,thatis,charactersformaword,andwordsformasentenceorphrase. On\na practical side, in many NLP tasks it is quite natural to consider the hierarchical nature of\nlanguage. Wewillseeafewexamplesofmakinguseoftherelationshipsbetweendifferent\nlevelsoflanguagerepresentationsinlaterchapters.\n4.6 Summary\nThischapterhasintroducedtherecurrentandconvolutionalneuralapproachestomodeling\nsequencesofwords. Ononehand, recurrentneuralnetworksaredesignedfordealingwith",
    "208 Chapter 4. Recurrent and Convolutional Sequence Models\nPr(·|w ...w )\n1 i\nRNN RNN RNN RNN\nWordVectors x 1 x 2 x 3 x 4\nwordw\n3\nCharacter-aware\nWordRepresentation\nCNN CNN CNN\nCharacter\ne3,1 e3,2 e3,3 e3,4 e3,5\nVectors\nFigure4.13: Alanguagemodelwithcharacter-awarewordrepresentations[Kimetal.,2016].\nAsalanguagemodel,thegoalofthismodelistocomputetheprobabilityPr(w |w ...w )\ni+1 1 i\nforeachi. Werepresenteachwordw asareal-valuedvectorx . Thisvectoristheoutputofa\ni i\nCNNthattakesasequenceofcharacterscorrespondingtothisword. Then,thesequenceofthe\nwordvectorsx ...x isusedastheinputtoanRNN+Softmaxmodel. Themodeloutputs\n1 m\nateachpositioniadistributionofwords,wheretheentryw describesPr(w |w ...w ).\ni+1 i+1 1 i\nThishierarchicalstructureprovidesamulti-scaleapproachtolanguagemodeling: asentence\nismodeledbyconsideringwords,andawordismodeledbyconsideringcharacters.\nsequentialdata,andhavebroadapplicabilityinNLP.Toimprovethemodelingpowerofthese",
    "models,thememorymechanismisgenerallyused. Inparticular,wehaveintroducedLSTM\nandGRUwhicharetwopopulartypesofmodelsindealingwithlongsequenceproblems. On\ntheotherhand,whileconvolutionalneuralnetworksarecommonlyusedtoprocessvisiondata,\ntheyarestraightforwardlyapplicabletosequencemodeling. Wehaveseenthatallthesemodels\ncan be used in several language and speech processing tasks, including text classification,\nspeechrecognition,sequencelabeling,andlanguagemodeling.\nTherootsofmodelingsequencesoflanguageunitscanbetracedback toearlyworkin\nseveraldifferentfields. Forexample, theprocessofgeneratingasequenceofwordscanbe\ndescribedasaMarkovchainwherethepredictionofawordonlydependsonalimitednumber\nof previous words [Markov, 1913]. This idea motivates the n-gram methods for sequence\nmodeling [Shannon, 1948a], as well as hidden Markov models which later appeared and\nbecamepopularinmodelingsequencesofpairsofobservedandunobservedvariables[Baum",
    "andPetrie,1966;Baumetal.,1970]. Thesemodelsandtheirvariantslaythefoundationsof\nmany successful NLP systems in past decades [Manning and Schütze, 1999; Jurafsky and",
    "4.6 Summary 209\nMartin,2008].\nThe idea of using neural networks in sequence modeling has also been investigated for\nsometime. Oneexampletoseehowneuralnetworksaredevelopedandappliedtosequence\nmodeling is speech recognition [Lippmann, 1989]. Most of the studies in the early days\nof this research area try to either combine neural networks with existing models [Bourlard\nandWellekens,1990;BourlardandMorgan,1993;TrentinandGori,2001],oraddresssub-\nproblems of speech recognition [Tank and Hopfield, 1987; Waibel et al., 1989; Lang et al.,\n1990;Bengio,1991]. However,scalingneuralnetworksupinsizewaschallengingbecause\ntrainingdeepneuralnetworksrequiresalotofcomputationresourcesanddata. Thefieldhad\nlongbeendominatedbyapproachesbasedonhiddenMarkovmodelsandGaussianmixture\nmodels(GMMs),withapipelineofseveralmodulesthatrequirecarefultuning. Ontheother\nhand,whilefullyneuralapproacheswerenotstate-of-the-artduringthattime,researcherswere",
    "awareoftheirpotentialinlearningrepresentationsofacousticinputsandfreeingthemfrom\nhand-craftedfeatures[LeCunandBengio,1995].\nAdramaticshiftfromconventionalpipelinedapproachestoend-to-endapproachescomes\nwiththerevivalofneuralnetworksinthe2000s[Hannunetal.,2014;Gravesetal.,2013b].\nTheshiftissoinfluentialthatabroadsetoffieldscomestogetherlikeneverbefore,e.g.,in\ncomputervisionandspeechprocessing,thepasttenyearshave,meanwhile,witnessedgreat\nperformancegainsbroughtbyverydeepneuralnetworksandend-to-endlearning[Hintonetal.,\n2006;Gravesetal.,2013b;Heetal.,2016a;Krizhevskyetal.,2017]. InNLP,theparadigm\nshiftstartswiththeworkonwordembeddings[Mikolovetal.,2013a;Penningtonetal.,2014],\nandcontinuesasmorepowerfulsequencerepresentationmodelsaredeveloped[Vaswanietal.,\n2017]. Asimpleapproachtosequencemodeling,thoughnotdiscussedindepthinthischapter,\niscompositionalmodels[Janssen,2012]. Forexample,wecanusethebag-of-wordsmodel",
    "to sum or average word vectors of a sequence. Despite the simple architectures of these\napproaches, they achieve satisfactory results in many tasks, providing strong baselines for\nfurtherresearchonmoreadvancedmodels[Conneauetal.,2018]. Asthenextstep,applying\nrecurrentandconvolutionalneuralmodelstosequencemodelingisstraightforward. Thisis\nnotsurprisingbecausethesemodelsarefairlywellstudiedinotherfields[Liptonetal.,2015;\nLietal.,2021d;Khanetal.,2020]. Inparticular,theLSTMmodeliswellsuitedtodealwith\nlongsequencesandthusofgreatinteresttoNLPresearchers[Sundermeyeretal.,2012;Huang\netal.,2015;Wuetal.,2016]. However,wearealwaysontheway. Learningsequencemodels\nisoneofthemostactiveresearchfieldswithnoendinsight. Therearemanymodelsthatare\nbasedonnewarchitecturesandshowstrongerperformanceinvarioustasks. Morediscussions\nonsomeofthesemodelscanbefoundinChapters6,7and8.\nNotethatthetermsequencemodelingiscurrentlyusedinmanydifferentways,referring",
    "todifferenttasks. Inmanycasesitismorecommontousethetermsencodingandencoder\ntoemphasizetheprocessofmappingasequenceofsymbolstoacontinuousrepresentation.\nAsdiscussedintheprevioussections,abenefitofviewingencodingasanindividualtaskis\nthatwecanlearnageneralrepresentationmodelthatisnotdependentonwhereweapplyit.\nIt opens the door to a wide range of pre-trained encoders for learning to represent various\ntypesofdata,suchastext[Petersetal.,2018;Devlinetal.,2019],speech[Oordetal.,2018;",
    "210 Chapter 4. Recurrent and Convolutional Sequence Models\nHsuetal.,2021;Chenetal.,2022], vision[ChenandHe,2021;Baoetal.,2021;Heetal.,\n2022],andcombinationsofthem[Chuangetal.,2020;Lietal.,2021c;Kimetal.,2021]. A\ncloselyrelatedconcepttotextencodingistextembeddingorsentenceembedding[Conneau\netal.,2017a;Ceretal.,2018]. Thesecanbebroadlyconsideredthesamething. Ingeneral,\nan embedding model in NLP means a process of transforming the input text into a single\nlow-dimensionalvectorratherthanproducingsequencesofcontextualizedvectors[Kirosetal.,\n2015;Hilletal.,2016].\nInmanyNLPproblems,systemsarenotnecessarilysequentialontheirinputand/oroutput.\nFor example, in text classification, a system may take tree-structured input and produces a\nlabel [Tai et al., 2015; Yang et al., 2016]. In this case we need some mechanism to encode\nhierarchicalstructures. Analternativeapproachistoconverttreestosequences(orlinearized\ntrees) so that we can directly make use of sequence models to handle non-sequential data",
    "[Vinyalsetal.,2015]. Thisisagreatideabecauseitopensupthepossibilityofdevelopinga\nuniversallyapplicableencodertorepresentvarioustypesofdataiftheinputoftheencodercan\nbelinearizedinsomeway. Forexample,byrepresentinganimageasasequenceofpatches,\nsequence models can be directly applied to image classification, achieving state-of-the-art\nresultsonseveraltasks[Chenetal.,2020a;Dosovitskiyetal.,2021].",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 5\nSequence-to-Sequence Models\n天下万物之理，无独必有对。\nAccording to the Principle of Heaven and Earth and all things, nothing\nexistsinisolationbuteverythingnecessarilyhasitsopposite.\n–《近思录》\nReflectionsonthingsathand\n朱熹/XiZhu(AD1130-1200)\n吕祖谦/ZuqianLv(AD1137-1181)\ntranslatedbyChang[1967]\nIntheworldoflanguage,thingsoftencomeinpairs. Ifthereisaquestion,therewouldbean\nanswer;ifthereisaChinesetext,therewouldbeanEnglishtranslation;ifthereisasentence,\nthere would be a parse of it according to some syntax. Many NLP systems are designed to\nmodelthecorrespondencebetweenthesepairs,i.e.,oneofthetwoistakenasinputandthe\notheristakenasoutput. Theseproblemscanbeexpressedinaformthatwehaveencountered\nseveraltimes,likethis\nyˆ = argmaxPr(y|x) (5.1)\ny\nwherexisaninputvariable,y isanoutputvariable,andPr(y|x)isamodelthatestimateshow\nlikelyy wouldbethetrueoutputgivenx.",
    "This chapter is more interested in a particular family of problems where both x and\ny are sequences of words, called sequence-to-sequence (or seq2seq) problems. Unlike\nclassificationproblemswheretheoutputyˆisselectedfromafixedsetofclasses,sequence-to-\nsequenceproblemsrequireproducinganoutputfromanexponentiallylargersetofsequences.\nObtaining yˆ in this case turns out to be a much more complex problem than the case of\nclassification,becauseweneedmorepowerfulmodelstodescribePr(y|x)andmoreefficient\nsearchalgorithmstosolveEq. (5.1).",
    "212 Chapter 5. Sequence-to-Sequence Models\nThis chapter will discuss the well-known encoder-decoder architecture for sequence-\nto-sequencemodeling. Also, thischapterwilldiscusstheattentionmechanismwhichisan\nimprovementonthisarchitecture. Bothofthesemodelslaythefoundationofdiscussionsof\nseveralstate-of-the-artmodelsinthefollowingchapters. Furthermore,thischapterwilldiscuss\nthesearchproblemwhichplaysanimportantroleinsequencegenerationandrelatedproblems.\n5.1 Sequence-to-Sequence Problems\nWechoosemachinetranslationasanillustrativeexamplethroughoutthischapter,becauseit\nisnowoneofthemostpopularsequence-to-sequencetasks. Weusex=x ...x todenote\n1 m\na sequence of words in one language (call it a source-side sequence or source sequence),\nandusey=y ...y todenoteasequenceofwordsinanotherlanguage(callitatarget-side\n1 n\nsequenceortargetsequence). WecanwriteEq. (5.1)usingthenewnotation,asfollows\nyˆ = argmaxPr(y|x)\ny\n= argmaxPr(y ...y |x ...x ) (5.2)\n1 n 1 m\ny1...yn",
    "As discussed in Chapter 1 and in [Brown et al., 1993], this formulation implies three\nfundamentalissues.\n• Modeling. First,weneedtodefinetheformofPr(y|x). Inthischapterweshowthat\nPr(y|x)canbecomputedusingasingleneuralnetworkbasedontheencoder-decoder\narchitectureandtheattentionmechanism. Notethatsometimeswejustneedamodelfor\ndiscriminating“good”from“bad”targetsequences. Inthiscase,itisnotnecessaryto\nrequirethemodeltomakeprobabilitysense,andwecantakeadiscriminantfunction\ninstead.\n• Training. Then,weneedtolearnparametersofthemodelPr(y|x)givensometraining\ndata. As Pr(y|x) is expressed as a neural network, we can train it in a regular way:\nweoptimizesomelossbygradientdescent. SeeChapter3forcommonapproachesto\ntrainingneuralnetworks. Wewillalsodiscusstechniquesthataretailoredforspecific\ntasksinthisandthefollowingchapters.\n• Search(orDecoding). Oncewehavelearnedamodel,wewillobtainyˆ bysearchingfor\nthetargetsequencethatmaximizesPr(y|x). Thisisacomputationalchallengebecause",
    "thenumberofcandidatesequencesgrowswiththemaximumlengthofthesequences\nand the size of the vocabulary. In Section 5.4, we will discuss efficient and effective\nsearchmethodsforsequence-to-sequenceproblems,particularlyformachinetranslation.\nMany NLP problems that fit the form of Eq. (5.2) can fall into sequence-to-sequence\nproblems,andtheresearchontheseproblemsislargelymotivatedbydiscussionsoftheabove\nissues. Table 5.1 shows common examples of sequence-to-sequence problems taken from\ntheliterature. Whenthetarget-sideisatext,theproblemscanbroadlybecategorizedasthe\ntext generation problems, although a general text generation system does not require the",
    "5.2 The Encoder-Decoder Architecture 213\nTask Source Target\nMachineTranslation Text Translation\ninOneLanguage inAnotherLanguage\nQuestionAnswering Question Answer\nDialogueSystems Text/SpeechforConversation Response\nSummarization LongText SummariesoftheText\nTextSimplification Text SimplerText\nTextStyleTransfer Text SameContent\ninOneStyle inAnotherStyle\nGrammarCorrection TextwithErrors CorrectedText\nSpeechRecognition Speech Transcription\nSpeechSynthesis Text Speech\nSpeechTranslation Speech Translation\ninOneLanguage inAnotherLanguage\nTable5.1: Examplesofsequence-to-sequenceproblems.\nsource-side to be sequential. In addition to language and speech processing, sequence-to-\nsequence problems can be generalized to cases where the input and/or output of a system\narenotnaturallysequential. Forexample,image-to-textgeneration(orimagecaptioning)\nand text-to-image generation systems both involve dealing with images that are typically",
    "representedas2Ddata. Byrepresentingimagesassequencesinsomeway(suchassequences\nofpatches),sequence-to-sequencemodelsaredirectlyapplicabletothesetasks.\nHistorically,mostsystemsinthesetasksweredevelopedsomewhatindependently,resulting\nindifferentarchitectures,features,andtrainingmethodsfordifferenttasks. However,asshown\nin this chapter, when we represent these models as neural networks and train them in an\nend-to-endfashion,thereappearstobea“universal”paradigmforalltheseproblems. Thisisa\nbigchangefortheAIcommunitybecausemanyresearchfieldscometogetherandsystemscan\nbesharedacrossthem. Wecangainsomeinsightintothecommonnatureofabroadvariety\nofproblems,thoughtherearemanytask-specificconsiderationsinpractice. Inthefollowing\nsections,wewilldiscusssomeofthecommonthreadsamongsequence-to-sequencemodels.\n5.2 The Encoder-Decoder Architecture\nIn this section we discuss the encoder-decoder architecture and a simple neural machine\ntranslationmodelbasedonthisarchitecture.\n5.2.1 Encoding and Decoding",
    "Fromasupervisedlearningviewpoint,wewouldideallyliketolearnamodelfromanumber\nof sequence pairs such that any source-side sequence can be mapped to the corresponding\ntarget-sidesequence. However,learningthemappingbetweensequencesofdiscretevariables",
    "214 Chapter 5. Sequence-to-Sequence Models\nistypicallyaproblemoflearningfromhigh-dimensionaldata. Itinevitablysuffersfromthe\ncurseofdimensionality,makingthemodelingandtrainingdifficult.\nOne approach to learning such a mapping is to divide the problem into “simpler” sub-\nproblems. We assume that there is a low-dimensional representation shared by x and y,\ndenoted by H. Then, the mapping x→y can be achieved by mapping x to H and then to\ny. Formally,givenasource-sidesequencex,wemapittotherepresentationHbyusingan\nencodingsystem(callitanencoder)\nH = Encode(x) (5.3)\nThen, we map H to the target-side sequence y by using a decoding system (call it a\ndecoder)1\ny = Decode(H) (5.4)\nThisarchitecture,alsoknownastheencoder-decoderarchitecture,iswidelyusedinrecent\nsequence-to-sequence systems (see Figure 5.1 for an illustration). It is easy to see that the\nformofEq. (5.3)isthesameasthoseofthesequencemodelsmentionedinChapter4,andso",
    "therearemanyencodingmodelstochoosefrom,suchasbi-directionalLSTM.Thegoalofthe\ndecoderistoproducea“best”target-sidesequencegiventherepresentationofthesource-side\nsequence. Likeclassificationmodels,thepredictionismadebyfirstproducingadistribution\noverallpossiblesequences,andthenselectingtheonewiththemaximumprobability. Assuch,\nwecanre-defineDecode(·)asaprobabilityfunction\nPr(·|H) = Decode(H)\n= Decode(Encode(x)) (5.5)\nInotherwords,givenatarget-sidesequencey,thedecoderassignsitaprobability\nPr(y|x) = Pr(y|H) (5.6)\nThen,theoptimalsequenceyˆ isobtainedbyperformingargmax Pr(y|x)asinEq. (5.2). In\ny\nmanysystemsbasedontheencoder-decoderarchitecture,bothEncode(·)andDecode(·)are\nmodelsconstructedfromneuralnetworks. Thus,wecantreatthesequence-to-sequencemodel\n1Itisimportanttodistinguishbetweentheconceptofdecoding(ordecoder)usedinconventionalsequence-to-\nsequencesystemsandthatusedintheencoder-decoderarchitecture.Thetwoareoftenconfused,thoughtheyare",
    "differentsomehow.Inmanymachinetranslationorspeechrecognitionsystems,decodinghasthesamemeaningas\ntranslationortranscription,thatis,werecovertheoptimalyfromx. AspointedoutinEq. (5.2),thisprocess\ninvolvesasearchoverallcandidatey. Therefore,theconventionaluseofdecodinginthesesystemsistorefer\ntoasearchprocess(i.e.,theargmaxoperationinEq.(5.2))[Koehn,2010].Bycontrast,intheencoder-decoder\narchitecturedecodingmeansaprocessofrecoveringthetarget-sidesequenceyfromtheintermediaterepresentation\nH.Itisallaboutmodelingratherthansearching.Itisalsoworthnotingthat,whilethetermdecoding(ordecoder)\nisusedindifferentways,itcanbethoughtofasaprocessofmappinganencodedmessagebacktotheoriginal\nmessageinacommunicationsystemasdefinedininformationtheory[Shannon,1948b].Inthissense,thedecoding\nprocessesinthesesystemsdothesamethingasthewordsoundslike:convertsomethingtoitsoriginalform.",
    "5.2 The Encoder-Decoder Architecture 215\nTarget-sidesequence: y=y ...y\n1 n\nDecoder\n(y=Decode(H))\n.2 -1 6 5 .7 -2 Representation: H\nEncoder\n(H=Encode(x))\nSource-sidesequence: x=x ...x\n1 m\nFigure5.1: Theencoder-decoderarchitecture. Inthecaseofsequence-to-sequenceproblems,\nittransformsasource-sidesequencex=x ...x toatarget-sidesequencey=y ...y . This\n1 m 1 n\nprocedureinvolvestwosteps: xisfirstencodedasarepresentationH,andthisrepresentation\nisthendecodedtoy.\nasasingleneuralnetworkandtrainitasusual,providedtheentiremodelissomecombination\nofEncode(·)andDecode(·).\nToapplytheencoder-decoderarchitecturetoareal-worldtask,weneedtomakeanumber\nof design choices, such as the forms of H, Encode(·) and Decode(·). As a very simple\nexample, consider the task of regenerating an input word. We can define Encode(·) as a\nfeed-forwardneuralnetworkthattakesaword(inone-hotrepresentation)andoutputsaword\nvector. Inthisway,Hisadistributedrepresentationoftheword. Then,wedefineDecode(·)",
    "asanotherfeed-forwardneuralnetworkthattakesthewordvectorandgeneratesadistribution\noverthevocabulary. Fortraining,wewishtolearnasystemthatassignsthelargestprobability\nto the input word. As discussed in Chapter 2, we can call this an auto-encoder which is a\nspecialinstanceoftheencoder-decoderarchitecture.\n5.2.2 Example: Neural Machine Translation\nNextweillustratetheapplicationoftheencoder-decoderarchitectureusingaworkingexample\n—neuralmachinetranslation(NMT).Weconsiderawell-knownNMTmodelwhichuses\nRNNoritsvariantsforbuildingboththeencoderanddecoder[Choetal.,2014;Sutskeveretal.,\n2014]. TheencoderoftheNMTmodelisastandardRNN-basedencoder. AstheRNN-based\nsequencemodelhasbeendiscussedindetailinChapter4,wejustgiveabriefreviewofthis\nmodelhere. Supposethatthesource-sidevocabularyisV andeachsource-sidewordx is\nx j\nrepresentedasaone-hotvectorinR|Vx|. Then,x\nj\nistransformedintoah\ns\n-dimensionalvector",
    "216 Chapter 5. Sequence-to-Sequence Models\n(orwordembedding)\nxe = Embed (x ) (5.7)\nj s j\nwhere Embed (·) is the word embedding function. More details about word embedding\ns\nmodelscanbefoundinChapter3.\nTheRNNmodeltakesthesequenceofthewordvectorsxe...xe andproducesasequence\n1 m\nofRNNstatevectorsh 1 ...h m . AnRNNstatevectorh j ∈Rd h isdefinedtobe\nh = RNN(h ,xe) (5.8)\nj j−1 j\nHereRNN(·)isanRNNunitthatsummarisestheinformationuptopositionj bycombining\nthepreviousstateh andthecurrentinputxe insomeway. Then,thelaststateh canbe\nj−1 j m\ntreatedasarepresentationoftheinputsequencex ...x ,andwecanuseh astheoutputof\n1 m m\ntheencoder,writtenas\nh = Encode(x ...x ) (5.9)\nm 1 m\nFigure 5.2 (a-b) shows an illustration of the encoding process. Note that the model\ndescribedabovejustinvolvesasingle-layerRNN.Inpracticalsystems,thisframeworkcanbe\neasilyextendedtoincludemultiplelayersandmorepowerfulrecurrentunits(suchasLSTM\nunits).",
    "The decoder of the NMT model is a standard RNN-based language model, that is, we\npredict the next word y given all previous words y ...y . To incorporate the source-side\ni+1 1 i\ninformationintotranslation,asimpleandstraightforwardmethodistotreath astheinitial\nm\nstate of the target-side RNN. Let ye ∈Rds be the word vector of the start symbol ⟨SOS⟩\n0\n(denotedbyy ). ThecorrespondingRNNstateisgivenby\n0\ns = RNN(h ,ye) (5.10)\n0 m 0\nHereRNN(·)hasthesameformastherecurrentunitusedintheencoder,butwithdifferent\nparameters.\nFori>0,thestatevectors\ni\n∈Rds isgivenintheform\ns = RNN(s ,ye) (5.11)\ni i−1 i\nThen,s isfedintoaSoftmaxlayertoproduceadistributionoverthetarget-sidevocabulary\ni\nV . TheoutputoftheSoftmaxlayerisgivenby\ny\nPr(·|y ...y ,x ...x ) = Pr(·|s )\n1 i 1 m i\n= Softmax(s U +b ) (5.12)\ni y y\nwhereU\ny\n∈Rds×|Vy|andb\ny\n∈R|Vy|aretheparametersoftheSoftmaxlayer. Pr(y\ni+1\n|y\n1\n...y\ni\n,x\n1\n...x\nm\n)\ncanbeseenastheprobabilityofpredictingwordy byconditioningonboththetranslated\ni+1",
    "5.2 The Encoder-Decoder Architecture 217\nh1 h1 h2 hm(H)\n...\nRNN RNN RNN RNN\nxe xe xe xe\n1 1 2 m\n(a)Theencodingprocessstarts (b)TheencoderrepresentsxasH\nPr(y1|y0,x) ··· Pr(yn+1|y0...yn,x)\nSoft. Soft. Soft.\ns0 s0 s1 sn\n...\nRNN RNN RNN RNN\nye ye ye ye\n0 0 1 n\nh1 h2 hm(H) h1 h2 hm(H)\n... ...\nRNN RNN RNN RNN RNN RNN\nxe xe xe xe xe xe\n1 2 m 1 2 m\n(c)Thedecodertakestherepresentationofx (d)Thedecoderpredictsthetarget-sidewords\nFigure5.2: TheencodinganddecodingstepsforanRNN-basedNMTsystem. Theencoderis\nastandardRNN.Theencodingprocessstartswiththefirstsource-sidewordandendsupwith\nthelastsource-sideword. ThelaststateoftheRNNistakentobetherepresentationofthe\nentiresource-sidesequence(i.e.,H=h ). ThedecoderisanotherRNN.Atthefirststep,it\nm\ntakesHfromtheencoder. Afterrepresenting(ye...he,H)ass atpositioni,asoftmaxlayer\n0 i i\nisbuilttopredictthenextwordy .\ni+1\nwordsy ...y andthesource-sidesequencex ...x . SeeFigure5.2(c-d)foranillustrationof\n1 i 1 m\nthewordpredictionsofadecoder.",
    "Armed with this model of word prediction, we turn to a form that is frequently used in\npapersonNMT,likethis\nPr(y|x) = Pr(y y|x)\n0\n= Pr(y y ...y |x ...x )\n0 1 n 1 m\n= Pr(y |x ...x )Pr(y ...y |y ,x ...x )\n0 1 m 1 n 0 1 m\nn−1\n(cid:89)\n= Pr(y |y ...y ,x ...x ) (5.13)\ni+1 0 i 1 m\ni=0",
    "218 Chapter 5. Sequence-to-Sequence Models\nSometimes,thisequationisalsowritteninanequivalentform\nn\n(cid:89)\nPr(y|x) = Pr(y |y ...y ,x ...x ) (5.14)\ni 0 i−1 1 m\ni=1\nHereweassumethatyalwaysstartswithy (i.e.,⟨SOS⟩)andsoPr(y |x ...x )=1. In\n0 0 1 m\nmanypracticalsystems,itisalsocommontoassumethatyendswithaspecialsymbol⟨EOS⟩.\nTherefore,wecanmodifythisequationtoinvolve⟨SOS⟩and⟨EOS⟩onboththesourceand\ntarget-sides,asfollows\nPr(y yy |x xx ) = Pr(y y ...y y |x x ...x x )\n0 n+1 0 m+1 0 1 n n+1 0 1 m m+1\n= Pr(y |x ...x )·\n0 0 m+1\nPr(y ...y y |y ,x ...x )\n1 n n+1 0 0 m+1\nn\n(cid:89)\n= Pr(y |y ...y ,x ...x ) (5.15)\ni+1 0 i 0 m+1\ni=0\nwherex =y =⟨SOS⟩,x =y =⟨EOS⟩,andPr(y |x x ...x x )=1.\n0 0 m+1 n+1 0 0 1 m m+1\nSincePr(y|x)canbeexpressedasaneuralnetwork,trainingthismodelisstraightforward.\nAsdescribedinChapter4,RNN-basedlanguagemodelsaretrainedbyusingthecross-entropy\nlossandgradientdescent. NMTcanusethissamemethodfortrainingmodelparameters. Once",
    "wehaveobtainedtheoptimizedmodel,wecanthenuseittotranslatenewsentences. Finding\nthebesttranslationforanygivensource-sidesentenceisastandardsearchproblem. Wewill\ndiscussitinSection5.4.\n5.3 The Attention Mechanism\nTheNMTmodeldiscussedintheprevioussectionwasbasedonafixed-lengthrepresentationof\nthesource-sidesequence. Whilethismodeliseasytoimplement,inmanypracticalapplications\nit is unsatisfactory because a fixed-length vector might not be sufficient for representing a\nvariable-lengthsequence, especiallywhenthesequenceislong. Thissystemwilltherefore\nneedsomemechanismtocoupletheencoderandthedecoderinafine-grainedmanner. Inthis\nsectionwediscusstheattentionmechanismbywhichasystemcanlearn,foreachwordofthe\ntarget-sidesequence,anadaptiverepresentationthatfocusesmoreonimportantpartsofthe\nsource-sidesequence.\nIn fact, the discussion here is related to the attention models in psychology because\ntranslationisitselfacognitiveprocess[Sternberg,1996;Neisser,2014]. Thekeyideabehind",
    "this type of model is natural: attention is generally concentrated on specific parts of the\ndata when we process something. This forms the basis of many state-of-the-art sequence-\nto-sequence models, and the attention mechanism has been the de facto standard for the\ndevelopmentofthesesystems.",
    "5.3 The Attention Mechanism 219\n... ...\nPr(y1|−)Pr(y2|−) Pr(yn+1|−) Pr(y1|−)Pr(y2|−) Pr(yn+1|−)\nDecoder Decode..r.\nye ye ... ye ye ye ... ye\n0 1 n 0 1 n\nAttention\nh h ... h h h ... h\n1 2 m 1 2 m\nEncoder Encoder\nxe xe ... xe xe xe ... xe\n1 2 m 1 2 m\n(a)AnNMTsystemwithoutattention (b)AnNMTsystemwithattention\nFigure5.3: NMTarchitectureswithout(left)andwith(right)theattentionmodel. Whenthe\nattention model is not involved, a fixed-length representation is considered for generating\nthe entire target-side sequence. By contrast, when the attention model is involved, a new\nrepresentationiscomputedspecificallyforeachtarget-sidestatesothatthedecodercanlearn\ntoconcentrateondifferentpartsofthesource-sidesequenceforpredictingatarget-sideword.\n5.3.1 A Basic Model\nRecallthatintheNMTmodeloftheprevioussection,theencoderrepresentsasource-side\nwordsequenceash ...h ,andthedecoderrepresentsatarget-sidewordsequenceass ...s .\n1 m 1 n",
    "Theattentionmechanismaddressesthequestionofhowarepresentationcanbelearnedfrom\nh ...h sothatthisrepresentationcanexplainthesource-sidesequencewellforagiventarget\n1 m\nstates 2. Fromaninformationprocessingperspective,solongasweignorethemeaningsof\ni\nh ...h and s in NMT, attention can be thought of as a generic process of processing the\n1 m i\ninputinformationh ...h byconsideringhoweachh isrelatedtotheinterests . Figure5.3\n1 m j i\ncomparesNMTarchitectureswithandwithouttheattentionmechanism.\nMore formally, an attention model produces a linear combination of {h ,...,h } in the\n1 m\nform\nm\n(cid:88)\nc = α ·h (5.16)\ni i,j j\nj=1\nwhereα istheattentionweightthatdescribeshowmuchthemodelshouldrelyonh when\ni,j j\n2Followingtheconventioninmachinetranslation[Brownetal.,1993],weusejtorepresentapositioninthe\nsource-sidesequence,anduseitorepresentapositioninthetarget-sidesequence.",
    "220 Chapter 5. Sequence-to-Sequence Models\ncomputingc fors . Sometimesc isalsocalledacontextvector.\ni i i\nAcommonapproachtocomputingattentionweightsistonormalizealignmentscoresin\nthefollowingform\nα = Softmax(a(s ,h ))\ni,j i j\nexp(a(s ,h ))\ni j\n= (5.17)\n(cid:80)m\nexp(a(s ,h ))\nj′=1 i j′\nHerethealignmentscorea(s ,h )measureshowstrongh isrelatedtos . Ingeneral,a(s ,h )\ni j j i i j\ncanbedefinedinseveraldifferentways[Gravesetal.,2014;Bahdanauetal.,2014;Luong\netal.,2015]. Acomprehensivelistofthesefunctionscanbefoundinsurveypapersonthis\nsubject[Chaudharietal.,2021]. Hereweintroducesomeofthecommonones.\n• Dot-product Attention. One of the simplest methods is to measure the similarity\nbetweenh ands . Thus,wecancalculatethedot-productofthetwovectors,asfollows\nj i\na(s ,h ) = s hT\ni j i j\nd\n(cid:88)h\n= s (k)·h (k) (5.18)\ni j\nk=1\nAvariantofthismodel,calledscaleddot-productattention,addsascalarfactor 1 to\nβ\ntheright-handsideofEq. (5.18),asfollows\ns hT\ni j\na(s ,h ) = (5.19)\ni j\nβ",
    "Wewillseeanexampleofthismodellaterinthissection.\n• CosineAttention. Anothercommonlyusedsimilaritymeasureinvectoralgebraisthe\ncosineoftheanglebetweentwovectors,givenby\na(s ,h ) = cos(s ,h )\ni j i j\ns hT\ni j\n= (5.20)\n∥s ∥ ·∥h ∥\ni 2 j 2\n1\nwhere∥a∥\n2\n=(a·a)2 istheEuclideannormofthevectora.\n• WeightedDot-productAttention. Thisattentionmodelinvolvesalinearmappingof\ntheinputvectorsbeforeperformingthedot-productoperation,givenby\na(s ,h ) = s W hT (5.21)\ni j i a j\nwhereW a ∈Rd h ×d h istheparametermatrixofthelinearmapping. Boththisapproach\nandthedot-productattentionapproacharealsocalledmultiplicativeattention[Ruder,\n2017].",
    "5.3 The Attention Mechanism 221\n• AdditiveAttention. Inadditiveattention,theentriesofthetwovectorsaresummedin\nsomeway. Awidely-usedformisgivenbyBahdanauetal.[2014]\na(s ,h ) = vTTanH(s W +h W ) (5.22)\ni j a i s j h\nwhere W h ,W s ∈Rd h ×da and v a ∈Rda are parameters. TanH(s i W s +h j W h ) pro-\nducesad -dimensionalvectorwhereeachentryisatransformedweightedsumofthe\na\nentriesofh ands . Itisfollowedbyadot-productwithanotherweightvectorv .\nj i a\nNowletusreturntoEqs. (5.16-5.17)andrethinktheroleofattentionweights. Eq. (5.17)\ninformallydefinesa“distribution”overh ...h ,writtenas\n1 m\nPr(h |s ) = α (5.23)\nj i i,j\nIfweconsiderharandomvariablethattakesavaluefrom{h ,...,h },thenα canbe\n1 m i,j\nthoughtofastheprobabilityofh=h ,conditionedons ,andEq. (5.16)canberewrittenas\nj i\nm\n(cid:88)\nc = Pr(h |s )·h\ni j i j\nj=1\n= E (h) (5.24)\nh∼Pr(h|si)\nInotherwords,c canbeviewedasanexpectedrepresentationofthesource-sidesequence\ni",
    "given the target-side state s , that is, the expectation of {h ,...,h } under the distribution\ni 1 m\nPr(h |s ). Thisprovidesageneralframeworkfordescribingthewaythedecoderreceivesthe\nj i\ninformationfromtheencoder: thedecoderisareceiverthatdetermineshowmuchinformation\nisacceptedfromeachsender. Forexample,intheNMTmodeloftheprevioussection,there\nis only one sender h , and so the receiver receives all the information the sender sends.\nm\nBy contrast, in the NMT model armed with the attention mechanism, there are m senders\n{h ,...,h }andthereceiverreceivesinformationaccordingtoadistributionofpreferences\n1 m\nforthesenders.\nItisstraightforwardtointroducetheattentionmodelintotheprocessofwordprediction.\nWe modify our treatment of s so as to make use of both the source-side and target-side\ni\ninformation at each decoding step. We slightly modify the definition of s to include the\ni\ncontextvectorcorrespondingtothepreviousstates ,asfollows\ni−1\ns = RNN(s ,c ,ye) (5.25)\ni i−1 i−1 i",
    "ComparedwiththemodelofEq. (5.11),themodelofEq. (5.25)takesc asanadditional\ni−1\ninput. Therefore,thismodelconsidersboththerepresentationofthetarget-sidewordsy ...y\n1 i−1\n(asencodedins andye)andtherepresentationoftheentiresource-sidesequencex ...x\ni−1 i 1 m\n(asencodedinc ). Then,thedistributionoftargetwordsatpositionicanbeconditionedon\ni−1\ns asusual\ni\nPr(·|y ...y ,x ...x ) = Pr(·|s ) (5.26)\n1 i 1 m i",
    "222 Chapter 5. Sequence-to-Sequence Models\ncompute\nPr(y |y ...y ,x ...x )\ni+1 1 i 1 m\n(i.e.,Pr(y |s ))\ni+1 i\nDecoderState DecoderState\natstepi atstepi+1\ns\ni−1\ns s\ni−1 i\nc\ni−1\nye\ni\nc =\n(cid:80)m\nα h\ni−1 j=1 i−1,j j\nweightα i−1,j of\nconnectings i−1andh\nj\n......\nααii−−11,,11 ααii−−11,,22 ααii−−11,,33 ααii−−11,,mm\nh h h ... h\n1 2 3 m\nEncoder\nFigure 5.4: An attention model for NMT. Suppose we have obtained the representations\n{h ,...,h }andthedecoderstates uptothispoint. Wewishtoobtainthedecoderstateat\n1 m i−1\nthenextstep. Tothisend,wefirstcomputeattentionweightsbynormalizingsomeattention\nscoresbetweens and{h ,...,h },andthencomputeacontextvectorc bysumming\ni−1 1 m i−1\nover{h ,...,h }withtheattentionweights. Anewdecoderstates iscreatedbytakingthe\n1 m i\ncontextvectorc ,thepreviousstates ,andthewordrepresentationye. s willbeusedas\ni−1 i−1 i i\naconditionforpredictingadistributionofwordsatstepi+1.\nwherePr(·|s )isgenerallyaSoftmaxlayer. ThisprocessisillustratedinFigure5.4.\ni",
    "WenowhaveamodelforcomputingPr(y |y ...y ,x ...x ). Abriefoutlineofthekey\ni+1 1 i 1 m\nstepsofthismodelisgivenby\n1. Encodethesource-sidesequenceash ...h whereh =RNN(h ,xe).\n1 m j j−1 j\n2. Repeatthefollowingprocedurefromi=1ton−1.",
    "5.3 The Attention Mechanism 223\na. Computethealignmentscorea(s ,h )foreachj.\ni−1 j\nb. Computetheattentionweights{α ,...,α }\ni−1,1 i−1,m\nwhereα =\nexp(a(si−1,hj))\n.\ni−1,j (cid:80)m j′=1 exp(a(si−1,h j′))\nc. Computethecontextvectorc =\n(cid:80)m\nα ·h .\ni−1 j=1 i−1,j j\nd. Computethetarget-sidestates =RNN(s ,c ,ye).\ni i−1 i−1 i\ne. Computethedistributionoftarget-sidewordsPr(·|s ).\ni\nf. ComputePr(y |y ...y ,x ...x )=Pr(y |s )foragivenwordy (asin\ni+1 1 i 1 m i+1 i i+1\ntraining),orselectthemostlikelywordyˆ =argmax Pr(y |y ...y ,x ...x )\ni+1 yi+1 i+1 1 i 1 m\n(asintesting).\nInreal-worldsystems,thisbasicmodelcanbemodifiedtobetterpredictthetarget-side\nwords. Forexample, wecanintroducefusionlayerstocombines , c , andye beforethe\ni i−1 i\nSoftmaxlayersothatwehaveadeepermodelforprediction[Bahdanauetal.,2014]. Another\ncommonlyusedapproachistostackmultipleRNNlayersonthetarget-side. Inthiscase,one\ncanperformattentionineithereachlayerofthestack[Wuetal.,2016]orthetop-mostlayer",
    "ofthestack[Luongetal.,2015]. SeeSection5.3.5formoreinformationaboutmulti-layer\napproachestoattention.\n5.3.2 The QKV Attention\nBecause the attention mechanism is such a powerful approach, many variants have been\ndeveloped. Perhapsthemostwidelyusedapproachistoreframetheattentionproblemasone\nof matching a query in a set of key-value pairs. It lays the foundation for the well-known\nsequencemodel—Transformer[Vaswanietal.,2017].\nHereweassumethatthereareanumberofkey-valuepairs{(k ,v ),...,(k ,v )}anda\n1 1 m m\nqueryq. Thegoalofthequery-key-valueattention(orQKVattention)modelistoobtaina\nvaluebyconsideringthecorrespondencebetweenthequeryandthekeys. Thisisastandard\nsearchingproblemindatabasesystemsinwhichinformationisreturnedinitsoriginalform\nor a new form when it matches the query. In the QKV attention, the result of searching is\nnotasinglevaluein{v ,...,v }butinsteadacombinationofthesevalues. Thisisthekey\n1 m\ndifferenceofthisattentionmodelcomparedwiththeconventionalmodelsofsearching.",
    "Formally,theresultoftheQKVattentionisdefinedtobe\nm\n(cid:88)\nc = α v (5.27)\nj j\nj=1\nwhere\nqkT\nj\nα = Softmax( ) (5.28)\nj\nβ\nistheattentionweight. Itturnsoutthattheabovemodelhaspreciselythesamegeneralform\nasthemodeldescribedintheprevioussubsection,andccanbesimplyviewedasacontext",
    "224 Chapter 5. Sequence-to-Sequence Models\nvector.\nWhilethebasicformoftheQKVattentionisnotsomething“new”,itcanhandleavariety\nofproblemsbygivingq,k andv appropriatemeanings. Hereweconsideramoregeneral\nj j\ncase where there are n queries {q ,...,q } and n output vectors {c ,...,c }. To simplify\n1 n 1 n\nnotation,weuseQtodenoteamatrixwherethei-throwvectorisq ,likethis\ni\n \nq\n1\n.\nQ =   . .   (5.29)\nq\nn\n     \nk v c\n1 1 1\n. . .\nLikewise,wecandefineK=  . .   ,V=  . .   ,andC=  . .   . Then,theattentionmodel\nk v c\nm m n\ncanbeformulatedas\nQKT\nC = Softmax( )V (5.30)\nβ\nFigure5.5showsanillustrationofthisequation.\nNotethatSoftmax(QKT\n)computesa\nβ\nmatrixofattentionweights\n \nα ... α\n1,1 1,m\nQKT\n. .\nSoftmax( β ) =   . . . .   (5.31)\nα ... α\nn,1 n,m\n(cid:104) (cid:105)\nwherearowvector α ... α representsadistributionover{v ,...,v }. Wecanthen\ni,1 i,m 1 m\nexpandEq. (5.30)foreasyunderstandingofthemodel\n \nc\n1\n.\nC =   . .  \nc\nn\n(cid:80)m\nα v\n\nj=1 1,j j\n.\n=   . .  ",
    "(cid:80)m\nα v\nj=1 n,j j\n  \nα ... α v\n1,1 1,m 1\n. . .\n=   . . . .     . .   (5.32)\nα ... α v\nn,1 n,m m\nIn sequence-to-sequence modeling, Q, K and V can be defined in several different\nways. Todescribethecorrespondencebetweenthesource-sideandtarget-sidesequences,one",
    "5.3 The Attention Mechanism 225\nQueries(e.g.,{s ,...,s }) Keys(e.g.,{h ,...,h }) Values(e.g.,{h ,...,h })\n1 n 1 m 1 m\nq 1 q 2 q 3 k 1 k 2 k 3 v 1 v 2 v 3\n q 1   k1   v 1 \nQ= q 2  K= k2  V= v 2 \nq 3 k3 v 3\nQKT\nq\n1\nq\n2\n× kT kT kT =\n1 2 3\nq\n3\nq3kT\n3\nα\n(cid:16) (cid:17)\nSoftmax =\nβ\nα3,3\nReturnedValuesC\nv c\n1 1\n× v 2 = c 2 c i = (cid:80)3 j=1 α i,j v j\nv c\n3 3\nFigure5.5: TheQKVattentionmodelforbatchesofqueries(Q),keys(K),andvalues(V). The\nfigureshowsadirectimplementationoftheformulaC=Softmax(QKT\n)V.\nSoftmax(QKT\n)\nβ β\ncomputestheattentionweightsbynormalizingascaleddot-productofQandKT. Thisresults\ninamatrixαinwhicharowvectordescribesweightsofdifferentvalues. Bymultiplyingα\nwithV,weobtainasequenceofnewvalues,eachexpressingaweightedsumoftheoriginal\nvalues.\napproach,calledencoder-decoderattention,istosimplyassumethat\n \ns\n1\n.\nQ =   . .   (5.33)\ns\nn",
    "226 Chapter 5. Sequence-to-Sequence Models\nand\n \nh\n1\n.\nK = V =   . .   (5.34)\nh\nm\nIn this case, C is a sequence of new representations of the source-side sequence given the\nrepresentations of the target-side sequence. As with the model described in the previous\nsubsection,eachc ∈Ccanbeusedtopredictthewordy .\ni i+1\nIn addition to applying the model to sequence-to-sequence problems, another type of\napproachistoregarditasasequencemodel,thatis,weusetheQKVattentiontorepresenta\nsequenceinonelanguage. Inthiscase,theQKVattentionisalsocalledself-attentionwhich\nformsthebasisofthewell-knownTransformermodel[Vaswanietal.,2017]. Consider,for\nexample,thesequenceofstatesh ...h . Theself-attentionmodelassumesthat\n1 m\n \nh\n1\n.\nQ = K = V =   . .   (5.35)\nh\nm\nThen,theoutputofthemodelisasequenceofrepresentationsc ...c . c isarepresentation\n1 m j\nwhichconsidersthecorrelationsbetweenh andanyotherelementoftheinputsequence. We\nj\nwillseeamoredetaileddiscussiononthismodelinChapter6.",
    "5.3.3 Multi-head Attention\nMulti-head attention is an interesting extension to the above models. The key idea is to\nperformattentionin differentsub-spacesofrepresentationssimultaneouslyrather than ina\nsinglespaceofrepresentations. Toillustrate,considerastandardattentionmodelthattakes\nsequencesofsource-sideandtarget-sidestatesandoutputsasequenceofnewstates,writtenas\nc ...c = Att(h ...h ,s ...s ) (5.36)\n1 n 1 m 1 n\nwhere h\nj\n,s\ni\n,c\ni\n∈Rd h, and Att(·) is the attention function. We can map h\nj\ninto τ vectors\n[1] [τ]\n{h ,...,h }viathefollowinglineartransformations\nj j\n[1] [1]\nh = h W (5.37)\nj j h\n.\n.\n.\n[τ] [τ]\nh = h W (5.38)\nj j h\nwhereh [ j 1] ,...,h j [τ] ∈Rd τ h ,andW h [1] ,...,W h [τ] ∈Rd h × d τ h .\n[1] [τ]\nSimilarly,wecanmaps intoτ vectors{s ,...,s }. Wethendefineτ featuresub-spaces\ni i i\ninwhichtheattentionfunctionisperformedindependently. Forthek-thfeaturesub-space,we",
    "5.3 The Attention Mechanism 227\nhave\nc [k] ...c[k] = Att(h [k] ...h[k],s [k] ...s[k]) (5.39)\n1 n 1 m 1 n\nTheoutputofthemodelisasequenceofd -dimensionalvectors,eachofwhichisobtained\nh\nbyconcatenatingthevectorsthatareproducedinallthesefeaturesub-spaces,followedbya\nlineartransformation. Thisprocedureisgivenby\n[1] [τ]\nc = [c ,...,c ]W (5.40)\n1 1 1 c\n...\nc = [c[1],...,c[τ]]W (5.41)\nn n n c\nwhereW c ∈Rd h ×d h.\nFollowing the notation used in the previous subsection, we can express a sequence of\n     \nh s c\n1 1 1\n. . .\nvectorsasamatrix,say,H=  . .   ∈Rm×d h,S=  . .   ∈Rn×d h,andC=  . .   ∈Rn×d h.\nh s c\nm n n\nUsingthisnotation,werewriteEq. (5.36)as\nC = Att(H,S) (5.42)\nTogiveaformaldefinitionofmulti-headattention,wefirstintroducethesplitandmerge\nfunctions. Thesplitfunctiondivideseachrowvectorofamatrixintoanumberofsub-vectors,\nresultingina3Dtensor. Forexample,splittingam×d matrixAwithτ producesaτ×m×d h\nh τ\ntensor3\nA = Split(A,τ) (5.43)\nheads",
    "Themergefunctionhasareverseformofthesplitfunction. Givenaτ×n×d h tensor(say\nτ\nA ),itmergeseachgroupofτ d h-dimensionalsub-arraysintheform\nheads τ\nA = Merge(A ,τ) (5.44)\nmerge heads\nThustheformofmulti-headattentionisgivenby\nC = C W\nmerge c\n= Merge(C ,τ)W\nheads c\n= Merge(Att(H ,S ),τ)W (5.45)\nheads heads c\nH = Split(HW ,τ) (5.46)\nheads h\nS = Split(SW ,τ) (5.47)\nheads s\n3Aa×b×ctensorcanbetreatedasanarrayofamatriceswhoseshapesareb×c.",
    "228 Chapter 5. Sequence-to-Sequence Models\nC∈Rn×d\nh\nMerge&Project\nMerge(C ,τ)W\nheads c\nC\nheads\n∈R3×n× d\n3\nh\nAtt(H ,S )\nheads heads\nH S\nheads heads\n∈R3×m× d\n3\nh ∈R3×n× d\n3\nh\nProject&Split Project&Split\nSplit(HW ,τ) Split(HW ,τ)\nh s\nH∈Rm×d\nh\nS∈Rn×d\nh\nFigure 5.6: An attention model with τ =3 heads. First, we transform the input matrices\nintomulti-headrepresentations,i.e.,3DtensorsH heads ∈R3×m× d 3 h andS heads ∈R3×n× d 3 h .\nThese tensors are then taken by an attention model. The output of this model is a tensor\nC heads ∈R3×n× d 3 h . WethenmergetheheadsofC heads ,followedbyalineartransformation.\nFinally,weobtainnvectorsofsized ,representedbyann×d matrix.\nh h\nwhereW h ,W s ∈Rd h ×d h aretheparameters. Split(HW h ,τ)implementstheprojectionsof\nEqs. (5.37-5.38)forallh . Likewise,wecanhavethemeaningofSplit(HW ,τ). Notethat\nj h\nhereAtt(·)isextendedtodealwithmulti-headinputs. SeeFigure5.6foranillustrationofthis\nmodel.\nMulti-headattentionisaverygeneralapproachthatcanbeextendedtomanymodels. Asa",
    "simpleexampleofthisextension,considertheQKVattentionmodeldiscussedintheprevious\nsubsection. LetAtt\nQKV\n(Q,K,V)betheattentionfunction,andQ∈Rd k,K∈Rd k,V∈Rdv",
    "5.3 The Attention Mechanism 229\nbetheinputs. Themulti-headQKVattentionmodelisgivenby\nC = Merge(Att (Q ,K ,V ))W (5.48)\nQKV heads heads heads c\nQ = Split(QW ,τ) (5.49)\nheads q\nK = Split(KW ,τ) (5.50)\nheads k\nV = Split(VW ,τ) (5.51)\nheads v\nwhereW q ∈Rd k ×d k,W k ∈Rd k ×d k,W v ∈Rdv×dv,W c ∈Rdv×dv arethemodelparameters.\nOneadvantageofmulti-headattentionisthatthefeaturesub-spaceswilleachdescribea\ndifferentperspectiveofattention(callitanattentionheadorheadforshort). Therefore,the\nconcatenationoftheoutputsovertheseheadsrepresentsanensembleofattentionmodelsthat\ndealwithdifferentpartsofthedata. Thisissimilartolearningagroupofmodelsindependently\nandcombiningthemtoformastrongermodel. Thistypeofmachinelearningapproachhas\nbeenproventobeusefulinmanyproblems[OpitzandMaclin,1999;Zhou,2012b]. Notethat\nthe multi-head attention models discussed here are parameterized by the linear projections\nontheinputandoutputspaces. Theuseoftheselinearprojectionsisgenerallyhelpfulasthe",
    "modelsbecomedeeperandcandescribemorecomplexproblems.\nFromanarchitecturedesignperspective,multi-headattentionfallsintoabroadclassof\nneural networks — those involving a number of branches of layer stacks for dealing with\nthe same input (call them multi-branch neural networks). However, unlike conventional\napproaches,whichrequiredifferentmodelarchitecturesfordifferentbranches,themulti-head\nattentionapproachisbasedonasinglemodelforalltheheads. Asaresult,suchsystemsare\nveryefficientinpracticebecausetheattentionprocedurecanruninparallelovertheseheads.\n5.3.4 Hierarchical Attention\nIn many cases the underlying structure of an NLP problem is hierarchical. For example,\ndocumentsmayhaveamulti-levelstructure: adocumentismadeupofsentences,asentenceis\nmadeupofwords,andawordismadeupofcharacters. Itisthereforedesirabletomodifythe\nattentionmodelstotakeintoaccountthehierarchicalnatureofthisdata[Yangetal.,2016].\nTo illustrate, we consider a simple problem where the source-side has a 2-level tree",
    "structure. Supposethesource-sidesequenceisaconcatenationofanumberofsub-sequences\n{u¯ ,...,u¯ }. Eachu¯ yieldsasequenceofwords\n1 T t\nu¯ = x ...x (5.52)\nt p(t,1) p(t,|u¯t|)\nwherep(t,i)isthepositionofthei-thwordofu¯ intheentiresource-sidesequencex ...x .\nt 1 m\nThen,thesequencex ...x canbewrittenasacompositionofT sub-sequences:\n1 m\nx ...x =x ...x x ...x ... x ...x (5.53)\n1 m p(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯ T|)\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nu¯1 u¯2 u¯\nT",
    "230 Chapter 5. Sequence-to-Sequence Models\nSimilarly,theencoderoutputh ...h canbewrittenas\n1 m\nh ...h = h ...h h ...h ... h ...h (5.54)\n1 m p(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯ T|)\nOn the target-side, we assume that there are two sequences of state vectors: one for\nplacing the standard representations of the target-side sequence (i.e., s ...s ) and one for\n1 n\nplacinghigher-levelrepresentationsofs ...s . Letϕ(i)denotethepositioninthehigher-level\n1 n\nsequenceofs ,ands¯ denotethecorrespondingstatevector. Foreachi,wethushaveapair\ni ϕ(i)\nofstatevectorss and¯s . Ingeneral,therelationshipbetweens and¯s comesfromthe\ni ϕ(i) i ϕ(i)\nhierarchicalstructureoftheproblem. Forexample,s istherepresentationofaword,and¯s\ni ϕ(i)\nistherepresentationofthesentencethewordbelongsto4.\nAsbefore, ourgoalistoobtainacontextvectorc foreachtarget-sidepositioni. Here\ni\nwe still take c to be a weighted sum of {h ,...,h }, as in Eq. (5.16). All that remains is\ni 1 m",
    "tospecifytheattentionweightforeachh . Asafirststepweattends toeachu . Thisisa\nj i t\nstandardprocedure. Wejustneedtoruntheattentionmodelonh ...h insteadof\np(t,1) p(t,|u¯t|)\nh ...h ,givenby\n1 m\nh¯ = Att(h ...h ,s )\nt p(t,1) p(t,|u¯t|) i\n|u¯t|\n(cid:88)\n= π h (5.55)\ni,k,t p(t,k)\nk=1\nwhereπ istheattentionweightrestrictedtou . h¯ isarepresentationofu ,andsowehave\ni,k,t t t t\nanewsequenceofrepresentationsh¯ ...h¯\n.\n1 T\nThen, we run the attention model on\nh¯ ...h¯\nto perform a second round of attention.\n1 T\nThisisdonebyattendings toh¯ ...h¯ . Theoutputisacontextvectorforthehierarchical\nϕ(i) 1 T\nattentionmodel,givenby\nc = Att(h¯ ...h¯ ,s )\ni 1 T ϕ(i)\nT\n(cid:88)\n= γ h¯ (5.56)\ni,t t\nt=1\nwhereγ istheweightofattendings toh¯ . SubstitutingEq. (5.55)intoEq. (5.56),wecan\ni,t ϕ(i) t\nwritec as\ni\nT |u¯t|\n(cid:88)(cid:88)\nc = γ π h\ni i,t i,k,t p(t,k)\nt=1k=1\nm\n(cid:88)\n= α h (5.57)\ni,j j\nj=1\nWhilethenotationinthissubsectionisabitcomplicated,theformoftheresultingmodel",
    "4Ifthea-thsentencecoverswordsfrompositionbtoc,thenϕ(b)=ϕ(b+1)=...=ϕ(c)=a.",
    "5.3 The Attention Mechanism 231\nc\ni\n2ndattention Att(·)\nh¯ 1 ... h¯ t ... h¯ T s ϕ(i)\n1stattention Att(·)\nh ... h ... h ... h s\np(1,1) p(t,1) p(t,|u¯t|) p(T,|u¯ T|) i\nsub-sequenceu¯\nt\nFigure 5.7: A 2-level hierarchical attention model. The input sequence h ...h is made\n1 m\nup of T sub-sequences. For each sub-sequence u¯ , an attention model is used to produce\nt\na context vector h¯ by considering the target-side state (i.e., s ) and the representations of\nt i\nthesub-sequence(i.e.,h ...h ). TheresultofrunningthisprocedureontheT sub-\np(t,1) p(t,|u¯t|)\nsequences is T level-1 representations h¯ ...h¯ . They are then taken by a second attention\n1 T\nmodeltoconsidertheattentionbetweentheserepresentationsandahigher-leveltarget-side\nstate s . This results in the context vector c which describes the attention between the\nϕ(i) i\ntarget-sidestates andtheentiresource-sidesequenceh ...h .\ni 1 m\nis simple. We still combine {h ,...,h } in a linear manner but with new weights [Maruf\n1 m",
    "etal.,2019]. Computingα describesagenerativeprocessinwhichwefirstdeterminethe\ni,j\nweightofeachsub-sequenceandthendeterminetheweightofeachwordinasub-sequence,as\nillustratedinFigure5.7. Seebelowforanalignmentamongdifferenttypesofattentionweight.\nsequence h ... h h ... h ...... h ... h\n1 |u1| |u1|+1 |u1|+|u2| (cid:80)T\nt=\n−\n1\n1|ut|+1 m\nweight(α) α ... α α ... α ...... α ... α\ni,1 i,|u1| i,|u1|+1 i,|u1|+|u2| i,(cid:80)\nt\nT\n=\n−\n1\n1|ut|+1 i,m\nsequence h ... h h ... h ...... h ... h\np(1,1) p(1,|u¯1|) p(2,1) p(2,|u¯2|) p(T,1) p(T,|u¯T|)\nweight(γ) γ ... γ γ ... γ ...... γ ... γ\ni,1 i,1 i,2 i,2 i,T i,T\nweight(π) π ... π π ... π ...... π ... π\ni,1,1 i,|u¯1|,1 i,1,2 i,|u¯2|,2 i,1,T i,|u¯T|,T",
    "232 Chapter 5. Sequence-to-Sequence Models\n5.3.5 Multi-layer Attention\nSo far wehave consideredthe case ofsingle-layer attention— theoutput of theattention\nmodelsiswrittenasalinearcombinationofthesource-siderepresentations. Nowweextendit\ninanaturalwaytomulti-layerattentioninwhichthesingle-layerattentionprocedurerunsa\nnumberoftimesforforminga“deeper”attentionmodel.\nTodothis,amulti-layerneuralnetworkiscreatedonthetarget-side. Themodelarchitecture\nisregular. Westackanumberofattentionlayers,eachinteractingwiththesource-sidesequence\nandfeedingitsoutputtothenextlayer. Inanattentionlayer,weperformattentionasusual.\nForthel-thlayerinthestack,thissteptakesthesource-sidesequence(denotedbyh ...h )as\n1 m\nwellastheoutputofthepreviouslayer(denotedbysl−1...sl−1),andproducesasequenceof\n1 n\nvectorsby\ncl...cl = Att(h ...h ,sl−1...sl−1) (5.58)\n1 n 1 m 1 n\nwhereAtt(·)couldbeanyattentionfunctiondescribedinthischapter.\nThen,wecreateanotherneuralnetworkf(·)togivemoremodelingpowertothemodel.",
    "Theoutputoftheattentionlayeristhusdefinedtobe\nsl...sl = f(cl...cl ,sl−1...sl−1) (5.59)\n1 n 1 n 1 n\nf(·)canbedesignedinmanyways[Sukhbaataretal.,2015;Wuetal.,2016;Vaswanietal.,\n2017]. A popular choice is to define f(·) as a feed-forward neural network with a residual\nconnection,givenby\nf(cl...cl ,sl−1...sl−1) = FFN(cl...cl ) + sl−1...sl−1 (5.60)\n1 n 1 n 1 n 1 n\nSubstitutingforthevectorscl...cl ,usingEq. (5.58),theoutputoflayeriiswritteninthe\n1 n\nform\nsl...sl = FFN(Att(h ...h ,sl−1...sl−1)) + sl−1...sl−1 (5.61)\n1 n 1 m 1 n 1 n\nAs with the models in the previous subsections, it is convenient to use a more compact\nnotation by expressing a sequence of vectors as a matrix. Thus this model can be given in\nanotherform\nSl = FFN(Att(H,Sl−1))+Sl−1 (5.62)\nHereFFN(·)isgenerallyamulti-layerneuralnetworkwithnon-linearactivationfunctions.\nThe identity mapping (i.e., +Sl−1) creates a direct path from the input to the output of the\nlayer,makingiteasiertotrainadeepneuralnetwork.",
    "Figure5.8showsthearchitectureofthismodel. Theattentionmodelstartswiththeinitial",
    "5.3 The Attention Mechanism 233\nS2\nFFN(·)\nC 2 Layer2\nAtt(·)\nH S1\nFFN(·)\nC 1 Layer1\nAtt(·)\nH H S0=S\nFigure 5.8: A 2-layer attention model. These layers take the same “key-value” pairs (i.e.,\nH) but each takes a different “query” (i.e., Sl). The attention model is standard: context\nvectors Cl are generated by taking both H and Sl. A feed-forward neural network is built\nto transform Cl, followed by an addition of Sl. So this model can be formulated as Sl =\nFFN(Att(H,Sl−1))+Sl−1. Sl isthenusedinthenextlayerasthequery,thatis,layerl+1\ntakesHandSl,andrepeatstheaboveprocess. Theoutputofthelastlayercanbeviewedasa\ndeeperrepresentationofHforS.\n \ns\n1\n.\nrepresentation of the target-side sequence, that is, S0 =S=  . .   . If there are L attention\ns\nn\nlayers,thenthefinaloutputwillbeSL.\n5.3.6 Remarks\nAboveweconsideredabasicattentionmodelandaseriesofrefinements. Theliteratureon\nattention and related topics contains a wide range of methods for modeling how a system",
    "concentrates on different parts of the input, as well as a wide range of applications of such",
    "234 Chapter 5. Sequence-to-Sequence Models\nmodels. Thissubsectionprovidesdiscussionsonsomeoftheinterestingissues.\n1. Alignment vs Attention\nAttention is related to a long line of research on alignment approaches to modeling the\ncorrespondencebetweentwogroupsoflanguageunits. InNLP,alignmentisaverygeneral\nconcept that is used to refer to several problems. For example, most statistical machine\ntranslation systems are trained on bilingual texts which are annotated with word-to-word\nalignment[Koehnetal.,2003;Chiang,2005]. Wordalignmentmodelsarethusdevelopedto\ngeneratelinksbetweenwordsintwosentences[Vogeletal.,1996;OchandNey,2003;Taskar\netal.,2005;Dyeretal.,2013]. Whiletheoutputsofthesesystemsarediscretevariables,the\nunderlyingmodelsaremostlyprobabilisticandcontinuous. Therefore, thecorrespondence\nbetweenwordalignmentandtheattentionmodelsdiscussedhereisapparentbecausetheyare\nbothlearnedtoassignaweighttoeachpairofwords.",
    "Notethatdespitethesimilaritybetweenalignmentandattentionproblems,theirgoalsare\ndifferent. In most cases word alignment models are used as individual systems to produce\nalignmentresultsfordownstreamsystems,whereasattentionmodelsaretypicallytreatedas\ncomponentsofbiggersystemsanddonotworkalone(seeFigure5.9foracomparisonofthese\nmodels). This makes them fit different types of sequence-to-sequence systems in practice:\nwordalignmentisonestepofapipelinedsystem,andattentionissomeintermediatestatesofa\nneuralnetwork.\nNevertheless,wordalignmentandattentionaretworelatedproblems,andcanhelpeach\notherinsomecases. Forexample,onewaytoseehowanattentionmodelbehavesistoinduce\nwordalignmentsfromitandmeasurethequalityofthesewordalignments[Tuetal.,2016;Li\netal.,2019;Gargetal.,2019]. Also,systemsequippedwiththeattentionmechanismcanbe\nguidedbyexternalwordalignmentresources[Mietal.,2016b;Liuetal.,2016b].\n2. Introducing Priors\nAsdiscussedinSection5.3.1,theattentionmodelsimplicitlydefineanattentiondistribution",
    "over{h ,...,h }bywhichwecancomputeaweightedsumoftheserepresentations. This\n1 m\ndistributionisexpressedintermsofthealignmentweightsandislearnedaspartofamodel.\nInadditiontolearningtheattentiondistributioninanend-to-endfashion,wecanalsodefine\nitbasedonourknowledgeabouthowweconcentrateondifferentpartsofasequencewhen\nreadingit.\nOneapproachistodirectlyimposesomestructureonthedistribution. Asimpleexample\nis that weconsider only a span ofthe sequence for attention anddiscard the rest. Let [ρ −\ni\nD,ρ +D]bea2D+1wordwindowcenteredatpositionρ ofthesource-sidesequence. We\ni i\ncanconnects onlytoh ...h andobtainalocalcontextvectorinthefollowingform\ni ρi−D ρi+D\nc = Att(h ...h ,s ) (5.63)\ni ρi−D ρi+D i\nThis approach is also called local attention. The problem of determining ρ is similar\ni\ntothereorderingprobleminmachinetranslation. Fortranslationbetweenlanguageswith",
    "5.3 The Attention Mechanism 235\nmean , while the statio h n asmade ne w progre in ss co mp c re o h n e s n t . r s u\niv\nc\ne\ntion\n同时\n,\n场\n站\n全面\n建设\n也\n取得\n了\n新\n成绩\n.\n(a)Aheatmapofwordalignments\nmean , while the statio h n asmade ne w progre in ss co mp c re o h n e s n t . r s u\niv\nc\ne\ntion\n同时\n,\n场\n站\n全面\n建设\n也\n取得\n了\n新\n成绩\n.\n(b)Aheatmapofattentionweights\nFigure5.9: HeatmapsofawordalignmentmodelandanattentionmodelforapairofChinese\nand English sentences. The heat maps are represented as shaded grids in which each cell\ndescribesthecorrespondenceofapairofChineseandEnglishwords. Thiscorrespondence\nisshownonacolorscalerangingfromwhitedenotingaweightof0topurebluedenotinga\nweightof1.",
    "236 Chapter 5. Sequence-to-Sequence Models\nsimilarwordorders,itiscommontoassumethatthetranslationismonotonicandρ islinear\ni\nwithrespecttoi[Koehn,2004],e.g.,ρ =⌊mi⌋or⌈mi⌉. Analternativemethodistousea\ni n n\nneuralnetworktopredictarelativepositioninthesource-sidesequence(denotedbyr )[Luong\ni\netal.,2015]. ρ canthenbedefinedtobe⌊mr ⌋or⌈mr ⌉.\ni i i\nIn another thread of research, a new distribution is derived by combining the original\nattentiondistributionandsomepriordistribution. Thesimplestsuchdistributiontakestheform\noflinearinterpolation\nP(cid:102)r(h\nj\n|s\ni\n) = η·Pr(h\nj\n|s\ni\n)+(1−η)·Prior (5.64)\nwhere Prior is the prior, and η is the interpolation coefficient. When η=1, it is astandard\nattentionmodel. Bycontrast,whenη=0,theattentioniscompletelydependentontheprior\n[Youetal.,2020].\nThe prior can be chosen in many ways. A simple choice is to assume Prior to be a\nGaussiandistributionGaussian(µ,σ2). Thismakesthemodelwellexplained: theattention",
    "is concentrated on some point of the sequence and decreases its strength as we spread the\nattentionfromthispoint. TodeterminethemeanandvarianceoftheGaussiandistribution,we\ncanusethesametechniquedescribedabove,say,wedeveloptwoneuralnetworkstocompute\nthemrespectively.\nTheinterpolationcanalsobeconsideredanintermediatestepofcomputingtheattention\ndistribution. For example, consider the QKV attention discussed in Section 5.3.2. The\ninterpolationcanbeplacedonthequery-keydot-product[Yangetal.,2018a;Guoetal.,2019].\nTothisend,wecanmodifyEq.(5.28)inthefollowingform\nqkT\nj\nα = Softmax( +ηPrior)\nj\nβ\ns hT\ni j\n= Softmax( +ηPrior) (5.65)\nβ\nAs\nqkT\nj (or\nsihT\nj )isnotconstrainedin[0,1],Priorisre-scaledbyahyper-parameterη.\nβ β\nSometimes, priors arise in the context where the knowledge of attention comes from\nexternalsources. Asdiscussedabove,incorporatingwordalignmentsintoattentionmodelsis\noneofthesimplestwaystodothis. Theideacanbeextendedtomakeuseofmorestructural",
    "information, such as fertility and coverage [Cohn et al., 2016; Feng et al., 2016; Tu et al.,\n2016], or more task-specific constraints, such as monotonic alignments between input and\noutputsequences[Raffeletal.,2017;ChiuandRaffel,2018]. Also,aswithsyntacticmachine\ntranslationsystems,parsetreescanbeusedtobiastheprocessofattentionasanauxiliaryinput.\nFor example, dependency trees are a widely used source of information in modeling word\ncorrespondenceforeithersequence-to-sequence[Chenetal.,2018a]orsequencemodeling\nproblems[Zhangetal.,2020c;Nguyenetal.,2020;Xuetal.,2021b].\nSince attention models can be computationally expensive in large-scale applications,\nresearchers have also developed efficient attention models by introducing more inductive",
    "5.3 The Attention Mechanism 237\nbiasesintomodeldesign[Tayetal.,2020b]. Thislineofresearchcanbroadlybecategorized\nintoefficientmethodsforNLP.InChapter6wewillpresentadiscussion.\n3. Attention in Memory Networks\nAswellasbeingofgreatinterestinsequence-to-sequencesystems,theattentionmechanism\nis extensively used in memory-based neural models [Sukhbaatar et al., 2015; Graves et al.,\n2014;Kumaretal.,2016]. AsdiscussedinChapter4,amemorysystemmaintainsacollection\nofdataitemsandallowsuserstoretainandretrieveinformation. Givenaquery,itcomputes,\ninsomeway,thematchbetweenthequeryandthekeyofeachdataitem. Thisprocedureis\nalso called addressing [Graves et al., 2014]. Such addressing is typically implemented by\nfirst representing the query and the data item as real-valued vectors, and then calculating a\nweightbyconsideringsomesimilaritybetweenthetwovectors. Theresultoftheretrievalisa\nweightedsumofallthedataitems. ThisformalismfitsperfectlywiththemodeloftheQKV\nattentiondiscussedinSection5.3.2.",
    "Providedtheattentionmechanismandthememorymechanismarecorrelated,thoughnot\nfromapsychologyperspective,wecanviewattentionasaprocessofretrievinginformationin\namemory(i.e., {h ,...,h })foragivenquery(i.e., s ). Thuswecaninterpretasequence-\n1 m i\nto-sequence system with the attention mechanism as follows. On the source-side, we store\ninformation in a memory represented as a sequence of vectors h ...h . Then, we retrieve\n1 m\nfromthismemorytorecoverstepbystepthesource-sideinformationonthetarget-side.\n4. Beyond Sequence-to-Sequence Problems\nWhilewerestrictourdiscussiontotheproblemoftransformationfromonesequencetoanother\nsequence in this section, the general approach of attention can be used to deal with other\nproblems. AsmentionedinSection5.3.2,andwillbediscussedinChapter6,awell-known\nvariantofthisapproachisself-attention. Inself-attention,theQKVattentionmodelisused\nasasequencemodel,andwehaveonlyonesequenceofvariablesasinput. Asaresult,the",
    "outputsof thisattentionmodelcan betreatedas newrepresentations oftheinput sequence.\nSelf-attention provides a general approach to modeling the interactions and dependencies\nbetween input variables, and so can be applied to a variety of problems. For example, we\ncanconcatenateh ...h ands ...s asanewsequenceh ...h s ...s ,andrunthismodelon\n1 m 1 n 1 m 1 n\nthesequence. Inthisway,self-attentioniseasilyextendedtoasequence-to-sequencemodel\n[LampleandConneau,2019;Raffeletal.,2020]. Suchanapproachevenworkswhenh ...h\n1 m\nands ...s representdifferenttypesofdata. Forexample, wecanuseh ...h torepresent\n1 n 1 m\natextanduses ...s torepresentanimage. Then,wehaveamulti-modalmodelthatfuses\n1 n\ntextualandvisualrepresentationsbyperformingself-attentiononthem[Chenetal.,2020c].\nAnotherapproachtojointrepresentationlearningofsequencesistobuildmultipleattention\nmodelssothateachsequencecanlearnfromothersequences. Anexampleofsuchmodels",
    "isco-attention,whichhasbeenwidelyusedinmulti-modallanguageprocessing[Luetal.,\n2016]. Forexample,forthepurposesofvisualquestionanswering(VQA),wewishtofigure\nout which parts of the image are related to a word of the question and to figure out which\nwordsofthequestionarerelatedtoagivenpartoftheimage. Todothiswewillbuildtwo",
    "238 Chapter 5. Sequence-to-Sequence Models\nattention models: one for image-to-text attention, and one for text-to-image attention. The\noutputsofbothmodelscanbethoughtofasjointrepresentationsfortheimageandtext,and\nthuscanbeusedasfeaturesforanswerprediction.\nTheattentionmodelsdiscussedinthissectionareorder-independentforinput. Thisisan\nissuefordealingwithsequentialdata,andcanbeaddressedbyencodingorderinformation\nin the inputs themselves (see Chapters 4 and 6). On the other hand, the simplicity of this\nformulationmakesthesemodelsgeneral: theinputdataofthemodelsneedsnottobesequential.\nAsaresult,theattentionmodelscanbedirectlyappliedtomorecomplexdata,suchasgraphs\n[Velicˇkovic´ etal.,2018;Leeetal.,2019].\n5.4 Search\nSearchisafundamentalissueinartificialintelligence,andplaysanimportantroleinmany\nNLPsystems. Thesearchproblemisacomputationalchallengeherebecausethenumberof\nhypothesesinthesearchspaceincreasesexponentiallywiththelengthofthesequenceand",
    "the size of the vocabulary on the target-side. Exhaustive search in this case is simply slow.\nTherefore, real-world systems often involve search algorithms or heuristics to ensure that\noptimalorsub-optimalsolutionscanbefoundinanacceptabletime.\nFormanypracticalsequence-to-sequenceapplications,thesearchproblem,alsocalledthe\ninferenceproblemsometimes,canbeformulatedasthefollowingequation\nyˆ = argmaxScore(x,y) (5.66)\ny∈Ω\nwhereScore(x,y)isamodelthatmeasuresthegoodnessofygivenx.\nThisequationtakesaslightlydifferentformfromthatofEq. (5.2). First,weuseScore(x,y)\ninsteadofPr(y|x)asthegoodnessfunction. Whileatypicalapproachtotrainingsequence-to-\nsequencemodelsistomaximizePr(y|x)(orPr(x,y)),weoftenneedtoconsidertask-specific\nproblemswhenperforminginferenceontestdata,forexampletheproblemoflengthbias. Itis\nthereforecommontoinvolveotherterms,aswellasPr(y|x),todefinetheobjectivefunction\nforsearch(seeSection5.4.1). AseconddifferencebetweenEq. (5.66)andEq. (5.2)arises",
    "fromtheformofthesearchspacewhichisconstrainedtoΩ. Ingeneral,Ωisaprunedsearch\nspaceandcontainsarelativelysmallnumberofhypotheses. Acommonwaytoachievethis\nisthroughtheuseofpruningtechniquesandadvancedsearchalgorithms(seeSection5.4.2).\nInthissectionweconsidersolutionstotheseproblemsandsomeoftherefinements. These\nmethodsarelargelymotivatedbythedevelopmentofmachinetranslation,butthediscussions\nherearegeneralandcanbeconsideredinmosttextgenerationproblems.\n5.4.1 The Length Problem\nRecallfromSection5.2.2thattheprobabilityofthetarget-sidesequenceygiventhesource-\nsidesequencexcanbewrittenasaproductofprobabilitiesofeachwordy givenboththe\ni\ngenerated words y ...y and x. Here we re-express Eq. (5.14) using simpler notation, as\n0 i−1",
    "5.4 Search 239\nfollows\nn\n(cid:89)\nPr(y|x) = Pr(y |y ,x) (5.67)\ni <i\ni=1\nwherey denotesthesequencey ...y . Thiscanbeequivalentlyexpressedintermsoflog\n<i 0 i−1\nprobabilities\nn\n(cid:88)\nlogPr(y|x) = logPr(y |y ,x) (5.68)\ni <i\ni=1\nSuch a simple form of modeling has clear advantages as practical models for NLP, but\nunfortunately,thisleadstoapreferenceforshortertarget-sidesequencesoverlongertarget-side\nsequences. SoitseemsimplausibletosimplytakeScore(x,y)=Pr(y|x)orlogPr(y|x)in\nsearch because the result is very probably a short sequence, say, a sequence of one or two\nwords. Thisproblemisadirectconsequenceoftheinductivebiasoftheabovemodel. Froma\nsupervisedlearningperspective,anotherreasonforthisisthatteacherforcingisusedtotrain\nthemodel: onlyaground-truthtarget-sidesequenceisconsideredintraining,andthemodelis\nforcedtooutputthisground-truth. Bycontrast,whenapplyingthismodeltotestdata,weneed\ntoexploreabigsetofyofdifferentlengths,andtocomparedifferentyintermsofafunction",
    "thatisdifferentfromtheonelearnedonthetrainingdata.\nThisproblemcanbeaddressedthroughatechniquecalledlengthreward,whichgives\nbonusestolongersequencesbyaddingatermtoScore(x,y)[Heetal.,2016c]. Asdiscussed\ninChapter3,acommonlyusedformoflengthrewardisgivenby\nScore(x,y) = logPr(y|x)+λ·n (5.69)\nHerethelengthrewardtermisthelengthofy(i.e.,n=|y|),weightedbytheparameterλ>0.\nImprovements on this approach involve replacing n with an estimated sequence length by\nusingalengthpredictionmodel. Forexample,wecanboundtherewardinthefollowingform\n[Huangetal.,2017b;Yangetal.,2018b]\nScore(x,y) = logPr(y|x)+λ·max(l ,n) (5.70)\np\nwhere l is a predicted length, and is generally defined to be a scaled length of x, that is,\np\nl =scalar ·m.\np p\nIfwesubstitutethelogprobabilitylogPr(y|x)givenbyEq. (5.68)intoEq. (5.69), we\nobtain\nn\n(cid:88)\nScore(x,y) = logPr(y |y ,x)+λ·n\ni <i\ni=1\nn\n(cid:88)\n= [logPr(y |y ,x)+λ] (5.71)\ni <i\ni=1",
    "240 Chapter 5. Sequence-to-Sequence Models\nThus,wecaninterpretthelengthrewardtermasarewardtoeachwordy . Suchamethodhas\ni\nbeen widely used in statistical machine translation (SMT) systems in which the rewards\naretreatedasfeaturesofalog-linearmodel[Koehnetal.,2003;Chiang,2007]. Tofindan\nappropriatevalueofλ,wecaneitheruseminimumerrorratetraining[Och,2003],following\ntheconventioninSMT,orusegradient-basedmethodsasinneuralnetwork-basedsystems\n[MurrayandChiang,2018].\nAsecondapproachtobiasingsearchtowardslongersequences,calledlengthnormaliza-\ntion,istodividelogPr(y|x)byalengthcorrectionterm,writteninthefollowingform\nlogPr(y|x)\nScore(x,y) = (5.72)\nn\ncorrect\nAsimpleexampleofthismodelistodefinethelengthcorrectiontermasthesequencelength\n[Jeanetal.,2015],likethis\nn = n\ncorrect\n= |y| (5.73)\nInthiscase,\nlogPr(y|x)\n=\n(cid:80)n\ni=1\nlogPr(yi|y<i,x)\ncanbeviewedasthelog-scalegeometricmean\nn n\noftheprobabilities{Pr(y |y ,x)}5.\ni <i",
    "Another example is the one used in the GNMT system [Wu et al., 2016]. It takes the\nexponentialoftheshifted,re-scaledn,asfollows\n(5+n)α\nn = (5.76)\ncorrect (5+1)α\nwherethepowerαisahyper-parameterandcanbedeterminedempiricallyonatuningset. To\ncomparedifferentmethods,Table5.2showsalistofscoringfunctionsforlengthrewardand\nlengthnormalization.\nInmachinetranslation,thelengthproblemisalsocloselyrelatedtothecoverageproblem\nwhichhasbeendiscussedextensivelyinSMT.Whentranslatingasource-sidesequence,we\nwishtoknowhowmanytimeseachwordistranslated. Then,wewillsaythatover-translation\noccurs(i.e.,alongertranslation)ifsomewordsaretranslatedtoomanytimes,andthatunder-\ntranslation occurs (i.e., a shorter translation) if some words are not sufficiently translated.\nTraditionally,thecoverageofasource-sidesequenceisdescribedintermsofanm-dimensional\n5Suppose{a\n1\n,...,an}arenvariables.Since\nexp (cid:18)(cid:80)n i=1 n loga i (cid:19) = (cid:32) (cid:89) n a i (cid:33) n 1 (5.74)\ni=1\nwehave",
    "(cid:80)n i=1 n loga i = log (cid:32) (cid:89) n a i (cid:33) n 1 (5.75)\ni=1",
    "5.4 Search 241\nMethod FormofScore(x,y)\nNoReward/Normalization Score(x,y)=logPr(y|x)\nLengthReward Score(x,y)=logPr(y|x)+λ·n\nBoundedLengthReward Score(x,y)=logPr(y|x)+λ·max(l ,n)\np\nlogPr(y|x)\nLengthNormalization(Basic) Score(x,y)=\nn\nlogPr(y|x)\nLengthNormalization(GNMT) Score(x,y)=\n(5+n)α/(5+1)α\nTable 5.2: Scoring functions for length reward and length normalization. m=|x|, n=|y|,\nandl =scalar ·m. λandαareparameters.\np p\n(cid:104) (cid:105)\nvector υ ... υ ,calledthecoveragevector. υ describestowhatextentthesource-side\n1 m j\nwordx istranslated. InSMTsystemsυ isabinaryvariable: 0denotesuntranslated,and\nj j\n1denotestranslated. However,NMTsystemshavenosuchsymboliccoveragemechanism.\nInstead,theyhavemodelsthatcomputetheattentionweightsbetweenx andallthetarget-side\nj\nwords. Therefore,onewaytodefinewhatwemeanbythecoverageofawordistoconsider\nhowstrongx connectstothetarget-sidewords. Todothis,weextendυ tobeacontinuous\nj j\nvariable,givenby\nn\n(cid:88)\nυ = α (5.77)\nj i,j\ni=1",
    "υ canthusbeviewedasthe“numberoftimes”x istranslated,say,υ =0meansthatx is\nj j j j\nnottranslatedatall,andυ =1meansthatx iscountedonlyonceintranslation. Considerthe\nj j\nexampleinFigure5.9. Forthesource-sideword建设,thecorrespondingattentionweightsare\nshownbelow.\n0.23 0.14 0.20 0.22 0.22 0.21 0.10 0.29 0.15 0.30 0.60 0.09\nelihwnaem\n,\neht\nnoitats\nsah\nedam\nwen\nssergorp\nni\nevisneherpmoc\nnoitcurtsnoc\n.\n建设\n(j =5)\nυ =\n(cid:80)12\nα =2.75\n5 i=1 i,5\nWewillsaythat建设istranslated2.75times. Itispossibletomakeuseof{υ ,...,υ }to\n1 m\ndefinehowmuchthesource-sidesequenceiscoveredintranslation. Asimplewaytodothis\nistodevelopacoveragescorecp(x,y)bycombining{υ ,...,υ }. Forexample,theGNMT\n1 m\nsystemdefinescp(x,y)inthefollowingform\nm\n(cid:88)\ncp(x,y) = β log(min(υ ,1)) (5.78)\nj\nj=1",
    "242 Chapter 5. Sequence-to-Sequence Models\nwhereβ isaweightforthecoveragemodel. Theunderlyingideaisthatwhenυ ≥1theword\nj\nx isassumedtobeadequatelytranslated;whenυ <1thewordx isassumedtobelackof\nj j j\ntranslation. Thuscp(x,y)penalizeshypothesesinwhichsomeofthesource-sidewordsmiss\npartsofthetranslations. AnimprovementtothisformisgivenbyLietal.[2018]\nm\n(cid:88)\ncp(x,y) = β log(max(υ ,γ)) (5.79)\nj\nj=1\nwhere γ is the hyper-parameter for truncation, giving a tolerance for under-translation. A\nsimilarformwasproposedin[ChorowskiandJaitly,2017]\nm\n(cid:88)\ncp(x,y) = β 1(υ >γ) (5.80)\nj\nj=1\nItjustcountsthenumberoftimesυ isgreaterthanγ.\nj\ncp(x,y)canbeeasilyintroducedintosearchbyaddingittoScore(x,y). Forexample,the\nGHKM-stylescoringfunctionisdefinedtobe\nlogPr(y|x)\nScore(x,y) = +cp(x,y) (5.81)\n(5+n)α/(5+1)α\nInpractice,modifyingScore(x,y)isnottheonlywaytoaddressthelengthproblemin\nsearch. An alternative approach is to have architecture changes for modeling the problem",
    "[Tu et al., 2016; Mi et al., 2016a; Sankaran et al., 2016; See et al., 2017; Malaviya et al.,\n2018]. Note that, sometimes the length of the target-side sequence has been specified or\npredicted in some way. In these cases, we can either develop models not dependent on the\nauto-regressiveassumption[Guetal.,2018],ordeveloplength-controllabletextgeneration\nsystemsforinterestingapplications[Rushetal.,2015;Kikuchietal.,2016].\n5.4.2 Pruning and Beam Search\nTherearemanywaystodefineasearchspace. Asageneralconceptincomputerscience,a\nsearchspaceisoftenreferredtoasthedomainoftheproblemthatissearched. Forsequence-to-\nsequenceproblems,wecanthinkofahypothesisasamappingfromasource-sidesequencex\ntoatarget-sidesequencey,andcanthinkofasearchspaceasacollectionofsuchhypotheses6.\nWecanimplementasearchprogrambyorganizinghypothesesinanunderstandableway\nso that we can look at the search space for the problem. Recall that in Eqs. (5.67-5.68)",
    "we assign a probability of y given x by using a left-to-right factorization. A typical search\nsystemmaintainsasetofhypotheses(orpartialhypotheses)andbuildsupthesehypotheses\nfromlefttoright7. ThesearchprocedurebeginswithaninitialhypothesissetZ containing\n0\n6Hereweuse(x,y)todenoteahypothesis.Whentherearemultiplemappingsfromxtoy,ahypothesiscan\nberepresentedas(x,y,d)whereddenotesthemapping.Forexample,ifwetransformxtoywithasynchronous\ngrammar,theremightbemultiplederivationsofgrammarrulestodothis.\n7Ahypothesisiscalledpartialwhenthecorrespondingtarget-sidesequencedoesnotendwith⟨EOS⟩,i.e.,an\nincompletetarget-sidesequence.Inthissectionweusethetermshypothesisandpartialhypothesisinterchangeably",
    "5.4 Search 243\nonlyonehypothesisz whosetarget-sideisy byconsideringy =⟨SOS⟩isthestartsymbol\n0 0 0\nfor all target-side sequences. Then, we extend this hypothesis set over a number of search\nsteps. SupposewehaveasequenceofhypothesissetsZ ...Z wheren isthemaximum\n0 nmax max\nnumberofsearchsteps. Atstepi,wewishtoextendeachhypothesisbyaddinganewwordv\nk\ndrawnfromthevocabularyV . Letz.srcbethesource-sideofz andz.tgtbethetarget-side\ny\nofz. Clearly,wehavez.src=xforanyz. Givenahypothesisz ∈Z ,wecanextendit\ncur i−1\nto|V |hypotheses{z1 ,...,z |Vy| },givenby\ny next next\n{z1 ,...,z |Vy| } = Extend(z ,V )\nnext next cur y\n(cid:91)\n= Extend(z ,v ) (5.82)\ncur k\nv\nk\n∈Vy\nHereExtend(z ,v )isafunctionthatextendstheinputhypothesisz withawordv ∈V .\ncur k cur k y\nThetarget-sideofaresultinghypothesisistheconcatenationofz .tgtandv ,writtenas8,\ncur k\nzk .tgt = z .tgt◦v (5.83)\nnext cur k\nThese new hypotheses {z1 ,...,z |Vy| } are then added to Z . Figure 5.10 illustrates a\nnext next i",
    "fewstepsinthishypothesisextensionprocess. Weseethatallthehypothesescaneasilybe\nrepresentedasatreestructure. HereZ correspondstoasetofthenodesatleveliofthesearch\ni\ntree,andwesimplyhave\n|Z | = |V|·|Z | (5.84)\ni i−1\nInotherwords,thesizeofZ growsexponentiallywiththenumberofsteps,say,|Z |=|V|i.\ni i\nEachhypothesiszisassociatedwithalogprobabilitylogPr(z.tgt|z.src). logPr(z.tgt|z.src)\nsimplytakestheformofEq. (5.68),andcanbedefinedinarecursivefashion\nlogPr(zk .tgt|zk .src) = logPr(z .tgt|z .src)+\nnext next cur cur\nlogPr(v |z .tgt,z .src) (5.85)\nk cur cur\nAsanexample,supposezk .tgt=y ...y . TheformofEq. (5.85)becomesclearfromthe\nnext 0 i+1\nfollowingrewriting\nlogPr(y ...y |x) = logPr(y ...y |x) +logPr(y |y ...y ,x)\n0 i+1 0 i i+1 0 i\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nlogPr(z n k ext .tgt|z n k ext .src) logPr(zcur.tgt|zcur.src) logPr(v k |zcur.tgt,zcur.src)\ni\n(cid:88)\n= logPr(y |y ,x)+logPr(y |y ...y ,x)",
    "k <k i+1 0 i\nk=1\ni+1\n(cid:88)\n= logPr(y |y ,x) (5.86)\nk <k\nk=1\nbecausetheirformsarethesame.\n8Weusea◦btodenotetheconcatenationoftwostringsaandb.",
    "244 Chapter 5. Sequence-to-Sequence Models\n.\n.\n.\n⟨SOS⟩A◦tail\ns: −1.3×1012\n.\n.\n.\n⟨SOS⟩◦“ ⟨SOS⟩A◦taxi\ns: −1.6×107 s: −2.7×1012\n⟨SOS⟩◦A ⟨SOS⟩A◦team\ns: −3.2×104 s: −5.0×1010\nz\n0\n⟨SOS⟩ ⟨SOS⟩◦An ⟨SOS⟩A◦text\ns: 0 s: −6.1×105 s: −1.0×109\n⟨SOS⟩◦Because ⟨SOS⟩A◦tiger\ns: −7.7×106 s: −3.2×1012\n.\n.\n.\n⟨SOS⟩◦If\ns: −3.9×106\n.\n.\n.\nStep0 Step1 Step2\n(Z ={z }) (Z ) (Z )\n0 0 1 2\nFigure5.10: Illustrationofhypothesisextensioninfirst3steps. Each(partial)hypothesisis\nrepresented as a box in which we show the corresponding target-side sequence and model\nscore. Each search step is associated with a hypothesis set Z . We start with a hypothesis\ni\nz ∈Z denotingthestartsymbol⟨SOS⟩. Instepi, weextendeveryhypothesisinZ by\n0 0 i−1\ntryingtoappendeverywordfromavocabularyV (seewordsinred). Thisoperationwillresult\nin|V|·|Z |hypotheses,formingthehypothesissetZ . Thehypothesisextensionprocedure\ni−1 i\nrepresentsabreadth-firstsearchalgorithm: wecreateallthenodes(orsearchstates)atdepth",
    "i−1beforemovingtodepthi. Atreestructureiscreatedalongwiththisprocedure,andaleaf\nnodeofthetreecantracethesearchpathbacktotherootnode.\nGiventhisprobability,wecanthencomputez.score=Score(z.src,z.tgt),asinSection\n5.4.1. Thisenablesustocomparedifferenthypothesesintermsofz.score. Ifahypothesisends\nwiththesymbol⟨EOS⟩,itiscalledcompleteandisnotextendedanymore. Onceahypothesis",
    "5.4 Search 245\niscomplete,itisaddedtoamax-heap9. Wecandumpthehypotheseswithmaximummodel\nscoresfromtheheap. Ingeneral,thesearchprocedurewillstopifwefindacertainnumber\nofcompletehypotheses. Forexample,wecanstopsearchingwhentheheapisfull(seemore\ndiscussionslaterinthissubsection). Theresultingsearchalgorithmisdescribedbelow.\nAlgorithm: ASimpleBreadth-firstSearchAlgorithm\nSimpleSearch(x)\n//Searchforthebesthypothesisgiventhesource-sidesequencex\n1. CreateaHeapwithsize elements\nheap\n2. Z ={z }wherez .src=xandz .tgt=y\n0 0 0 0 0\n3. Foreachstepi=1ton\nmax\n4. Foreachhypothesisz ∈Z\ncur i−1\n5. Foreachwordv ∈V\nk y\n6. z =Extend(z ,v ,x)\nnext cur k\n7. Ifz .tgtendswith⟨EOS⟩,then\nnext\n8. Addz toHeap\nnext\n9. Else\n10. Addz toZ\nnext i\n11. IfHeapisfulland/orotherstoppingcriteriaaremet,then\n12. Breakalltheloops\n13. returnHeap.Pop()\nExtend(z ,v ,src)\ncur k\n//Createanewhypothesisbyappendinganewwordv tothetarget-sideofz\nk cur\n1. Createanewhypothesisz\nnext\n2. z .src=src\nnext\n3. z .tgt=z .tgt◦v\nnext cur k",
    "4. z .prob=z .prob+logPr(v |z .tgt,z .src) //seeEq. (5.85)\nnext cur k cur cur\n5. z .score=score(z .src,z .tgt) //seeSection5.4.1\nnext next next\n6. Returnz\nnext\nIfthehypothesisheaphasaninfinitecapacity(size =∞),thisalgorithmwillperform\nheap\nanexhaustivesearchoveraspaceofallhypotheseswhosetarget-sidelengthsareupton ,\nmax\nresultinginatmost1+|V\ny\n|+|V\ny\n|2+···+|V\ny\n|nmax = |Vy|n\n|V\nm\ny\na\n|−\nx+\n1\n1−1 completehypotheses. This\nisanextremelyhugesearchspacewhichiscomputationallyintractableinpractice10. Therefore,\ninpracticalsystemsitiscommontoprunethesearchspaceinordertomakethesearchtractable.\nInlaterpartsofthissubsectionwewillintroducetwopopularsearchalgorithms,bothadopting\npruningforefficientsearch.\n9Givenamax-heapa,weusea.Pop()todenoteafunctionpoppingthetop-1itemofa,andusea.PopAll()to\ndenoteafunctionpoppingalltheitemsofa.\n10Consider,forexample,avocabularysizeof20,000(|Vy|=20,000)andalengthlimitof20(nmax=20).\n|Vy|nmax+1−1 wouldbe1.05×1086.\n|Vy|−1",
    "246 Chapter 5. Sequence-to-Sequence Models\n1. Greedy Search\nThe greedy strategy is one of the most common concepts that one learns in textbooks on\nalgorithms. Itisbasedonaheuristicthatthegloballyoptimalsolutioncanbeapproximatedby\nmakinglocallyoptimaldecisions. Althoughsuchanapproximationcanonlyobtainalocally\noptimalsolution,thisissufficientformanypracticalapplicationsanditslowcomputational\ncomplexityisaclearadvantage.\nApplying the greedy strategy to the search problem here is straightforward. In each\nextensiongivenstepi,weonlyconsiderthebesthypothesisuptoi. Tobemoreprecise,for\nanyZ ,weonlykeepthehypothesiswiththehighestmodelscoreanddiscardtherest. The\ni\noutputofeachstepofthegreedysearchisgivenby\nz = argmax z .score (5.87)\nbest next\nznext∈Extend(Zi−1,Vy)\nHerethefunctionExtend(Z ,V )hasthesamemeaningasthatinEq. (5.82),butoperates\ni−1 y\nonasetofhypotheses,thatis,\n(cid:91)\nExtend(Z ,V ) = Extend(z,V ) (5.88)\ni−1 y y\nz∈Zi−1\nThen,Z isdefinedtobe\ni\nZ = {z } (5.89)\ni best",
    "Agreedysearchalgorithmforsequence-to-sequenceproblemsisdescribedbelow.\nAlgorithm: AGreedySearchAlgorithm\nGreedySearch(x)\n//Searchforthe“best”hypothesisinagreedymanner\n1. Createahypothesisz\nbest\n2. Z ={z }wherez .src=xandz .tgt=y\n0 0 0 0 0\n3. Foreachstepi=1ton\nmax\n4. z .score=−∞\nbest\n5. Foreachhypothesisz ∈Z\ncur i−1\n6. Foreachwordv ∈V\nk y\n7. z =Extend(z ,v ,x)\nnext cur k\n8. Ifz .score<z .score,then\nbest next\n9. z =z\nbest next\n10. Ifz .tgtendswith⟨EOS⟩and/orotherstoppingcriteriaaremet,then\nbest\n11. Breaktheloop\n12. Z ={z }\ni best\n13. Returnz\nbest\nIneachstepofsearch,wehaveonlyoneactivehypothesistoextend(i.e.,|Z |=1)and\ni−1",
    "5.4 Search 247\nthereforeneed|V|extensionsfromwhichweselectthebestoneforthenextstepofsearch.\nThetotalnumberoftimesExtend(z ,v )iscalledis|V|·n . ProvidedExtend(z ,v )\ncur k max cur k\nisafixed-costfunction,thetimecomplexityofthealgorithmislinearwithrespectto|V|and\nn .\nmax\n2. Beam Search\nBeamsearchisanaturalextensionoftheabove1-bestgreedysearchalgorithm. Itisbased\non the greedy heuristics as well, and is thus a type of greedy algorithm. The idea of beam\nsearchistokeepateachstepanumberofthemostpromisinghypothesesratherthanthe1-best\nhypothesis. Abeamisadatastructurethatstoresthebesthypotheseswehavegeneratedso\nfar. Thenumberofhypothesesinabeamisapredeterminedparameter,calledbeamwidthor\nbeamsize. HerewecansimplyviewZ asabeam,writtenas\ni\nZ = {z1 ,...,zsize beam} (5.90)\ni best best\nwheresize isthebeamsize. z1 isthebesthypothesisintheextensionExtend(Z ,V )\nbeam best i−1 y\n(seeEq. (5.87)),z2 isthe2ndbesthypothesisinExtend(Z ,V ),andsoon.\nbest i−1 y",
    "Thefollowingpseudo-codedescribesabeamsearchalgorithmforsequence-to-sequence\nproblems.\nAlgorithm: ABeamSearchAlgorithm\nBeamSearch(x)\n//Searchforthe“best”hypothesisbyconsideringanumberofbestcandidates\n//ineachstep\n1. CreateaHeapwithsize elements\nheap\n2. Z ={z }wherez .src=xandz .tgt=y\n0 0 0 0 0\n3. Foreachstepi=1ton\nmax\n4. CreateaheapBeamwithsize elements\nbeam\n5. Foreachhypothesisz ∈Z\ncur i−1\n6. Foreachwordv ∈V\nk y\n7. z =Extend(z ,v ,x)\nnext cur k\n8. Ifz .tgtendswith⟨EOS⟩,then\nnext\n9. Addz toHeap\nnext\n10. Else\n11. UpdateBeam(Beam,z )\nnext\n12. IfHeapisfulland/orotherstoppingcriteriaaremet,then\n13. Breakalltheloops\n14. Z =Beam.PopAll()\ni\n15. ReturnHeap.Pop()\nUpdateBeam(Beam,z )\nnext\n//UpdateBeamwithanewly-generatedhypothesisz\nnext",
    "248 Chapter 5. Sequence-to-Sequence Models\n1. Addz toBeama\nnext\naBeamisamax-heapwithsize\nbeam\nelements.So,ifznext.scoreislowerthanalltheelementsinthe\nheap,theheapwillbeleftunchanged.Inotherwords,Beamonlystorestop-size besthypothesesand\nbeam\nignorestherest.\nThefunctionUpdateBeam(Beam,z )isadirectimplementationofhistogramprun-\nnext\ning. Note that this general-purpose framework provides a simple way to implement other\npruningmethods,andonecanmodifyUpdateBeam(Beam,z )asneeded. Forexample,\nnext\nanalternativemethod,calledthresholdpruning,retainsthehypotheseswhosedifferencesin\nmodelscoreswiththebesthypothesisinBeamarebelowathresholdθ ,say,wediscard\nbeam\nz inUpdateBeam(Beam,z )if\nnext next\nz .score < z .score−θ (5.91)\nnext best beam\nwherez isthebesthypothesisinBeam. Alternatively,wecanconsiderarelativethreshold\nbest\nmethod[FreitagandAl-Onaizan,2017],givenby\nz .score < z .score·θ (5.92)\nnext best beam\nFigure5.11showsacomparisonofexhaustivesearch,(1-best)greedysearchandbeam",
    "search. Atoneextreme,theoptimalsolutionisguaranteed,butanexponentiallylargenumber\nofsearchstatesarevisited. Attheotherextreme,onlytheminimumnumberofsearchstatesare\nvisited,butthesolutionissub-optimal. Bycontrast,beamsearchmakesatrade-offbetweenthe\ntwomethods. Alargerbeamsizemeansmoresearcheffortandahigherpossibilityoffinding\ntheoptimum,whileasmallerbeamsizemeansfastersearchandahigherriskofmissingthe\noptimum. Itisalsopossibletouseavariablebeamsizetomakeabettertrade-offduringsearch\n[Buckmanetal.,2016;PostandVilar,2018;Kulikovetal.,2019].\nAnimportantproblemrelatedtothesesearchalgorithmsistheproblemofsearcherrors.\nIngeneral,searcherrorscanbedefinedinseveraldifferentways. Herewesaythatasearch\nerroroccursifthesearchresultisnotthesameasthatofexhaustivesearch. Commonsense\ntellsusthatfewersearcherrorsarehelpfulforfinding“better”results. Thus,weoftenwishto\nhaveamoredesirabletarget-sidesequencebyenlargingthebeamsize. However,thisisnot",
    "thecaseforsomesequence-to-sequencesystems. Forexample,asearchwithalargerbeam\nsizemayleadtoalowertranslationqualityforneuralmachinetranslationsystems[Koehnand\nKnowles,2017]. Thisinspiresveryinterestingstudiesonthedeteriorationissueoflargebeam\nsearch[Ottetal.,2018b;Yangetal.,2018b;StahlbergandByrne,2019].\n3. Stopping Criteria\nAlthoughthetimecomplexitiesoftheabovealgorithmsareboundedbythemaximumnumber\nofsearchsteps(i.e.,n ),itisimportanttohavemoreefficientalgorithmstostopsearchingas\nmax\nearlyaspossible,especiallyforlatency-sensitiveapplications. Thistypicallyrequiresheuristics\ntodesignadditionalcriteriaforstoppingthesearchprocedureattheappropriatepoint. Some\nofthesestoppingcriteriaare:",
    "5.4 Search 249\nS S S\nte te te\np p p\n3 3 3\nS S S\nte te te\np p p\n2 2 2\nS S S\nte te te\np p p\n1 1 1\n(a)ExhaustedSearch (b)(1-best)GreedySearch (c)BeamSearch\nFigure5.11: Acomparisonofexhaustivesearch,(1-best)greedysearchandbeamsearch. Balls\nrepresentsearchstatesorpartialhypotheses. Exhaustedsearchexploresallsearchstatesinthe\nsearchspace. Bycontrast,greedysearchkeepsonlythe1-bestpathofsearchstatesandprunes\nawaytherest. Beamsearchisatrade-offbetweenthemandkeepsthemostpromisingsearch\nstatesinabeamineachstep.\n• If a given number of complete hypotheses are created, then we stop searching. For\nexample,inthebeamsearchalgorithmdescribedinthissubsection,thesearchprogram\nterminates when we have size complete hypotheses. Another way to implement\nheap\nthis idea is to shrink the beam as the number of complete hypotheses increases. In\nBahdanauetal.[2014]’ssystem,onceanewcompletehypothesisiscreated,thebeam\nsize decreases by 1. Therefore, the search program will terminate if the beam size is\nreducedto0.",
    "• Ifeveryhypothesisatstepihasascorelowerthanthatofthebestcompletehypothesisin\nHeapbysomemargin,thenwestopsearching. Supposez isthebesthypothesis\nbestinall\nwe have generated so far (i.e., z =Heap.Pop()). If every hypothesis z at\nbestinall next\nstepisatisfies\nz .score−z .score ≥ θ (5.93)\nbestinall next all\nthen we will finish the search process at this step. Here θ is a parameter. One\nall\ncan specify it with an appropriate value through multiple tries. A simple choice is\nθ =0,whichisemployedinsomeofthepopularsequence-to-sequencesystems[Ott\nall\netal.,2019]. Undersomecircumstances,suchanearly-stopstrategycanguaranteethe",
    "250 Chapter 5. Sequence-to-Sequence Models\noptimalityofsearch[Huangetal.,2017b;Yangetal.,2018b].\n• Ifeveryhypothesisatstepihasascorelowerthanthatofthelastcompletehypothesisin\nHeapbysomemargin,thenwestopsearching. Thisisaweakconditionforearly-stop.\n• Ifthetoprankedhypothesesatstepiareallcompletehypotheses,thenwestopsearching.\nThisisamoreaggressiveversionofearly-stop. Forexample, inKleinetal.[2017]’s\nsystem, the search program terminates at step i if the top-1 hypothesis is a complete\nhypothesis.\n• If the search program consumes a certain amount of computing resources, such as a\ncertainnumberoffloating-pointinstructionsandacertainamountofwallclocktime,\nthen we stop searching. In applications where computer performance is limited and\nlatency plays an important role, we will often be interested in this kind of stopping\ncriterion.\nSometimes,thesearchalgorithmwillnotfindanycompletehypothesisuntilhittingthe\nlengthlimitn . Asapracticalmatteritmightbeeasyinthiscasetoforcethebestpartial\nmax",
    "hypothesistobecompletebyadding⟨EOS⟩toitsend.\nNotethatchoosingappropriatestoppingcriteriareflectsatrade-offbetweenfastcomputa-\ntionandaccuratepredictionatinferencetime(callitthespeed-accuracytrade-off). Whileit\nisnotalwaysthecasethatmoretimeasearchprogramtakescouldresultinbetterresultsfora\nsequence-to-sequencesystem,wewouldalwayswanttoknowhowclosewecangettoabetter\nsolutiontotheproblembysearchingthroughalargerregionofthesearchspace. Adiscussion\nofaccuratesearchalgorithmscanbefoundinSection5.4.4.\n5.4.3 Online Search\nSo far in our general discussion of sequence-to-sequence problems, we have assumed that\nallthesource-sidewordscometogetherasawholeandcanbeaccessedintheentiresearch\nprocess. However,insomepracticalapplications,theinputsarereceivedinorder,andwewish\ntomakepredictionsconditionedonsomeoftheobservedinputs. Anexampleofthisisonline\nautomaticspeechrecognitioninwhichthesystemcontinuallytakesnewacousticsignalsand\natthesametimeoutputsthecorrespondingtranscriptionunits.",
    "Intuitively,wemightthinkofthegenerationofthei-thtarget-sidewordasaproblemof\nmappingaprefixofthesource-sidesequencetothetarget-sidevocabulary. Wecanformulate\nthisbyintroducingafunctiong(i)whichdenotesthemaximumlengthoftheprefixofxwe\nuseingeneratingy . Thus,theprobabilityofy giventheentiresequencexandthepreviously\ni i\ngeneratedwordsy canbeapproximatedby\n<i\nPr(y |y ,x) ≈ Pr(y |y ,x ) (5.94)\ni <i i <i ≤g(i)\nwherex denotesthesub-sequencex ...x . Then,thelogprobabilityofthetarget-side\n≤g(i) 1 g(i)",
    "5.4 Search 251\nsequenceygiventhesource-sidesequencexiswrittenas\nn\n(cid:88)\nlogPr(y|x) = logPr(y |y ,x)\ni <i\ni=1\nn\n(cid:88)\n≈ logPr(y |y ,x ) (5.95)\ni <i ≤g(i)\ni=1\nThisequationframesasequence-to-sequenceproblemasaprefix-to-prefixproblem,thatis,\ntheprefixy isonlydependentontheprefixx . Inferenceforthismodelissimple. For\n≤i ≤g(i)\neachi,thesearchsystemwaitsuntilallg(i)source-sidewordsarereceived,andthenextends\nthehypothesesasusual. Thiscanbedonebyreusingthealgorithmsdescribedintheprevious\nsubsection. Forexample,wecanmodifythebeamsearchalgorithmandobtainthefollowing\nonlinesearchalgorithm.\nAlgorithm: AnOnlineBeamSearchAlgorithm\nOnlineBeamSearch(x,g(·))\n//Onlinesearchinwhichthesearchisoperatedonceanadequatenumberofinput\n//wordsarereceived. Ineachsearchstep,anumberofthemostpromisingcandidates\n//areconsidered.\n1. CreateaHeapwithsize elements\nheap\n2. Z ={z }wherez .tgt=y\n0 0 0 0\n3. j =0\n4. i=1\n5. input=ϕ\n6. Whilei≤n do\nmax\n7. Ifj <g(i),then //readawordfromtheinputstream\n8. input=input◦x\nj",
    "9. Else //makeapredictionatstepi\n10. //wheng(i)inputwordsareobserved(storedininput)\n11. CreateaheapBeamwithsize elements\nbeam\n12. Foreachhypothesisz ∈Z\ncur i−1\n13. Foreachwordv ∈V\nk y\n14. z =Extend(z ,v ,input)\nnext cur k\n15. Ifinputequalsxandz .tgtendswith⟨EOS⟩,then\nnext\n16. Addz toHeap\nnext\n17. Else\n18. UpdateBeam(Beam,z )\nnext\n19. IfHeapisfulland/orotherstoppingcriteriaaremet,then\n20. Breakalltheloops\n21. OutputPartial(Beam)\n22. Z =Beam.PopAll()\ni",
    "252 Chapter 5. Sequence-to-Sequence Models\n23. i++\n24. ReturnHeap.Pop()\nOutputPartial(Beam)\n//Outputapartialresult\n1. DisplaythebesthypothesisinBeam\nAnadvantageofthissystemisthattheoutputatstepiisimmediateoncewehaveseen\nx . Thisresultsinanonlinesequence-to-sequencesysteminwhichinputwordsarrive\n≤g(i)\ninacontinuousstreamandpredictionscanbemadejustaftera“sufficient”numberofinput\nwordsareseen.\nWhile the search problem here seems simple, much remains to be done to define g(i).\nClearly,g(i)isamonotonicallynon-decreasingfunction. Asasimpleexample,wecandefine\ng(i)=mforanyi. Thiswillmaketheabovealgorithmpreciselythesameasthestandardbeam\nsearchalgorithmthatworkswithacompleteinputsequence. Bycontrast,inonlinesequence-\nto-sequencetasks,wewantg(i)tobeassmallaspossible,andsowecanstartcomputationas\nearlyaspossibleininference. Thesimplestcaseoftheseisthattheinputandoutputsequences\naresynchronousinsomeway. Forexample,anautomaticspeechrecognitionsystemassigns",
    "eachspectralframeatranscriptionunit. Inthiscase,wehaveasimplecorrespondencebetween\ninputs and outputs: m=n (i.e., |x|=|y|), and x corresponds to y . Then, we can simply\ni i\ndefineg(i)=i,inotherwords,eachtimeanewinputarrives,wemakeaprediction.\nAmorecomplicatedcaseisonlinesequence-to-sequenceproblemswithreordering,such\nassimultaneoustranslation,inwhichatarget-sidewordmaydependonsource-sidewords\nwith long-range dependencies. A simple way to address this is to delay the predictions for\nanumberofsteps. Forexample,thewait-k methodforceseachpredictiontolagbehindthe\ninputsbyk words[Maetal.,2019]. Moreformally,thewait-k versionofthefunctiong(i)is\ndefinedtobe\ng(i) = min(m,k+i−1) (5.96)\nHere k is a hyper-parameter that controls how large a source-side context is considered in\npredictingtarget-sidewords. Whenk=∞,itisthesameasthestandardsearchmethodsfor\nsequence-to-sequenceinference. Insimultaneoustranslationandrelatedtasks,resultsarein",
    "generalsatisfactorybyusingasmallvalueofk. Acomparisonofdifferentg(i)isshownin\nFigure5.12.\nIn some applications of online sequence-to-sequence problems, we may know when to\nperformsearchandwhentoreadinputs. Forexample,ininteractivemachinetranslation\n[Casacuberta et al., 2009], the translation of a partial input sequence is triggered by some\nbehaviors of users (such as the action of pressing buttons). In this case, we do not need to\ndefineg(i),butviewitasaninputvariableofthemodel.\nNote that while one can directly employ pre-trained sequence-level models for online\ninference,developingsuchsystemsoftenrequiresadditionaltrainingeffort. Amoreprincipled\napproachtoonlinesequence-to-sequencemodelingistomodelthetransformationfromxto\nyasasequenceofactions[GrissomIIetal.,2014;ChoandEsipova,2016;Guetal.,2017;",
    "5.4 Search 253\n1 2 3 4 5 6\n= = = = = =\ni i i i i i\nj=6\nj=5 StandardSeq2Seq g(i)=m\nj=4\n1-to-1Monotonic\nj=3 g(i)=i (m=n)\nTransduction\nj=2\nj=1 Wait-k g(i)=min(m,k+i−1)\nj=0\n(a)Visualizationofg(i).\nStandardSeq2Seq\nx x x x x x\n1 2 3 4 5 6\ny y y y y y y\n0 1 2 3 4 5 6\n1-to-1MonotonicTransduction\nx x x x x x\n1 2 3 4 5 6\ny y y y y y y\n0 1 2 3 4 5 6\nWait-k(k=3)\nx x x x x x\n1 2 3 4 5 6\ny y y y y y y\n0 1 2 3 4 5 6\n(b)Actionsequences.\nFigure5.12: Visualization(top)andactionsequences(bottom)ofdifferentg(i)forapairof\nsequences(x=x ...x ,y=y ...y ). Inanactionsequence,acircledx standsfortheaction\n1 6 1 6 j\nof reading a source-side word (x ), and a circled y stands for the action of predicting the\nj i\nprobabilityofy givenx andy . Arrowsherestandfordependenciesbetweenwords.\ni ≤g(i) <i\nBecausey denotesthestartsymbol⟨SOS⟩,itcouldbegeneratedwithoutdependencieson\n0\nanywords.\nZhengetal.,2019]. Forexample,anactioncanbeeitherapredictoperationthatperforms",
    "searchatthecurrentstep,orareadoperationthatacceptsanewinputword. Then,wecan\nframethetaskofdesigningthefunctiong(i)aslearningapolicytodeterminewhichactionis",
    "254 Chapter 5. Sequence-to-Sequence Models\ntakengivenasource-sideprefixx andatarget-sideprefixy . Andsequence-to-sequence\n≤j <i\nmodels can be trained on the states of these action sequences so that they can make better\npredictionsconditionedonpartoftheinput. However,adiscussionoftrainingonlinesequence-\nto-sequence models lies outside the scope of this section. We refer the reader to the above\npapersformoredetailsonthesemethods.\n5.4.4 Exact Search\nFromaformalpointofview,wewouldideallyliketodevelopasystemwithnosearcherrors.\nAlthoughapproximatesearchalgorithmshavebeenusedsuccessfullyinmanyapplications,itis\nimportanttostudymodelerrorsofthesesystems,andthustofocusontheprobleminprinciple,\nnotjustinpractice. Sodevelopingexactsearchalgorithmsforsequence-to-sequencemodels\nhaslongbeenaninterestingtopicinNLPresearch. However,thesearchproblemforasimple\nword-basedmachinetranslationsystemwithn-gramlanguagemodelshasbeenfoundtobe",
    "anNP-hardproblem[Knight,1999]. Muchofearlierresearchformulatedthesearchproblem\nasclassicalcombinatorialoptimizationproblems,suchasthelinearprogrammingproblem\nandthetravelingsalesmanproblem,andemployedthecorrespondingsolvers[Germannetal.,\n2004;Zaslavskiyetal.,2009]. Additionalresearcheffortsexploredexactsearchalgorithmsfor\nstatisticalmachinetranslationsystemsbyusingtheLagrangianrelaxationtechnique[Chang\nandCollins,2011;RushandCollins,2012]andfinite-stateautomata[deGispertetal.,2010;\nAllauzenetal.,2014].\nUnlike these methods, which are more or less dependent on the integration of n-gram\nlanguagemodelsintosequence-to-sequencemodels,themodelsdescribedinthischaptertake\nasimplerform. Webeginwithabasicmodelinwhichthescoringfunctionscore(x,y)isthe\nlog probability logPr(y|x). Eq. (5.68) tells us that logPr(y|x) can be written as a sum of\nword-levellogprobabilities,andlogPr(y|x)becomessmallerasmoretarget-sidewordsare",
    "generated(i.e.,alargern)11. Inotherwords,logPr(y|x)isamonotonicdecreasingfunction\nwithrespecttothetarget-sidelengthn: foranyi,wehave\nlogPr(y |x) = logPr(y |x)+logPr(y |y ,x)\n≤i <i i <i\n≤ logPr(y |x) (5.97)\n<i\nThisisalsocalledthemonotonicityofthescoringfunction.\nThen,bymakinguseofthemonotonicnatureofmodelscores,wecandevelopaheuristic\ntoruleouthypothesesthatwouldneverbethebest. Letz betheglobalbestcomplete\nbestinall\nhypothesiswehavefound. Ifanewhypothesishasamodelscorelowerthanz .score,\nbestinall\nthenwewillnotneedtoextendit. Thuswecanexplorearegionthatissignificantlysmaller\nthantheoriginalsearchspace,withoutlossofoptimality. Notethatz .scorecontinues\nbestinall\nto become larger in search. It will be more difficult to find a better hypothesis and more\nhypotheseswillbeprunedawayasthesearchprocessproceeds. Seethepseudo-codebelow\nforanexactsearchalgorithmofthesequence-to-sequencemodelofEq. (5.68).",
    "11Consider logPr(y|x) = (cid:80)n i=1 logPr(y i |y <i ,x). Since logPr(y i |y <i ,x) has a non-positive value,\nlogPr(y|x)willbesmallerorunchangedifngrows.",
    "5.4 Search 255\nAlgorithm: AnExactSearchAlgorithm\nExactSearch(x)\n//Searchforthe“best”hypothesisbymakinguseofthemonotonicityofthe\n//scoringfunction(score(x,y)=logPr(y|x)).\n1. Createapriorityqueue(max-heap)Queue\n2. Createahypothesisz withz .score=−∞\nbest best\n3. WhileQueueisnotemptydo\n4. z =Queue.Pop()\ncur\n5. If|z .tgt|>n ,then\ncur max\n6. skipz andcontinuetheloop\ncur\n7. Foreachwordv ∈V\nk y\n8. z =Extend(z ,v ,x)\nnext cur k\n9. bound=z .score //alowerboundonmodelscores\nbest\n10. Ifbound<z .score,then//admissiblepruning\nnext\n11. Ifz .tgtendswith⟨EOS⟩,then\nnext\n12. z =z\nbest next\n13. bound=z .score\nnext\n14. Else\n15. Addz toQueue\nnext\n16. Returnz\nbest\nThisisageneralalgorithmforexactsearch,anditssearchefficiencyisgreatlyinfluenced\nbythedesignofthepriorityqueue[Meisteretal.,2020]. Forexample,wecanviewscore(x,y)\nasthepriorityofeachhypothesisinthepriorityqueue,asinamax-heap12. Then,theresulting\nalgorithmperformsaprocedureofbreadth-first-likesearch,sinceahypothesiswithashorter",
    "target-sidesequenceismorelikelytohaveahighermodelscoreandtobeatop-rankeditem\ninthepriorityqueue. Forefficientsearch,however,wewishtofindcompletehypothesesas\nearly as possible, such that more unpromising hypotheses can be thrown away in the early\nstageofsearch. Todothis,wecanbiasthepriorityofahypothesistowardsalongertarget-side\nsequence. Thisprovidesadepth-firstsearchalgorithmwhichismorelikelytofindcomplete\nhypothesesinashortertime[StahlbergandByrne,2019].\nWhiletheexactsearchalgorithmbecomesapparentbyconsideringthemonotonicityof\nPr(y|x), in practical systems, as discussed in Section 5.4.1, score(x,y) often has a more\ncomplexforminvolvinglengthrewardornormalization,andsothemonotonicpropertydoes\nnothold. Fortunately,theassumptionofmonotonicitycanbedroppedattheexpenseofslightly\nrelaxingthelowerboundonmodelscoresforpruning. Herewedefineboundtobethelowest\nmodel score that a hypothesis should have so that it can at best be extended to an equally",
    "good hypothesis with z . For example, consider a simple word reward model described\nbest\nin Eq. (5.69): Score(x,y)=logPr(y|x)+λ·n. For a hypothesis z , there are at most\nnext\n12Wecanimplementapriorityqueueusingamax-heap.",
    "256 Chapter 5. Sequence-to-Sequence Models\nn −|z .tgt|wordswecanpredicttoobtainacompletehypothesis. Supposeallthese\nmax next\nn −|z .tgt|wordsarepredictedwithaprobabilityof1. Then,themodelscoreofthe\nmax next\nresultinghypothesis(denotedbyz )willbegivenby\nnew\nn (cid:88)max\nz .score = z .score+ (log1+λ)\nnew next\ni=|znext.tgt|+1\n= z .score+λ·(n −|z .tgt|) (5.98)\nnext max next\nUsingthisresult,wecandefineboundas\nbound = z .score−λ·(n −|z .tgt|) (5.99)\nbest max next\nAnalternativewaytoderivethelowerboundistosimplyconsidern timesofword\nmax\nreward,givenby\nbound = z .score−λ·n (5.100)\nbest max\nThisisalooselowerboundandleadstolesspruning.\nInthecaseoflengthnormalization,wecandothisinasimilarway. Forexample,consider\nlogPr(y|x)\nthe length normalization model Score(x,y)= , as in Eqs. (5.72-5.73). A lower\nn\nboundonadmissiblemodelscoresisgivenby\nPr(z .tgt|x)\nnext\nbound = (5.101)\nn\nmax\nIn practice, such a lower bound can be defined in several different ways to guarantee the",
    "optimalityofsearch,dependingonwhichmodelandsearchstrategyareusedinthesequence-\nto-sequencesystems[Huangetal.,2017b;StahlbergandByrne,2019].\nWecaneasilyapplytheselowerboundstotheaboveexactsearchalgorithmbyreplacing\nline9withEq. (5.99)or(5.101). Asasideeffect,thesearchwillexploremorehypothesesand\nthusbemuchslower.\n5.4.5 Differentiable Search\nWehaveaddressedthesearchproblemthroughtheintroductionofheuristicsearchalgorithms\ninwhichwetrytominimizethescoringfunctiononasetofsequencesofdiscretevariables.\nAn alternative possibility is to relax these discrete variables to continuous variables and to\nformulatetheproblemusingtheframeworkofcontinuousoptimization[Hoangetal.,2017;\nKumar et al., 2021]. While we try to use a consistent notation throughout this book, it is\nconvenientheretointroducesomenewnotationthatisslightlydifferentfromthatadoptedin\nthepreviouschapters. Wewilluseavectoryw∈{0,1}|Vy|todenotetheone-hotrepresentation\ni",
    "ofy . SupposetheoutputatstepiisadistributionoverV ,denotedbyPr(·|y ,x). Then,we\ni y <i",
    "5.4 Search 257\ncanwritethelogprobabilityofy atstepiasadotproductoftwovectors,likethis\ni\nlogPr(y |y ,x) = yw·logPr(·|y ,x)\ni <i i <i\n= yw·logPr(·|yw...yw ,x) (5.102)\ni 0 i−1\nwherey =y ...y isrepresentedasasequenceofone-hotvectorsyw...yw . Asdiscussed\n<i 0 i−1 0 i−1\ninChapter3,theright-handsideoftheaboveequationmeanstheselectionoftheentryy of\ni\nthevectorlogPr(·|y ,x)(orlogPr(·|yw...yw ,x)).\n<i 0 i−1\nUsingthisnotation,wecanwritelogPr(y|x)as\nn\n(cid:88)\nlogPr(y|x) = logPr(y |y ,x)\ni <i\ni=1\nn\n(cid:88)\n= yw·logPr(·|yw...yw ,x) (5.103)\ni 0 i−1\ni=1\nProvided we use logPr(y|x) as the objective function (i.e., score(x,y)=logPr(y|x)),\nthesearchproblemcanbeformulatedas\nn\n(cid:88)\nyˆw...yˆw = argmax yw·logPr(·|yw...yw ,x) (5.104)\n0 n i 0 i−1\nyw...yw\n1 n i=1\nThisisequivalenttothestandardformforinferenceofsequence-to-sequencemodels,givenby\nyˆ = yˆ ...yˆ\n0 n\n= argmaxPr(y ...y |x) (5.105)\n0 n\ny0...yn\nGivenEq. (5.104),wecannowrelaxeachone-hotvectortoareal-valuedvectorwitha",
    "constraintthatthesumofallitsentriesisequalto1,thatis,\nyw ∈ +R|Vy| (5.106)\ni\ns.t. ||yw|| = 1 (5.107)\ni 1\nInthisway,yw canbeinformallytreatedasa|V |-dimensionalembeddingofy ,though\ni y i\nit has much more dimensions than the usual embeddings used in NLP. Now yw does not\ni\ncorrespondtoaspecificwordinthevocabulary,butdescribesadistributionoverthevocabulary.\nInHoangetal.[2017]’swork,yw·logPr(·|yw...yw ,x)iscalledtheexpectedembedding\ni 0 i−1\nunderthedistributionlogPr(·|yw...yw ,x). Whatisinterestingaboutthisformulationisthat\n0 i−1\nEq. (5.104) in fact defines a “new” task in which we try to maximize a sum of continuous\nvariables(i.e.,asumofnexpectedembeddings).\nWe can solve Eq. (5.104) by using the off-the-shelf toolkits in optimization. Since we\nhave a constraint that yw is a variable in a simplex13, it is straightforward to apply general\ni\n13Simplexisatermusedingeometry.InaEuclideanspace,ak-simplexisak-dimensionalpolytopedescribed",
    "258 Chapter 5. Sequence-to-Sequence Models\nconstrainedoptimizationalgorithmstothisproblem. Analternativewayistousealgorithms\nthataredesignedtosolvetheoptimizationproblemwithsimplexconstraints. Thedetailsof\nthesealgorithmscanbefoundinmanybooksonoptimization.\nA third choice of solving Eq. (5.104) is to formulate the constraints in the objective\nfunctionexplicitlyandtousegradientdescentmethodstooptimizethisfunction. Forexample,\nHoangetal.[2017]modifyEq. (5.104)andobtainanewformforoptimization\nn\n(cid:88)\nyˆw...yˆw = argmax Softmax(yw)·logPr(·|yw...yw ,x) (5.111)\n0 n i 0 i−1\nyw...yw\n1 n i=1\nHereweremovethesimplexconstraintfromyw,andimposeitonanewoutputthatisproduced\ni\nbyaSoftmaxfunction.\nOnce we have obtained the optimal sequence yˆw...yˆw, we need to map each yw to a\n0 n i\nuniqueword. Asimplemethodistotakethewordcorrespondingtotheentryofyw withthe\ni\nlargestvalue. However,thismaybreaktheoptimalityofthesolutionbecausethecondition",
    "yw...yw is changed when these variables are discretized. A more practical method is to\n0 i−1\nperform optimization to predict the next word given a prefix, say, we fix yw...yw to the\n0 i−1\none-hotrepresentationsoftheoptimalprefix,andmaximize (cid:80)n yw·logPr(·|yw...yw ,x).\nk=i k 0 k−1\nThen,weselectthebestwordatpositioniandmoveontothenextposition.\nSo far we have assumed that the search objective is derived from the log probability\nlogPr(y|x)andthelengthoftheoutputisgiveninadvance. Tohaveasearchoversequences\nwithdifferentlengths,wecanrepeattheaboveoptimizationprocedureforeveryn∈[1,n ],\nmax\nandselectthesequencewiththemaximumscore. Thisalsomakesiteasytointroducelength\nnormalization and reward into search. We can ignore the length bias issue in each search\nwith a fixed n, and add the length models after optimization, that is, we leave the search\nobjectiveunchanged,but,inthefinalstep,weselectthebestsequenceinasetofcandidates\nwithdifferentnintermsofscore(x,y).\n5.4.6 Hypothesis Diversity",
    "Multipleoutputsareoftenrequiredwhenonewantstorescoretheseoutputsand/orinteract\nwith the system. One of the most widely used methods is to use beam search to generate a\nnumberoftop-rankedhypotheses. Forexample,wecansimplyviewtheelementsofHeap\nasthek-besthypothesesinbeamsearch(seeSection5.4.2). However,thisapproachsuffers\nfromtheproblemthatthereisoftenlittledifferenceamongthehypothesesinthebeam,and\nbyasetofk+1independentpoints{p\n0\n,p\n1\n,...,p\nk\n}.Thispolytopeisdefinedasasetofpoints\nP k-simplex = {a 0 ·p 0 +a 1 ·p 1 +...+a k ·p k } (5.108)\nwhere\nk\n(cid:88)\na i = 1 (5.109)\ni=0\na i ≥ 0 foranyi∈[0,k] (5.110)",
    "5.4 Search 259\nRank Output\n1 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor\nanewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek\nfor“ThePriceIsRight”whenBobBarker,91,showeduptorunhisoldshow.\n2 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor\nanewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek\nfor\"ThePriceIsRight\"whenBobBarkershoweduptorunhisoldshow.\n3 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor\nanewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek\nfor\"ThePriceIsRight\"whenBobBarker,91,showeduptoruntheshow.\n4 ManuelaArbelaezaccidentallyrevealedthecorrectanswertoaguessinggamefor\nanewHyundaiSonata. HostDrewCareycouldn’tstoplaughing. It’sbeenabusyweek\nfor\"ThePriceIsRight\"whenBobBarker,91,showeduptorunhisshow.\nTable5.3: 4-bestoutputsofatextsummarizationsystemonasampleintheCNN/DailyMail\ndataset(beamsize=4). Weseethatthesetextsdifferonlybyafewwords.",
    "it is difficult to figure out which one is better though more options are available to users.\nTable 5.3 shows the 4-best outputs of a text summarization system. We see that these texts\narefairlysimilartoeachother. Onereasonforthisphenomenonisthatdiversehypotheses,\nthough probably with high model scores when completed, will be pruned away if they are\nlow-rankedinsomestagesofbeamsearch. Fromamodelingperspective,wecaninterpretthis\nasaproblemwiththelocallynormalizedmodelsthatweusehere: everypredictionismade\nonanintermediatestepofsearch,andthereisnowayforthefollowingstepstoescapeifthe\nprefixisfixed[MurrayandChiang,2018].\nOneapproachtoimprovingthehypothesisdiversityistogivepenaltiestocaseswherethe\nhypothesesinthebeamarelessdiverse[LiandJurafsky,2016;Vijayakumaretal.,2018]. A\nsimpleexampleofsuchobjectivefunctionsisgivenby\nscore (x,y) = score(x,y)−λ·dp (5.112)\nd\nItcombinestheoriginalmodelscorescore(x,y)andadiversitypenaltydp. dpcanbedefined",
    "inafewdifferentways. Anideaistopenalizehypothesesthatarecloseinthesearchtree. For\nexample,onecandefinedpastherankofahypothesisinthesetofitssiblingsthatareextended\nfrom the same parent hypothesis, and so the beam can spread its members over a larger\nregionofthespaceofhypotheses[LiandJurafsky,2016]. Anotherwaytointroducediversity\nmeasuresistoconsiderthedifferencesbetweenthetarget-sidesequencesofthehypotheses\ninthebeam. Forexample,wecandefinedpastheaveragestringsimilaritybetweenagiven\nhypothesisandotherhypothesesinthebeam[Xiaoetal.,2013].\nThe above idea can also be expressed as constraints imposed on the search procedure.\nFor example, we can constrain the beam to include only the hypotheses that are rooted at",
    "260 Chapter 5. Sequence-to-Sequence Models\ndifferentparentsinthelaststep[Boulanger-Lewandowskietal.,2013]. Moreprecisely,for\neachhypothesisz ∈Z ,weseekthebestnext-stephypothesisby\ncur i−1\nzˆ = argmax Pr(z .tgt|x) (5.113)\nnext next\nznext∈Extend(zcur,Vy)\nThehypothesiszˆ isthenaddedtoZ . Notethatthisisessentiallyasub-spacemethodthat\nnext i\ndividesaspaceofhypothesesintosub-spacesofhypotheses,andcollectsresultsoverthese\nsub-spaces. Anintuitionbehindthismethodisthatdifferentsub-spacescandescribedifferent\naspectsoftheproblem,andsowecanhavediversesolutions.\nAnotherapproachtoaddressingthediversityissueistoperturbbeamsearchbyintroducing\nrandomly generated hypotheses into the beam [Holtzman et al., 2020a; Wiher et al., 2022].\nOnecommonwaytodothisistochoosesomerandomwordsforextendingahypothesis,and\nto add the extended hypotheses to the beam. In general, these words can be sampled from\nthe distribution Pr(·|y ,x) over the entire vocabulary or its subset. Randomness can also\n<i",
    "beaddedtotheinputsofasystemattesttime. Forexample,onecanexpressaninputword\nasalinearcombinationofitsoriginalembeddingandtheembeddingofawordofarandom\nsequence drawn from the training data [Li et al., 2021b]. In problems having many local\nminima,addingrandom“noise”tosearchproceduresisgenerallyhelpful,aswecanexplore\nmorediversehypothesesandpreventthesystemsfromgettingstuckincertainregionsofthe\nsearchspace.\nInsteadofperformingsearchusingasinglesystem,wecanusemultiplesystemstoobtain\ndiversehypotheses. Thesesystemscanbebuiltoneitherdifferentarchitecturesordifferent\nhidden structures/configurations [He et al., 2018; Shen et al., 2019; Wu et al., 2020a; Sun\netal.,2020a]. Althoughmethodsofthistypedonotfallunderthesearchframeworkthatwe\nhavebeendiscussing,combiningtheresultsfrommultiplesystemsisgenerallyhelpful. The\nfollowingsectionwillpresentadiscussiononthisissue.\n5.4.7 Combining Multiple Models\nFrom a machine learning point of view, ensembling are methods for addressing modeling",
    "issues,notsearchissues. Inthissubsection,wediscussthesemethodsbecausetheirimplemen-\ntationstypicallyrequiremodificationstothesearchmodules,andwecangainsomeinsight\nintotheresultingsystembyviewingitfromthesearchperspective.\nIn machine learning, ensemble methods aim to make better predictions by combining\npredictions of a number of constituent systems or component systems. The problem of\ncombiningmultiplesystemshasbeendiscussedextensivelyintimeswhenstatisticalmodels\nemerged in NLP, and is sometimes called system combination methods for emphasizing\nits practical use. For sequence-to-sequence models discussed here, a widely used form of\nsystemcombinationisanaverageofpredictions[Sutskeveretal.,2014]. SupposewehaveK\nsequence-to-sequencemodelsthathavebeentrained. Thelogprobabilityofthetarget-side\nwordy givenitsleftcontexty andthesource-sidesequencexcanbedefinedbyusingthe\ni <i",
    "5.4 Search 261\ngeometricaverage\nK\n1 (cid:88)\nlogPr(y |y ,x) = logPr (y |y ,x) (5.114)\ni <i k i <i\nK\nk=1\noralternativelybyusingthearithmeticaverage\nK\n1 (cid:88)\nlogPr(y |y ,x) = log Pr (y |y ,x) (5.115)\ni <i k i <i\nK\nk=1\nwherePr (y |y ,x)istheoutputofthek-thcomponentsystem. Theseformsaresosimple\nk i <i\nthatonecanimplementthemforanysequence-to-sequencemodelswithoutsignificantmodifi-\ncationstoexistingsystems,andtheyhavebeenusedasthebasisofmanysuccessfulsystems\ninvariousevaluationtasks[Barraultetal.,2020;Akhbardehetal.,2021].\nA problem with prediction averaging is that all the component systems are required to\nfollowthesamebasicformofmodeling(seeEq. (5.68))andweneedtohaveaccesstothe\nprobabilities{Pr (y |y ,x)}. Whenwehaveonlyasetofblack-boxsystemsinhand,we\nk i <i\nneedtoperformsequenceensembling. Acommonideaistovotefromtheensembleofthe\nsequencesproducedbythecomponentsystems. Forexample,oneofthesimplestwaystodo\nthisishypothesisselection[HildebrandandVogel,2008],inwhichwesimplyselectthe“best”",
    "sequencefromtheensembleusingsomecriterion. Analternativewayofsequenceensembling\nistoregenerateanewsequencedifferingfromanyoftheoriginalsequences[Matusovetal.,\n2006;Rostietal.,2007]. Thistypicallyrequiresamodelthatrepresentsthesequencesintoa\ncompactrepresentation(suchasalattice),aswellasanadditionalsearchpassbywhichwe\ncanfindthebestoutputinthisnewrepresentationofhypotheses(suchaslatticesearchand\nrescoring)[Deorasetal.,2011;Stahlbergetal.,2016;Khayrallahetal.,2017].\nNotethattheensemblingofsequence-to-sequencemodelsisrelatedtothediversityissue\ndiscussedintheprevioussubsection. Itisoftenthoughtthatcomponentsystemsneedtobe\ndiverse for a better ensembling result, and so we need to build these systems in some way\nthatwecanmakethemdifferent[Sutskeveretal.,2014;Zhouetal.,2017]. Oneofthemost\npopularmethodsischeckpointensembling. Ittakesanumberofcopiesofamodelatdifferent\ncheckpointsduringtraining,andcombinesthesemodelcopiesviapredictionaveraging. This",
    "methodcanbeusefulforalleviatingtheoverfittingprobleminpractice. Also,differentmodels\ncanbecreatedfromabasemodelunderdifferentsettings. Forexample,wecanbuildmodels\nwith different numbers of parameters on the basis of a backbone model. A more general\napproachistotakemodelsbasedondifferentarchitectures,althoughthisisattheexpenseof\nmoredevelopmenteffort.\nAnotherwaytoviewsequenceensemblingisthatitprovidesatwo-passsearchscheme.\nInthefirstpassofsearch,multiplesystemsareusedtoperforminferenceindividually. Each\nofthesesystemshasitsownbiasformodelingandsearch,andexploresdifferentregionsof\nthesearchspace. Ahypothesisexploredbyonesystemmightnotbeseenandevaluatedby",
    "262 Chapter 5. Sequence-to-Sequence Models\nanothersystem. Theresultofthispassisadiverseensembleofhypothesesthatare“optimal”\nfromsomeperspectives. Inthesecondpassofsearch,weusethisensembletodefineanew\nspaceofhypotheses,anduseafine-grainedmodeltosearchforthefinalresult.\n5.4.8 More Search Objectives\nIn this subsection, we consider more objective functions that can be applied to the search\nproblem.\n1. Search with Future Scores\nMostofthealgorithmsdescribedinthissubsectioncanbeviewedassomeoptimizationsof\nbest-firstsearchalgorithms[Meisteretal.,2020]. Asanotherexampleofbest-firstsearch,A*\nsearchiswidelyconsideredtobeagoodsolutiontothegeneralsearchproblem. VanillaA*\nsearchrequiresthatallstatesofsearcharesortedineverysearchstep,whichisintractablein\nourproblems. Wethereforestillconsiderbeamsearchandgreedysearchforourdiscussion,\nbutuseanA*search-likeobjectivefunctioninstead. Specifically,givenasearchstate(x,y ),\n≤i\ntheA*search-likeobjectivefunctioncanbedefinedas",
    "score (x,y ) = g(x,y )+h(x,y ) (5.116)\nA* ≤i ≤i ≤i\nHereg(x,y )istherewardofthepathfromthestartstateto(x,y ),andh(x,y )isthe\n≤i ≤i ≤i\nestimatedrewardofthe“optimal”pathfrom(x,y )tothefinalgoal. Becauseg(x,y )and\n≤i ≤i\nh(x,y )canhavearbitraryforms,thisframeworkisverygeneral. Forexample,ifwedefine\n≤i\ng(x,y ) = score(x,y ) (5.117)\n≤i ≤i\nh(x,y ) = 0 (5.118)\n≤i\nthenscore (x,y)isexactlythesameastheobjectivefunctionsdiscussedpreviously.\nA*\nTomakefulluseofthisformulation,itseemsnaturaltoseekafunctionoffuturereward\norfuturecost. Ideally,wewouldlikeh(x,y )tobeabletocomputehowmuchadditional\n≤i\nrewardwecanobtainifweextend(x,y )tothebestcompletehypothesis. Thisis,however,\n≤i\nintractablebecauseweneedtoexploreallthehypothesesextendedfrom(x,y )andfindthe\n≤i\nbestone. Itiscommonpracticetouseacomputationallycheapermodelanalogoustothereal\nfuturerewardmodel. Conventionalapproachesrelyonheuristicstodefineh(x,y )[Koehn\n≤i\netal.,2007],suchasestimatingtheweightsofthewordsthatcouldbefurthergenerated. These",
    "heuristicscanbegeneralizedtotheknowledgeofthemodeldesignofsequence-to-sequence\nsystems[Heetal.,2017;Zhengetal.,2018]. Amoregeneralapproachistouseavalue-based\ntreatment of the problem [Ren et al., 2017; Li et al., 2017a; Leblond et al., 2021]. We can\ndevelop a policy that learns to predict the distribution of y given x and y , and a value\ni <i\nfunction for this policy that learns to predict future rewards. Eq. (5.116) can therefore be\ninterpretedasalinearcombinationofthepolicyscoreof(x,y )andthecorrespondingvalue.\n≤i\nSuchatreatmentofsearchobjectivesfallsintotheframeworkofvalue-basedsearch,andhas\nbeensuccessfullyemployedinreinforcementlearning[Silveretal.,2017].",
    "5.4 Search 263\n2. Search with Language Models\nFor a long time, language models played an important role in text generation tasks. For\nexample, statisticalmachinetranslationsystemsandautomaticspeechrecognitionsystems\ntypically rely on large n-gram language models to produce fluent texts. While modern\nsequence-to-sequence models are not required to have separate language models, applying\nthemtosequence-to-sequencesearchstillmakesintuitivesenseformachinetranslationand\nrelatedproblems.\nFollowingtheconventionthatalanguagemodelcanbetreatedasafeatureofalog-linear\n(orlinear)model[OchandNey,2002],thelanguagemodel-augmentedobjectivecanbedefined\nas\nscore (x,y) = logPr(y|x)+λ·logPr(y) (5.119)\nlm\nThisformulationdoesnotinvolvelengthrewardandnormalizationterms,buteitherofthem\ncan be easily used as an additional feature of the model. In general, the language model\nPr(y)istrainedsolelyontarget-sidesequences,enablingtheuseoflarge-scalemonolingual",
    "datainsequence-to-sequencemodels[Gulcehreetal.,2017]. Interestingly,ithasbeenfound\nthatcurrentsequence-to-sequencemodelsarestronglanguagemodelsthemselvesiftheyare\ntrainedsufficiently,andabetterwaytomakeuseoftarget-sidedatamightbetouseittocreate\nsyntheticdata,calleddataaugmentation. Anexampleofthisisbacktranslationinwhichwe\nuseabackwardtranslationsystemtotranslatetarget-sidesentencestosource-sidesentences,\nandthenusethissyntheticbilingualdataasadditionaldatafortrainingaforwardtranslation\nsystem[Sennrichetal.,2016a;Edunovetal.,2018]. Inmanytasks,suchasimplemethodcan\nachievesignificantimprovementsintranslationquality,butthisresultquestionsthenecessity\nofusingadditionallanguagemodelsinneuralmachinetranslation.\nNotethatthemodelofEq. (5.119)dependsonourchoiceforthecoefficientλ. Formachine\ntranslation,weareusuallyinterestedinapositivevalueofλsothatoursystemcanproduce\nmore fluent texts. By contrast, a negative value of λ means that we want some output that",
    "is less frequent. For example, if λ=−1, then Eq. (5.119) can be written as the point-wise\nmutualinformationofxandy\nscore (x,y) = logPr(y|x)−logPr(y)\nlm\nPr(x,y)\n= log (5.120)\nPr(x)·Pr(y)\nThis scoring function has been shown to be useful for generating more diverse outputs for\nneuralconversationsystems[Lietal.,2016].\n3. Minimum Bayes Risk Search\nSo far, our discussion of search objectives has focused on the use of the decision rule of\nchoosingthehighestscorehypothesis,calledmaximumaposteriori(MAP)search14. An\n14Instatistics,MAPisamethodforinferenceoftheparametersofastatisticalmodel.Supposewehaveamodel\nthatdescribesthedistributionofavariablexandthemodelisparameterizedbyθ.MAPseekstheoptimalvalueof",
    "264 Chapter 5. Sequence-to-Sequence Models\nassumptionbehindthismethodisthattheposteriorprobabilityPr(y|x)(orthemodelscore\nscore(x,y))correlateswiththetruequalityofoutputs. Inpractice,thisassumptionleadsto\nseveral useful properties, e.g., the search system is easy to implement, and the objective of\nsearchisconsistentwiththatoftraining. However,therearesomeshortcomingswithMAP\nsearch,whichcausesresearcherstoconsidermorepowerfulmethods. OneproblemwithMAP\nsearch is that the objective does not reflect the way one evaluates the system. The metrics\nused in end-to-end evaluation of a system may have very different forms from Pr(y|x). A\nsecondproblemisthatMAPisjustaspecialcaseoftheBayesiantreatmentofdetermining\nposterior probabilities. It provides a point estimate of θ with no uncertainty measure, and\nissometimesoverconfident. Insomeapplications,sequence-to-sequencemodelsspreadtoo\nmuchprobabilitymassacrossmanydifferenthypotheses[Ottetal.,2018a],andMAPmaynot",
    "describethemajorportionofthedistribution.\nHereweconsiderminimumBayesrisk(MBR)searchthatprovideswaystointroduce\nevaluationmeasuresintosearch,aswellaswaystomakeuseofthedistributionsoverhypothe-\nses. TheMBRmethodassumesariskfunctiononapairofsequences,denotedbyR(y,y ). It\nr\ncomputesthecostofreplacingy withy intermsofsomeevaluationmetric. Forexample,\nr\nwecandefinetheriskscoretobe1−BLEUformachinetranslation. Then,theriskforyon\nasetofsequencesΩisgivenbytheexpectationofR(y,y )withrespecttothedistribution\nr\nPr(y |x)\nr\nRisk(y) = E R(y,y )\nyr∼Pr(yr|x) r\n(cid:88)\n= R(y,y )·Pr(y |x) (5.124)\nr r\nyr∈Ω\nHowever,thesummationoverallpossibletarget-sidesequencesiscomputationallyinfeasi-\nble. WethereforedefineΩtobethek-bestoutputsorsampledoutputsofasystem[Eikema\nandAziz,2020],denotedbyΩ . Then,wetakescore(x,y)=−Risk(y)andobtainthe\nsystem\nθbymaximizingtheprobabilityofθgivenx,writtenas\nθˆ = argmaxPr(θ|x) (5.121)\nMAP\nθ\nθˆ isalsocalledthemodeoftheposteriordistributionofθ. FortheMAPsearchproblemhere,wesimply\nMAP",
    "denoteθbyyandseekthemodeofPr(y|x).\nAsaBayesianmethod,wecanre-expresstheaboveequationusingtheBayes’rule\nPr(x|θ)·Pr(θ)\nθˆ = argmax\nMAP Pr(x)\nθ\n= argmaxPr(x|θ)·Pr(θ) (5.122)\nθ\nwhereθistreatedasavariablehavingapriordistributionPr(θ).\nBycontrast,MLEdirectlymaximizesthelikelihoodfunctionPr(x|θ)\nθˆ = argmaxPr(x|θ) (5.123)\nMLE\nθ\nThus,theMAPresultcanbeviewedasanestimationofθthatconsidersbothMLEofxgivenθandthepriorofθ.\nNotethatMAPandMLEwillbeequivalentifPr(θ)isauniformdistribution.",
    "5.5 Summary 265\nfollowingobjectiveforMBRsearch\nyˆ = argmax−Risk(y)\ny\n(cid:88)\n= argmin R(y,y )·Pr(y |x) (5.125)\nr r\ny\nyr∈Ωsystem\nThis model is very general and applies to a wide range of NLP problems in which one\nneedstosearchforanoptimalhypothesisinalargesetofcandidates[Goodman,1996b;Goel\nand Byrne, 2000; Kumar and Byrne, 2004b]. It allows for flexible forms of risk functions,\nfor instance having various factors considered in evaluating hypotheses. MBR search has\nrecentlybeenofinteresttoNLPresearchersastheyarefoundtobeeffectiveineliminatingthe\nbiasescausedbyMAPsearch[MüllerandSennrich,2021;Freitagetal.,2022]. Inadditionto\nprovidingaformulationofsearchobjectives,MBRmethodscanbeusedfortrainingsequence-\nto-sequencemodels,andarethoughttobesolutionstothediscrepancyissuebetweenobjectives\noftrainingandevaluation[Shenetal.,2016].\n5.5 Summary\nInthischapter,weattemptedtoprovideanoverviewofsequence-to-sequencemodelingwhich",
    "can serve as the basis for many NLP systems. Sequence-to-sequence modeling is a very\nrich area of research, and has been widely discussed in different disciplines, even beyond\nNLP. This chapter is not a review of all the literature on this subject (this would be a big\nproject),butfocusesonsomeofthecoremethodsandideas. Westartedwithanintroduction\nofsequence-to-sequenceproblems,aswellastheencoder-decoderarchitecturewhichlaysthe\nfoundationsformostofthestate-of-the-artsequence-to-sequencesystems. Asanillustrationof\ntheapplicationofthisarchitecture,weconsideredtheproblemofneuralmachinetranslation,\nandbuiltasimpleneuralmachinetranslationmodelusingthebasicknowledgewehavelearned\nsofar.\nWealsopresentedtheattentionmechanismandaseriesofrefinements. Ifwelookback\nto the past few years, we will find that exploring attention models is the next natural step\nin developing sequence-to-sequence models. While these models are well known for their",
    "applicationandimpressiveperformanceinmachinetranslation,theyhavedominatedtheNLP\ncommunity. Thereisalsogreatinterestinattentionmodelsinsomeothersub-fieldsofAI,such\nascomputervision[BorjiandItti,2012;Xuetal.,2015;Jaderbergetal.,2015]andspeech\nprocessing[Chorowskietal.,2015;Chanetal.,2016;Bahdanauetal.,2016]. Theresultis\nthatthepastfewyearswereanexcitingtimeforpeopleintheseareas.\nSequence-to-sequencemodelsaresosuccessfulthatwetrytoputeverythinginthesame\npocket. Notonlyhavewedevelopedpowerfulsequence-to-sequencemodelstodealwithvery\ngeneralproblems,butcurrentresearchisforcedtobeunifying. AnexampleisthatTransformer,\naself-attention-basedsequence-to-sequencemodel,hasbecomeoneofthefundamentalmodels\nformanytasksrangingoverdifferenttypesofdata,fromtextualtovisualandacousticdata. It\ncanevenbeextendedtodealwithmultimodalproblemswhicharesometimesmorechallenging.",
    "266 Chapter 5. Sequence-to-Sequence Models\nThismakesthingsmoreinterestingandexciting: animprovementtoonemodelcanbeused\nto improve systems in a variety of tasks. And we are seeing a significant change in our\nresearch paradigm in which the NLP and machine learning fields are marrying and results\nin NLP research are becoming more influential. However, on the other side of the coin is\nthatwearemakingmuchroomforsomeoftheproblemsbutleavinglessroomfortheothers.\nIn recent NLP conferences, we can see many, many papers talking about how to train big\nsequence-to-sequencemodelsandapplythemtodifferenttextgenerationtasks,buttherearea\nrelativelysmallnumberofpapersonparsing. Therehavealwaysbeendebatesonthisoverthe\npastfewdecades,forexample,whatandhowmuchpriorknowledgedoweneedtobuildan\nNLPsystem? [Church,2011;See,2018]Gettinginvolvedinsuchdebatesissimplybeyondthe\ndiscussionsinthischapter. Fortunately,NLPresearchpromisestocontinuetobediverseand",
    "active,andwecanalwayshearandlearnfrombothsidesofthedebates. Forexample,there\nareinterestingfindingsthattheneuralsequencemodelscanlearnsomelinguisticproperties\nfromdata,andlinguisticstructurescanhelpsystemdesign. InChapter6,wewillseeafew\nexamples.\nThe“bias”ofresearchfocusalsoexistsonthemachinelearningsideofproblem-solving.\nForexample,forsequence-to-sequenceproblemsdiscussedhere,recentyearshavewitnessed\nadrasticincreaseofinterestinmodeldesignandtrainingmethods,butonlyarelativelysmall\ngroup of people discuss the search problem. While search is a classical problem in AI and\nplaysanimportantroleinpracticalsystems[RussellandNorvig,2010],itisevennotdiscussed\ninrecenttutorialsandsurveysinNLP.Thismotivatesustowriteasectiononthissubjectso\nthatwecanhaveamorecompletepictureoftheproblem. However,ourgeneraldiscussion\ndoesnotcoverallaspectsofthesearchproblem. Atopicweleftoutisefficiency[Birchetal.,\n2018;Heafieldetal.,2021]. Whilethischapterincludessomediscussionsontheefficiency",
    "issue,suchasstoppingcriteriaofsearchalgorithms,efficientmethodsareawide-rangingtopic\nandaregenerallydependentonmodelarchitectures. Amoredetaileddiscussionofthemcan\nbe found in Chapter 6. Another topic that one may be interested in is constrained search\ninwhichconstraintsareimposedonthesearchprocess[HokampandLiu,2017;Anderson\netal.,2017]. Ingeneral,theseconstraintscomefromourpriorknowledgeorinteractionswith\nusers. Forexample,constrainedsearchhasbeenusedtoenforcetermtranslationconstraints\nonmachinetranslation[Hasleretal.,2018;PostandVilar,2018].\nOne last note on limitations of this chapter. The formulation of the general sequence-\nto-sequence problem described here is based on the left-to-right factorization of Pr(y|x),\nresultinginanautoregressivemodel. Onelimitationofthisformulationisthateachprediction\natsomestepdependsonlyontheprecedingwords,andsothemodelcannotaccesstheright\ncontext. To make use of the right context of a word, a simple approach is to build another",
    "modelthatperformsright-to-leftgeneration. Theleft-to-rightandright-to-leftmodelscanthen\nbecombinedtogenerateabetteroutputsequence[Liuetal.,2016a;Hoangetal.,2017;Zhang\netal.,2018b;2020a]. Analternativeapproachisgivenbynon-autoregressivegenerationor\nnon-autoregressivedecodinginwhichtheconstraintofautoregressivegenerationisremoved\nandeachwordpredictionisconditionedontheglobalcontext[Guetal.,2018;Ghazvininejad\netal.,2019;Leeetal.,2020]. Anicepropertyofnon-autoregressivegenerationisthepossibility",
    "5.5 Summary 267\nofsystemspeed-up,sinceallthewordsinasequencecanbegeneratedinparallelandwecan\ndothisefficientlyusingGPUs.",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 6\nTransformers\nSo far we have discussed several basic models for solving sequence-to-sequence problems.\nWe now explore a new class of models which are based on a powerful architecture, called\nTransformer. TransformersdifferinseveralwaysfromthemodelsgiveninChapters4and\n5. First, they do not depend on recurrent or convolutional neural networks for modeling\nsequences of words, but use only attention mechanisms and feed-forward neural networks.\nSecond,theuseofself-attentioninTransformersmakesiteasiertodealwithglobalcontexts\nanddependenciesamongwords. Third,Transformersareveryflexiblearchitecturesandcan\nbe easily modified to accommodate different tasks. The past few years have seen the rise\nofTransformersnotonlyinNLPbutalsoinseveralotherfields. AsTransformersandtheir\nvariants continue to mature, these models are playing an increasingly important role in the\nresearchandapplicationofartificialintelligence.",
    "Inthischapter,wewilldiscussthecoreideasofTransformers. Wewillbeginourdiscussion\nby looking at the standard Transformer architecture. Then we will look at some notable\ndevelopments,suchasimprovementstothebasicarchitectureandefficientmethods. Wewill\nalso present several applications in which Transformer models have been extensively used.\nHowever,thediscussionofTransformerisawide-rangingtopic,andtherehavemany,many\nrelatedpapers. Thischapterisnotintendedtoprovideacomprehensivesurveyoftheliterature\nbutacollectionofselectedtopicsthatNLPpeoplemaybeinterestedin.\n6.1 The Basic Model\nHereweconsiderthemodelpresentedinVaswanietal.[2017]’swork. Westartbyconsidering\ntheTransformerarchitectureanddiscussthedetailsofthesub-modelssubsequently.\n6.1.1 The Transformer Architecture\nFigure6.1showsthestandardTransformermodelwhichfollowsthegeneralencoder-decoder\nframework. A Transformer encoder comprises a number of stacked encoding layers (or",
    "encodingblocks). Eachencodinglayerhastwodifferentsub-layers(orsub-blocks),called\ntheself-attentionsub-layerandthefeed-forwardneuralnetwork(FFN)sub-layer. Suppose",
    "270 Chapter 6. Transformers\nwe have a source-side sequence x=x ...x and a target-side sequence y =y ...y . The\n1 m 1 n\ninputofanencodinglayerisasequenceofmvectorsh ...h ,eachhavingd dimensions\n1 m model\n(or d dimensions for simplicity). We follow the notation adopted in the previous chapters,\nusingH∈Rm×d todenotetheseinputvectors1. Theself-attentionsub-layerfirstperformsa\nself-attentionoperationAtt (·)onHtogenerateanoutputC:\nself\nC = Att (H) (6.1)\nself\nHereCisofthesamesizeasH,andcanthusbeviewedasanewrepresentationoftheinputs.\nThen,aresidualconnectionandalayernormalizationunitareaddedtotheoutputsothatthe\nresultingmodeliseasiertooptimize.\nTheoriginalTransformermodelemploysthepost-normstructurewherearesidualcon-\nnectioniscreatedbeforelayernormalizationisperformed,likethis\nH = LNorm(C+H) (6.2)\nself\nwhere the addition of H denotes the residual connection, and LNorm(·) denotes the layer\nnormalization function. Substituting Eq. (6.1) into Eq. (6.2), we obtain the form of the",
    "self-attentionsub-layer\nLayer (H) = H\nself self\n= LNorm(Att (H)+H) (6.3)\nself\nThedefinitionsofLNorm(·)andAtt (·)havebeengiveninChapters2and5,andwewill\nself\nalsodiscussthemlaterinthesection.\nTheFFNsub-layertakesH andoutputsanewrepresentationH ∈Rm×d. Ithasthe\nself ffn\nsame form as the self-attention sub-layer, with the attention function replaced by the FFN\nfunction,givenby\nLayer (H ) = H\nffn self ffn\n= LNorm(FFN(H )+H ) (6.4)\nself self\nHere FFN(·) could be any feed-forward neural networks with non-linear activation func-\ntions. The most common structure of FFN(·) is a two-layer network involving two linear\ntransformationsandaReLUactivationfunctionbetweenthem.\nFordeepmodels,wecanstacktheaboveneuralnetworks. LetHl betheoutputoflayer\nl. Then, we can express Hl as a function of Hl−1. We write this as a composition of two\nh \n1\n1Providedh j ∈Rdisarowvector,wehaveH=  . . .   .\nhm",
    "6.1 The Basic Model 271\nPr(·|y ,x ...x )\n0 1 m\n. . . . . .\nPr(·|y ...y ,x ...x )\n0 n−1 1 m\nSoftmax(SLWo)\nDecoder\nAdd&LayerNorm\nFeed-ForwardNetwork\nEncoder Layer ffn (·)\n×L\nAdd&LayerNorm Add&LayerNorm\nFeed-ForwardNetwork Encoder-DecoderAttention\nL×\nLayer (·) Layer (·)\nffn cross\nAdd&LayerNorm Add&LayerNorm\nSelf-Attention Self-Attention\nLayer (·) Layer (·)\nself self\nWord Position Word Position\nx ...x y y ...y\n1 m 0 1 n−1\nFigure6.1: TheTransformerarchitecture[Vaswanietal.,2017]. ThereareLstackedlayerson\neachoftheencoderanddecodersides. Anencodinglayercomprisesaself-attentionsub-layer\nand an FFN sub-layer. Both of these sub-layers share the same structure which involves a\ncorefunction(eitherLayer (·)orLayer (·)),followedbyaresidualconnectionandalayer\nself ffn\nnormalizationunit. Eachdecodinglayerhasasimilararchitecturewiththeencodinglayers,but\nwithanadditionalencoder-decoderattentionsub-layersandwichedbetweentheself-attention",
    "andFFNsub-layers. Aswithmostsequence-to-sequencemodels,Transformertakesx ...x\n1 m\nand y ...y for predicting y . The representation of an input word comprises a sum of a\n0 i−1 i\nwordembeddingandapositionalembedding. Thedistributions{Pr(·|y ...y ,x ...x )}are\n0 i−1 1 m\ngeneratedinsequencebyaSoftmaxlayer,whichoperatesonalineartransformationofthe\noutputfromthelastdecodinglayer.\nsub-layers\nHl = Layer (Hl ) (6.5)\nffn self\nHl = Layer (Hl−1) (6.6)\nself self",
    "272 Chapter 6. Transformers\nIfthereareLencodinglayers,thenHL willbetheoutputoftheencoder. Inthiscase,HL can\nbeviewedasarepresentationoftheinputsequencethatislearnedbytheTransformerencoder.\nH0 denotestheinputoftheencoder. Inrecurrentandconvolutionalmodels,H0 cansimplybe\nwordembeddingsoftheinputsequence. Transformertakesadifferentwayofrepresenting\ntheinputwords,andencodesthepositionalinformationexplicitly. InSection6.1.2wewill\ndiscusstheembeddingmodelusedinTransformers.\nTheTransformerdecoderhasasimilarstructureastheTransformerencoder. Itcomprises\nLstackeddecodinglayers(ordecodingblocks). LetSl betheoutputofthel-thdecoding\nlayer. Wecanformulateadecodinglayerbyusingthefollowingequations\nSl = Layer (Sl ) (6.7)\nffn cross\nSl = Layer (HL,Sl−1) (6.8)\ncross cross self\nSl = Layer (Sl−1) (6.9)\nself self\nHeretherearethreedecodersub-layers. Theself-attentionandFFNsub-layersarethesame\nas those used in the encoder. Layer (·) denotes a cross attention sub-layer (or encoder-\ncross",
    "decodersub-layer)whichmodelsthetransformationfromthesource-sidetothetarget-side. In\nSection6.1.6wewillseethatLayer (·)canbeimplementedusingthesamefunctionas\ncross\nLayer (·).\nself\nTheTransformerdecoderoutputsadistributionoveravocabularyV ateachtarget-side\ny\nposition. Thisisachievedbyusingasoftmaxlayerthatnormalizesalineartransformationof\nSL todistributionsoftarget-sidewords. Todothis,wemapSL toann×|V |matrixOby\ny\nO = SL·W (6.10)\no\nwhereW\no\n∈Rd×|Vy| istheparametermatrixofthelineartransformation.\nThen,theoutputoftheTransformerdecoderisgivenintheform\n \nPr(·|y ,x)\n0\n.\n  . .   = Softmax(O)\nPr(·|y ...y ,x)\n0 n−1\n \nSoftmax(o )\n1\n.\n=   . .   (6.11)\nSoftmax(o )\nn\nwhereo denotesthei-throwvectorofO,andy denotesthestartsymbol⟨SOS⟩. Underthis\ni 0\nmodel,theprobabilityofygivenxcanbedefinedasusual,\nn\n(cid:88)\nlogPr(y|x) = logPr(y |y ...y ,x) (6.12)\ni 0 i−1\ni=1\nThisequationresemblesthegeneralformoflanguagemodeling: wepredictthewordat",
    "6.1 The Basic Model 273\ntimeigivenallofthewordsuptotimei−1. Therefore,theinputoftheTransformerdecoder\nisshiftedonewordleft,thatis,theinputisy ...y andtheoutputisy ...y .\n0 n−1 1 n\nTheTransformerarchitecturediscussedabovehasseveralvariantswhichhavebeensuc-\ncessfully used in different fields of NLP. For example, we can use a Transformer encoder\ntorepresenttexts(callittheencoder-onlyarchitecture),canuseaTransformerdecoderto\ngeneratetexts(callitthedecoder-onlyarchitecture),andcanuseastandardencoder-decoder\nTransformermodeltotransformaninputsequencetoanoutputsequence. Intherestofthis\nchapter,mostofthediscussionisindependentoftheparticularchoiceofapplication,andwill\nbemostlyfocusedontheencoder-decoderarchitecture. InSection6.5,wewillseeapplications\noftheencoder-onlyanddecoder-onlyarchitectures.\n6.1.2 Positional Encoding\nIntheiroriginalform,bothFFNsandattentionmodelsusedinTransformerignoreanimportant\nproperty of sequence modeling, which is that the order of the words plays a crucial role in",
    "expressingthemeaningofasequence. Thismeansthattheencoderanddecoderareinsensitive\ntothepositionalinformationoftheinputwords. Asimpleapproachtoovercomingthisproblem\nistoaddpositionalencodingtotherepresentationofeachwordofthesequence. Moreformally,\nawordx canberepresentedasad-dimensionalvector\nj\nxp = x +PE(j) (6.13)\nj j\nHerex ∈Rdistheembeddingofthewordwhichcanbeobtainedbyusingthewordembedding\nj\nmodels,asdescribedChapter3. PE(j)∈Rd istherepresentationofthepositionj. Vanilla\nTransformeremploysthesinusoidalpositionalencodingmodelswhichwewriteintheform\n1\nPE(i,2k) = sin(i· ) (6.14)\n100002k/d\n1\nPE(i,2k+1) = cos(i· ) (6.15)\n100002k/d\nwherePE(i,k)denotesthek-thentryofPE(i). Theideaofpositionalencodingistodistin-\nguishdifferentpositionsusingcontinuoussystems. Hereweusethesineandcosinefunctions\nwith different frequencies. The interested reader can refer to Chapter 4 to see that such a\nmethodcanbeinterpretedasacarryingsystem. Becausetheencodingisbasedonindividual",
    "positions, it is also called absolute positional encoding. In Section 6.3.1 we will see an\nimprovementtothismethod.\nOncewehavetheaboveembeddingresult,xp ...xp istakenastheinputtotheTrans-\n1 m\nformerencoder,thatis,\n \nxp\n1\n.\nH 0 =   . .   (6.16)\nxp\nm\nSimilarly,wecanalsodefinetheinputonthedecoderside.",
    "274 Chapter 6. Transformers\n(a)RNN (b)CNN(r=3) (c)Self-attention\nFigure6.2: Informationflowsinrecurrent,convolutionalandself-attentionmodels,shownas\narrowlinesbetweenpositions.\n6.1.3 Multi-head Self-attention\nThe use of self-attention is perhaps one of the most significant advances in sequence-to-\nsequencemodels. Itattemptstolearnandmakeuseofdirectinteractionsbetweeneachpair\nofinputs. Fromarepresentationlearningperspective,self-attentionmodelsassumethatthe\nlearnedrepresentationatpositioni(denotedbyc )isaweightedsumoftheinputsoverthe\ni\nsequence. Theoutputc isthusgivenby\ni\nm\n(cid:88)\nc = α h (6.17)\ni i,j j\nj=1\nwhereα indicateshowstrongtheinputh iscorrelatedwiththeinputh . Wethuscanview\ni,j i j\nc asarepresentationoftheglobalcontextatpositioni. α canbedefinedindifferentwaysif\ni i,j\noneconsidersdifferentattentionmodels. Hereweusethescaleddot-productattentionfunction\ntocomputeα ,asfollows\ni,j\nα = Softmax(h hT/β)\ni,j i j\nexp(h hT/β)\ni j\n= (6.18)\n(cid:80)m exp(h hT/β)\nk=1 i k\n√",
    "whereβ isascalingfactorandissetto d.\nCompared with conventional recurrent and convolutional models, an advantage of self-\nattentionmodelsisthattheyshortenthecomputational“distance”betweentwoinputs. Figure\n6.2 illustrates the information flow in these models. We see that, given the input at posi-\ntion i, self-attention models can directly access any other input. By contrast, recurrent and\nconvolutionalmodelsmightneedtwoormorejumpstoseethewholesequence.\nWe can have a more general view of self-attention by using the QKV attention model.\n \nq\n1\n.\nSupposewehaveasequenceofκqueriesQ=  . .   ,andasequenceofψkey-valuepairs(K=\nq\nκ\n   \nk v\n1 1\n. .\n  . .   ,V=  . .   ). Theoutputofthemodelisasequenceofvectors,eachcorrespondingto\nk v\nψ ψ",
    "6.1 The Basic Model 275\naquery. TheformoftheQKVattentionisgivenby\nQKT\nAtt (Q,K,V) = Softmax( √ )V (6.19)\nqkv\nd\nWecanwritetheoutputoftheQKVattentionmodelasasequenceofrowvectors\n \nc\n1\n.\nC =   . .  \nc\nκ\n= Att (Q,K,V) (6.20)\nqkv\nToapplythisequationtoself-attention,wesimplyhave\nHq = HWq (6.21)\nHk = HWk (6.22)\nHv = HWv (6.23)\nwhereWq,Wk,Wv ∈Rd×d representslineartransformationsofH.\nByconsideringEq. (6.1),wethenobtain\nC = Att (H)\nself\n= Att (Hq,Hk,Hv)\nqkv\nHq[Hk]T\n= Softmax( √ )Hv (6.24)\nd\nHq[Hk]T\nHereSoftmax( √ )isanm×mmatrixinwhicheachrowrepresentsadistributionover\nd\n{h ,...,h },thatis\n1 m\n(cid:104) (cid:105)\nrowi = α ... α (6.25)\ni,1 i,m\nWecanimprovetheaboveself-attentionmodelbyusingatechniquecalledmulti-head\nattention. This method can be motivated from the perspective of learning from multiple\nlower-dimensionalfeaturesub-spaces,whichprojectsafeaturevectorontomultiplesub-spaces\nandlearnsfeaturemappingsonindividualsub-spaces. Specifically,weprojectthewholeof",
    "theinputspaceintoτ sub-spaces(callthemheads), forexample,wetransformH∈Rm×d\nintoτ matricesofsizem×d,denotedby{Hhead,...,Hhead}. Theattentionmodelisthenrun\nτ 1 τ\nτ times,eachtimeonahead. Finally,theoutputsofthesemodelrunsareconcatenated,and\ntransformedbyalinearprojection. Thisprocedurecanbeexpressedby\nC = Merge(Chead,...,Chead)W (6.26)\n1 τ c\n(6.27)",
    "276 Chapter 6. Transformers\nForeachheadh,\nHq[Hk]T\nChead = Softmax( h√ h )Hv (6.28)\nh h\nd\nHq = HWq (6.29)\nh h\nHk = HWk (6.30)\nh h\nHv = HWv (6.31)\nh h\nHere Merge(·) is the concatenation function, and Att (·) is the attention function de-\nQKV\nscribed in Eq. (6.20). W h q,W h k,W h v ∈Rd× τ d are the parameters of the projections from a\nd-dimensionalspacetoa d-dimensionalspaceforthequeries,keys,andvalues. Thus,Hq ,Hk,\nτ h h\nHv, andChead areallm×d matrices. Merge(Chead,...,Chead)producesanm×dmatrix.\nh h τ 1 τ\nItisthentransformedbyalinearmappingW ∈Rd×d,leadingtothefinalresultC∈Rd×d.\nc\nWhilethenotationhereseemssomewhattedious,itisconvenienttoimplementmulti-head\nmodelsusingvariousdeeplearningtoolkits. AcommonmethodinTransformer-basedsystems\nistostoreinputsfromalltheheadsindatastructurescalledtensors,sothatwecanmakeuse\nofparallelcomputingresourcestohaveefficientsystems. Amoregeneraldiscussionofthe\nQKVattentionandmulti-headattentionmodelscanbefoundinChapter5.\n6.1.4 Layer Normalization",
    "Layer normalization provides a simple and effective means to make the training of neural\nnetworks more stable by standardizing the activations of the hidden layers in a layer-wise\nmanner. As introduced in Ba et al. [2016]’s work, given a layer’s output h∈Rd, the layer\nnormalizationmethodcomputesastandardizedoutputLNorm(h)∈Rd by\nh−µ\nLNorm(h) = g⊙ +b (6.32)\nσ+ϵ\nHereµ∈Rd andσ∈Rd arethemeanandstandardderivationoftheactivations. Leth bethe\nk\nk-thdimensionofh. µandσ aregivenby\nd\n1 (cid:88)\nµ = · h (6.33)\nk\nd\nk=1\n(cid:118)\n(cid:117) d\n(cid:117)1 (cid:88)\nσ = (cid:116) · (h −µ)2 (6.34)\nk\nd\nk=1\nHereg∈Rd andb∈Rd aretherescalingandbiasterms. Theycanbetreatedasparameters\noflayernormalization,whosevaluesaretobelearnedtogetherwithotherparametersofthe\nTransformermodel. Theadditionofϵtoσ isusedforthepurposeofnumericalstability. In\ngeneral,ϵischosentobeasmallnumber.\nWe illustrate the layer normalization method for the hidden states of an encoder in the",
    "6.1 The Basic Model 277\nfollowingexample(assumethatm=4,d=3,g=1,b=0,andϵ=0.1).\n   \nh 1 1 2 µ=1.3, σ=0.5 1−1.3 1−1.3 2−1.3\n1 0.5+0.1 0.5+0.1 0.5+0.1\nh 0.9 0.9 0 µ=0.6, σ=0.4 0.9−0.6 0.9−0.6 0−0.6 \n2  =⇒ 0.4+0.1 0.4+0.1 0.4+0.1\nh 0.7 0.8 0 µ=0.5, σ=0.4 0.7−0.5 0.8−0.5 0−0.5 \n3  0.4+0.1 0.4+0.1 0.4+0.1\nh 3 1 7 µ=3.7, σ=2.5 3−3.7 1−3.7 7−3.7\n4 2.5+0.1 2.5+0.1 2.5+0.1\nAs discussed in Section 6.1.1, the layer normalization unit in each sub-layer is used to\nstandardizetheoutputofaresidualblock. Herewedescribeamoregeneralformulationfor\nthis structure. Suppose that F(·) is a neural network we want to run. Then, the post-norm\nstructureofF(·)isgivenby\nH = LNorm(F(H )+H ) (6.35)\nout in in\nwhereH andH aretheinputandoutputofthismodel. Clearly,Eq. (6.4)isaninstance\nin output\nofthisequation.\nAnalternativeapproachtointroducinglayernormalizationandresidualconnectionsinto\nmodelingistoexecutetheLNorm(·)functionrightaftertheF(·)function,andtoestablishan",
    "identitymappingfromtheinputtotheoutputoftheentiresub-layer. Thisstructure,knownas\nthepre-normstructure,canbeexpressedintheform\nH = LNorm(F(H ))+H (6.36)\nout in in\nBothpost-normandpre-normTransformermodelsarewidelyusedinNLPsystems. See\nFigure 6.3 for a comparison of these two structures. In general, residual connections are\nconsideredaneffectivemeanstomakethetrainingofmulti-layerneuralnetworkseasier. In\nthissense,pre-normTransformerseemspromisingbecauseitfollowstheconventionthata\nresidualconnectioniscreatedtobypassthewholenetworkandthattheidentitymappingfrom\ntheinputtotheoutputleadstoeasieroptimizationofdeepmodels. However,byconsidering\nthe expressive power of a model, there may be modeling advantages in using post-norm\nTransformer because it does not so much rely on residual connections and enforces more\nsophisticatedmodelingforrepresentationlearning. InSection6.3.2,wewillseeadiscussion\nonthisissue.\n6.1.5 Feed-forward Neural Networks",
    "The use of FFNs in Transformer is inspired in part by the fact that complex outputs can be\nformed by transforming the inputs through nonlinearities. While the self-attention model\nitself has some nonlinearity (in Softmax(·)), a more common way to do this is to consider\nadditionallayerswithnon-linearactivationfunctionsandlineartransformations. Givenaninput\nH ∈Rm×d and an output H ∈Rm×d, the H =FFN(H ) function in Transformer\nin out out in",
    "278 Chapter 6. Transformers\n.\n.\n.\nH F(·) LNorm(·) H\nin out\nAdd&LayerNorm\n(a)Post-norm\nCoreFunction\nF(·)\nH LNorm(·) F(·) H\nin out\n. . (b)Pre-norm\n.\nFigure6.3: Thepost-normandpre-normstructures. F(·)=corefunction,LNorm(·)=layer\nnormalization,and⊕=residualconnection.\nhasthefollowingform\nH = H W +b (6.37)\nout hidden f f\nH = ReLU(H W +b ) (6.38)\nhidden in h h\nwhereH hidden ∈Rm×d ffn isthehiddenstates,andW h ∈Rd×d ffn,b h ∈Rd ffn,W f ∈Rd ffn ×d\nandb ∈Rd aretheparameters. Thisisatwo-layerFFNinwhichthefirstlayer(orhidden\nf\nlayer)introducesanonlinearitythroughReLU(·)2 andthesecondlayerinvolvesonlyalinear\ntransformation. ItiscommonpracticeinTransformertousealargersizeofthehiddenlayer.\nForexample,acommonchoiceisd =4d,thatis,thesizeofeachhiddenrepresentationis4\nffn\ntimesaslargeastheinput.\nNote that using a wide FFN sub-layer has been proven to be of great practical value in\nmanystate-of-the-artsystems. However,aconsequenceofthisisthatthemodelisoccupied",
    "bytheparametersoftheFFN.Table6.1showsparameternumbersandtimecomplexitiesfor\ndifferentmodulesofastandardTransformersystem. WeseethatFFNsdominatethemodel\nsizewhend islarge,thoughtheyarenotthemosttimeconsumingcomponents. Inthecase\nffn\nofverybigTransformmodels,wethereforewishtoaddressthisproblemforbuildingefficient\nsystems.\n6.1.6 Attention Models on the Decoder Side\nA decoder layer involves two attention sub-layers, the first of which is a self-attention sub-\nlayer,andthesecondisacross-attentionsub-layer. Thesesub-layersarebasedoneitherthe\npost-normorthepre-normstructure,butdifferbydesignsoftheattentionfunctions. Consider,\nforexample,thepost-normstructure,describedinEq. (6.35). Wecandefinethecross-attention\n2ReLU(x)=max{0,x}.",
    "6.1 The Basic Model 279\nSub-model #ofParameters TimeComplexity ×\nMulti-headSelf-attention 4d2 O(m2·d) L\nEncoder Feed-forwardNetwork 2d·d +d+d O(m·d·d ) L\nffn ffn ffn\nLayerNormalization 2d O(d) 2L\nMulti-headSelf-attention 4d2 O(n2·d) L\nMulti-headCross-attention 4d2 O(m·n·d) L\nDecoder\nFeed-forwardNetwork 2d·d +d+d O(n·d·d ) L\nffn ffn ffn\nLayerNormalization 2d O(d) 3L\nTable6.1: NumbersofparametersandtimecomplexitiesofdifferentTransformermodules\nunderdifferentsetups. m=source-sequencelength,n=target-sequencelength,d=default\nnumber of dimensions of a hidden layer, d = number of dimensions of the FFN hidden\nffn\nlayer,τ =numberofheadsintheattentionmodels,andL=numberofencodingordecoding\nlayers. Thecolumn×meansthenumberoftimesasub-modelisappliedontheencoderor\ndecoderside. Thetimecomplexitiesareestimatedbycountingthenumberofmultiplicationof\nfloating-pointnumbers.\nandself-attentionsub-layersforadecodinglayertobe\nS = Layer (H ,S )\ncross cross enc self\n= LNorm(Att (H ,S )+S ) (6.39)",
    "cross enc self self\nS = Layer (S)\nself self\n= LNorm(Att (S)+S) (6.40)\nself\nwhereS∈Rn×d istheinputoftheself-attentionsub-layer,S ∈Rn×d andS ∈Rn×d\ncross self\naretheoutputsofthesub-layers,andH ∈Rm×d istheoutputoftheencoder3.\nenc\nAs with conventional attention models, cross-attention is primarily used to model the\ncorrespondencebetweenthesource-sideandtarget-sidesequences. TheAtt (·)function\ncross\nisbasedontheQKVattentionmodelwhichgeneratestheresultofqueryingacollectionof\nkey-valuepairs. Morespecifically,wedefinethequeries,keysandvaluesaslinearmappings\nofS andH ,asfollows\nself enc\nSq = S Wq (6.41)\nself self cross\nHk = H Wk (6.42)\nenc enc enc\nHv = H Wv (6.43)\nenc enc enc\nwhereWq ,Wk ,Wv ∈Rd×d aretheparametersofthemappings. Inotherwords,the\ncross enc enc\nqueriesaredefinedbasedonS ,andthekeysandvaluesaredefinedbasedonH .\nself enc\n3ForanencoderhavingLencoderlayers,Henc=HL.",
    "280 Chapter 6. Transformers\ni−3 i−2 i−1 i i+1 i−3 i−2 i−1 i i+1\n(a)Encoder-sideSelf-attention (b)Decoder-sideSelf-attention\nFigure6.4: Self-attentionontheencoderanddecodersides. Eachlineconnectsaninputand\nanoutputoftheself-attentionmodel,indicatingadependencyofanoutputstateonaninput\nstate. Forencoderself-attention,theoutputatanypositioniscomputedbyhavingaccesstothe\nentiresequence. Bycontrast,fordecoderself-attention,theoutputatpositioniiscomputedby\nseeingonlyinputsatpositionsuptoi.\nAtt (·)isthendefinedas\ncross\nAtt (H ,S ) = Att (Sq ,Hk ,Hv )\ncross enc self qkv self enc enc\nSq [Hk ]T\n= Softmax( self√ enc )Hv (6.44)\nenc\nd\nTheAtt (·)functionhasasimilarformasAtt (·),withlinearmappingsofStaken\nself cross\nasthequeries,keys,andvalues,likethis\nAtt (S) = Att (Sq,Sk,Sv)\nself qkv\nSq[Sk]T\n= Softmax( √ +M)Sv (6.45)\nd\nwhereSq=SWq ,Sk=SWk ,andSv=SWv arelinearmappingsofSwithparameters\ndec dec dec\nWq ,Wk ,Wv ∈Rd×d.\ndec dec dec",
    "This form is similar to that of Eq. (6.20). A difference compared to self-attention on\nthe encoder side, however, is that the model here needs to follow the rule of left-to-right\ngeneration(seeFigure6.4). Thatis,givenatarget-sidewordatthepositioni,wecanseeonly\nthetarget-sidewordsintheleftcontexty ...y . Todothis,weaddamaskingvariableM\n1 i−1\nSq[Sk]T Sq[Sk]T\ntotheunnormalizedweightmatrix √ +M. BothMand √ +Mareofsizen×n,\nd d\nandsoalowervalueofanentryofMmeansalargerbiastowardsloweralignmentscoresfor\nSq[Sk]T\nthecorrespondingentryof √ +M. Inordertoavoidaccesstotherightcontextgiveni,\nd\nMisdefinedtobe\n(cid:40)\n0 i≤k\nM(i,k) = (6.46)\n−∞ i>k\nwhereM(i,k)indicatesabiastermforthealignmentscorebetweenpositionsiandk. Below",
    "6.1 The Basic Model 281\nweshowanexampleofhowthemaskingvariableisapplied(assumen=4).\nSq[Sk]T\nSoftmax( √ +M)\nd\n   \n2 0.1 1 1 0 −∞ −∞ −∞\n 0 0.9 0.9 0.9 0 0 −∞ −∞\n= Softmax( + )\n0.2 0.8 0.7 2  0 0 0 −∞\n   \n0.3 1 0.3 3 0 0 0 0\n \n2 −∞ −∞ −∞\n 0 0.9 −∞ −∞\n= Softmax( )\n0.2 0.8 0.7 −∞\n \n0.3 1 0.3 3\n \n1 0 0 0\n0.3 0.7 0 0 \n=   (6.47)\n0.2 0.4 0.4 0 \n \n0.05 0.1 0.05 0.8\nAs noted in Section 6.1.3, it is easy to improve these models by using the multi-head\nattention mechanism. Also, since decoders are typically the most time-consuming part of\npracticalsystems,thebulkofthecomputationaleffortinrunningthesesystemsisverymuch\nconcernedwiththeefficiencyoftheattentionmodulesonthedecoderside.\n6.1.7 Training and Inference\nTransformerscanbetrainedandusedinaregularway. Forexample,wecantrainaTransformer\nmodel by performing gradient descent to minimize some loss function on the training data\n(seeChapter2),andtestthetrainedmodelbyperformingbeamsearchontheunseendata(see",
    "Chapter5). Belowwepresentsomeofthetechniquesthataretypicallyusedinthetraining\nandinferenceofTransformermodels.\n• LearningRateScheduling. Asstandardneuralnetworks,Transformerscanbedirectly\ntrainedusingback-propagation. Thetrainingprocessisgenerallyiteratedmanytimesto\nmakethemodelsfitthetrainingdatawell. Ineachtrainingstep,weupdatetheweightsof\ntheneuralnetworksbymovingthemasmallstepinthedirectionofnegativegradientsof\nerrors. Therearemanywaystodesigntheupdateruleoftraining. Apopularchoiceisto\nusetheAdamoptimizationmethod[KingmaandBa,2014]. Toadjustthelearningrate\nduringtraining,Vaswanietal.[2017]presentalearningrateschedulingstrategywhich\nincreases the learning rate linearly for a number of steps and then decay it gradually.\nTheydesignalearningrateoftheform\nlr = lr ·min (cid:8) n−0.5, n ·(n )−1.5(cid:9) (6.48)\n0 step step warmup\nwhere lr denotes the initial learning rate, and n denotes the number of training\n0 step\nstepswehaveexecuted,andn denotesthenumberofwarmupsteps. Inthefirst\nwarmup",
    "282 Chapter 6. Transformers\nn steps, the learning rate lr grows larger as training proceeds. It reaches the\nwarmup\nhighestvalueatthepointofn =n ,andthendecreasesasaninversesquare\nstep warmup\nrootfunction(i.e.,lr ·n−0.5).\n0 step\n• BatchingandPadding. Tomakeatrade-offbetweenglobaloptimizationandtraining\nconvergency,itiscommontoupdatetheweightseachtimeonarelativelysmallcollection\nofsamples,calledaminibatchofsamples. Therefore,wecanconsiderabatchversion\nofforwardandbackwardcomputationprocessesinwhichthewholeminibatchisused\ntogethertoobtainthegradientinformation. Oneadvantageofbatchingisthatitallows\nthesystemtomakeuseofefficienttensoroperationstodealwithmultiplesequences\ninasinglerun. Thisrequiresthatalltheinputsequencesinaminibatcharestoredina\nsinglememoryblock,sothattheycanbereadinandprocessedtogether. Toillustrate\nthisidea,consideraminimatchcontainingfoursampleswhosesource-sidesare\nA B C D E F\nM N\nR S T\nW X Y Z\nWecanstorethesesequencesina4×6continuousblockwhereeach“row”representsa",
    "sequence,likethis\nA B C D E F\nM N □ □ □ □\nR S T □ □ □\nW X Y Z □ □\nHere padding words □ are inserted between sequences, so that these sequences are\naligned in the memory. Typically, we do not want padding to affect the operation of\nthesystem,andsowecansimplydefine□asazerovector(callitzeropadding). On\ntheotherhand,insomecasesweareinterestedinusingpaddingtodescribesomething\nthatisnotcoveredbytheinputsequences. Forexample,wecanreplacepaddingwords\nwith the words in the left (or right) context of a sequence, though this may require\nmodificationstothesystemtoensurethatthenewlyaddedcontextwordsdonotcause\nadditionalcontenttoappearintheoutput.\n• SearchandCaching. Attesttime,weneedtosearchthespaceofcandidatehypotheses\n(orcandidatetarget-sidesequences)toidentifythehypothesis(ortarget-sidesequence)\nwiththehighestscore.\nyˆ = argmax score(x,y) (6.49)\ny\nwherescore(x,y)isthemodelscoreofthetarget-sidesequenceygiventhesource-side",
    "sequence x. While there are many search algorithms to achieve this, most of them\nshareasimilarstructure: thesearchprogramoperatesbyextendingcandidatetarget-side",
    "6.2 Syntax-aware Models 283\ny y y y y y\n1 1 2 1 2 3\nCache Cache\n... ... ... ... ... ...\n... ... ... ... ... ...\ny y y y y y\n0 0 1 0 1 2\n(a)decodingstep1 (b)decodingstep2 (c)decodingstep3\nFigure6.5: IllustrationofthecachingmechanisminTransformerdecoders. Rectanglesindicate\nthestatesofdecodinglayersorsub-layers. Atstepi,allthestatesatpreviousstepsarestored\ninacache(seedottedboxes),andweonlyneedtocomputethestatesforthisstep(seeblue\nrectanglesandarrows). Then,weaddthenewlygeneratedstatestothecache,andmoveonto\nstepi+1.\nsequencesinapoolatatime. Inthisway,theresultingalgorithmcanbeviewedasa\nleft-to-rightgenerationprocedure. Foramoredetaileddiscussionofsearchalgorithms\nandmodelscoresofgeneralsequence-to-sequencemodels,seeChapter5. Notethatall\nofthedesignsofscore(x,y),nomatterhowcomplex,arebasedoncomputingPr(y|x).\nBecausetheattentionmodelsusedinTransformerrequirecomputingthedot-productof\neachpairoftheinputvectorsofalayer,thetimecomplexityofthesearchalgorithmisa",
    "quadraticfunctionofthelengthofy. Itisthereforenotefficienttorepeatedlycompute\ntheoutputsoftheattentionmodelsforpositionsthathavebeendealtwith. Thisproblem\ncan be addressed by caching the states of each layer for words we have seen. Figure\n6.5 illustrates the use of the caching mechanism in a search step. All the states for\npositions<iaremaintainedandeasilyaccessedinacache. Atpositioni,allweneedis\ntocomputethestatesforthenewlyaddedword,andthentoupdatethecache.\n6.2 Syntax-aware Models\nAlthoughTransformerissimplyadeeplearningmodelthatdoesnotmakeuseofanylinguistic\nstructure or assumption, it may be necessary to incorporate our prior knowledge into such\nsystems. This is in part because NLP researchers have long believed that a higher level of\nabstractionofdataisneededtodevelopidealNLPsystems,andtherehavebeenmanysystems\nthatusestructureaspriors. However,structureisawide-rangingtopicandthereareseveral\ntypesofstructureonemayrefertoSee[2018]’swork. Forexample,theinductivebiasesused",
    "inourmodeldesigncanbethoughtofassomestructuralprior,whileNLPmodelscanalso\nlearntheunderlyingstructureofproblemsbythemselves. Inthissubsectionwewilldiscuss",
    "284 Chapter 6. Transformers\nsome of these issues. We will focus on the methods of introducing linguistic structure into\nTransformermodels. AsTransformercanbeappliedtomanyNLPtasks,whichdiffermuchin\ntheirinputandoutputformats,wewillprimarilydiscussmodificationstoTransformerencoders\n(callthemsyntax-awareTransformerencoders). Ourdiscussion,however,isgeneral,and\nthemethodscanbeeasilyextendedtoTransformerdecoders.\n6.2.1 Syntax-aware Input and Output\nOne of the simplest methods of incorporating structure into NLP systems is to modify the\ninput sequence, leaving the system unchanged. As a simple example, consider a sentence\nwhereeachwordx isassignedasetofκsyntacticlabels{tag1,...,tagκ}(e.g.,POSlabels\nj j j\nanddependencylabels). Wecanwritethesesymbolstogethertodefineanew“word”\nx /tag1/.../tagκ\nj j j\nThen,theembeddingofthiswordisgivenby\nxp = e(x /tag1/.../tagκ)+PE(j) (6.50)\nj j j j\nwheree(x /tag1/.../tagκ)∈Rdistheembeddingofx /tag1/.../tagκ. Sincex /tag1/.../tagκ\nj j j j j j j j j",
    "isacomplexsymbol,wedecomposethelearningproblemofe(x /tag1/.../tagκ)intoeasier\nj j j\nproblems. Forexample,wecandevelopκembeddingmodels,eachproducinganembedding\ngiven a tag. Then, we write e(x /tag1/.../tagκ) as a sum of the word embedding and tag\nj j j\nembeddings\ne(x /tag1/.../tagκ) = x +e(tag1)+...+e(tagκ) (6.51)\nj j j j j j\nwhere{e(tag1),...,e(tagκ)}aretheembeddingsofthetags. Alternatively,wecancombine\nj j\ntheseembeddingsviaaneuralnetworkintheform\ne(x /tag1/.../tagκ) = FFN (x ,e(tag1),...,e(tagκ)) (6.52)\nj j j embed j j j\nwhereFFN (·)isafeed-forwardneuralnetworkthathasonelayerortwo.\nembed\nWecandothesamethingforsentencesonthedecodersideaswell,andtreaty /tag1/.../tagκ\ni i i\nasasyntax-augmentedword. However,thismayleadtoamuchlargertarget-sidevocabulary\nandposesacomputationalchallengefortrainingandinference.\nAnotherformthatiscommonlyusedtorepresentasentenceissyntaxtree. Inlinguistics,the\nsyntaxofasentencecanbeinterpretedinmanydifferentways,resultinginvariousgrammars",
    "and the corresponding tree (or graph)-based representations. While these representations\ndifferintheirsyntacticforms,ageneralapproachtousetheminsequencemodelingistree\nlinearization. Considerthefollowingsentenceannotatedwithaconstituency-basedparsetree",
    "6.2 Syntax-aware Models 285\nS\nNP VP .\nPRP VBZ ADJP !\nIt ’s JJ\ninteresting\nWecanwritethistreestructureasasequenceofwords,syntacticlabelsandbracketsviaatree\ntraversalalgorithm,asfollows\n(S (NP (PRP It ) ) (VP (VBZ ’s ) (ADJP (JJ\nPRP NP VBZ\ninteresting ) ) ) (. ! ) )\nJJ ADJP VP . S\nThissequenceofsyntactictokenscanbeusedasaninputtothesystem,thatis,eachtoken\nisrepresentedbywordandpositionalembeddings,andthenthesumoftheseembeddingsis\ntreatedasaregularinputoftheencoder. Anexampleoftheuseoflinearizedtreesistree-to-\nstringmachinetranslationinwhichasyntaxtreeinonelanguageistranslatedintoastringin\nanotherlanguage[Lietal.,2017b;CurreyandHeafield,2018]. Linearizedtreescanalsobe\nusedfortreegeneration. Forexample,wecanframeparsingtasksassequence-to-sequence\nproblemstomapaninputtexttoasequentialrepresentationofitscorrespondingsyntaxtree\n[Vinyals et al., 2015; Choe and Charniak, 2016]. See Figure 6.6 for illustrations of these",
    "models. ItshouldbenotedthatthemethodsdescribedherearenotspecifictoTransformerbut\ncouldbeappliedtomanymodels,suchasRNN-basedmodels.\n6.2.2 Syntax-aware Attention Models\nForTransformermodels,italsomakessensetomakeuseofsyntaxtreestoguidetheprocess\noflearningsequencerepresentations. Intheprevioussectionwesawhowrepresentationsofa\nsequencecanbecomputedbyrelatingdifferentpositionswithinthatsequence. Thisallowsus\ntoimposesomestructureontheserelationswhicharerepresentedbydistributionsofattention\nweightsoverallthepositions. Todothisweusetheencoderself-attentionwithanadditive\nmask\nHq[Hk]T\nAttSyn (H) = Softmax( √ +M)Hv (6.53)\nself\nd\noralternativelywithamultiplicativemask\nHq[Hk]T\nAttSyn (H) = Softmax( √ ⊙M)Hv (6.54)\nself\nd\nwhereM∈Rm×misamatrixofmaskingvariablesinwhichalargervalueofM(i,j)indicates",
    "286 Chapter 6. Transformers\n很 好 ！\nEncoder Decoder\n(ADJP (JJ Great ) JJ (. ! ). ) ADJP ⟨SOS⟩ 很 好\n(a)Tree-to-StringMachineTranslation\n(ADJP (JJ Great ) JJ (. ! ). ) ADJP\nEncoder Decoder\nGreat ! ⟨SOS⟩ (ADJP (JJ Great ) JJ (. ! ).\n(b)ConstituencyParsing\nFigure 6.6: Illustration of tree linearization on either the encoder or decoder side. For tree-\nto-string machine translation, the encoder takes sequential representation of an input parse\ntree,andthedecoderoutputsthecorrespondingtranslation. Forparsing,theencodertakesa\nsentence,andthedecoderoutputsthecorrespondingsyntaxtree.\na stronger syntactic correlation between positions i and j. In the following description we\nchooseEq. (6.54)asthebasicform.\nOnecommonwaytodesignMistoprojectsyntacticrelationsoftheinputtreestructure\nintoconstraintsoverthesequence. Hereweconsiderconstituencyparsetreesanddependency\nparsetreesforillustration. Generally,twotypesofmaskingmethodsareemployed.",
    "• 0-1Masking. ThismethodassignsM(i,j)avalueof1ifthewordsatpositionsiandj\nareconsideredsyntacticallycorrelatedandavalueof0otherwise[Zhangetal.,2020c;\nBai et al., 2021]. To model the relation between two words in a syntax tree, we can\nconsiderthedistancebetweentheircorrespondingnodes. Oneofthesimplestformsis\ngivenby\n(cid:40)\n1 ω(i,j)≤ω\nmax\nM(i,j) = (6.55)\n0 otherwise\nwhere ω(i,j) is the length of the shortest path between the nodes of the words at\npositions i and j. For example, given a dependency parse tree, ω(i,j) is the number",
    "6.2 Syntax-aware Models 287\nof dependency edges in the path between the two words. For a constituency parse\ntree,allthewordsareleafnodes,andsoω(i,j)givesatreedistancebetweenthetwo\nleavesinthesamebranchofthetree. ω isaparameterusedtocontrolthemaximum\nmax\ndistancebetweentwonodesthatcanbeconsideredsyntacticallycorrelated. Forexample,\nassuming that there is a dependency parse tree and ω =1, Eq. (6.55) enforces a\nmax\nconstraint that the attention score between positions i and j is computed only if they\nhaveaparent-dependentrelation4.\n• Soft Masking. Instead of treating M as a hard constraint, we can use it as a soft\nconstraint that scales the attention weight between positions i and j in terms of the\ndegreetowhichthecorrespondingwordsarecorrelated. Anideaistoreducetheattention\nweightasω(i,j)becomeslarger. Averysimplemethodtodothisistotransformω(i,j)\ninsomewaythatM(i,j)holdsanegativecorrelationrelationshipwithω(i,j)andits\nvaluefallsintotheinterval[0,1]\nM(i,j) = DNorm(ω(i,j)) (6.56)",
    "There are several alternative designs for DNorm(·). For example, one can compute\na standardized score of −ω(i,j) by subtracting its mean and dividing by its standard\ndeviation [Chen et al., 2018a], or can normalize 1/ω(i,j) over all possible j in the\nsequence[Xuetal.,2021b]. Incaseswhereparserscanoutputascorebetweenpositionsi\nandj,itisalsopossibletousethisscoretocomputeM(i,j). Forexample,adependency\nparsercanproducetheprobabilityofthewordatpositionibeingtheparentoftheword\natpositionj [Strubelletal.,2018]. WecanthenwriteM(i,j)as\nM(i,j) = Pr (i|j) (6.57)\nparent\noralternatively\nM(i,j) = max{Pr (i|j),Pr (j|i)} (6.58)\nparent parent\nwhere Pr (i|j) and Pr (j|i) are the probabilities given by the parser. See\nparent parent\nFigure6.7foranexampleofinducingasoftmaskingvariablefromadependencyparse\ntree.\n6.2.3 Multi-branch Models\nIntroducing syntax into NLP systems is not easy. This is partially because automatic parse\ntreesmayhaveerrors,andpartiallybecausetheuseofsyntaxmayleadtostrongassumptionof",
    "theunderlyingstructureofasentence. Ratherthancombiningsyntacticandwordinformation\n4For multiplicative masks, M(i,j)=0 does not mean that the attention weight between j and i is zero\nbecausetheSoftmaxfunctiondoesnotgiveazerooutputforadimensionwhosecorrespondinginputisofazero\nvalue.\nAmethodto“mask”anentryofSoftmax(H√HT\n)istouseanadditivemaskandsetM(i,j)=−∞if\nd\nω(i,j)>ωmax.",
    "288 Chapter 6. Transformers\nul\nT h e c o n c\ner\nw\nt\na s w o n d\nerf\n!\nThe\nconcert\nwas\nThe concert was wonderful !\nwonderful\n!\n(a)DependencyParseTree (b)MaskM(darkercolormeanslargervalue)\nFigure6.7: Priorsinducedfromadependencyparsetree. TherowiofthematrixMrepresents\nadistributionthatdescribeshowmuchweightwecangivetoM(i,j)intermsofthesyntactic\ndistancebetweeniandj.\nintoone“big”model,itmaybemoreflexibleandeffectivetobuildonemodeltoencodesyntax\nandadifferentonetoencodewordsequences. Onewaytoachievethisisthroughtheuseof\nmultipleneuralnetworks(calledbranchesorpaths),eachdealingwithonetypeofinput. The\noutputsofthesebranchesarethencombinedtoproduceanoutput[Xieetal.,2017;Fanetal.,\n2020;Linetal.,2022b]. Variousmethodshavethereforebeenusedtocombinedifferenttypes\nofinputforneuralmodelslikeTransformer.\nOnecommonly-usedapproachistobuildtwoseparateencoders,inwhichonemodelis\ntrainedtoencodethesyntacticinput(denotedbyt),andtheotheristrainedtoencodetheusual",
    "input(denotedbyx). Figure6.8(a)illustratesthismulti-encoderarchitecture. Thesyntactic\nencoderEncode (t)isbasedonmodelspresentedinSections6.2.1and6.2.2,andthetext\nsyn\nencoderEncode (x)isastandardTransformerencoder. Therepresentationsgeneratedby\ntext\ntheseencodersarethenfedintothecombinationmodelasinput,andcombinedintoahybrid\nrepresentation,givenby\nH = Combine(H ,H )\nhybrid syn text\n= Combine(Encode (t),Encode (x)) (6.59)\nsyn text\nThereareseveraldesignsforCombine(·),dependingonwhatkindofproblemsweapply\nthe encoders to. For example, if we want to develop a text classifier, Combine(·) can be a\nsimplepoolingnetwork. Formorecomplicatedtasks,suchasmachinetranslation,Combine(·)\ncanbeaTransformerencoderaswell,andwecanfuseinformationfromdifferentsourcesby\nperformingself-attentionon[H ,H ].\nsyn text\nWhilewerestrictattentiontosyntacticmodelsinthissection,thegeneralmulti-encoder\narchitecturecanbeusedinmanyproblemswhereinputsfromadditionalsourcesarerequired.",
    "For example, one can use one encoder to represent a sentence, and use another encoder to",
    "6.2 Syntax-aware Models 289\n)·(\nedocnE\nnys\n)·(\nedocnE\ntxet\n... ...\nCombine(·)\nt x x\n(a)Multi-encoder\n1hcnarB 2hcnarB\n...\nt\nt Head\n1\nx\n(b)Multi-branchasaSub-model (c)Multi-headAttention\nFigure6.8: Multi-brancharchitectures. Therearetwoinputs: asentence(denotedbyx)and\nthesyntaxtreeofthesentence(denotedbyt). Inthemulti-encoderarchitecture(seesub-figure\n(a)), two encoders are constructed to encode x and t, respectively. A combination model\nthen takes the outputs of the encoders and produces a combined representation of x and t.\nTheideaofmulti-branchnetworkscanbeusedfordesigningsub-modelsoftheencoder. A\nsimpleexampleisthatwecreatemultiplepathsinparallelforsomelayersoftheencoder(see\nsub-figure (b)). Another example is multi-head attention (see sub-figure (c)) where we use\ndifferentheadstolearndifferentrepresentations.\nrepresenttheprevioussentenceinthesamedocument. Wethushaveacontext-awaremodelby\ncombiningthetwoencoders[Voitaetal.,2018;Lietal.,2020a]. Furthermore,thearchitectures",
    "oftheencodersdonotneedtoberestrictedtoTransformer,andwecanchoosedifferentmodels\nfordifferentbranches. Forexample,asawidely-used2-branchencodingarchitecture,wecan\nuseaCNN-basedencodertomodellocalcontext,andaTransformerencodertomodelglobal\ncontext[Wuetal.,2020b].\nSub-modelsofaTransformermodelcanalsobemulti-branchneuralnetworks. SeeFigure\n6.8(b)foranexampleinvolvingtwoself-attentionbranches. Oneisthestandardself-attention\nnetworkAtt (H). Theotheristhesyntax-awareself-attentionnetworkAttSyn (H). The\nself self\noutputoftheself-attentionmodelisalinearcombinationoftheoutputsofthesetwobranches\n[Xuetal.,2021b],givenby\nH = α·Att (H)+(1−α)·AttSyn (H) (6.60)\nself self self\nwhereαisacoefficientofcombination. H canbeusedasusualbytakingalayernormal-\nself\nizationfunctionandaddingaresidualconnection,andsotheoverallarchitectureisthesame\nasstandardTransformermodels.\nMulti-headattentionnetworkscanalsobeviewedasformsofmulti-branchmodels. There-",
    "fore,wecanprovideguidancefromsyntaxtoonlysomeoftheheadswhilekeepingtherest\nunchanged[Strubelletal.,2018]. ThisapproachisillustratedinFigure6.8(c)whereonlyone",
    "290 Chapter 6. Transformers\nheadoftheself-attentionsub-layermakesuseofsyntaxtreesforcomputingattentionweights.\n6.2.4 Multi-scale Models\nIn linguistics, syntax studies how sentences are built up by smaller constituents. Different\nlevelsoftheseconstituentsareingeneralorganizedinahierarchicalstructure,calledsyntactic\nhierarchy. Itisthereforepossibletousemultiplelevelsofsyntacticconstituentstoexplain\nthe same sentence, for example, words explain how the sentence is constructed from small\nmeaningfulunits,andphrasesexplainhowthesentenceisconstructedfromlargerlinguistic\nunits.\nMulti-scaleTransformersleveragevaryingabstractionlevelsofdatatorepresentasentence\nusingdiversefeaturescales. Acommonapproachistowriteasentenceinmultipledifferent\nformsandthentocombinethemusingamulti-branchnetwork[Haoetal.,2019]. Forexample,\nconsiderasentence\nTheoldestbeer-makingfacilitywasdiscoveredinChina.\nWecantokenizeitintoasequenceofwords,denotedby\nx =Theoldestbeer-makingfacilitywasdiscoveredinChina.\nwords",
    "Alternatively,wecanwriteitasasequenceofphrasesbyusingaparser,denotedby\nx =[Theoldestbeer-makingfacility] [wasdiscoveredinChina] [.]\nphrases NP VP .\nThesimplestwaytobuildamulti-scalemodelistoencodex andx usingtwo\nwords phrases\nseparateTransformerencoders. Then,theoutputsoftheseencodersarecombinedinsomeway.\nThisleadstothesameformasEq. (6.59),andwecanviewthismodelasaninstanceofthe\ngeneralmulti-encoderarchitecture.\nBothx andx canbeviewedassequencesoftokens,forexample,x has\nwords phrases words\nnine word-based tokens, and x has three phrase-based tokens5. However, involving\nphrases\nall possible phrases will result in a huge vocabulary. We therefore need some method to\nrepresent eachphrase asan embeddingin a cheapway. By treatingphrase embeddingas a\nsequencemodelingproblem,itisstraightforwardtolearnsub-sequencerepresentationssimply\nbyconsideringthesequencemodelsdescribedinthepreviouschaptersandthischapter. Now\nwehaveatwo-stagelearningprocess. Inthefirststage,welearntheembeddingsofinputunits",
    "ondifferentscalesusingseparatemodels. Inthesecondstage,welearntoencodesequences\nondifferentscalesusingamulti-branchmodel.\nMoregenerally,wedonotneedtorestrictourselvestolinguisticallymeaningfulunitsin\nmulti-scalerepresentationlearning. Forexample,wecanlearnsub-wordsegmentationsfrom\ndataandrepresentaninputsentenceasasequenceofsub-words. Thisresultsinahierarchical\n5x comprisesthreetokensTheoldestbeer-makingfacility,wasdiscoveredinChina,and..\nphrases",
    "6.2 Syntax-aware Models 291\nrepresentationofthesentence,forexample,sub-words→words→phrases. Whilethelearned\nsub-wordsmaynothavelinguisticmeanings,theyprovideanewinsightintomodelingwords\nand phrases, as well as a new scale of features. Also, we do not need to develop multiple\nencoders for multi-scale modeling. An alternative approach is to take representations on\ndifferentscalesinthemulti-headself-attentionattentionmodules, whichmakesiteasierto\nmodeltheinteractionsamongdifferentscales[Guoetal.,2020;Lietal.,2022b].\nAproblemwiththeapproachesdescribedabove,however,isthattherepresentations(or\nattention weight matrices) learned on different scales are of different sizes. For example,\nin the above examples, the representation learned from x is a 9×d matrix, and the\nwords\nrepresentation learned from x is a 3×d matrix. A simple solution to this problem\nphrases\nis to perform upsampling on the phrase-based representation to expand it to a 9×d matrix.",
    "Likewise,wecanperformdownsamplingontheword-basedrepresentationtoshrinkittoa\n3×dmatrix. Then,thecombinationmodelCombine(·)canbethesameasthosedescribedin\nSection6.2.3.\nIt is worth noting that multi-scale modeling is widely discussed in several fields. For\nexample,incomputervision,multi-scalemodelingisoftenreferredtoasaprocessoflearning\na series of feature maps on the input image [Fan et al., 2021; Li et al., 2022f]. Unlike the\nmulti-branchmodelspresentedhere,themulti-scalevisionTransformermodelsmakeuseof\nthehierarchicalnatureoffeaturesinrepresentingimages. Systemsofthiskindareoftenbased\non a stack of layers in which each layer learns the features on a larger scale (e.g., a higher\nchannelcapacity)fromthefeaturesproducedbythepreviouslayer.\n6.2.5 Transformers as Syntax Learners\nSofarwehavediscussedsyntaxtreesasbeingconstraintsorpriorsontheencodingprocessso\nthatwecanmakeuseoflinguisticrepresentationsinlearningneuralnetworks. Itisnaturalto",
    "wonderwhethertheseneuralmodelscanlearnsomeknowledgeoflinguisticstructurefrom\ndatawithouthumandesignlinguisticannotations. Thisreflectsoneofthegoalsofdeveloping\nNLPsystems: linguisticknowledgecanbelearnedfromdataandencodedinmodels.\nInordertoexplorethelinguisticpropertieslearnedbyNLPsystems,asimplemethodisto\nexaminethesyntacticbehaviorsoftheoutputsofthesystems. Forexample,wecanexamine\nwhethertheoutputsoflanguagegenerationsystemshavegrammaticalerrors. Anotherexample\nistoaskthesesystemstoaccomplishtasksthatmakesenseforlinguistics, thoughtheyare\nnot trained to do so [Brown et al., 2020]. However, examining and explaining how model\npredictionsexhibitsyntacticabilitiesisnotsufficienttoanswerthequestion. Itisalsothecase\nthattheneuralnetworkshavelearnedsomeknowledgeaboutlanguage,butitisnotusedin\nprediction[Clarketal.,2019a]. Therefore,weneedtoseewhatismodeledandlearnedinside\ntheseneuralnetworks.\nOne approach to examining the latent linguistic structure in Transformer models is to",
    "developprobestoseewhetherandtowhatextentthesemodelscapturenotionsoflinguistics,\nsuch as dependency relations and parts-of-speech. A general approach to probing is to\nextracttheinternalrepresentationsofthemodelsandprobethemforlinguisticphenomena.\nForTransformer, itisusuallyachievedbyexaminingtheattentionmapand/oroutputofan",
    "292 Chapter 6. Transformers\nLoss(ProbingTraining) Output(Probing)\nPredictor Predictor\nLoss(Pre-training)\n... ... ...\n... ... ...\nInput(Pre-training) Input(ProbingTraining) Input(Probing)\n(a)TrainingtheTransformerModel (b)TrainingtheProbingPredictor (c)ProbingonNewData\nFigure 6.9: An overview of probing for Transformer-based models. Given a Transformer\nmodel(e.g.,aTransformer-basedlanguagemodel),wefirstoptimizethemodelparameterson\nsomeunlabeleddata. Then,wedevelopapredictorwhichtakesthestatesofahiddenlayer\noftheTransformermodelandgeneratesoutputsforaprobingtask(seesub-figure(a)). The\npredictorcanbetrainedasusualinwhichonlytheparametersofthepredictorareoptimized\nandtheparametersoftheTransformermodelarefixed(seesub-figure(b)). TheTransformer\nmodelandthepredictorareusedtogethertomakepredictionsonnewdataforprobing(see\nsub-figure(c)).\nattention layer. Then, we construct a probing predictor (or probing classifier) that takes",
    "these internal representations as input and produces linguistic notions as output [Belinkov,\n2022]. Theprobingpredictorcanbebasedoneithersimpleheuristicsorparameterizedmodels\noptimizedontheprobingtask. Recentworkshowsthatlarge-scaleTransformer-basedlanguage\nmodelsexhibitgoodbehaviors,calledemergentabilities,invariousprobingtasks. However,\nwewillnotdiscussdetailsoftheselanguagemodelingsystemsinthischapter,butleavethem\ninthefollowingchapters. Nevertheless,weassumeherethatwehaveaTransformerencoder\nthathasbeenwelltrainedonunlabeleddataandcanbeusedforprobing. Figure6.9illustrates\ntheprocessofprobing.\nMany probing methods have been used in recent work on analyzing and understanding\nwhatislearnedinneuralencoders. Herewedescribesomeofthepopularones.\n• Trees. GivenatrainedTransformerencoder,itiseasytoknowhow“likely”twowordsof\nasentencehavesomelinguisticrelationshipbycomputingtheattentionweightbetween\nthem. We can use this quantity to define a metric measuring the syntactic distance",
    "betweenthetwowordsatpositionsiandj\nd (i,j) = 1−α(i,j) (6.61)\ns\nByusingthismetricitisstraightforwardtoconstructtheminimum-spanningtreefor\nthesentence,thatis,weconnectallthewordstoformatreestructurewiththeminimum",
    "6.2 Syntax-aware Models 293\ntotaldistance. Thetreestructurecanbeseenasalatenttreerepresentationofthesentence\nthatisinducedfromtheneuralnetwork. Whilethisdependency-tree-likestructurecan\nbeusedasasourceoflearnedsyntacticinformationindownstreamtasks,itsaysnothing\nabout our knowledge of syntax. An approach to aligning the representations in the\nencoderwithlinguisticstructureistolearntoproducesyntaxtreesthatareconsistent\nwithhumanannotations. Todothis,weneedtodevelopaprobingpredictorthatcanbe\ntrainedontree-annotateddata. Supposethatthereisahumanannotateddependencytree\nofagivensentence. Foreachpairofwords,wecanobtainadistanceω(i,j)bycounting\nthenumberofedgesbetweenthem. Then,wecanlearnadistancemetricbasedonthe\ninternalrepresentationsoftheencodertoapproximateω(i,j). Asimpleformofsucha\nmetricisdefinedtobetheEuclideandistance[Manningetal.,2020]. LetA∈Rd×ks be\naparametermatrix. TheformoftheEuclideandistanceisgivenby\n(cid:113)\nd (i,j) = ||(h −h )A||2 (6.62)\ns i j 2",
    "whereh andh aretherepresentationsproducedbyanencodinglayeratpositionsi\ni j\nandj6. Givenasetoftree-annotatedsentencesS,wecanoptimizethemodelby\n(cid:88) 1 (cid:88)\nAˆ = argmax |ω(i,j)−d2(i,j)| (6.63)\n|s|2 s\nA\ns∈S i∈s,j∈s\nwhere |s| is length of the sentence s, and (i,j) indicates a pair of words in s. The\noptimized model is then used to parse test sentences via the minimum-spanning tree\nalgorithm,andwecancomparetheparsetreesagainstthehuman-annotatedtrees. To\nobtaindirectedtrees,whicharestandardformsofdependencysyntax,onecanupdate\ntheabovemodelbyconsideringtherelativedistanceofawordtotheroot. Moredetails\ncan be found in Manning et al. [2020]’s work. Here the probing predictor functions\nsimilarlytoaneuralparser,trainedtopredictasyntaxtreebasedonarepresentationof\ntheinputsentence. Thisideacanbeextendedtootherformsofsyntacticstructure,such\nasphrasestructuretrees[Shietal.,2016].\n• Syntactic and Semantic Labels. Many syntactic and semantic parsing tasks can be",
    "framedasproblemsofpredictinglinguisticlabelsgivenasentenceoritssegments. A\nsimpleexampleispart-of-speechtagginginwhicheachwordofasentenceislabeled\nwith a word class. A probe for part-of-speech tagging can be a classifier that takes a\nrepresentation h each time and outputs the corresponding word class. One general\nj\nprobingapproachtotheseproblemsisedgeprobing[Tenneyetal.,2019b;a]. Givena\nsentence,alabelededgeisdefinedasatuple\n(span ,span ,label)\n1 2\nwherespan isaspan[i ,j ],andspan isanotherspan[i ,j ](optionally),andlabel\n1 1 1 2 2 2\n6Ingeneral,h iandh j aretheoutputsofthelastlayeroftheencoder.Alternatively,theycanbeweightedsums\noftheoutputsofallthelayers.",
    "294 Chapter 6. Transformers\nis the corresponding label. Our goal is to learn a probe to predict label given span\n1\nandspan . Forexample,forpart-of-speechtagging,span isaunitspan[j,j]foreach\n2 1\npositionj,span isanemptyspan,andlabelisthepart-of-speechtagcorresponding\n2\nto the j-th word of the sentence; for dependency parsing and coreference resolution,\nspan andspan aretwowordsorentities,andlabelistherelationshipbetweenthem;\n1 2\nforconstituencyparsing,span isaspanofwords,span isanemptyspan,andlabel\n1 2\nisthesyntacticcategoryofthetreenodeyieldingspan . Insimplecases,theprobing\n1\nmodelcanbeamulti-layerfeed-forwardneuralnetworkwithaSoftmaxoutputlayer.\nAsusual,thismodelistrainedonlabeleddata,andthentestedonnewdata.\n• SurfaceFormsofWordsandSentences. Probingtaskscanalsobedesignedtoexamine\nwhethertherepresentationsembedthesurfaceinformationofsentencesorwords[Adi\netal.,2016;Conneauet al.,2018]. Asimplesentence-levelprobingtaskissentence",
    "length prediction. To do this, we first represent the sentence as a single vector h7,\nandthenbuildaclassifiertocategorizehintothecorrespondinglengthbin. Similarly,\nprobescanbebuilttopredictwhethertwowordsatpositionsiandj arereorderedinthe\nsentencegivenh andh . Also,wecandevelopprobestoaddressconventionalproblems\ni j\ninmorphology. Forexample,wereconstructthewordatpositionj orpredictitssense\nwith the representation h . In addition, probing tasks can be focused on particular\nj\nlinguisticproblems,forexample,numeracy[Wallaceetal.,2019]andfunctionwords\n[Kimetal.,2019].\n• Cloze. Ofcourse,wecanprobeneuralmodelsforproblemsbeyondsyntaxandmorphol-\nogy. Oneperspectiveonlarge-scalepre-trainedTransformermodelsistoviewthemas\nknowledgebasescontainingfactsabouttheworld. Itisthereforetemptingtoseeifwe\ncanapplythemtotestfactualknowledge. Asimplemethodistoaskaprobetorecover\nthemissingitemofasentence[Petronietal.,2019]. Forexample,ifwehaveaclozetest\nShijiwaswrittenby .",
    "wewishtheprobetogiveananswerSimaQianbecausethereisasubject-object-relation\nfact(Shiji,SimaQian,written-by). Thisprobecansimplybeamaskedlanguagemodel\nthatiswidelyusedinself-supervisedlearningofTransformerencoders.\nInNLP,probingiscloselyrelatedtopre-trainingoflargelanguagemodels(seeChapters\n7and8). Ingeneral,wecanseeprobingtasksasapplicationsofthesepre-trainedlanguage\nmodels,thoughprobingisordinarilyusedtogiveaquicktestofthemodels. Ideallywewould\nliketodevelopaprobethatmakesbestuseoftherepresentationstodealwiththeproblems.\nHowever,whenaprobeiscomplexandsufficientlywell-trained,itmightbedifficulttosay\niftheproblemissolvedbyusingtherepresentationsortheprobeitself. Acommonwayto\nemphasizethecontributionofprobesinproblem-solvingistocomparethemwithreasonable\nbaselines or conduct the comparison on control tasks [Hewitt and Liang, 2019; Belinkov,\n2022].\n7hcanbecomputedbyperformingapoolingoperationon{h\n1\n,...,hm}",
    "6.3 Improved Architectures 295\n6.3 Improved Architectures\nIn this section we present several improvements to the vanilla Transformer model. Unlike\ntheprevioussection,mostoftheimprovementsarefromtheperspectiveofmachinelearning,\nratherthanlinguistics.\n6.3.1 Locally Attentive Models\nMethods of self-attention, as discussed in Section 6.1.3, can also be viewed as learning\nrepresentations of the entire input sequence. The use of this global attention mechanism\ncan lead to a better ability to deal with long-distance dependencies, but this model has a\nshortcoming: localinformationisnotexplicitlycaptured. Hereweconsiderafewtechniques\nthatattempttomodelthelocalnessofrepresentations.\n1. Priors of Local Modeling\nOneofthesimplestwaysofintroducinglocalmodelsintoTransformersistoaddapenalty\ntermtotheattentionfunctioninordertodiscouragelargeattentionweightsbetweendistant\npositions. Ontheencoder-side,thisleadstoaformthatwehavealreadyencounteredseveral\ntimesinthischapter.\nHq[Hk]T",
    "AttLocal (H) = Softmax( √ −γ·G)Hv (6.64)\nself\nd\nwhereγ istheweight(ortemperature)ofthepenaltyterm,andG∈Rm×m isthematrixof\npenalties. EachentryG(i,j)indicateshowmuchwepenalizethemodelgivenpositionsiand\nj. AsimpleformofG(i,j)isadistancemetricbetweeniandj,forexample\nG(i,j) = |i−j| (6.65)\nOrG(i,j)canbedefinedasaGaussianpenaltyfunction[Yangetal.,2018a]\n(i−j)2\nG(i,j) = (6.66)\n2σ2\ni\nwhere σ is the standard deviation of the Gaussian distribution. For different j, both of the\ni\nabove penalty terms increase, linearly or exponentially, away from the maximum at i with\ndistance|i−j|.\nThismethodcanbeextendedtothecross-attentionmodel,likethis\nSq[Hk]T\nAttLocal (H,S) = Softmax( √ −γ·G)Hv (6.67)\ncross\nd\nwhereGisann×mmatrix. EachentryofGcanbedefinedas\n(µ −j)2\ni\nG(i,j) = (6.68)\n2σ2\ni",
    "296 Chapter 6. Transformers\nwhere µ is the mean of the Gaussian distribution over the source-side positions. Both µ\ni i\nand σ can be determined using heuristics. Alternatively, we can develop additional neural\ni\nnetworkstomodelthemandlearncorrespondingparameterstogetherwithotherparametersof\ntheTransformermodel. Forexample,wecanuseafeed-forwardneuralnetworktopredictµ\ni\ngivens .\ni\nOnealternativetoEq. (6.64)(orEq. (6.67))treatsthepenaltytermasaseparatemodeland\ncombinesitwiththeoriginalattentionmodel. Forexample,wecandefinetheself-attention\nmodelas\n(cid:18) Hq[Hk]T (cid:19)\nAttLocal (H) = (1−β)·Softmax( √ )+β·Softmax(−γ·G) Hv (6.69)\nself\nd\nwhere β ∈[0,1] is the coefficient of the linear combination. Note that, to avoid empirical\nchoicesofthevaluesofαandβ,wecanusegatingfunctionstopredictαandβ andtrainthese\nfunctionsasusual.\nAnotheralternativeistouseamultiplicativemasktoincorporatethepriorintomodeling,\nasinEq. (6.54). Thisisgivenby\nHq[Hk]T\nAttLocal (H) = Softmax( √ ⊙G′)Hv (6.70)\nself\nd",
    "HereG′∈[0,1]m×m isamatrixofscalars. ThescalarG′(i,j)givesavalueof1wheni=j,\nandasmallervalueasj movesawayfromi. G′(i,j)canbeobtainedbynormalizing−G(i,j)\noverallj orusingalternativefunctions.\n2. Local Attention\nThe term local attention has been used broadly to cover a wide range of problems and to\nrefer to many different models in the NLP literature. The methods discussed above are\nthosethatimposesoftconstraintsonattentionmodels. Infact,localattentionhasitsorigins\nin attempts to restrict the scope of attention models for considerations of modeling and\ncomputationalproblems[Luongetal.,2015]. Researchinthisareaoftenlooksintointroducing\nhardconstraints,sothattheresultingmodelscanfocusonpartsoftheinputandignorethe\nrest. Forexample,wecanpredictaspanofsource-sidepositionsforperformingtheattention\nfunction given a target-side position [Sperber et al., 2018; Yang et al., 2018a; Sukhbaatar\netal.,2019]. Also,attentionspanscanbeinducedfromsyntaxtrees,forexample,knowing",
    "sub-treestructuresofasentencemayhelpwinnowthefieldthatthemodelconcentrateson\ninlearningtherepresentation. Thus,manyofthesyntax-constrainedmodelsareinstancesof\nlocalattention-basedmodels(seeSection6.2.4). Inaddition,theconceptoflocalattention\ncanbeextendedtodeveloparichsetofmodels,suchassparseattentionmodels,although\nthesemodelsareoftendiscussedinthecontextofefficientmachinelearningmethods. Wewill\nseeafewexamplesoftheminSection6.4.\nIndeeplearning,oneofthemostwidelyusedmodelsforlearningfeaturesfromarestricted\nregionoftheinputisCNNs. ItisthusinterestingtoconsidermethodsofcombiningCNNs\nandTransformermodelstoobtainthebenefitsofbothapproaches,forexample,CNNsdeal",
    "6.3 Improved Architectures 297\nwithshort-termdependencies, andself-attentionmodelsdealwithlong-termdependencies.\nOneapproachistobuildatwo-branchsequencemodelwhereonebranchisbasedonCNNs\nand the other is based on self-attention models [Wu et al., 2020b]. Another approach is to\nincorporateCNNlayersintoTransformerblocksinsomewaythatwecanlearnbothlocaland\nglobalrepresentationsthroughadeepmodel[Wuetal.,2019;Gulatietal.,2020].\n3. Relative Positional Embedding\nRelativepositionalembedding,alsoknownasrelativepositionalrepresentation(RPR),is\nanimprovementtotheabsolutepositionalembeddingmethodusedinstandardTransformer\nsystems[Shawetal.,2018;Huangetal.,2018]. TheideaofRPRisthatwemodelthedistance\nbetweentwopositionsofasequenceratherthangivingeachpositionafixedrepresentation.\nAs a result, we have a pair-wise representation PE(i,j) for any two positions i and j. One\nsimplewaytodefinePE(i,j)istoconsideritasalookuptableforallpairsofiandj. More",
    "specifically, let u be a d-dimensional representation for a given distance π. The form of\nπ\nPE(i,j)inthevanillaRPRmethodisgivenby\nPE(i,j) = u (6.71)\nclip(j−i,krpr)\nwhereclip(x,k )isafunctionthatclipsxintheinterval[−k ,k ]\nrpr rpr rpr\nclip(x,k ) = max{−k ,min{x,k }} (6.72)\nrpr rpr rpr\nThus,wehaveamodelwithparameters\n \nu\n−krpr\n . . \n . \n \nU =  u  (6.73)\nrpr  0 \n . \n . . \n \nu\nkrpr\nWhilethismatrixnotationisusedinarelativelyinformalway,wecanviewU asamatrix\nrpr\n∈R(2krpr+1)×d,andselectarowcorrespondingtoclip(j−i,k\nrpr\n)whenRPRisrequiredfor\ngiveniandj.\nUsing the above method, we can define three RPR models PEq(i,j), PEk(i,j) and\nPEv(i,j)forqueries,keys,andvalues,respectively. Then,followingtheformofEq. (6.17),\ntheoutputoftheself-attentionmodelatpositionicanbewrittenas\nm\nc = (cid:88) α (cid:2) hv+PEv(i,j) (cid:3)\ni i,j j\nj=1\nm m\n(cid:88) (cid:88)\n= α hv+ α PEv(i,j) (6.74)\ni,j j i,j\nj=1 j=1",
    "298 Chapter 6. Transformers\n··· ···\nblock2 block2\nFFNSub-layer FFNSub-layer\nSelf-attentionSub-layer Self-attentionSub-layer\nblock1 block1\nFFNSub-layer FFNSub-layer\n... PE(i,j)(krpr=3)\n···\n5 u−3u−3u−2u−1 u0\n4 u−3u−2u−1 u0 u1\nSelf-attentionSub-layer Self-attentionSub-layer\n3 u−2u−1 u0 u1 u2\n2 u−1 u0 u1 u2 u3\n1 u0 u1 u2 u3 u3\ni\nx x ··· x x x ··· x j 1 2 3 4 5 ···\n1 2 m 1 2 m\n+ + + + + +\nPE(1) PE(2) ··· PE(m) PE(1) PE(2) ··· PE(m)\n1 2 ··· m\n(a)TransformerwithoutRPR (b)TransformerwithRPR\nFigure6.10: Transformerencoderswithoutandwithrelativepositionalrepresentation(RPR).\nInRPR,eachpairofpositionsisrepresentedasavectorPE(i,j)usingamodelparameterized\nby U . PE(i,j) is fed into each self-attention sub-layer so that we can make use of the\nrpr\npositionalinformationinintermediatestepsoflearningrepresentations.\nwhere hv is the j-th row vector of Hv. This representation comprises two components:\nj\n(cid:80)m α hv isthebasicrepresentation,and (cid:80)m α PEv(i,j)isthepositionalrepresenta-",
    "j=1 i,j j j=1 i,j\ntion.\nTheattentionweightα iscomputedinaregularway,butwithadditionaltermsPEq(i,j)\ni,j\nandPEk(i,j)addedtoeachqueryandkey.\n[hq+PEq(i,j)][hk+PEk(i,j)]T\nα = Softmax( i √ j ) (6.75)\ni,j\nd\nFigure6.10showstheTransformerencoderarchitectureswithandwithoutRPR.When\nRPRisadopted,PEq(i,j),PEk(i,j),PEv(i,j)aredirectlyfedtoeachself-attentionsub-layer,\nandsowecanmakebetteruseofpositionalinformationforsequencemodeling. Notethat,\nthe use of the clipping function (see Eq. (6.72)) makes the modeling simple because we\ndo not need to distinguish the relative distances for the cases |j−i|≥k . This clipped\nrpr\ndistance-basedmodelcanlead,inturn,tobettermodelinginlocalcontextwindows.\nEqs. (6.74)and(6.75)provideageneralapproachtoposition-sensitivesequencemodeling.\nThere are many variants of this model. In Shaw et al. [2018]’s early work on RPR, the",
    "6.3 Improved Architectures 299\npositionalrepresentationsforqueriesareremoved,andthemodelworksonlywithPEk(i,j)\nandPEv(i,j),likethis\nhq[hk+PEk(i,j)]T\nα = Softmax( i j √ ) (6.76)\ni,j\nd\nBy contrast, there are examples that attempt to improve the RPR model in computing\nattention weights but ignore PEv(i,j) in learning values [Dai et al., 2019; He et al., 2021].\nInsteadoftreatingRPRasanadditivetermtoeachrepresentation,researchersalsoexplore\notherwaysofintroducingRPRintoTransformer[Huangetal.,2020b;Raffeletal.,2020]. We\nrefertheinterestedreaderstothesepapersformoredetails.\n6.3.2 Deep Models\nMany state-of-the-art NLP systems are based on deep Transformer models. For example,\nrecentlargelanguagemodelsgenerallycomprisetensofTransformerlayers(ormoreprecisely,\nhundreds of layers of neurons), demonstrating strong performance on many tasks [Ouyang\netal.,2022;Touvronetal.,2023a]. BystackingTransformerlayers,itisstraightforwardto",
    "obtain a deep model. However, as is often the case, training very deep neural networks is\nchallenging. Adifficultyarisesfromthefactthattheerrorsurfacesofdeepneuralnetworksare\nhighlynon-convexandhavemanylocaloptimathatmakethetrainingprocesslikelytogetstuck\ninthem. Whilethereareoptimizationalgorithmsthatcanhelpalleviatethisproblem,most\nofthepracticaleffortsexploretheuseofgradient-basedmethodsforoptimizingdeepneural\nnetworks. Asaresult,trainingamodelwithmanyTransformerlayersbecomeschallenging\nduetovanishingandexplodinggradientsduringback-propagation. Hereweconsiderseveral\ntechniquesfortrainingdeepTransformermodels.\n1. Re-thinking the Pre-Norm and Post-Norm Architectures\nAsintroducedpreviously,aTransformersub-layerisaresidualnetworkwhereashortcutis\ncreated to add the input of the network directly to the output of this sub-layer. This allows\ngradients to flow more directly from the output back to the input, mitigating the vanishing",
    "gradientproblem. Ingeneral,aresidualconnectioninTransformerisusedtogetherwithalayer\nnormalizationunittoformasub-layer. Thisleadstotwotypesofarchitecture,calledpost-norm\nandpre-norm. Tobespecific,recallfromSection6.1.4thatthepost-normarchitecturecanbe\nexpressedas\nzl = LNorm(Fl(zl−1)+zl−1) (6.77)\nwherezl andzl−1 aretheoutputandinputofthesub-layerl,andFl(·)isthecorefunction\nofthissub-layer. Thepre-normarchitecturetakestheidentitymappingzl outsidethelayer\nnormalizationfunction,givenintheform\nzl = LNorm(Fl(zl−1))+zl−1 (6.78)",
    "300 Chapter 6. Transformers\nConsiderthedifferencebetweentheinformationflowinthesetwoarchitectures:\n• Thepost-normarchitecturepreventstheidentitymappingoftheinputfromaddingtothe\noutputofthesub-layer. Thisisnotatrueresidualnetwork,becausealltheinformation\nispassedonthroughanon-linearfunction(i.e.,thelayernormalizationunit). Thus,the\npost-normarchitectureisnotvery“efficient”forback-propagation. Wangetal.[2019a]\nshowthatthegradientofthelossofanLsub-layerTransformernetworkwithrespectto\nzl isgivenby\n∂E ∂E L (cid:89) −1 ∂LNorm(vk) L (cid:89) −1(cid:18) ∂Fk(zk) (cid:19)\n= · · 1+ (6.79)\n∂zl ∂zL ∂vk ∂zk\nk=l k=l\nwherezL istheoutputofthelastlayer,vk isashortforFk(zk−1),andE istheerror\n∂LNorm(vk) ∂Fk(zk)\nmeasuredbysomelossfunction. and arethegradientsofthelayer\n∂vk ∂zk\nnormalizationfunctionandthecorefunction,respectively. Althoughtheequationhere\nappearsabitcomplex,weseethat\n(cid:81)L−1∂LNorm(vk)\nissimplyaproductofL−lfactors.\nk=l ∂vk",
    "ThismeansthattheerrorgradientwillberescaledmoretimesifLbecomeslarger,and\nthereisahigherriskofvanishingandexplodinggradientsforadeepermodel.\n• Thepre-normarchitecturedescribesastandardresidualneuralnetworkwheretheinput\nofawholenetworkisaddedtoitsoutput. Wecanwritethegradientoftheerroratzl as\n∂E ∂E (cid:32) L (cid:89) −1 ∂Fk(LNorm(zk)) (cid:33)\n= · 1+\n∂zl ∂zL ∂zk\nk=l\n∂E ∂E L (cid:89) −1 ∂Fk(LNorm(zk))\n= + · (6.80)\n∂zL ∂zL ∂zk\nk=l\nItiseasytoseethat ∂E receivesdirectfeedbackregardingtheerrorsmadebythemodel,\n∂zl\nbecausethefirsttermofthesummationontheright-handside(i.e., ∂E )isthegradient\n∂zL\nofthemodeloutputwhichisindependentofthenetworkdepth.\nTheuseofthepre-normarchitecturealsohelpsoptimizationduringearlygradientdescent\nsteps. For example, it has been found that pre-norm Transformer models can be trained by\nusingalargerlearningrateintheearlystageoftraininginsteadofgraduallyincreasingthe\nlearningratefromasmallvalue[Xiongetal.,2020].",
    "Whilethepre-normarchitectureleadstoeasieroptimizationofdeepTransformermodels,\nwe would not simply say that it is a better choice compared to the post-norm architecture.\nIn fact, both post-norm and pre-norm Transformer models have been successfully used in\nmany applications. For example, the post-norm architecture is widely used in BERT-like\nmodels,whilethepre-normarchitectureisamorepopularchoiceinrecentgenerativelarge\nlanguage models. Broadly, these two architectures provide different ways to design a deep\nTransformermodel,aswellasdifferentadvantagesanddisadvantagesindoingso. Thepost-\nnormarchitectureforcestherepresentationtobelearnedthroughmorenon-linearfunctions,",
    "6.3 Improved Architectures 301\nbut in turn results in a complicated model that is relatively hard to train. By contrast, the\npre-normarchitecturecanmakethetrainingofTransformermodelseasier,butwouldbeless\nexpressivethanthepost-normcounterpartifthelearnedmodelsareoverlydependentonthe\nshortcutpaths.\nAnimprovementtothesearchitecturesistocontroltheextenttowhichwewantto“skip”\na sub-layer. A simple way to do this is to weight different paths rather than treating them\nequally. Forexample,ascalarfactorofaresidualconnectioncanbeintroducedtodetermine\nhowheavilyweweightthisresidualconnectionrelativetothepathofthecorefunction[He\netal.,2016b;Liuetal.,2020a;b]. Amoregeneralformofthismodelisgivenby\nzl = LNorm(Fl(zl−1)+β·zl−1)+γ·zl−1 (6.81)\nwhereβ istheweightoftheidentitymappinginsidethelayernormalizationfunction,andγ\nistheweightoftheidentitymappingoutsidethelayernormalizationfunction. Clearly,both\nthepost-normandpre-normarchitecturescanbeseenasspecialcasesofthisequation. That",
    "is, if β =1 and γ =0, then it will become Eq. (6.77); if β =0 and γ =1, it will become\nEq. (6.78). Thismodelprovidesamulti-branchviewofbuildingresidualblocks. Theinput\ntothisblockcanbecomputedthroughmultiplepathswithdifferentmodelingcomplexities.\nWhenβ andγ aresmall,therepresentationisforcedtobelearnedthrougha“deep”model\nwith multiple layers of cascaded non-linear units. In contrast, when β and γ are large, the\nrepresentation is more likely to be learned using a “shallow” model with fewer layers. To\ndetermine the optimal choices of β and γ, one can give them fixed values by considering\nsometheoreticalpropertiesorsystemperformanceonvalidationsets,orcomputethesevalues\nbyusingadditionalfunctionsthatcanbetrainedtodoso[Srivastavaetal.,2015]. Itshould\nbe emphasized that many other types of architecture can be considered in the design of a\nTransformersub-layer. Itispossible,forinstance,tointroducemorelayernormalizationunits",
    "intoasub-layer[Dingetal.,2021;Wangetal.,2022b],or,onthecontrary,tosimplyremove\nthemfromasub-layer[Bachlechneretal.,2021].\n2. Parameter Initialization\nAs with other deep neural networks, there is interest in developing parameter initialization\nmethodsfordeepTransformermodelsinordertoperformoptimizationonsomeregionaround\na better local optimum. However, initialization is a wide-ranging topic for optimization of\nmachine learning models, and the discussion of this general topic lies beyond the scope\nof this section. Here we will discuss some of the parameter initialization methods used in\nTransformer-basedsystemsratherthanthegeneraloptimizationproblems.\nWhile the parameters of a neural network can be set in various different ways, most\npracticalsystemsadoptsimpletechniquestogiveappropriateinitialvaluesofmodelparameters.\nConsider,forexample,theXavierinitializationforaparametermatrixW∈Rdin×dout [Glorot\nandBengio,2010]. Wedefineavariableη by\n(cid:114)\n6\nη = gain· (6.82)\nd +d\nin out",
    "302 Chapter 6. Transformers\nwhere gain is a hyper-parameter which equals 1 by default. Then, each entry of W can be\ninitializedbyusingauniformdistribution\nW ∼ U(−η,η) (6.83)\nor,alternatively,usingaGaussiandistribution\nW ∼ Gaussian\n(cid:0) 0,η2(cid:1)\n(6.84)\nThismethodcanbeeasilyadaptedtoinitializeTransformermodelshavingalargenumber\noflayers. Onecommonwayistofindamoresuitablevalueofgainbytakingintoaccountthe\nfactthattheinitialstatesofoptimizationmightbedifferentforneuralnetworksofdifferent\ndepths. Forexample,onecanincreasethevalueofgainasthedepthofthemodelgrows. Then,\ngaincanbedefinedasafunctionofthenetworkdepthintheform\ngain = a·Lb (6.85)\nwhereaisthescalar,andLb isthenetworkdepthraisedtothepowerofb. Typically,aand\nb can be positive numbers, which means that it is preferred to have larger initial values for\ntheparametersfordeepermodels. Forexample,Wangetal.[2022a]showthat,bychoosing\nappropriatevaluesforaandb,averydeepTransformermodelcanbesuccessfullytrained.",
    "Eq. (6.85)assignsgainthesamevalueforallofthesub-layers. However,itisfoundthat\nthenormofgradientsbecomessmallerwhenasub-layermovesawayfromtheoutputlayer.\nThisconsistentapplicationofgainacrosstheentiremodelcouldresultinunder-trainingofthe\nlowerlayersduetothegradientvanishingproblem. Forthisreason,onecandevelopmethods\nthataresensitivetothepositionofasub-layerintheneuralnetwork. Thegeneralformofsuch\nmethodsisgivenby\na\ngain = (6.86)\nlb\nHereldenotesthedepthofasub-layer. Iflislarger(i.e.,thesub-layerisclosertotheoutput),\ngain will be smaller and the corresponding parameters will be set to smaller values. An\nexampleofthismethodcanbefoundinZhangetal.[2019a]’swork.\nItisalso,ofcourse,straightforwardtoapplygeneralmethodsofinitializingdeepmulti-\nlayer neural networks to Transformer models. An example is to consider the Lipschitz\nconstantinparameterinitialization,whichhasbeenshowntohelpimprovethestabilityof\ntraining deep models [Szegedy et al., 2014b; Xu et al., 2020]. Another approach is to use",
    "second-ordermethodstoestimatethepropervaluesoftheparameters. Forexample,onecan\ncomputetheHessianofeachparametermatrixtomodelitscurvature[Skorskietal.,2021].\nFormodelswithalargenumberoflayers,itisalsopossibletopre-trainsomeofthelayers\nviasmallermodelsandusetheirtrainedparameterstoinitializebiggermodels[Chenetal.,\n2015]. Thatis,wefirstobtainaroughestimationoftheparametersinacheapway,andthen\ncontinuethetrainingprocessonthewholemodelasusual. Thesemethodsfallintoaclassof",
    "6.3 Improved Architectures 303\ntrainingmethods,calledmodelgrowthordepthgrowth.\nAsasimpleexample,consideraTransformermodel(e.g.,aTransformerencoder)of2L\nsub-layers. Wecantrainthismodelbyusingtheshallow-to-deeptrainingmethod[Lietal.,\n2020b]. First,wetrainanL-sub-layermodel(callittheshallowmodel)inaregularway. Then,\nwecreatea2L-sub-layermodel(callitthedeepmodel)bystackingtheshallowmodeltwice,\nandfurthertrainthisdeepmodel. Toconstructdeepermodels,thisprocedurecanberepeated\nmultiple times, say, we start with a model of L sub-layers, and obtain a model of L2I after\nI iterations. Note that many of the pre-training models are used in the same manner. For\nexample,forBERT-likemethods,atransformerencoderistrainedonlarge-scaledata,andthe\noptimizedparametersarethenusedtoinitializedownstreamsystems.\n3. Layer Fusion\nAnotherproblemwithtrainingadeepTransformermodelisthatthepredictionisonlycondi-\ntionedonthelastlayeroftheneuralnetwork. Whiletheuseofresidualconnectionsenables",
    "thedirectaccesstolower-levellayersfromahigher-levellayer,thereisstilla“long”pathof\npassinginformationfromthebottomtothetop. Onesimplewaytoaddressthisistocreate\nresidualconnectionsthatskipmorelayers. Forexample,consideragroupofLTransformer\nsub-layers. For the sub-layer at depth l, we can build l−1 residual connections, each con-\nnectingthissub-layerwithaprevioussub-layer. Inthisway,wedevelopadenselyconnected\nnetworkwhereeachsub-layertakestheoutputsofallprevioussub-layers[Huangetal.,2017a].\nTheoutputofthelastsub-layercanbeseenassomecombinationoftheoutputsatdifferent\nlevelsofrepresentationoftheinput.\nFollowing the notation used in the previous subsections, we denote the output of the\nsub-layeratdepthlbyzl,anddenotethefunctionofthesub-layerbyLayerl(·). Then,zl can\nbeexpressedas\nzl = Layerl(z1,...,zl−1) (6.87)\nWecansimplyviewLayerl(·)asafunctionthatfusestheinformationfrom{z1,...,zl−1}.\nThere are many possible choices for Layerl(·). For example, a simple form of Layerl(·) is\ngivenby",
    "Layerl(z1,...,zl−1) = LNorm(Fl(Zl)) (6.88)\nZl = ϕ(z1,...,zl−1) (6.89)\nHereϕ(·)takesthelayeroutputs{z1,...,zl−1}andfusesthemintoasinglerepresentationZl.\nAsimpleinstanceofϕ(·)isaveragepoolingwhichcomputesthesumof{z1,...,zl−1}divided\nbyl−1. SeeTable6.2formoreexamplesofϕ(·).\nTakingasimilararchitectureofaTransformersub-layer,wecanalsoconsiderapost-norm",
    "304 Chapter 6. Transformers\nEntry Function\nAveragePooling ϕ(z1,...,zl−1)= 1 (cid:80)l−1 zk\nl−1 k=1\nWeightedSum ϕ(z1,...,zl−1)= (cid:80)l−1 weight ·zk\nk=1 k\nFeedforwardNetwork ϕ(z1,...,zl−1)=FFN([z1,...,zl−1])\nSelfAttention ϕ(z1,...,zl−1)=FFN([Att (z1,...,zl−1)])\nself\nTable6.2: Fusionfunctions. FFN(·)=feedforwardneuralnetwork, [·]=concatenatingthe\ninput vectors, and Att (·) = self-attention function. All of the fusion functions can be\nself\nfollowedbyalayernormalizationfunction,forexample,wecanwritetheweightedsumof\n{z1,...,zl−1}asϕ(z1,...,zl−1)=LNorm( (cid:80)l−1 weight ·zk).\nk=1 k\nform\nLayerl(z1,...,zl−1) = LNorm(Zl) (6.90)\nZl = ϕ(Fl(zl−1),z1,...,zl−1) (6.91)\norapre-normform\nLayerl(z1,...,zl−1) = Zl (6.92)\nZl = ϕ(LNorm(Fl(zl−1)),z1,...,zl−1) (6.93)\nThesemodelsareverygeneral. Forexample,astandardpost-normencodersub-layercan\nberecoveredasaspecialcaseofEqs. (6.90-6.91),ifweremovethedependenciesofsub-layers\nfrom1tol−2,anddefineϕ(·)tobe\nϕ(Fl(zl−1),z1,...,zl−1) = Fl(zl−1)+zl−1 (6.94)",
    "Denselyconnectednetworkmakestheinformationeasiertoflowthroughdirectconnections\nbetweensub-layers,buttheresultingmodelsareabitmorecomplex,especiallywhenweuse\nparameterizedfusionfunctions. Inpractice,wetypicallyadddenseconnectionsonlytosome\nofthesub-layers,andsotheoverallnetworksarenotverydense. Forexample,weonlyadd\nconnections from bottom sub-layers to the last few sub-layers. Thus, the prediction can be\nmadebyhavingdirectaccesstodifferentlevelsofrepresentation[Wangetal.,2018a].\n4. Regularization\nInmachinelearning,regularizationisusedtoavoidoverfittingintrainingdeepneuralnetworks.\nItisthereforestraightforwardtoapplyregularizationtechniquestoTransformermodels. Since\ntheregularizationissuehasbeendiscussedinChapter2,hereweconsidersomeofthemethods\nthathavenotbeencoveredyetinthisbookbutcouldbeusedfortrainingdeepTransformer\nmodels.\nOneapproachtoregularizingadeepTransformermodelistorandomlyskipsub-layers",
    "or layers during training [Huang et al., 2016; Pham et al., 2019]. In each run of the model,\nsuch as running the backpropgation algorithm on a batch of samples, we select each of the",
    "6.3 Improved Architectures 305\nsub-layerswithaprobabilityρ,andstacktheselectedsub-layerstoforma“new”model. Thus,\nwe essentially train different neural networks with shared architectures and parameters on\nthesamedataset. Inthisway,asub-layerlearnstooperatesomewhatindependently,andso\noverfittingisreducedbypreventingtheco-adaptionofsub-layers. Infact,droppingoutsub-\nlayers(orlayers)anddroppingoutneuronsaretwodifferentmethodsonatheme. Sometimes,\nthemethoddescribedhereiscalledsub-layerdropoutorlayerdropout.\nAttesttime,weneedtocombineallthepossiblenetworkstomakepredictionsofsome\noutput. Asimplemethodtoachievethisistorescaletheoutputsofthestochasticcomponents\nof the model [Li et al., 2021a]. As an example, suppose each sub-layer has a pre-norm\narchitecture. Then,theoutputofthesub-layeratdepthlisgivenby\nzl = ρ·LNorm(Fl(zl−1))+zl−1 (6.95)\nAnotherideaistoforcetheparameterstobesharedacrosssub-layers. Oneofthesimplest\nmethodsistousethesameparametersforallthecorrespondingsub-layers[Dehghanietal.,",
    "2018],forexample,alltheFFNsub-layersarebasedonthesamefeedforwardnetwork. This\nmethodhasasimilareffectasthemethodsthataddnormsofparametermatricestotheloss\nfunctionforpenalizingcomplexmodels. Forpracticalsystems,therecanbesignificantbenefit\ninadoptingasharedarchitecturebecausewecanreusethesamesub-modeltobuildamulti-\nlayerneuralnetworkandreducethememoryfootprint. Wewillseemorediscussionsonthe\nefficiencyissueinSection6.4.4.\n6.3.3 Numerical Method-Inspired Models\nA residual network computes its output through the sum of the identity mapping and some\ntransformation of the input. Such a model can be interpreted as an Euler discretization of\nordinarydifferentialequations(ODEs)[Ee,2017;HaberandRuthotto,2017]. Toillustrate\nthisidea,weconsiderageneralformofresidualnetworks\nzl = fl(cid:0) zl−1(cid:1) +zl−1 (6.96)\nwherefl(zl−1)denotesafunctiontakesaninputvariablezl−1 andproducesanoutputvariable\nin the same space. Clearly, a Transformer sub-layer is a special case of this equation. For",
    "example,forpre-normTransformer,wehavefl(·)=LNorm(Fl(·)).\nFornotationalsimplicity,werewritetheaboveequationinanequivalentform\n(cid:0) (cid:1)\nz(l) = f z(l−1),l +z(l−1) (6.97)\nWeusethenotationsz(l)andf(z(·,l))toemphasizethatz(·)andf(·)arefunctionsofl. Here\nwe assume that l is a discrete variable. If we relax l to a continuous variable and z(l) to a\ncontinuousfunctionofl,thenwecanexpressEq. (6.97)as\n(cid:0) (cid:1)\nz(l) = △l·f z(l−△l),l +z(l−△l) (6.98)",
    "306 Chapter 6. Transformers\nThiscanbefurtherwrittenas\nz(l)−z(l−△l) (cid:0) (cid:1)\n= f z(l−△l),l (6.99)\n△l\nTakingthelimit△l→0,wehaveanODE\ndz(l) (cid:0) (cid:1)\n= f z(l),l (6.100)\ndl\nWe say that a pre-norm Transformer sub-layer (i.e., Eqs. (6.97) and (6.96)) is an Euler\ndiscretization of solutions to the above ODE. This is an interesting result! A sub-layer is\nactuallyasolveroftheODE.\nEqs. (6.97) and (6.96) are standard forms of the Euler method. It computes a new\nestimation of the solution by moving from an old estimation one step forward along l. In\ngeneral,twodimensionscanbeconsideredindesignofnumericalmethodsforODEs.\n• LinearMulti-stepMethods. Alinearmulti-stepmethodcomputesthecurrentestima-\ntionofthesolutionsbytakingtheestimationsandderivativeinformationfrommultiple\nprevioussteps. Ageneralformulationofp-stepmethodscanbeexpressedas\np p+1\n(cid:88) (cid:88) (cid:0) (cid:1)\nz(l) = a ·z(l−i)+h b ·f z(l−i), l−i+1 (6.101)\ni i\ni=1 i=1",
    "where h is the size of the step we move each time8, that is, △l in Eqs. (6.98) and\n(6.99). {a }and{b }arecoefficientsofthesolutionpointsandderivativesinthelinear\ni i\ncombination. Giventhisdefinition,wecanthinkoftheEulermethodasasingle-step,\nlow-ordermethodofsolvingODEs9.\n• (Higher-order)Runge-KuttaMethods. Runge-Kutta(RK)methodsandtheirvariants\nprovidewaystocomputethenextstepsolutionbytakingintermediateresultsinsolving\nanODE.Asaresult,weobtainhigher-ordermethodsbutstillfollowtheformofsingle-\nstepmethods,thatis,theestimatedsolutionisdependentonlyonz(l−1)ratherthanon\ntheoutputsatmultipleprevioussteps.\nInfact,linearmulti-stepmethods,thoughnotexplicitlymentioned,havebeenusedinlayer\nfusion discussed in Section 6.3.2. For example, taking Eqs. (6.92) and (6.93) and a linear\nfusionfunction,apre-normsub-layerwithdenseconnectionstoallprevioussub-layerscanbe\nexpressedas\nLayerl(z1,...,zl−1) = a ·zl−1+...+a ·z1+b ·LNorm(Fl(zl−1)) (6.102)\n1 l−1 1\n8Let{t\n0\n,...,t\ni",
    "}denotethevaluesofthevariablelatsteps{0,...,i}.Inlinearmulti-stepmethods,itisassumed\nthatt\ni\n=t\n0\n+ih.\n9Innumericalanalysis,thelocaltruncationerrorofamethodofsolvingODEsatastepisdefinedtobethe\ndifferencebetweentheapproximatedsolutioncomputedbythemethodandthetruesolution.Themethodiscalled\norderpifithasalocaltruncationerrorO(hp+1).",
    "6.3 Improved Architectures 307\nThisequationisaninstanceofEq. (6.101)whereweseth=1andremovesomeoftheterms\nontheright-handside.\nItisalsostraightforwardtoapplyRunge-KuttamethodstoTransformer[Lietal.,2022a].\nGivenanODEasdescribedinEq. (6.100),anexplicitp-orderRunge-Kuttasolutionisgiven\nby\np\n(cid:88)\nz(l) = z(l−1)+ γ ·g (6.103)\ni i\ni=1\ni−1\n(cid:0) (cid:88) (cid:1)\ng = h·f z(l−1)+ β ·g , l−1+λ ·h (6.104)\ni i,j j i\nj=1\nHereg representsanintermediatestepwhichispresentonlyduringtheaboveprocess. {γ },\ni i\n{β } and {λ } are coefficients that are determined by using the Taylor series of z(l). To\ni,j i\nsimplifythemodel,weassumethatthesamefunctionf isusedforall{g }. Then,weremove\ni\nthedependencyoftheterml−1+λ ·hinf,andrewriteEq. (6.104)as\ni\ni−1\n(cid:0) (cid:88) (cid:1)\ng = h·f z(l−1)+ β ·g (6.105)\ni i,j j\nj=1\nwheref(·)isafunctionindependentofi.\nAsanexample,considerthe4th-orderRunge-Kutta(RK4)solution\n1\nz(l) = z(l−1)+ (g +2g +2g +g ) (6.106)\n1 2 3 4\n6\ng = h·f(z(l−1)) (6.107)\n1\n1",
    "g = h·f(z(l−1)+ g ) (6.108)\n2 1\n2\n1\ng = h·f(z(l−1)+ g ) (6.109)\n3 2\n2\ng = h·f(z(l−1)+g ) (6.110)\n4 3\nThese equations define a new architecture of sub-layer. For example, by setting h=1 and\nf(·)=LNorm(Fl(·)),weobtainanRK4Transformersub-layer,asshowninFigure6.11. This\nmethodleadstoadeepmodelbecauseeachsub-layerinvolvesfourrunsoff(·)insequence.\nOn the other hand, the resulting model is parameter efficient because we reuse the same\nfunctionf(·)withinthesub-layer,withoutintroducingnewparameters.\nSo far in this subsection our discussion has focused on applying dynamic systems to\nTransformermodelsbydesigningarchitecturesofTransformersub-layers. Whilethebasic\nODEmodeliscontinuouswithrespecttothedepthl,thesemethodsstillfollowthegeneral\nframeworkofneuralnetworksinwhichlisadiscretevariableandtherepresentationalpower\nofthemodelsislargelydeterminedbythishyper-parameter. Analternativeapproachistouse\nneuralODEmodelstorelaxthe“depth”toatrulycontinuousvariable. Inthisway,wecan",
    "have a model with continuous depth for computing the solution of ODEs. However, as the",
    "308 Chapter 6. Transformers\nz(l) z(l) z(l)\n1\n6\n1\n2 2\n6\nf\n2\n1 6\n2 f\nf 1\n1 2\n6 f\nf f 1\n2\nf\nz(l−1) z(l−1) z(l−1)\n(a)Pre-norm (b)RK2 (c)RK4\nFigure6.11: Pre-norm(a)andRunge-Kutta(bandc)sub-layerarchitectures. z(l−1)denotes\nthe input of a sub-layer at depth l, z(l) denotes the output of the sub-layer, and f (in blue\nboxes)denotesthefunctionf(·)=LNorm(Fl(·))\ndiscussionofneuralODEliesbeyondthescopeofthischapter,wereferthereadertorelated\npapersformoredetails[Chenetal.,2018c;Kidger,2022].\n6.3.4 Wide Models\nMostofthemethodsthatwehavestudiedsofarinthissectionareexamplesoflearningand\nusingdeepmodels. Anotherdesignchoicewegenerallyfaceistodeterminethewidthfora\nneuralnetwork. Typically,thewidthofaTransformermodelcanbedefinedasthenumberof\ndimensionsofarepresentationatsomepositionoftheinputsequence,thatis,theparameter\nd. Increasingthiswidthisacommonmethodtoobtainamorecomplexandmorepowerful\nmodel. Forexample,inVaswanietal.[2017]’swork,awidemodel(calledTransformerbig)",
    "leadstosignificantimprovementsintranslationqualityformachinetranslationsystems. More\nrecently,widermodelshavebeenproposedtoboostsystemsonlarge-scaletasks[Lepikhin\netal.,2021;Fedusetal.,2022b].\nHowever, developing very wide Transformer models is difficult. One difficulty is that\ntrainingsuchsystemsiscomputationallyexpensive. Whilethenumberofthemodelparameters\n(or model size) grows linearly with d, the time complexity of the models grows quadratic\nwithd(seeTable6.1). InsomeNLPtasks,itisfoundempiricallythatthetrainingeffortthat\nweneedtoobtainsatisfactoryperformanceisevenanexponentialfunctionofthemodelsize\n[Kaplanetal.,2020]. Theseresultssuggestwaystoimprovetheefficiencyoftrainingwhen\nweenlarged.\nOnesimplemethodistoincrementallygrowthemodelalongthedimensionofd,rather\nthantrainingthemodelfromscratch. Supposewehaveaninitialmodelinvolvingad ×d\n1 1\nparametermatrixW , forexample, thelineartransformationofeachqueryorkeyinsome\n1",
    "layer. We can train this model to obtain optimized W in a regular way. Then, we want to\n1\nextendthismodeltoawidermodelwhereW isreplacedbyad ×d parametermatrixW .\n1 2 2 2",
    "6.3 Improved Architectures 309\nLetusassumeforsimplicitythatd =kd . Thereareseveralwaystoexpandad ×d matrix\n2 1 1 1\ntoakd ×kd matrix. ThesimplestofthesemaybetouseW tofillW . WecanwriteW\n1 1 1 2 2\nintheform\nk times\n \nW1 ··· W1\nρ ρ\nW 2 =   . . . . . .  \n \nW1 ··· W1\nρ ρ\nsemit\nk\n(6.111)\nwhereρisahyper-parameterthatisusedtocontrolthenormofW . Forexample,ifρ=k,\n2\nW will have thesame l norm asW . The above equation providesa goodstarting point\n2 1 1\nfortrainingthewidemodel,andwecantrainW asusualafterinitialization. Theprocedure\n2\ncanberepeatedanumberoftimesforconstructingamodelwitharbitrarywidth. Boththis\nmethodandthedepthgrowthmethoddescribedinSection6.3.2areinstancesofthegeneral\nmethodofmodelgrowth. Inotherwords,wecanobtainalargermodelbyextendingasmall\nmodeleitherverticallyorhorizontally,orboth. AlternativemethodsfortransformingW to\n2\nW involvethoseconsideringothermathematicalpropertiesofthetransformation[Chenetal.,\n1",
    "2015]. Thesemodelscanfallunderthereusableneuralnetworkswhereweareconcernedwith\nmodelsandalgorithmsfortransferringparametersfromsmallmodelsto(significantly)larger\nmodels[Wangetal.,2023b].\nAseconddifficultyinbuildingawideTransformermodelisthelargememoryrequirement.\nSincethefeedforwardnetworkgenerallyhasalargerhiddenlayerthanotherpartsofthemodel,\nitdemandsrelativelymorememoryasthemodelbecomeswider. Considerthefeedforward\nnetworkdescribedinSection6.1.5\nH = FFN(H )\nout in\n= ReLU(H ·W +b )·W +b (6.112)\nin h h f f\nwhereW h ∈Rd×d ffn andW f ∈Rd ffn ×d aretheparametersofthelineartransformations. d ffn\nistypicallyseveraltimeslargerthand. Therefore,W andW willoccupythemodelifd\nh f\nandd haveverylargevalues.\nffn\nInsomecases,thesizeofthefeedforwardnetworkmayexceedthememorycapacityofa\nsingledevice. Thisproblemcanbeaddressedbyusingthemixture-of-experts(MoE)models\n[Shazeeretal.,2017]. AnMoEmodelconsistsofM expertmodels{e (·),...,e (·)}. Given\n1 M",
    "an input h ∈Rd, each expert model produces an output e (h ). The output of the MoE\nin k in\nmodelisalinearcombinationof{e (h ),...,e (h )},givenby\n1 in M in\nM\n(cid:88)\nh = g (h )·e (h ) (6.113)\nout i in i in\ni=1\nwhere g(·) is a gating model (also called routing model). Its output is a vector g(h )=\nin\n(cid:104) (cid:105)\ng (h ) ... g (h ) inwhicheachentryg (h )indicatestheweightofthecorresponding\n1 in M in i in",
    "310 Chapter 6. Transformers\nexpertmodel. Inmanyapplications,itisassumedthatg(h )isasparsevector. Thismeans\nin\nthatonlyasmallnumberofexpertmodelsareinvolvedincomputingtheoutput. Awidely-used\nformofg(h )isgivenbyusingtheSoftmaxlayer\nin\ng(h ) = Softmax(h ·W ) (6.114)\nin in g\nwhereW ∈Rd×M istheparametermatrixofthelayer. Toenforcesparsityong(h ),wecan\ng in\nsimplyselectthetop-k entriesofg(h ),thatis,wesetnon-top-k entriesto0. Analternative\nin\nmethod is to first perform top-k selection on h ·W and then normalize the top-k entries\nin g\nusingtheSoftmaxfunction.\nLet π be the set of the indices of the top-k expert models. The MoE model with top-k\nroutinghasthefollowingform\n(cid:88)\nh = g (h )·e (h ) (6.115)\nout i in i in\ni∈π\nAnadvantageofthisapproachisthatwecandistributedifferentexpertmodelstodifferent\nprocessors,makingitpossibletoexecutethesemodelsonparallelcomputingmachines. In\neachrunoftheMoEmodel,eitherduringtrainingorinference,weonlyneedtoactivateand",
    "use k expert models rather than all of the expert models. In this way, the MoE approach is\nautomaticallylearningasparsemodelbylimitingthenumberofactiveexpertmodelseach\ntime in training and inference. The sparsity is determined by the hyperparameter k, say, a\nsmallvalueofk leadstoasparsemodel,andalargevalueofk leadstoadensemodel.\nLet us return to the discussion of Eq. (6.112). It is straightforward to apply the MoE\napproach to feedforward neural networks. To simplify the discussion, consider the linear\ntransformationofthefirstlayerasshowninEq. (6.112),thatis,H ·W . Wecanapproximate\nin h\nH ·W inanMoEform\nin h\n(cid:88)\nH ·W ≈ g (H )·e (H )\nin h i in i in\ni∈π\n(cid:88)\n= g (H )·[H ·Wi] (6.116)\ni in in h\ni∈π\nHereW isdividedintoM slides(orsub-matrices){W1,...,WM},writtenas\nh h h\n(cid:104) (cid:105)\nW = W1 ... WM (6.117)\nh h h\nHence each expert model e (H )=H ·Wi solves a sub-problem of the original linear\ni in in h\nmapping, andEq. (6.116)canbethoughtofasadivide-and-conquersolutiontothematrix",
    "multiplicationproblem.\nWecan,ofcourse,treatanyfeedforwardneuralnetworkasanexpertmodel,resultingin",
    "6.3 Improved Architectures 311\n.\n.\n.\nHout\nFFNsub-layer\n(cid:80)\ni∈π\ngi(Hin)·FFNi(Hin)\nAdd&LayerNorm\nFeed-ForwardNetwork\nGating\nModel\nFFN1(·) FFN2(·) ··· FFNM(·)\n.\n. . Hin Hin Hin ··· Hin\nFigure6.12: AnillustrationoftheMoEmodelappliedtoanFFNsub-layer. ThereareM FFNs\n(call them expert models) and a gating model. Each FFN is weighted by the gating model.\nTheoutputofthemodelisthesumoftheweightedoutputsofthetop-k FFNs(denotedbyπ).\nBecausetheseFFNsworkindependentlyandcanbeplacedondifferentcomputingdevices,\nthemodelcanbeeasilyscaledupasM islarger.\nthefollowingmodel\n(cid:88)\nH = g (H )·FFN (H ) (6.118)\nout i in i in\ni∈π\nwhereFFN (·)isa“small”feedforwardneuralnetworkthathasthesameformasEq. (6.112).\ni\nThis model is illustrated with an example in Figure 6.12. In practical implementations, all\ntheseexpertmodelscanberuninparallelondifferentdevices,andsotheresultingsystemis\nefficient.\nNotethat,fromaperspectiveofmachinelearning,MoEisageneralapproachtocombining",
    "different neural networks, each of which is developed to address a different aspect of the\nproblem [Yuksel et al., 2012; Masoudnia and Ebrahimpour, 2014]. The application here is\njustaspecialinstanceofthegeneralframeworkofMoE.Theapproachisalsooftenusedto\nimprovetheoverallperformanceofpredictors,whichcanbediscussedinthefieldofensemble\nlearning[Zhou,2012a].\nAnotherdifficultyindevelopinglargeTransformermodelsisthetraininginstabilityprob-\nlem. Aswithmanyotherlargeneuralnetworks,straightforwardoptimizationofaTransformer\nmodelwithalargenumberofparametersmayleadtogettingtrappedinlocalminimums,and,\noccasionally,largespikesinthelossduringtraining[Lepikhinetal.,2021;Fedusetal.,2022b;\nChowdheryetal.,2022]. Evenwithcarefulchoicesabouthyperparameters,trainingstrategies,\nandinitialmodelparameters,westillencounterthesituationthatwehavetorestartthetraining\natsomepointinordertojumpoutofthetoughregionsinoptimization. Oneofthereasonsfor",
    "thistrainingdifficultyisthattheusualimplementationsofthelinearalgebraoperations,such\nasmatrixmultiplication,willbenumericallyunstableiftheyoperatesonverylargevectors\nandmatrices. Itisthereforepossibletoimprovethetrainingbyconsideringnumericallystable\nmethodsinstead.",
    "312 Chapter 6. Transformers\n6.4 Efficient Models\nEfficiencyisanimportantconsiderationformanypracticalapplicationsofTransformermodels.\nForexample,wemaywishtorunand/ortrainaTransformermodelgivenmemoryandtime\nconstraints. Efficiencyisnotasingleproblem,butcoversawiderangeofproblems. While\ntheseproblemscanbecategorizedinseveraldifferentways,therearetwofundamentalaspects\nonemayconsiderinanefficiencyproblem.\n• TimeandSpaceEfficiencies. Foragivenproblem,wewishthemodeltobesmalland\nfast,andmeanwhiletobeasaccurateaspossibleinsolvingtheproblem. Forexample,\ninsomemachinetranslationapplications,wemaylearnamodelwithasmallnumberof\nparameterstofitthemodeltolimitedmemory,andmaydevelopafastsearchalgorithm\ntoachievelow-latencytranslation. Apracticaldifficultyhereisthatimprovingefficiency\noftenleadstoworsepredictions. Inmanycases,weneedtoseekatrade-offbetween\nefficiencyandaccuracy.\n• Scalability. Whentheproblemisscaledup,wewishthattheadditionaleffortwemade",
    "forsolvingthisproblemisassmallaspossible. Forexample,thetrainingofaneural\nnetwork is called efficient if it takes a reasonably short time to optimize it as more\ntrainingsamplesareinvolved. Anotherexampleofefficiencyisthatusedtomeasure\ntheamountofresourcesconsumedinprocessingmoreinputs. Forexample,amachine\ntranslationsystemisinefficientintranslatinglongsentencesifthememoryfootprintand\nlatencygrowexponentiallywiththenumberofinputwords.\nIn this section, we will not discuss all the issues related to efficiency, which is a very\nbroadtopic. Weinsteadconsiderthewidely-usedefficientapproachestoTransformer-based\nsequencemodelingandgeneration,someofwhicharerefinementsofmodelarchitectures,and\nsomeofwhicharemodel-freeapproachesandcouldbeusedinothersystemsaswell. Mostof\nthediscussionsherearefocusedondevelopinglightweightandfastTransformermodelsthat\narerelativelyrobusttolonginputandoutputsequences.\nIngeneral,thesameoptimizationmethodcanbeappliedtodifferentmodulesofaTrans-",
    "formersystem. Tosimplifythediscussion,wewillmostlyconsiderself-attentionsub-layers\nandFFNsub-layersinthissection. Ourdiscussion,however,isgeneralandthemethodspre-\nsentedherecanbeappliedtootherpartsofaTransformersystem,forexample,cross-attention\nsub-layers.\n6.4.1 Sparse Attention\nInpractice,theattentionapproachesusedinTransformeraretimeconsuming,especiallywhen\nthe input sequences are long. To illustrate, consider a Transformer decoder that predicts a\ndistribution of words at a time given the previous words. Suppose the sequence generated\nby the decoder is of size n and the input of a self-attention sub-layer is an n×d matrix S.\nFirst,SislinearlytransformedtoobtainthequeriesSq ∈Rn×d,keysSk ∈Rn×d,andvalues\nSv ∈Rn×d. Tosimplifythenotationinthissubsection,weuseQ,KandVtorepresentSq,\nSk,andSv,respectively.",
    "6.4 Efficient Models 313\nTheoutputoftheself-attentionsub-layercanthenbecomputedusing\nAtt (S) = AV (6.119)\nself\nwhereAisann×nattentionmatrixorattentionmap\nQKT\nA = Softmax( √ +M) (6.120)\nd\nMisamaskingmatrixthatisusedtopreventthemodelfromseeingtherightcontextwords\nateachposition,thatis,forapositioni,M(i,j)=0forj ≤i,andM(i,j)=−∞otherwise.\nBoththetimeandspacecomplexitiesoftheself-attentionsub-layerarequadraticfunctionsof\nn10. Therefore,ifnislarge,themodelwouldbecomputationallyexpensive.\nTheusualimplementationoftheabovemodeldependsondensematrixcomputation,for\nexample,thedensematrixmultiplicationsinEqs. (6.119-6.120). Oneapproachtoreducing\ntheamountofmemoryandthenumberoffloating-pointcalculationsinadensecomputation\nsystemistosparsifytheproblem. Todothis,weassumethatAisasparsematrix,forexample,\nonly ϱ·n2 entries of M have non-zero values, where ϱ indicates how sparse the matrix is,\nalso called sparsity ratio. Since we only need to store these non-zero entries, the memory",
    "requirementofAcanbereducedbyusingsparsematrixrepresentations. Anotheradvantage\nofusingasparseattentionmatrixisthatthemodelsof Q √ KT andAVcanbesimplified,aswe\nd\nconsideronlya“small”numberofrelatedpositionswhenlearningarepresentation.\nGiven a position i, we define the attention field π to be the set of positions that are\ni\nconsideredincomputingtherepresentationatthisposition. Wethereforeonlyneedtocompute\nthedot-productattentionbetweenthegivenpositioniandeachpositionj ∈π . Thisresultsin\ni\nasparseattentionmatrixA′ where\n(cid:40)\nsomeweight j ∈π andj ≤i\nA′(i,j) = i (6.121)\n0 otherwise\nAsimpleimplementationofthismodelinvolvesaslightmodificationtoM,leadingtoanew\nmaskingvariableM′\n(cid:40)\n0 j ∈π andj ≤i\nM′(i,j) = i (6.122)\n−∞ otherwise\nInpracticalimplementation,amoreefficientapproachistoemploysparseoperationsforQKT\nandA′VbyconsideringM′ andA′,respectively. Thatis,wesaveoncomputationforpairs\nofpositionswhoseattentionweightsarenon-zero,andskiptherest.",
    "Thereareseveralapproachesthatwecantaketothesparsemodelingofself-attention. We\ndescribebrieflysomeofthemasfollows\n10Moreprecisely,theamountofmemoryusedbytheself-attentionfunctionisn2+n·d,andsoitwillbe\ndominatedbythequadratictermn2ifn>>d.",
    "314 Chapter 6. Transformers\n• Span-based Attention/Local Attention. As discussed in Section 6.3.1, the use of\ncontextinsequencemodelingislocalinmanycases. Thebasicideaoflocalattentionis\ntospantheattentionweightstoarestrictedregionoftheinputsequence. Wecanthen\nwriteπ as\ni\nπ = [al,ar] (6.123)\ni i i\nwhereal andar andtheleftandrightendsofπ . ar−al+1determineshowsmallthe\ni i i i i\nregionis,andsowecanuseittocontrolthesparsityoftheattentionmodel,forexample,\nifar−al+1<<n,themodelwouldbeverysparse. al andar canbeobtainedbyusing\ni i i i\neitherheuristicsormachinelearningmethods. Thereadermayrefertorelatedpapers\nformoredetails[Luongetal.,2015;Sperberetal.,2018;Yangetal.,2018a;Sukhbaatar\netal.,2019]. SeeFigure6.13(b)foranillustrationoflocalattention.\n• ChunkedAttention. Whenaproblemistoodifficulttosolve,onecantransformitinto\neasierproblemsandsolveeachofthemseparately,asisoftenthecaseinpractice. This\nmotivatesthechunkedattentionapproachinwhichwesegmentasequenceintochunks",
    "and run the attention model on each of them [Parmar et al., 2018; Qiu et al., 2020a].\nGivenasequence{1,...,n},wedefine{chunk ,...,chunk }tobeasegmentationofthe\n1 q\nsequence. Achunkcanbeexpressedasaspan\nchunk = [cl,cr] (6.124)\nk k k\nIntheattentionstep,wetreateachchunkasasequenceandperformself-attentiononit\nasusual. Inotherwords,therepresentationatpositioniiscomputedbyusingonlythe\ncontextinthechunkthatibelongsto. Inthissense,thismodelcanbethoughtofassome\nsortoflocalattentionmodel. Figure6.13(c)showsanillustrationofthismodel. There\nremainstheissueofhowtosegmentthesequence. Thereareseveralwaystodothis.\nForexample,asdiscussedinSection6.2.4,wecandosegmentationfromalinguistic\nperspective,andsegmentthesequenceintolinguisticallymotivatedunits. Inpractical\nsystems,itissometimesmoreconvenienttosegmentthesequenceintochunksthatare\nofequallength. Thus,thesparsityofthemodeliscontrolledbythesizeofthesechunks,\nforexample,theuseofsmallerchunkswouldleadtoamoresparseattentionmodel.",
    "• StridedAttention. Sincethechunkedattentionapproachenforcesahardsegmentation\non the input sequence, it may lose the ability to learn representations from inputs in\ndifferentchunks. Analternativewaytoachievechunk-wiseattentionistoallowoverlap\nbetween chunks [Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020]. This\napproachisanalogoustothefamilyofapproachesthatarecommonlyusedtoapplya\nlocalmodelto1Dor2Ddatatogenerateoutputsofthesameshape. LikeCNNs,we\nuseacontextwindowtorepresentthefieldofinputoftheattentionmodel. Thecontext\nwindowslidesalongthesequence,eachtimemovingforwardastepofsizestride. Asa\nspecialcase,ifstrideequalsthesizeofthecontextwindow,thismodelisthesameas\nthechunkedattentionmodelmentionedabove. Ifstridechoosesavaluesmallerthan",
    "6.4 Efficient Models 315\nthe size of the context window, the attention model will become denser. Figure 6.13\n(d)showsthecaseofstrdie=1wherethechunkoverlappingismaximized. Awayto\nachieverelativelysparserattentionistouseadilatedcontextwindow. Figure6.13(e)\nshowsanexampleofthedilatedstridedattentionmodel,wherethecontextwindowis\ndiscontinuous,withgapsofsize1.\n• Learning Attention Fields. Because the attention field π can be any sub-set of\ni\n{1,...,n},wecandevelopmoregeneralsparseattentionmodelsbyconsideringattention\nmaps beyond chunk-based patterns. The only question is how to determine which\npositions the model attends to for a given position. One simple approach is to use a\ncomputationally cheaper model to estimate the “importance” of each position. Then,\nattention weights are computed only for some of the positions which are thought to\nbemostimportant[Zhouetal.,2021]. Asecondapproachisgrouping: positionsare",
    "grouped, and then the attention weights are computed only for positions in the same\ngroup. It is often relatively easy to achieve this by running clustering algorithms on\nkeysandqueries. Forexample,wecanclusterkeysandqueriesviak-meansclustering.\nThe centroids of the clusters can be treated as additional parameters of the attention\nmodel, and so can be learned during optimization [Roy et al., 2021]. One benefit of\nlearning attention fields is that the model can spread its attention broader over the\nsequence. ThisisausefulpropertyformanyNLPproblemsbecauseworddependencies\naresometimeslong-range,notrestrictedtoalocalcontextwindow. SeeFigure6.13(f)\nforanexampleoftheattentionmaplearnedthroughthismodel. Alternativeapproaches\nto learning to attend are to use sorting or hashing functions to group similar key and\nquery vectors [Kitaev et al., 2020; Tay et al., 2020a]. These functions can be either\nheuristicallydesignedfunctionsorneuralnetworkswithlearnableparameters. Byusing",
    "thesefunctions,wecanreorderthesequencesothattheinputsinthesamegroupare\nadjacentinthereorderedsequence. Inthisway,theresultingattentionmapfollowsa\nchunk-wisepattern,andthemodeliscomputationallyefficientthroughtheuseofthe\nchunkedattentionapproach.\n• HybridMethods. Above,wehavediscussedarangeofdifferentsparseattentionmodels.\nIt is natural to explore methods that combine multiple models together to make use\nof their benefits in some way. A simple way to do this is to combine the attention\nfieldsofdifferentmodels. Forexample,inZaheeretal.[2020]’ssystem,theattention\nmapisgeneratedbyconsideringthreedifferentsparsemodels,includinglocalattention\n(chunked attention), global attention, and random attention11. The resulting model\nis still a sparse model, but is somewhat more robust as it involves multiple patterns\nfromdifferentperspectivesofattentionmodeling. Anotherwayofcombiningmultiple\nattentionmodelsistousedifferentmodelsfordifferentheadsinmulti-headattention",
    "[Childetal.,2019;Beltagyetal.,2020]. Forexample,onecanuseoneheadasalocal\nattentionmodel,anduseanotherheadasaglobalattentionmodel(seeFigure6.13(g-h)).\n11Here the global attention model attends each word only to a special word which accounts for the entire\nsequenceandisoftenplacedatthebeginningofthesequence.Therandomattentionmodelattendseachwordtoa\nrandomsetofthewordsofthesequence.",
    "316 Chapter 6. Transformers\n(a)StandardAttention (b)Span-basedAttention\n(c)ChunkedAttention (d)StridedAttention\n(e)DilatedStridedAttention (f)LearningAttentionFields\n(g)GlobalAttention (h)HybridMethods\nFigure6.13: Illustrationoftheattentionmapsofdifferentmodels(self-attentiononthedecoder\nside). DarkcellsmeanA′(i,j)̸=0(i.e.,iattendstoj),andlightcellsmeanA′(i,j)=0(i.e.,\nidoesnotattendtoj). Inalltheseattentionmaps,weassumethateverypositionattendsto\nitselfbydefault(seediagonals).",
    "6.4 Efficient Models 317\nOnedisadvantageofsparsemodelscomparedtodensemodelsisthattheyarenotcom-\nputationally efficient on GPUs or CPUs. While sparse models can ideally reduce both the\nmemoryandcomputationrequirements,theactualrateatwhichworkcanbedonebysparse\nmodels is much slower than by dense models. In practice, it is difficult for sparse models\nto approach the peak FLOPS of a GPU or CPU12. Therefore, they are often used for the\npurposeofhighmemoryefficiency, notreallyforthepurposeofefficientcomputation. On\nthe other hand, sparse models are still of great use to NLP practitioners in the context of\nmemory-efficient Transformer, especially when Transformer systems are used to deal with\nextremelylongsequences.\n6.4.2 Recurrent and Memory Models\nFor sequence generation problems, Transformer can also be thought of as a memory sys-\ntem. Consider again the general setting, in which we are given the states of previous i−1",
    "positions, and we wish to predict the next state. In self-attention, this is done by using\nthe query at position i (i.e., q ) to access the key-value pairs of the previous positions (i.e.,\ni\n{(k ,v ),...,(k ,v )}). Then,wemovetopositioni+1,andadd(k ,v )tothecollec-\n1 1 i−1 i−1 i i\ntionofkey-valuepairs. Thisprocedurecanbeinterpretedintermsofthememorymechanism\n(seeChapter4). TheTransformermodelmaintainsamemorythatretainstheinformationof\nthepast. Whenmovingalongthesequence,werepeatthesameoperation,eachtimegenerating\nsomeoutputbyreadingthememory,andthenupdatingthememorysothatnewinformation\ncouldbestoredinsomeway. ThisisillustratedinFigure6.14.\n1. Cache-based Memory\nThememoryherecanbeviewedasadatastoreofvectors. Fromamachinelearningperspective,\nthis is a non-parametric model, and the cost of accessing the model grows as a longer sub-\nsequenceisobserved. Clearly,suchavariable-lengthmemorywillgenerallybeinfeasibleif",
    "themodeldealswithavery,verylongsequence. Forthemodelingproblemofarbitrarylength\nsequences,itiscommontouseafix-lengthmemoryinstead. AsinmanyNLPproblems,oneof\nthesimplestwaystodothisistoconsideracachesavingrecentinformation,thatis,werestrict\nthemodelingtoacontextwindow. Letn bethesizeofthecontextwindow. Themodelkeeps\nc\ntrack of the n −1 latest states to the current position, so that its closest successors can be\nc\nconsideredateachstep. Thismeansthat,foreachposition,aself-attentionsub-layerattendsto\nn −1positionsahead,likethis\nc\nOutput\nInput\ni−5 i−4 i−3 i−2 i−1 i\nIfwestackmultipleself-attentionsub-layers,alargercontextwindowwouldbeconsidered.\n12FLOPS=floatingpointoperationspersecond.",
    "318 Chapter 6. Transformers\nRead(self-attention)\nMemory({1,...,i−1}) State\nUpdate\n1 2 3 ... i−2 i−1 i i+1\nPositioni\nRead(self-attention)\nMemory({1,...,i−1,i}) State\nUpdate\n1 2 3 ... i−2 i−1 i i+1\nPositioni+1\nFigure6.14: Transformerasamemorysystem. Atpositioni,thecollectionofthekey-value\npairsofpositions{1,...,i−1}isusedasamemoryofthepastinformation. TheTransformer\nmodel accesses this memory to generate some output, and then adds the key-value pair of\nposition i to the memory. Moving to the next position, we repeat the same procedure of\nmemoryaccessandupdate.\nFor example, a model involving two self-attention sub-layers has a context window of size\n2n −1,asfollows\nc\nOutput\nLayer2\nLayer1\ni−5 i−4 i−3 i−2 i−1 i\nTherefore, we can take a sufficiently large context by using a multi-layer Transformer\nmodel. Notethatthecontextwindowmodelhereisessentiallythesameasthestridedattention\nmodelpresentedintheprecedingsection. Systemsofthistypeareofteneasytoimplement:",
    "we slide a window along the sequence, and, in each move, we make predictions at the last\npositionofthewindow(forinference),orback-propagateerrors(fortraining).\nAnalternativewaytotrainthiscontextwindowmodelisbychunkedattention. Wedivide\nthe sequence into chunks (or sub-sequences) which are of the same length n . Then, we\nc\ntreat these chunks as individual training samples, and run the training program on each of",
    "6.4 Efficient Models 319\nthemasusual. Thisapproach,however,completelyignorestherelationshipbetweeninputsin\ndifferentchunks. Onewaytoaddressthisissueistointroducedependencebetweenchunks.\nForexample,theTransformer-XLmodelallowseverychunktoaccessoneormorepreceding\nchunks[Daietal.,2019]. Inthesimplestcase,consideranexampleinwhichchunk cansee\nk\nitssuccessorchunk . Eachpositioninchunk canattendtoallitsprecedingpositionsin\nk−1 k\nbothchunk andchunk .\nk k−1\nInTransformer-XL,thisapproachisimplementedinasimplifiedform. First,eachposition\nisconstrainedtoattendton −1previouspositionssothatthesizeoftheattentionfieldofa\nc\npositionisthesameinthetrainingandinferencestages. Suchamethodturnstheproblemback\ntostridedattention,makingtheimplementationoftheattentionmodelstraightforward. Onthe\notherhand,thedifferencebetweenthestandardstridedattentionmodelandtheTransformer-XL\nmodelisthatinTransformer-XL,weperformtraininginachunk-wisemanner. Oncewefinish",
    "thetrainingonachunk,wedirectlymovetothenextchunk,ratherthanslidingthecontext\nwindowasmallstepforward. Second,whilethisapproachallowsforconnectionsbetween\nchunks, the parameters of the sub-network on chunk are fixed, and we only update the\nk−1\nparametersofthesub-networkonchunk inthek-thstep. SeeFigure6.15foranillustration.\nk\nTheabovemodelissimilarinspirittorecurrentmodelsbecauseallofthemrequirethe\ncomputationinonesteptodependonthestatesoftheprecedingsteps. However,itisnotin\nthestandardformofarecurrentmodel,inwhichtheoutputofarecurrentunitinonestepis\nthe input in the next step. Instead, the “recurrence” is expressed by involving connections\nbetweentwodifferentlayers,thatis,theoutputofonelayerinchunk isusedastheinput\nk−1\nofahigher-levellayerinchunk .\nk\n2. Encoding Long-term Memory\nAnother idea for representing the states of a sequence is to frame the task as an encoding\nproblem. Instead of storing all the key-value vectors during left-to-right generation, we",
    "constructthememoryoftheentire“history”asafixednumberofencodedkey-valuevectors.\nTheseencodedkey-valuevectorscanbeeitherasmallsub-setof{(k ,v ),...,(k ,v )}\n1 1 i−1 i−1\norasmallsetofnewly-generatedvectorsthatencodes{(k ,v ),...,(k ,v )}.\n1 1 i−1 i−1\nOnewaytodotheencodingistoapplyapoolingoperationto{(k ,v ),...,(k ,v )}\n1 1 i−1 i−1\n[Rae et al., 2019a]. For example, by using average pooling, the memory contains only one\nkey-valuepair(k¯,v¯)\ni−1\n1 (cid:88)\nk¯ = k (6.125)\nj\ni−1\nj=1\ni−1\n1 (cid:88)\nv¯ = v (6.126)\nj\ni−1\nj=1\nThisleadstoaveryefficientmodel,andweonlyneedtoupdatethevectors(k¯,v¯)atatime\n[Zhangetal.,2018a].\nLet(k¯[i],v¯[i])bethestateofthememoryatpositioni.\nAmoregeneral",
    "320 Chapter 6. Transformers\nchunk chunk chunk\nk−1 k k+1\nOutput\nLayer2\nLayer1\ni−5 i−4 i−3 i−2 i−1 i i+1 i+2 i+3\n(a)Stepkofchunk-wisetraining\nchunk chunk chunk\nk−1 k k+1\nOutput\nLayer2\nLayer1\ni−5 i−4 i−3 i−2 i−1 i i+1 i+2 i+3\n(b)Stepk+1ofchunk-wisetraining\nFigure 6.15: Illustration of chunk-wise training [Dai et al., 2019]. The input sequence is\ndividedintochunksofthesamelengthn . Trainingisperformedonthesechunks,eachtime\nc\ndealingwithachunk. Inchunk ,theattentionfieldforeverypositioninthischunkisaleft\nk\ncontextwindowofsizen . Hencethismodelallowsforattentionacrosschunks,forexample,\nc\npositioni−2inchunk canattendtopositionsi−3andi−4inchunk (seesub-figure\nk k−1\n(a)). For training, errors are back-propagated only in the sub-network for chunk , leaving\nk\notherpartsofthemodelunchanged. Hereweusedashedlinestodenoteinformationflowthat\nweconsiderintheforwardpassbutnotinthebackwardpass. Oncewefinishthetrainingon\nchunk ,wemovetothenextchunk,andrepeatthesametrainingprocedure.\nk",
    "definitionof(k¯[i],v¯[i])isgiveninarecursiveform\nk¯[i] = KMem(k¯[i−1],k ) (6.127)\ni−1\nv¯[i] = VMem(v¯[i−1],v ) (6.128)\ni−1\nwhereKMem(·)andVMem(·)arefunctionsthatupdatethememorybytakingboththestates\nofthememoryatthepreviousposition(i.e.,k¯[i−1]andv¯[i−1])andthenewstates(i.e.,k\ni−1\nandv ). TherearemanyformsofthefunctionslikeKMem(·)andVMem(·)incommon\ni−1\nuse. Forexample,ifKMem(·)andVMem(·)areweightedsumfunctions,wecanderivethe\nsame forms as Eqs. (6.125) and (6.126). If KMem(·) and VMem(·) are recurrent cells in",
    "6.4 Efficient Models 321\nRNNsorLSTM,weobtainarecurrentmodelofmemory.\nExtension of the above model to memories having more than one key-value pair is\nstraightforward. One approach is to use the memory to represent sub-sequences. Let\n{(k¯ ,v¯ ),...,(k¯ ,v¯ )} be a memory of size κ. Each (k¯ ,v¯ ) is a snapshot of a chunk of\n1 1 κ κ j j\nlengthn . Thus,thismemorycanencodeasequencewithmaximumlengthκ·n . Then,we\nc c\ncancompute(k¯ ,v¯ )onthecorrespondingchunkusingEqs. (6.127)and(6.128). Asecond\nj j\napproachistoorganize{(k¯ ,v¯ ),...,(k¯ ,v¯ )}intoapriorityqueue. Wedesignsomefunction\n1 1 κ κ\nto assign a score to any given key-value pair. The key-value pair can be inserted into the\npriorityqueuethroughthepushoperation. Ideally,wewishtodevelopascoringfunctionto\nestimatethevalueofakey-valuepair,forexample,weuseanotherneuralnetworktoevaluate\nthekey-valuepair. Inthisway,thememoryisacollectionofthemostvaluablekey-valuepairs\novertheinputsequence.",
    "Althoughrepresentingthememoryasasetofvectorsisanobviouschoiceforthemodel\ndesigninTransformer,thememoryisdiscreteanditscapacityisdeterminedbythenumber\nofthevectors. Analternativeformofmemoryiscontinuousmemory. Thistypeofmodel\ntypicallybuildsontheideaoffunctionapproximation,inwhich{k ,...,k }or{v ,...,v }\n1 i−1 1 i−1\nisviewedasaseriesofdatapoints,andacontinuousfunctionisdevelopedtofitthesedata\npoints. Then, we no longer need to store {k ,...,k } and {v ,...,v }. Instead, the\n1 i−1 1 i−1\nmemoryisrepresentedbythefunctionsfittingthesevectors. Asimplemethodistocombine\nsimplefunctionstofitcomplexcurvesofdatapoints. Forexample,wecandevelopasetof\nbasisfunctionsandusealinearcombinationofthemtoapproximatethekeyorvaluevectors\n[Martinsetal.,2022]. Theresultingmodelisparameterizedbythesebasisfunctionsandthe\ncorrespondingweightsinthecombination.\nItisalsostraightforwardtouseashort-termmemoryandalong-termmemorysimultane-\nouslysothatwecancombinethemeritsofboth. Forexample,weuseacache-basedmemory",
    "tocapturelocalcontext,anduseanefficientlong-termmemorythatencodestheentirehistory\ntomodellong-rangedependency. Thisideaisalsosimilartothatusedincombiningdifferent\nsparseattentionmodelsasdiscussedintheprevioussubsection.\n3. Retrieval-based Methods\nSofarinthissubsection,wehavediscussedapproachesbasedonfixed-lengthmodels. Itis\nalsopossibletodevelopefficientmemorymodelsbyimprovingtheefficiencyofaccessingthe\nmemories,insteadofjustreducingthememorycapacities. Onewaytoachievethisistostore\nthepastkey-valuepairsinadatabase(callitavectordatabase),andtofindthemostsimilar\noneswhenqueryingthedatabase. Tobemoreprecise,givenaqueryq,weusethedatabaseto\nfindasetoftop-prelevantkey-valuepairs(denotedbyΩ )byperformingsimilaritysearch\np\nbasedonthedot-productsimilaritymeasurebetweenqueryandkeyvectors. Then,weattend\nqtoΩ asinstandardself-attentionmodels. Theideabehindthismethodistoconsideronlya\np\nsmallnumberofelementsthatcontributemosttotheattentionresult. Therefore,themodelis",
    "essentiallyasparseattentionmodelwhichiscomputationallyefficient. Anotheradvantageof\nthismethodisthatitallowsforfastsimilaritysearchoveraverylargesetofvectorsbecauseof\nthehighlyoptimizedimplementationofvectordatabases. Buildingamemoryasaretrieval",
    "322 Chapter 6. Transformers\nsystemcanfallunderthegeneralframeworkcalledtheretrieval-augmentedapproach. It\nprovidesasimplewaytoincorporateexternalmemoriesintoneuralmodelslikeTransformer\n[Guuetal.,2020;Lewisetal.,2020b;Wuetal.,2021].\n6.4.3 Low-dimensional Models\nInmanypracticalapplications,Transformermodelsare“high-dimensional”models. Thisis\nnotonlybecausetheinputand/oroutputdataisinhigh-dimensionalspaces,butalsobecause\nsomeoftheintermediaterepresentationsofthedatainthemodelarehigh-dimensional. As\ndiscussedinSection6.4.1,thishighdimensionalityarisesinpartfromthestepsofcomputing\ntheattentionmatrixasinEq. (6.119)(foreaseofpresentation,werepeattheequationhere)\nAtt (S) = AV (6.129)\nself\nandtheweightedsumofvaluevectorsasinEq. (6.120)\nQKT\nA = Softmax( √ +M) (6.130)\nd\nwhichinvolveslargematrixmultiplicationsQKT andAVwhenthelengthnandthehidden\ndimensionalitydhavelargevalues.\nTheAVandQKT operationshaveatimecomplexityofO(n2·d)andaspacecomplexity",
    "ofO(n2+n·d). Severalpreviouslydescribedapproacheshavereducedthiscomplexityby\nusingsparsemodels. Inthissubsection,wefocusonmethodsthatapproximatetheseoperations\nviadensecomputation. OnesimpleideaistotransformQ,K,andVintosmallermatrices,\nandthustoreducethecomputationalburdenofmatrixmultiplication. SinceQ,K,andVare\nall in Rn×d, we can achieve this by reducing either the n dimension or the d dimension, or\nboth.\n1. Reducing n\nNotethattheoutputAtt (S)isrequiredtobeann×dmatrix,andsowecannotreducethe\nself\nnumberofqueries. Weinsteadconsiderreducingthenumberofkeysandvalues. Supposen′\nisanumberlessthann,andKandVcanbetransformedinton′×dmatricesK′ andV′ in\nsomeway. Wecanobtaina“smaller”modelsimplybyreplacingKandVwithK′ andV′ ,\ngiving\n′\nAtt (S) = AV (6.131)\nself\nQ[K′]T\nA = Softmax( √ +M) (6.132)\nd\nThismodelisinthestandardformofself-attention,buthaslowertimeandspacecomplexities,\nthatis,O(n′·n·d)<O(n2·d)andO(n′·n+n′·d)<O(n2+n·d). Ifn′<<n,theresulting\nmodelwillbelinearwithn.",
    "The key problem here is how to obtain K′ and V′ in a way that retains much of the",
    "6.4 Efficient Models 323\ninformationinKandV. Thereareseveralwaystodoso. Onesimplemethodistoselectthe\nkeysandvaluesthatarethoughttobeimportant. Theimportanceofakey(orvalue)canbe\ncomputedintermsofsomecomputationallycheapmeasure. Forexample,wecansamplea\nsmallnumberofquery-keydot-productsandestimatetheimportanceofakeybycollecting\nthesedot-productresults.\nTheabovemethodisstraightforwardbutstillrequiressparseoperations,suchassampling\nand collection. As an alternative, we can use dense computation to transform K and V to\nK′ and V′ . A typical choice is to use CNNs [Liu et al., 2018]. Let Conv(·) be a function\ndescribingasetoffiltersthatslidealongthendimension. K′ isthengivenby\nK′ = Conv(K,W ,size ,stride) (6.133)\nc r\nwhere W is the parameter matrix of the filters, size is the size of the receptive field, and\nc r\nstride is the number of units the filters are translated at a time. In general, we can achieve\na high compression rate by choosing large values for size and stride. Likewise, we can\nr",
    "computeV′ usinganotherconvolutionalfunction. Itisworthnotingthat,iftheparametern′\nis fixed for all samples, compression of K and V along the length dimension is essentially\nthe same as the fixed-length memory model as described in the preceding subsection. The\nmethodspresentedherearemoregeneralandcouldbeappliedtovariable-lengthmemories.\nWe might also be tempted to model the attention function by considering the attention\nmatrixAasahigh-dimensionalrepresentationofdataandthenapplyingconventionaldimen-\nsionalityreductionmethods. Formanyproblems,itisfoundthatA(ormorepreciselyQKT)\nisalow-rankmatrix. Inthiscase,wecancompressAwhileretainingasmuchinformation\naspossible. Therearemanywaystodoso. Forexample,wemightuseaproductofsmaller\nmatrices as an approximation to A via the SVD technique (see Chapter 3). However, this\nintroducescomputationaloverheadinusingSVDcomparedwiththestandardattentionmodel.\nAsimplerideatodirectlytransformKandVintosmaller-sizedmatricesvialinearmappings,\ngivenby",
    "K′ = UkK (6.134)\nV ′ = UvV (6.135)\nwhere Uk ∈Rn′×n and Uv ∈Rn′×n are parameter matrices. Clearly, this leads to a model\nwhich is equivalent to that described in Eqs. (6.131) and (6.132). While such a method is\nintuitiveandsimple,itisproventoobtainasufficientlysmallapproximationerrorϵifn′ isa\nlinearfunctionofd/ϵ2 [Wangetal.,2020b].\n2. Reducing d\nAnotherapproachtoworkinginalow-dimensionalspaceistoreducetheddimension. Oneof\nthesimplestmethodsistoprojectallqueriesandkeysontoad′-dimensionalspace(d′<d),\nandtocomputethedot-productofanykey-valuepairinthenewspace. Formodeling,weonly\nneedtoreplaceQ∈Rn×dandK∈Rn×dbynewrepresentationsQ′∈Rn×d′ andK′∈Rn×d′\n.",
    "324 Chapter 6. Transformers\nWecaneasilymodifyEq. (6.130)touseQ′ andK′ incomputingtheattentionmatrix\nQ′[K′]T\nA = Softmax( √ +M) (6.136)\nd\nQ′ andK′ aregivenby\nQ′ = QUq (6.137)\nK′ = KUk (6.138)\nwhereUq ∈Rd×d′ andUk ∈Rd×d′ areparametermatricesoflineartransformations.\nItisalsopossibletoexploitkernelmethodstoobtainanefficientdot-productattention\nmodel. The basic idea is to map all data points (represented as vectors) from one space to\nanotherspace,sothattheproblem,whichmightbedifficulttosolveintheoriginalspace,is\neasiertosolveinthenewspace. The“trick”ofkernelmethodsisthatweactuallydonotneed\ntoknowthemappingfunction,butonlyneedtoknowhowtocomputetheinnerproductof\nvectors in the new space in one operation13. This operation of the inner product is usually\ncalledthekernelanddenotedbyK(·,·).\nItisinterestingtoapproximateAinafashionanalogoustoK(·,·)inkernelmethods. To\nillustrate,noteinEq. (6.130)Aisafractiondenotingthenormalizedattentionweights. The\nnumeratorcanbewrittenintheform\nQKT",
    "A(cid:101) = Mask(exp( √ )) (6.140)\nd\nHereMask(·)isafunctionwhichhasthesameeffectasusingtheadditivemaskingvariable\nM. Then,Acanbeexpressedas\nA = D−1A(cid:101) (6.141)\nwhereDisann×ndiagonalmatrix. Eachentryofthemaindiagonalisthesumoftheentries\nofthecorrespondingrowinA(cid:101),denotingthenormalizationfactorofSoftmax. Substitutingthis\nequationintoEq. (6.130),wehave\nAtt\nself\n(S) = D−1A(cid:101)V (6.142)\nInthismodel,A(cid:101)(i,j)canbeviewedasasimilarityfunctionoverallquery-keypairsina\n13Inmathematicalanalysis,theinnerproductisageneralizednotionofthedot-product.Itistypicallydenoted\nby⟨·,·⟩.Aformaldefinitionoftheinnerproductrequiresthat⟨·,·⟩satisfiesseveralpropertiesinavectorspace.\nAlthoughtheinnerproducthasdifferentformsindifferentcontexts,intheEuclideanspaceRd,itisthesamething\nasthedot-product,thatis,giventwovectorsa∈Rdandb∈Rd,wehave\n⟨a,b⟩ = a·b\nd\n(cid:88)\n= a i ·b i (6.139)\ni=1",
    "6.4 Efficient Models 325\nd-dimensionalspace. Hereweassumethatthisfunction,whichisintheformofthedot-product\nofvectors,canbeapproximatedbyakernelfunction\nA(cid:101)(i,j) = K(q\ni\n,k\nj\n)\n= ⟨ϕ(q ),ϕ(k )⟩\ni j\nϕ(·)isamappingfromRd toRd′ . Wecanrepresentthequeriesandkeysinthefollowingform\nQ′ = ϕ(Q)\n \nϕ(q )\n1\n.\n=   . .   (6.143)\nϕ(q )\nn\nK′ = ϕ(K)\n \nϕ(k )\n1\n.\n=   . .   (6.144)\nϕ(k )\nn\nThen,wedevelopakernelizedattentionmodelbyapproximatingtheattentionweightα\ni,j\nintheform\nϕ(q )ϕ(k )T\ni j\nα ≈ (6.145)\ni,j (cid:80)n ϕ(q )ϕ(k )T\nj′=1 i j′\nThekeyideabehindthiskernelizedattentionmodelisthatwecanremovetheSoftmax\nfunctionifthequeriesandkeysaremappedtoanewspace. Usingthisapproximation,thei-th\noutputvectoroftheattentionmodel(i.e.,thei-throwvectorofAtt (S))isgivenby\nself\nn\n(cid:88)\nc = α ·v\ni i,j j\nj=1\n(cid:88) n (cid:16) ϕ(q i )ϕ(k j )T (cid:17)\n≈ ·v\n(cid:80)n ϕ(q )ϕ(k )T j\nj=1\nj′=1 i j′\n(cid:80)n ϕ(q )ϕ(k )Tv\nj=1 i j j\n=\n(cid:80)n ϕ(q )ϕ(k )T\nj′=1 i j′\nϕ(q )( (cid:80)n ϕ(k )Tv )\ni j=1 j j",
    "= (6.146)\nϕ(q )( (cid:80)n ϕ(k )T)\ni j′=1 j′\nAlthoughtheequationappearsabitcomplicated,theideaissimple: insteadofattendingthe\nquerytoallkeystoobtaintheattentionweightα ,wecancomputethesumofthemultiplica-\ni,j\ntions (cid:80)n ϕ(k )Tv ∈Rd′×dandthenmultiplyitwiththekernelizedqueryϕ(q ). Returning\nj=1 j j i\ntothenotationusedinEq. (6.142),wedefinethei-thentryofDtobeϕ(q ) (cid:80)n ϕ(k )T.\ni j′=1 j′",
    "326 Chapter 6. Transformers\nThen,theattentionmodelcanbere-expressedintheform\nAtt (S) = D−1ϕ(Q)ϕ(K)TV\nself\n=\nD−1Q′K′T\nV\n=\nD−1(cid:0) Q′(K′T\nV)\n(cid:1)\n(6.147)\nHerewechangetheorderofcomputationfromleft-to-righttoright-to-leftusingparentheses.\nGiven that Q′ ∈ Rn×d′ and K′ ∈ Rn×d′ , this model has time and space complexities of\nO(n·d·d′) and O(n·d+n·d′+d·d′), respectively. Therefore, the model is linear with\nrespecttothesequencelengthn,andissometimescalledthelinearattentionmodel. One\ncomputationaladvantageofthismodelisthatweneedonlycomputethemultiplicationK′TV\n(i.e., (cid:80)n ϕ(k )Tv )andthecorrespondingnormalizationfactor(i.e., (cid:80)n ϕ(k )T)once.\nj=1 j j j′=1 j′\nTheresultscanthenbeusedforanyquery[Katharopoulosetal.,2020]. Thememoryneeds\nto maintain (cid:80)n ϕ(k )Tv and (cid:80)n ϕ(k )T and update them when new key and value\nj=1 j j j′=1 j′\nvectorscome.\nStill, there are several problems regarding this kernelized model, for example, how to",
    "developthefeaturemapϕ(·)toobtainagoodapproximationtothestandardattentionmodel.\nInterestedreadersmayrefertoChoromanskietal.[2020]’sworkformoredetails.\nA second idea for reducing d is to take sub-space models, in which a problem in a d-\ndimensional space is transformed into sub-problems in lower-dimensional spaces, and the\nsolutiontotheoriginalproblemisapproximatedbysomecombinationofthesolutionstothese\nsub-problems. In a general sub-space model, a d-dimensional key vector k can be mapped\nintoasetofd′-dimensionalvectors{K′ ,...,K′ }. Tosimplifymodeling,wecandothisby\n1 η\nvectorsegmentation,thatis,wesegmentkintoη sub-vectors,eachhavingd′= d dimensions.\nη\nWecantransformallqueryandvaluevectorsinthesameway. Then,theattentionmodelis\nappliedineachofthesesub-spaces.\nThismethod,however,doesnotreducethetotalamountofcomputation. Aspresentedin\nLampleetal.[2019]’swork,wecaninsteadapproximatethedot-productattentionoveraset",
    "ofkey-valuepairsbyconsideringtop-pcandidatesineachsub-space. Moreprecisely,wefind\np-best key-value pairs in each sub-space, which is computationally cheaper. The Cartesian\nproductofthesep-bestkeysetsconsistsofpη productkeys. Likewise,weobtainpη product\nvalues. Theremainingworkissimple: thed-dimensionalqueriesattendtothesed-dimensional\nproduct keys and values. An interesting difference between this sub-space model and the\nd-dimensionalspacemodelisthatthegeneratedproductkeysandvaluesmaybedifferentfrom\nanyoftheoriginalkey-values{(k ,v ),...,(k ,v )}. Thisprovidesawayforlearning\n1 1 i−1 i−1\nnewrepresentationsofthepastinformation.\nSofarwehavediscussedapproachestodimensionalityreductionalongeitherthenord\ndimension. Itisstraightforwardtocombinethemtodevelopa“lower-dimensional”model. As\nanexample,supposethatwehavethen→n′ reductionforkeysandvalues,andthed→d′",
    "6.4 Efficient Models 327\nreductionforqueriesandkeys. Themodeltakestheform\n′\nAtt (S) = AV\nself\nQ′K′T\nA = Softmax( √ +M) (6.148)\nd′\nwhere Q′ ∈Rn×d′ , K′ ∈Rn′×d′ , and V′ ∈Rn′×d′ are low-dimensional representations for\nqueries,keysandvalues. Asusual,wecaneasilyobtaintheserepresentationsthroughthelinear\nmappingsofQ,KandV. ThetimeandspacecomplexitiesofthismodelareO(n′·n·d′)and\nO(n′·n+n′·d′).\n6.4.4 Parameter and Activation Sharing\nRedundancy is common to most large-scale neural networks. As a result, many of these\nmodelsareover-parameterized,makingthetrainingandinferencelessefficient. Onecommon\napproachtoredundancyreductionistosimplifythemodelingbyremovinguselesscomponents\nofthemodels,forexample,wecaneitherpruneacomplexmodelorsharesub-modelsamong\ndifferentcomponentsofittoobtainareasonablysmallmodel. Inthissubsection,wediscuss\nmethods of parameter and intermediate state sharing in Transformer models. We leave the\ndiscussionofmodeltransferandpruningtoSection6.4.7.",
    "Shared-parameterarchitecturesarewidelyusedinneuralnetwork-basedsystems. Well-\nknownexamplesincludeCNNsandRNNs, wherethesamesetofparameters(orlayers)is\nappliedacrossdifferentregionsoftheinput. Thisproducesa“big”neuralnetwork,partsof\nwhichhavethesamearchitectureandthesamesharedparameters. ForTransformersaswellas\nothersequencemodels,thesharingmechanismcanbeappliedtodifferentlevelsofmodeling.\nAsimpleexample,whichmightbenotrelatedtoarchitecturedesign,issharedembedding. In\nmachinetranslation,atypicalstrategyfordealingwithwordsintwolanguagesistodevelop\ntwo separate embedding models. Alternatively, one can use a single embedding model for\nbothlanguages. Theparametersofthemodelarethenlearnedduringthetrainingofboththe\nsource-sideandtarget-sidenetworks. Suchastrategyisalsooftenadoptedinmulti-lingual\nsequencemodels,suchaslanguagemodelsthatareabletodealwithtextsinmanydifferent\nlanguages.\nFormulti-layerneuralnetworks,apopularmethodislayer-wisesharing. Supposethereis",
    "astackoflayers,allofwhichhavethesameform\nSl = Layer(Sl−1;θl) (6.149)\nWecantietheparametersforsomeoralloftheselayers. Forexample,givenasetoflayers\n{l\n1\n,l\n2\n,...,l\nn\n},weenforcetheconstraintθl1 =θl2 =...=θln,sothatwecanobtainasmaller\nmodelandtheoptimizationofthemodelcanbeeasier. Inpractice,thisshared-layermodelis\nhighlyadvantageousifmanylayersareinvolved,becausewecanrepeatthesameprocessmany\ntimestoconstructaverydeepneuralnetwork[Dehghanietal.,2018]. Forexample,sharinga\nsingleFFNsub-layeracrosstheTransformerencoderisfoundtobeeffectiveinreducingthe\nredundancyinmachinetranslationsystems[Piresetal.,2023].",
    "328 Chapter 6. Transformers\nForTransformers,sharingcanalsobeperformedinmulti-headattention. Anexampleof\nthisismulti-queryattention[Shazeer,2019]. RecallfromSection6.1.3thattheoutputofa\nheadhinstandardmulti-headself-attentioncanbewrittenas\nChead = Att (Sq,Sk,Sv)\nh qkv h h h\n= Att (SWq,SWk,SWv) (6.150)\nqkv h h h\nHere Sq = SWq , Sk = SWk, and Sv = SWv are the query, key, and value, which are\nh h h h h v\nobtainedbylinearlytransformingtheinputSwithdistinctparametermatricesWq ,Wk,and\nh h\nWv. Inmulti-queryattention,wesharethesamekeyandvalueacrossalltheheads,butuse\nh\ndifferentqueriesfordifferentheads. Theformofthismodelisgivenby\nChead = Att (SWq,SWk,SWv) (6.151)\nh qkv h 0 0\nHerethekeySWk andvalueSWv areirrelevanttoh. Henceweneedonlycomputethem\n0 0\nonceratherthancomputingthemseveraltimes. Asaresult,wecanmakeasignificantsaving\nincomputationalcost,especiallyifthenumberofheadsislarge. Multi-queryattentionhas\nbeensuccessfullyincorporatedintorecentlargelanguagemodels,suchasLlama2[Touvron",
    "etal.,2023b]andFalcon14.\nByextendingtheideaofsharingtomoregeneralsituations,anyintermediatestatescanbe\nsharedacrossaneuralnetwork. Forexample,reusingneuronactivationsallowsasub-modelto\nbeappliedmultipletimes. ForTransformers,sharingcanbeconsideredinsidetheprocessof\nself-attention. ItisfoundthattheattentionmapsofdifferentlayersaresimilarinsomeNLP\ntasks[Xiaoetal.,2019]. Therefore,itisreasonabletocomputetheattentionmaponlyonce\nandthenuseitinthefollowinglayers.\nIfwemakeafurthergeneralizationofthesharingmechanism,wecanviewitasaprocess\nby which we use the result produced previously rather than computing it on the fly. It is\nthus possible to reuse the information across different runs of a neural network. A related\nexampleisreversibleresidualnetworks,inwhichactivationsofonelayercanberecovered\nfrom the activations of the following layer [Gomez et al., 2017]. Hence we only keep the\noutput of the latest layer in the forward pass. Then, in the backward pass of training, we",
    "reconstruct the output of each layer from its successor. One advantage of this reversible\ntreatment is that the information produced in the forward pass is shared implicitly, and the\nmodelismemory-efficient[Kitaevetal.,2020].\n6.4.5 Alternatives to Self-Attention\nWehaveseenthattheuseofself-attentionisaprimarysourceofthelargecomputationand\nmemoryrequirementsforTransformersystems. Itisnaturaltowonderifthereareefficient\nalternativestoself-attentionmodels. HerewepresentbrieflysomeoftheTransformervariants\nin which self-attention sub-layers are not required and we instead replace them with other\ntypesofneuralnetworks.\n14https://falconllm.tii.ae/index.html",
    "6.4 Efficient Models 329\n1. CNN as A Replacement of Self-Attention\nCNNsaresimpleandwidelyusedneuralnetworks,andareconsideredaspotentialalternatives\nto self-attention models. To apply CNNs to Transformers, all we need is to construct a\nconvolutionalsub-layertoreplacetheself-attentionsub-layerinaTransformerblock. While\nafilterofCNNshasarestrictedreceptivefieldandthustakesinputsfroma“local”context\nwindow,largecontextscanbeeasilymodeledbystackingmultipleconvolutionalsub-layers.\nOne key advantage of CNNs is that the number of elementary operations required to run\nCNNs is a linear function of the sequence length n, compared with the quadratic function\nfor self-attention networks. In practical systems, there have been many highly-optimized\nimplementationsforCNNs,makingiteasiertoapplythemtosequencemodeling. Forfurther\nimprovements to memory efficiency, we can use lightweight CNN variants, for example,\ndepth-wiseCNNs[Wuetal.,2018a]15.\n2. Linear Attention",
    "Aswithmanypracticalapproachestosequencemodeling,thereisalsoconsiderableinterestin\ndevelopinglinearmodelsinordertospeeduptheprocessingoflongsequences. Whilethere\naremanywaystodefinealinearmodel,onegeneralformthatiscommonlyusedinsequence\nmodelsis\nz = f(a·z +b·s ) (6.153)\ni i−1 i\nHere s represents some intermediate states of the model at step i, and z represents the\ni i\nsummaryofthehistorystatesuptostepi. Itiseasytoseethatthisisarecurrentmodel: the\noutput at step i depends only on the input at the current step and the output at the previous\nstep. Aswiththepopulardesignchoicesinneuralnetwork-basedsystems,thelinearpartis\nfollowedbyatransformationf(·)whichcanbeeitheranactivationfunctionorafeedforward\nneuralnetwork. Notethat,Eq. (6.153)definesastandardlinearmodelonlyiff(·)isalinear\nfunction. Theuseoff(·)givesgreaterflexibilityinmodelingtheproblem,althoughtheterm\nlinearmodelmaynotbeappliediff(·)choosesanon-linearform.\nTheaboveformuladescribesalinearlystructuredmodelwhichcanbeseenasaninstance",
    "ofageneralfamilyofmathematicalmodels. Typically,itcanberepresentedasachainstructure,\n15RecallfromChapter2thatinCNNsafilter(orasetoffilters)combinestheinputvariablesinthereceptive\nfieldintoanoutputvariable(orasetofoutputvariables)vialinearmapping.Supposethattheinputandoutput\nofaproblemarerepresentedassequencesoffeaturevectors. Givenafilterhavingad×kreceptivefield,we\nslideitalongthesequence.Ateachstep,thefiltertakesd×kinputfeaturesandproducesanoutputfeature.This\nprocedureistypicallyexpressedby\ny = ReduceSum(x⊙W) (6.152)\nwherex∈Rk×disthevectorrepresentationoftheinput,y∈Ristheoutputfeature,andW∈Rk×distheweight\nmatrix. ThefunctionReduceSum(·)computesthesumofallelement-wiseproductsbetweenxandW. Ifwe\nwanttheinputandoutputtohavethesamenumberoffeatures,wecandesigndfiltersandthenumberofparameters\nwillbed2·k.\nIndepth-wiseCNNs,wetietheweightsacrossdifferentfeaturedimensions. Moreprecisely,allthecolumn\nvectorsofWarethesame.Thus,thenumberoftheuniqueparametersofthemodelisreducedtod·k(eachW",
    "correspondingtoafilterhavingkuniqueparameters).",
    "330 Chapter 6. Transformers\nor an ordered set of nodes. The model repeats the same computation process from the first\nnode to the last, each time taking the information from the current and previous steps and\nproducinganoutputvectorthatisusedinthefollowingtimesteps. Asaresult,thespaceand\ntimecostofthemodelscaleslinearlywiththelengthofthechain.\nWecanextendEq. (6.153)toastandardRNNmodelbysimplymakingalineartransfor-\nmationofthecurrentinputandthepreviousstate,thatis,z =f(z ·W +s ·W ). Itis\ni i−1 z i s\nthusstraightforwardtoapplyRNNanditsvariantstoTransformertoobtainahybridmodel.\nFor example, we can use LSTM and GRUs in building some of the Transformer layers to\ncombinethemeritsofbothrecurrentmodelsandself-attentivemodels[Chenetal.,2018b].\nAstheconventionalrecurrentmodelshavebeendiscussedatlengthinChapter2,weskipthe\ndiscussionofthemhere.\nIn fact, we may be more interested in developing linear attention models, so that we",
    "canobtainanefficientsystem,whilestillretainingthebenefitofgloballyattentivesequence\nmodeling. Partofthedifficultyindoingthisisthattheformofself-attentionisnotlinear. Let\nustakeamomenttoseehowthisdifficultyarises. Recallthattheresultofself-attentioncanbe\nwritteninthefollowingform\nAtt = A·V\nself\n= ψ(Q·KT)·V (6.154)\nHere ψ(·) is a function that is composed by taking the scaling, exponentiating, masking\nandnormalizationoperations(i.e.,ψ(a)=Normalize(Mask(exp(√a )))). Becauseψ(·)isa\nd\ncomplexnon-linearfunction,thereisnoobviousequivalentthatsimplifiesthecomputation,\nandwehavetocalculatethetwomatrixmultiplicationsseparately(oneinsideψ(·)andone\noutsideψ(·)). Asaconsequence,weneedtostoreallthekey-valuepairsexplicitly,andvisit\neachofthemgivenaquery. Notsurprisingly,thisleadstoamodelwhosecomputationalcost\ngrowsquadraticallywiththesequencelengthn.\nAlthough in self-attention keys and values are coupled, they are used in separate steps.",
    "Anelegantformofthismodelmightbethatallowsforadirectinteractionbetweenthekeys\nandqueries,sothatwecanencodethecontextinformationinawaythatisirrelevanttothe\nqueries. A trick here is that we can remove the non-linearity from ψ(·) by using a feature\nspacemappingϕ(·)onthequeriesandkeys,andreformulateψ(Q·KT)(i.e.,A)inaformof\nmatrixproducts. Forexample,recallfromSection6.4.3thatwecantransformQandKto\nQ′=ϕ(Q)∈Rn×d′ andK′=ϕ(K)∈Rn×d′ throughthemappingϕ(·). Then,wedefinethe\nformoftheattentionmodeltobe\nAtt ≡\nψ′(Q′·K′T\n)·V\nself\nQ′·K′T\n= ·V\nD\nQ′· (cid:0) K′T·V (cid:1)\n= (6.155)\nD\nwhereψ′(a)= a. Fromthisdefinition,weseethat,inthecaseoftransformedqueriesandkeys,\nD",
    "6.4 Efficient Models 331\nthequery-keyproductneedsnotbenormalizedviaSoftmax,butneedsonlybenormalizedviaa\nsimplefactorD. Hencethemodelhasaverysimpleforminvolvingonlymatrixmultiplication\nanddivision,allowingustochangetheorderoftheoperationsusingtheassociativityofmatrix\nmultiplication.\nThisleadstoaninterestingprocedure:\nkeysandvaluesarefirstencodedviaK′T·V,and\ntheneachqueryattendstothisencodingresult.\nGiventhatK′T·V= (cid:80)n k′T·v\n,wecan\nj=1 j j\nwriteK′T·VintheformofEq.\n(6.153),asfollows\nµ = µ\n+k′T\n·v (6.156)\nj j−1 j j\nHere µ ∈Rd′×d is a variable that adds k′T·v at a time. Likewise, we can define another\nj j j\nvariableν ∈Rd′\nj\nν = ν\n+k′T\n(6.157)\nj j−1 j\nThen,theoutputofself-attentionforthej-thquerycanbewrittenas(seealsoEq. (6.146))\nq′ ·µ\nj n\nAtt = (6.158)\nself,j q′ ·ν\nj n\nClearly,thisisalinearmodel,becauseµ andν arelinearwithrespectton. Insimple\nn n\nimplementationsofthismodel,onlyµ andν arekept. Eachtimeanewqueryisencountered,\nj j",
    "weupdateµ andν usingEqs. (6.156)and(6.157),andthencomputeAtt =\nq′\nj\n·µj16.\nj j self,j q′\nj\n·νj\nOnestraightforwardextensiontothelinearattentionmodelistoallowEqs. (6.156)and\n(6.157)tocombinedifferenttermswithdifferentweights. Forexample,wecanredefineµ\nj\nandν as\nj\nµ = a·µ\n+(1−a)·k′T\n·v (6.159)\nj j−1 j j\nν = a·ν\n+(1−a)·k′T\n(6.160)\nj j−1 j\nand train the parameter a as usual. Also, we can treat a as a gate and use another neural\nnetworktocomputea[Pengetal.,2021]. AnothermodeldesignistoaddmoretermstoEqs.\n(6.156)and(6.157)inordertogiveamorepowerfultreatmentofthelinearattentionapproach\n[Bello,2020;Schlagetal.,2021].\nWehaveseenageneralideaofdesigninglinearmodelsfortheattentionmechanism. The\nkey design choice of such models is to remove the Softmax-based normalization, thereby\ntakinglinearformsofrepresentationsbasedonvariousintermediatestatesofthemodels. This\nmotivatesseveralrecentlydevelopedalternativestoself-attentioninwhichefficientinference",
    "systemsaredevelopedonthebasisofrecurrentmodelsofsequencemodeling[Pengetal.,2023;\nSunetal.,2023]. Whilethesesystemshavedifferentarchitectures,theunderlyingmodelshave\na similar form, as described in Eq. (6.153). Note that, by using the general formulation of\n16Inautoregressivegeneration,wegenerateasequencefromlefttoright.Inthiscase,weneednotconsiderthe\nkeysandvaluesforpositions>j.",
    "332 Chapter 6. Transformers\nrecurrentmodels,weneednotrestrictthemodelingtothestandardQKVattention. Insteadwe\nmaygivenewmeaningsandformstothequeries,keys,andvalues.\nThe discussion here is also related to the memory models discussed in Section 6.4.2.\nFromthememoryviewpoint,thekeysandvaluescanbetreatedasencodingsofthecontext.\nTherefore,inthelinearattentionmodelabovewehaveamemorysysteminwhichtwosimple\nvariables µ and ν are used to represent all the context information up to position j. This\nj j\nresultsinafixed-lengthmemorywhichisveryusefulinpractice. Therearealsootherlinear\napproachestoencodinglongsequences. Forexample,wecanviewthemovingaveragemodel\nasaninstanceofEq. (6.153),andaverageaseriesofstatevectorsofaTransformersystem,\neitherweightedorunweighted.\n3. State-Space Models\nIncontrolsystems,state-spacemodels(SSMs)arerepresentationsofasystemwhoseinput\nand output are related by some state variables (or states for short), and whose dynamics",
    "is described by first-order differential equations of these states. As a simple example, we\nconsideracontinuoustime-invariantlinearsystemwhichisgivenintheformofthestate-space\nrepresentation\ndz(t)\n= z(t)·A+s(t)·B (6.161)\ndt\no(t) = z(t)·C+s(t)·D (6.162)\nHeres(t),o(t),andz(t)arethevaluesoftheinputvariable,outputvariableandstatevariable\nattimet17. Inageneralsetting,s(t),o(t),andz(t)mayhavedifferentnumbersofdimensions.\nTosimplifythediscussionhere,weassumethats(t),o(t)∈Rd andz(t)∈Rdz18. Eq. (6.161)\niscalledthestateequation,whereA∈Rdz×dz isthestatematrixandB∈Rd×dz istheinput\nmatrix. Eq. (6.162)iscalledtheoutputequation,whereC∈Rdz×d istheoutputmatrixand\nD∈Rd×d isthefeedforwardmatrix.\nThese equations describe a continuous mapping from the variable s(t) to the variable\no(t)overtime. Theyare,therefore,oftenusedtodealwithcontinuoustimeseriesdata. To\napply this model to the sequence modeling problem discussed in this chapter, we need to",
    "modifytheaboveequationstogiveadiscreteformofthestate-spacerepresentation. Suppose\nthat {s ,s ,...,s } is a sequence of input data points sampled from s(t) with time step ∆t.\n0 1 n\nSimilarly,wedefine{z ,z ,...,z }and{o ,o ,...,o }assequencesofthestateandoutput\n0 1 n 0 1 n\nvectors. Giventhisnotation,wenowhaveadiscretizedversionoftheSSM,writtenas\nz = z ·A+s ·B (6.163)\nt t−1 t\no = z ·C+s ·D (6.164)\nt t t\n17Weuseboldfaceletterstoemphasizethatthevariablesarevectors.\n18Inageneralstate-spacemodel,allthesevariablesarerepresentedasvectorsofcomplexnumbers.Becausethe\nmodelsdefinedonthefieldofcomplexnumbersisapplicabletocaseofrealnumber-basedstate-spaces,werestrict\nourdiscussiontovariablesinthemulti-dimensionalrealnumberfield.",
    "6.4 Efficient Models 333\nThisformulationoftheSSMdefinesanRNNwitharesidualconnection. Tobemoreprecise,\nEq. (6.163)describesarecurrentunitthatreadstheinputatsteptandthestateatstept−1,\nwithoutusinganyactivationfunction. Eq. (6.164)describesanoutputlayerthatsumsboththe\nlineartransformationsofthestatez andtheidentitymappings .\nt t\nTheparametersA,B,C,andDcanbeinducedfromA,B,CandDinseveraldifferent\nways,dependingonhowEq. (6.161)isapproximatedbyEq. (6.163)19. Oneapproachtotime\ndiscretization, called bilinear transform or Tustin’s method, gives a model in which the\nparameterstaketheform\n∆t ∆t\nA = (I− ·A)·(I− ·A)−1 (6.171)\n2 2\n∆t\nB = ∆t·B·(I− ·A)−1 (6.172)\n2\nC = C (6.173)\nD = D (6.174)\nAn alternative approach is to use the Zero-Order-Hold (ZOH) discretization which has the\nform\nA = exp(∆t·A) (6.175)\nB = ∆t·B·(exp(∆t·A)−I)·(∆t·A)−1 (6.176)\nC = C (6.177)\nD = D (6.178)\nA detailed discussion of these approaches lies beyond the scope of this book, and we refer",
    "theinterestedreadertostandardtextbooksoncontroltheoryforfurtherdetails[Åströmand\n19Thediscretizationprocesscanbeinterpretedasanumericalmethodofsolvingthedifferentialequation.Note\nthatEq.(6.161)isanODE\ndz(t)\n= g(z(t),t) (6.165)\ndt\nwhere\ng(z(t),t) = z(t)·A+s(t)·B (6.166)\nTherearemanynumericalapproximationstothesolutionstotheODE.Forexample,theEulermethodofsolving\ntheODEcanbeexpressedintheform(seeinSection6.3.3)\nzt = z t−1 +∆t·g(z t−1 ,t) (6.167)\nSubstitutingEq.(6.166)intoEq.(6.167)yields\nzt = z\nt−1\n+∆t(z\nt−1\n·A+st·B)\n= z t−1 ·(I+∆t·A)+st·(∆t·B) (6.168)\nThisgivesoneofthesimplestformsofthediscretizedstateequations[Guetal.,2022b],thatis,\nA = I+∆t·A (6.169)\nB = ∆t·B (6.170)",
    "334 Chapter 6. Transformers\nWittenmark,2013].\nTherecurrentformofEq. (6.163)makesiteasytocomputethestatesandoutputsovera\nsequenceofdiscretetimesteps. Wecanunrollz ando inafeedforwardfashion\nt t\nz =s ·B o =s ·B·C+s ·D\n0 0 0 0 0\nz =s ·B·A+s ·B o =s ·B·A·C+s ·B·C+s ·D\n1 0 1 1 0 1 1\n2 2\nz =s ·B·A +s ·B·A+s ·B o =s ·B·A ·C+s ·B·A·C+\n2 0 1 2 2 0 1\ns ·B·C+s ·D\n2 2\n...... ......\nItiseasytowrite\nt\n(cid:88) t−i\nz = s ·B·A (6.179)\nt i\ni=0\nt\n(cid:88) t−i\no = s ·B·A ·C+s ·D (6.180)\nt i t\ni=0\nClearly,theright-handsideofEq. (6.180)canbeinterpretedasamergedoutputofaconvolu-\ntionallayerandalinearlayer. Giventhat\nt\n(cid:88) t−i (cid:104) (cid:105)\ns ·B·A ·C = s s ... s ·\ni 0 1 t\ni=0\n(cid:104) (cid:105)\nt t−1\nB·A ·C B·A ·C ... B·C (6.181)\nwedefineafilterhavingtheparameters\n(cid:104) (cid:105)\nW = B·A\nnmax·C\nB·A\nnmax−1\n·C ... B·C (6.182)\nssm\nwhere n is the maximum length of the sequence20. Then, the output of the state-space\nmax\n \ns\n0\n.\nmodelforasequenceS=  . .   canbeexpressedas\ns\nn",
    "O = Conv(S,W )+Linear(S,D) (6.183)\nssm\nwhereConv(·)istheconvolutionoperation,andLinear(·)isthelineartransformationopera-\ntion. Suchatreatmentofthestate-spacemodelenablesthesystemtobeefficientlyimplemented\nusingfastparallelconvolutionalgorithms.\nUnfortunately,theabovemodelperformspoorlyinmanycases. Aswithmanydeepneural\nnetworks,carefulinitializationofthemodelparametersplaysanimportantroleinsuchmodels.\n20HereWssmcanberepresentedasannmax×d×dtensor.",
    "6.4 Efficient Models 335\nForexample,restrictingthestatematrixtoparticulartypesofmatricesisfoundtobeusefulfor\nlearningandgeneralizingonlongsequences[Guetal.,2022a].\nAnother problem with the basic state-space model is that it involves multiplication of\nn\nmultiplematrices. Ifthesequenceislong(i.e.,nisalargenumber),computingA willbe\ncomputationallyexpensiveandnumericallyunstable. Oneofthemostpopularapproachesto\ndevelopingpracticalstate-spacemodelsforsequencemodelingisdiagonalization. Thebasic\nideaisthatwecantransformastate-spacemodelintoanewstate-spacewhereA(orA)is\ndiagonalized. Givenastate-spacemodelparameterizedby(A,B,C,D),wecandefineanew\nstate-spacemodel(UAU−1,BU−1,UC,D)byintroducinganinvertiblematrixU. Itiseasy\ntoprovethatthetwomodelsareequivalentunderthestate-spacetransformationU21. Byusing\nthis state-space transformation, and by noting that A (or A) can be written as a canonical\nformP−1ΛP22,wecanenforcetheconstraintthatA(orA)isadiagonalmatrix,givingrise",
    "to diagonal state-space models. To illustrate, consider the filter used in the convolutional\nrepresentationofthestate-spacemodel(seeEq. (6.181)). AssumingthatA=P−1ΛP,we\nt\ncanwriteB·A ·Cas\nB·A t ·C = B·(P−1ΛP)t·C\n= B·(P−1ΛP)·(P−1ΛP)···(P−1ΛP)·C\n= (B·P−1)·Λt·(P·C) (6.185)\nSinceΛisadiagonalmatrix,wecanefficientlycomputeΛtbysimplyraisingalltheentries\nofΛtothet-thpower. Wethenhaveacomputationallycheapermodel,inwhich\n′\nA = Λ (6.186)\nB ′ = B·P−1 (6.187)\n′\nC = P·C (6.188)\n′\nD = D (6.189)\nMoredetaileddiscussionsofdiagonalstate-spacemodelsinsequencemodelingcanbefound\ninGuetal.[2021]’swork.\nTheapplicationofstate-spacemodelstoTransformerissimple. Eachself-attentionsub-\nlayerisreplacedinthiscasebyanSSMsub-layerasdescribedinEqs. (6.163)and(6.164).\nAswehaveseenthereisacloserelationshipbetweenstate-spacemodelsandbothCNNsand\nRNNs. For sequence modeling, we can deal with a sequence of tokens either sequentially\nas in RNNs, or in parallel as in CNNs. This leads to a new paradigm that takes both the",
    "sequentialviewandtheparallelviewofthesequencemodelingproblem—fortraining,the\n21Astatespacetransformationcanbeseenasaprocessofmappingallstatesfromtheoldspacetothenewspace,\nby\ns′(t) = s(t)·U (6.184)\n22Λdenotesadiagonalmatrix.",
    "336 Chapter 6. Transformers\nsystem operates like CNNs to make use of fast parallel training algorithms; for prediction,\nthe problem is re-cast as a sequential update problem which can be efficiently solved by\nusing RNN-like models. It should be noted, however, that state-space models are found to\nunderperformTransformermodelsforNLPproblems,suchaslanguagemodeling,although\nthey have achieved promising results in several other fields. Further refinements are often\nneededtomakethemcompetitivewithotherwidelyusedsequencemodels[Fuetal.,2022].\nWhile the formalism of state-space models is different from those we discussed in this\nchapter,itprovidesageneralframeworkofsequencemodelinginwhichtheproblemcanbe\nviewed fromeither oftwo differentperspectivesand wechoosedifferent onesfordifferent\npurposes. Several recent sequence models were motivated by this idea, leading to systems\nexhibitingpropertiesofbothparalleltrainingandRNN-styleinference[Orvietoetal.,2023;\nSunetal.,2023].",
    "6.4.6 Conditional Computation\nSo far in our discussion of efficient Transformer models, we have assumed that the model\narchitectureisgivenbeforebeginningthetrainingofamodelandisthenfixedthroughout. We\nnowturntothecaseoflearningefficientmodelarchitectures. Withoutlossofgenerality,we\ncanwriteamodelintheform\ny = Model(x,g(x)) (6.190)\nwherexandy aretheinputandoutputofthemodel. g(x)isamodelfunctionthatreturns\nthe model architecture and corresponding parameters for the given input x. In general, we\nadopttheconventionprevalentinlearningproblemsofusingafixedmodelarchitectureand\nlearning only the parameters, say, g(x)=θ. In this case, the goal of learning is to find the\noptimalvaluesoftheparametersgiventhemodelarchitectureandtrainingdata. Ontestdata,\nwemakepredictionsusingthesamemodelarchitecturealongwiththeoptimizedparameters.\nAnaturalextensionofthisapproachistoconsiderthelearningofboththemodelarchi-\ntectureandparameters. Inarchitecturelearning,wewouldliketofindamodelfunctiongˆ(x)",
    "thatproducestheoptimalmodelarchitectureandparametervaluesgiventheinputx. However,\nsearching a hypothesis space of all possible combinations of architectures and parameter\nchoices is extremely difficult, and so we need practical methods to achieve the goal. Two\nclassesofmethodscanbeapplied.\n• Neural Architecture Search (NAS). In automated machine learning (AutoML),\nneuralarchitecturesearchistheprocessofexploringaspaceofneuralnetworkstofind\nonethatbestfitssomecriterions[ZophandLe,2016;Elskenetal.,2019b]. Oncethe\noptimalneuralnetworkisdetermined,itsparameterswillbetrainedasusual,andthenbe\nappliedtonewdata. Inordertomakesearchtractable,severaladditionaltechniques,such\nassearchspacepruningandfastsearchalgorithms,aretypicallyused. Applyingneural\narchitecturesearchtothedevelopmentofefficientneuralnetworksisstraightforward\n[Howardetal.,2019;TanandLe,2019]. Weneedonlyincorporateefficiencymeasures\ninto the performance estimation of neural networks, for example, the search can be",
    "6.4 Efficient Models 337\nguided by a criterion that penalizes neural networks with high latency or excessive\nmemoryrequirements.\n• Dynamic Neural Networks. The key idea of dynamic neural networks is to adapt a\nneural network dynamically to various inputs [Gupta et al., 2004; Han et al., 2021b].\nIdeally,wewouldliketolearngˆ(·),andthen,foranyinputx ,weapplythemodel\nnew\nModel(x ,gˆ(x )). Asaresult,attesttimewemayhavedifferentmodelstructures\nnew new\nand/or different parameters for different inputs. However, it is infeasible to develop\na function gˆ(·) that can model arbitrary neural networks. In practice, gˆ(·) is often\nconsideredtorepresentafamilyofsub-networksofasuper-network. Theproblemis\nthereforereframedasasimplerproblemtolearntochoosewhichsub-networkisused\nforagiveninput.\nFrom a machine learning perspective, the approaches to neural architecture search are\ngeneral and can be applied to any neural network. On the other hand, from a practical",
    "perspective,itisstilldifficulttofindanefficientneuralnetworkthatissufficientlypowerful\nandgeneralizeswell. Whileneuralarchitecturesearchprovidesinterestingideasfordeveloping\nefficientTransformermodels,wemakenoattempttodiscussithere. Instead,thereadercan\nrefertotheabovepaperstohaveageneralideaofit,andrefertoSoetal.[2019],Wangetal.\n[2020a],andHuetal.[2021]’sworkforitsapplicationtoTransformers.\nIn this subsection, we focus on a particular family of approaches to dynamic neural\nnetworks, called conditional computation. This concept was originally motivated by the\ndynamicselectionofneuronsofaneuralnetwork[Bengioetal.,2013;2015]. Morerecently,it\nhasoftenbeenusedtorefertoasaprocessofdynamicallyselectingpartsofaneuralnetwork.\nAnarrowviewofconditionalcomputationistoseeg(·)asanadaptiveneuralnetworkwhich\ndynamicallyreducesorgrowsthenumberofcomputationunits(suchasneuronsandlayers).\nAsaresult,computationcanadapttochangingconditions,andwecanseekagoodaccuracy-",
    "latencytrade-offbythisadaptationmechanism.\nA common way to achieve this is to learn how to skip some computation steps so that\nwe can work with a necessary sub-set of the network [Xu and Mcauley, 2023]. One of the\nsimplestmethods,sometimescalledearlystopping,istostopthecomputationatsomepoint\nduringreadingorgeneratingasequence. Thistechniqueisoftenusedinpracticalsequence\ngeneration applications where a low latency is required. Suppose y ...y is the longest\n1 nmax\nsequence that the system can generate, and s ...s is the corresponding sequence of the\n1 nmax\nstates of the top-most Transformer layer. Then we develop a model f (·) that takes one\nstop\nhiddenstates atatimeandproducesadistributionofabinaryvariablec∈{stop,nonstop}\ni\nPr(c|s ) = f (s ) (6.191)\ni stop i\nThegenerationprocessterminatesifPr(stop|s )issufficientlylarge,forexample\ni\nPr(stop|s ) ≥ Pr(nonstop|s )+θ (6.192)\ni i stop",
    "338 Chapter 6. Transformers\nwhereθ denotestheminimalmarginfordistinguishingthetwoactions23. Thisformulation\nstop\nisalsorelatedtothestoppingcriterionproblemthatisfrequentlydiscussedinsearchalgorithms\nforsequencegeneration(seeChapter5). f (·)canbedesignedinseveraldifferentways. For\nstop\nexample,inmanypracticalapplications,thestoppingcriterionisbasedonsimpleheuristics.\nAlternatively,wecandefinethefunctionf (·)asaneuralnetworkandtrainitusinglabeled\nstop\ndata.\nThe above approach can be easily extended to handle situations in which some of the\ntokensareskipped. Thislearning-to-skipapproachistypicallyusedintheencodingstagein\nwhich all input tokens are given in advance. Let h ...h be low-level representations of a\n1 m\nsequencex ...x . LikeEq. (6.191),wecandevelopamodelPr(c|s )(c∈{skip,nonskip})\n1 m i\ntodeterminewhetherthetokenx canbeskipped. Figure6.16(a)and(b)showillustrations\ni\nofearlystoppingandskipping. Notethatthelearning-to-skipmethodhasoverlapwithother",
    "linesofresearchontrainingneuralnetworks. Forexample,erasingsomeoftheinputtokensin\ntrainingisfoundtobeusefulforachievinghighergeneralizationofTransformermodels[Shen\netal.,2020a;KimandCho,2021]. Thismethodisalsorelatedtothedownsamplingmethods\nwhichwillbediscussedinSection6.4.8.\nAsecondapproachtoconditionalcomputationistoresorttosparseexpertmodels, or\nitspopularinstance—MoE[Yukseletal.,2012]. Indeeplearning, amodelofthiskindis\ntypicallybuiltfromanumberofexpertswhichareneuralnetworkshavingthesamestructure\nbutwithdifferentparameters. Inthisway,wecanconstructabigmodelbysimplyincreasing\nthe number of experts. When running this model, during either training or prediction, we\nactivateonlyasmallnumberoftheexpertsbysomeroutingalgorithms(seeFigure6.16(c)).\nAnMoEmodelisanadaptivenetworksinceeachtimewehaveanewinput,themodelroutes\nittodifferentexperts. InSection6.3.4,wepresentedthebasicformofMoE,andshowedhow\nTransformermodelscanbescaledupbythissparsemethod. Foracomprehensivereviewof",
    "therecentadvancesinMoE,werefertheinterestedreadertoFedusetal.[2022a]’swork.\nAthirdapproachthatcanbeusedtoadaptaTransformermodeltochanginginputisto\ndynamicallyshrinkthenumberoflayers. Severalmethodshavebeenproposedtodothisin\nan attempt to improve inference efficiency. The simplest of these is to exit at some hidden\nlayersbywhichwecanstillmakeaccuratepredictionsforthesample(seeFigure6.16(d)and\n(e)). Todothis, wecaneitherdeterminetheappropriatedepthfortheentiresequence(call\nitasentence-leveldepth-adaptivemodel),oruseanadaptivedepthforeachtoken(callita\ntoken-leveldepth-adaptivemodel). Hereweconsidertoken-leveldepth-adaptivemodelsbut\nthemethodscanbeeasilyextendedtosequence-leveldepth-adaptivemodels.\nSupposethereareLstackedlayersatpositioni24. Wewouldideallyliketofindalayer\nin the stack, which can be used as the last hidden layer for making predictions, and whose\ndepthisaslowaspossible. However,wecannotsimplyusetheL-thlayerofthestackasthe",
    "oracleforthisproblem,becauseweneverknowinadvancewhatthelastlayergeneratesduring\ninference. Instead,weneedtodeterminewhetherthenetworkshouldstopgrowingatdepthi,\nconsideringthelayersgeneratedsofar.\n23AnequivalentformofEq.(6.192)isPr(stop|s\ni\n)≥ 1+θ\n2\nstop.\n24AlayerisastandardTransformerblockconsistingofafewsub-layers.",
    "6.4 Efficient Models 339\ny 1 y 2 y 3 stop ... ... ... ... ...\nh1 h2 h3 h4 h5\ny 0 y 1 y 2 − − x 1 x 2 x 3 x 4 x 5\n(a)EarlyStopping (b)TokenSkipping\n(Decoder) (Encoder)\n... ... ... ... ... y y y y y\n1 2 3 4 5\nexpert1 expert2 expert3 expert4\nx x x x x y y y y y\n1 2 3 4 5 0 1 2 3 4\n(c)MoE (d)Sentence-levelDepthAdaptation\n(Encoder) (Decoder)\ny y y y y y y y y y\n1 2 3 4 5 1 2 3 4 5\ny y y y y y y y y y\n0 1 2 3 4 0 1 2 3 4\n(e)Token-levelDepthAdaptation (f)LayerSkipping\n(Decoder) (Decoder)\nFigure6.16: Methodsofconditionalcomputation,includingearlystopping,tokenskipping,\nMoE,sentence-leveldepthadaptation,token-leveldepthadaptation,andlayerskipping. While\nthesemethodsareillustratedusingeithertheencodingordecodingprocess,mostofthemcan\nbeappliedtobothTransformerencodersanddecoders.",
    "340 Chapter 6. Transformers\nNowsupposewehaveaTransformerdecoderwhichproducesadistributionoveravocabu-\nlaryV ateachstep. Asusual,wedenotetheoutputofthel-thlayeratstepibysl. Foreachsl,\ni i\nwecreateanoutputlayerthatproducesadistributionpl overthevocabulary(callitanearly\ni\nexitclassifier),givenby\npl = Softmax(sl·Wl) (6.193)\ni i o\nwhereWl ∈Rd×|V| istheparametermatrix. HencewehaveL−1additionaloutputlayers,\no\neachcorrespondingtoahiddenlayerfromdepth1toL−1. Attrainingtime,weconsiderthe\ncross-entropylossesof{p1,...,pL−1},andtraintheselayerstogetherwiththeTransformer\ni i\nmodel. Attesttime,thedepthofthenetworkgrowsasusual,andweuse{p1,...,pl}and/or\ni i\n{s1,...,sl}todeterminewhetherweshouldexitatthel-thlayer. Thereareseveralexitcriteria,\ni i\nforexample,\n• Common criteria are based on measures of the confidence of predictions. A simple\nmethodistocomputetheentropyofpl,andexitifthisentropyisaboveapre-defined\ni\nvalue.\n• Alternatively,onecanviewthemaximumprobabilityoftheentriesofpl astheconfi-\ni",
    "denceoftheprediction.\n• Insteadofconsideringtheoutputofasinglelayer,wecanalsoexaminethechangein\ntheoutputsorhiddenstatesoveranumberoflayers. Forexample,wecanmeasurethe\nsimilaritybetweenpl−1 andpl orbetweensl−1 andsl. Ifthesimilarityisaboveagiven\ni i i i\nthreshold,thenwesaythattheoutputoftheneuraltendstoconvergeandthenumberof\nlayerscanstopgrowing.\n• Theabovemethodscanbeextendedtoexaminethechangeinthepredictionsmadeby\ntheclassifiersassociatedwiththelayers. Forexample,themodelcanchoosetoexitif\nthepredictionsmadebytheclassifiersremainunchangedforanumberoflayers.\nDiscussions of these criteria can be found in the related papers [Xin et al., 2020; Zhou\net al., 2020; Schuster et al., 2022]. There are a variety of ways to improve these early exit\nmethods. Oneistoexploreotherformsofthepredictionforeachlayer. Forexample,wecan\ndevelopamodelthatdirectlypredictshowmanylayersweneedtomodeltheinput[Elbayad\netal.,2020]. Anotherlineofresearchonearlyexitfocusesonbettertrainingforthesemodels,",
    "forexample,wecanconsidervariouslossfunctionsfortrainingtheclassifiers[Schwartzetal.,\n2020; Schuster et al., 2022]. In addition, there is also interest in learning the combination\noftheoutputsofmultiplelayerssothatwecanmakepredictionsbyusingmultiplelevelsof\nrepresentation[Zhouetal.,2020;Liaoetal.,2021].\nAproblemwithtoken-leveladaptive-depthmodelsisthattherepresentationsatcertain\ndepthsmaybeabsentintheprevioussteps. Inthiscase,standardself-attentionisnotdirectly\napplicable,becausewemaynotattendtotheprevioustokensinthesamelevelofrepresentation.\nFortraining,thiscanbeaddressedbyusingalltheLlayersofthefullmodel. Forinference,\nwe can either duplicate the layer from which we exit to fill up the layer stack, or modify\ntheself-attentionmodeltoenableittoattendtotherepresentationsoftheprevioustokensat",
    "6.4 Efficient Models 341\ndifferentdepths.\nItisalsopossibletoselectanysub-setofthelayersforconstructingashallownetwork.\nThe adaptive models therefore can be generalized to skipping models (see Figure 6.16 (f)).\nAs with the early exit problem, the skipping problem can be framed as a learning task, in\nwhichaclassifieristrainedtodecidewhetheralayershouldbedropped. Thelearning-to-skip\nproblemhasbeenstudiedinthefieldofcomputervision[Wangetal.,2018b;Wuetal.,2018b].\nHowever, learning a skipping model for large-scale, deep neural networks is difficult. For\npracticalsystems,itstillseemsreasonabletouseheuristicsorcheapmodelstoobtainaneural\nnetworkhavingskippedlayers,whichhasbeendiscussedinrecentpre-trainedNLPmodels\n[Wangetal.,2022c;DelCorroetal.,2023].\n6.4.7 Model Transfer and Pruning\nManylargeTransformermodelshavebeensuccessfullydevelopedtoaddressNLPproblems.\nAcommonquestionis: canwetransformalarge,well-trainedmodelintoasmalleronethat",
    "allows for more efficient inference? At a high level, this can be thought of as a transfer\nlearningprobleminwhichtheknowledgeistransferredfromonemodeltoanother. Butwe\nwill not discuss this general topic, which spans a broad range of issues and models, many\noutsidethescopeofthischapter. Instead,wenarrowourdiscussiontotwokindsofapproaches\nthatarewidelyusedinlearningsmallneuralnetworksfromlargeneuralnetworks.\n1. Knowledge Distillation\nKnowledgedistillationisaprocessofcompressingtheknowledgeinalargeneuralnetwork(or\nanensembleofneuralnetworks)intoasmallneuralnetwork[Hintonetal.,2015]. Insupervised\nlearningofneuralnetworks,theobjectivefunctionsaregenerallydesignedtorepresentsome\nlossofreplacingthetrueanswerwiththepredictedanswer. Hencewecanminimizethisloss\nsothatthemodelsaretrainedtooutputthetrueanswer. Whilemodelsaretypicallyoptimized\non the training data in this manner, what we really want is to generalize them to new data.",
    "This is, however, difficult because we have no information about generalization in training\nwiththeground-truth. Inknowledgedistillation,insteadofforcingamodeltostaycloseto\ntheground-truthoutput,wetrainthismodeltogeneralize. Todothis,wedirectlytransferthe\nknowledge(i.e.,thegeneralizationability)ofapre-trainedmodeltothemodelthatwewantto\ntrain.\nA frequently used approach to knowledge distillation is teacher-student training. A\nteachermodelistypicallyarelativelylargeneuralnetworkthathasalreadybeentrainedandcan\ngeneralizewell. Astudentmodelisarelativelysmallneuralnetwork,suchasaneuralnetwork\nwithfewerlayers,towhichwetransfertheknowledge. Asimplewaytodistilltheknowledge\nfromtheteachermodelintothestudentmodelistousetheoutputoftheteachermodelasthe\n“correct”answerfortrainingthestudentmodel. SupposewehaveateacherTransformermodel\nthatcangenerateasequenceofdistributions{Pr(·|y ,x),...,Pr(·|y ...y ,x)}fortheinput\n0 0 n−1",
    "x. Tokeepthenotationsimple,wedenotethedistributionPr(·|y ...y ,x)asp . Similarly,\n0 i−1 (cid:101)i\nwedenotetheoutputofthestudentTransformermodelforthesameinputasp . Asusual,we\ni\nconsideralossfunctionLoss(p ,p )(suchasthecross-entropyfunction)forcomputingsome\n(cid:101)i i",
    "342 Chapter 6. Transformers\ndistancebetweenp andp . Then,wecandefinethelossovertheentiresequenceas\n(cid:101)i i\nn\n1(cid:88)\nL(x,θ) = Loss(p ,p ) (6.194)\n(cid:101)i i\nn\ni=1\nwhereθ denotestheparametersofthestudentmodel25. Usingthisloss, wecanoptimizeθ,\nforanygivensetofsourcesequences{x ,...,x },insuchawayastominimizethequality\n1 K\n(cid:80)N\nL(x ,θ).\nk=1 k\nSeveraldifferentextensionstothisbasicmethodhavebeendevelopedtomodeltheproblem\nofknowledgetransferbetweentwomodels. Asimplewayistousethehiddenstatesinstead\nof the output probabilities as the training targets [Romero et al., 2014]. In this case, the\nobjectiveistominimizethedifferencebetweensomehiddenstatesoftheteachermodeland\nthecorrespondingstatesofthestudentmodel. Ratherthanusingtheoutputsofvariouslayersas\nthetargetsfortrainingthestudentmodel,anothertechniqueistomodeltherelationsbetween\nsamples and train the student model by minimizing some differences between the relation",
    "encodingsoftheteacherandstudentmodels[Parketal.,2019;Pengetal.,2019]. Forexample,\nwecandeveloparelationencodingmodelbasedontheTransformerarchitecture. Thegoalis\nthentooptimizethestudentmodelsothatitscorrespondingrelationencodingofagroupof\nsamplesisascloseaspossibletothatoftheteachermodel.\nForsequencegenerationproblems,aspecialcaseofknowledgedistillation,whichcanbe\nviewedasameansofdataaugmentation,isoftenusedfordevelopinglightweightmodels\n[KimandRush,2016]. Forexample,considertheproblemoftransferringthetranslationability\nofawell-developedmachinetranslationmodel(i.e.,theteachermodel)toanewmodel(i.e.,\nthestudentmodel). Givenasetofsource-sidesentences{x ,...,x },wecanusetheteacher\n1 K\nmodeltotranslateeachx toatarget-sidesentencey . Then,bytreatingx andy aspaired\nk (cid:101)k k (cid:101)k\nsentences, we obtain a bilingual dataset consisting of {(x ,y ),...,(x ,y )}. We can use\n1 (cid:101)1 K (cid:101)K\nthisbilingualdatasetasthelabeleddatasettotrainthestudentmodelasusual. Oneadvantage",
    "ofthisdataargumentationmethodisthatitisarchitecturefree,andwedonotevenneedto\nunderstandtheinternalarchitecturesoftheteacherandstudentmodels. Hencewecanapply\nthismethodifwehaveablack-boxteachermodel. Moredetaileddiscussionsofknowledge\ndistillationcanbefoundinGouetal.[2021]andWangandYoon[2021]’ssurveys.\n2. Structured Pruning\nPruningisamongthemostpopularofthemodelcompressionmethodsandhasbeenapplied\nto a broad range of systems. One common approach to pruning is unstructured pruning,\nby which we activate only some of the connections between neurons. However, as with\nmostsparsemodels,modelsprunedinthiswaytypicallyrequirespecialimplementationsand\nhardwaresupport,whichinturnreducestheirefficiencyinsomeapplications. Asimplebut\nmoreaggressivewaytodopruningistousestructuredpruning. Indeeplearning,structured\npruningisatechniquethatremovesagroupofneuronsorconnectionstogether. Forexample,\nwecanremoveanentirelayerofneuronfromaneuralnetworktoobtainashallowermodel.",
    "25Weomittheparametersoftheteachermodelbecausetheyarefixedthroughoutthetrainingprocess.",
    "6.4 Efficient Models 343\nAsmulti-layer, multi-headneuralnetworks, Transformersarenaturallysuitedtostructured\npruning,andwecanpruneaTransformernetworkinseveraldifferentways. Forexample,we\ncanprunesomeoftheheadsinmulti-headattention[Voitaetal.,2019;Micheletal.,2019],or\nsomeofthelayersinthelayerstack[Houetal.,2020;KimandAwadalla,2020].\nFormally, we can represent a neural network as a set of parameter groups {θ ,...,θ },\n1 R\neachcorrespondingtoacomponentorsub-modelofthemodel. Ourgoalistofindasub-set\nof{θ ,...,θ }bywhichwecanbuildamodelthatyieldsgoodperformance,whilehavinga\n1 R\nlowermodelcomplexity. However,asimplesearchofsuchamodelisinfeasiblebecausethere\nareacombinatoriallylargenumberofpossiblemodelcandidatesandevaluatingallofthese\nmodelsiscomputationallyexpensive.\nOneapproachto structuredpruningistorandomly prunecomponentsofa model. One\ncanruntherandompruningprocessanumberoftimestogenerateapoolofmodelcandidates\nandselectthebestonefromthepool. Anotherapproachistouseheuristicstodecidewhich",
    "componentsarenotimportantandcanberemoved. Commonmeasuresoftheimportanceofa\nparametergroupθ includevariousqualitiesbasedonnormsoftheweightsorgradientsofθ\nr r\n[Santacroceetal.,2023]. Wecanpruneθ ifthevaluesofthesemeasuresarebelow(orabove)\nr\ngiventhresholds. Athirdapproachistoframethepruningproblemasanoptimizationtaskby\nintroducingtrainablegatesindicatingthepresenceofdifferentcomponents[McCarleyetal.,\n2019;Wangetal.,2020d;Lagunasetal.,2021]. Theprunedmodelcanbeinducedbyusing\nthetrainedgates. Notethat,inmanycases,pruningisnotapost-processingstepforagiven\ntrainedmodel,butpartofthetraining.\n6.4.8 Sequence Compression\nInsequencemodelingandgenerationproblems,thetimeandspacecomplexitiesarestrongly\ninfluencedbythelengthoftheinputoroutputsequence,andwepreferthesequencetobeshort.\nThisisparticularlyimportantforTransformermodels,astheirtimeandspacecomplexitiesare\nquadraticwiththesequencelength,andthememoryfootprintandlatencycanbeheavyburdens",
    "if the sequence is very long. In the previous subsections, we have discussed modifications\nto the Transformer architecture for dealing with long sequences. Here we instead consider\nmethodsforcompressingthesequencesintooneswithacceptablelengths.\nOne simple approach is to map the input sequence to a fixed-size representation. For\nexample, using the recurrent models discussed in Section 6.4.2, we can encode a sequence\nof vectors into a single vector. This method can be easily extended to generate a “larger”\nrepresentation so that this representation can retain more information of the original input.\nForexample,wecanselectafixednumberofthehiddenstatesoverthesequencetoforma\nnewsequenceoffixed-length. Anotherwaytorepresentavariable-lengthsequenceasafixed-\nlengthsequenceistoattendtheinputvectorstosomehiddenstates,usuallyafixednumberof\nlearnablehiddenrepresentations. InJaegleetal.[2021]’swork,thisisdonebyintroducingr",
    "hiddenrepresentations{u ,...,u },andthenattendingtheinputvectors{x ,...,x }tothese\n1 r 1 m\nhiddenrepresentations. TheattentionmodelcanbeastandardQKVattentionmodelinwhich\nweview{u ,...,u }asqueriesand{x ,...,x }askeysandvalues. Theoutputofthismodel\n1 r 1 m\nisasequenceofr vectors,whichcanbeusedasfixed-lengthinputtodownstreamsystems.",
    "344 Chapter 6. Transformers\nAsecondapproachistousedownsamplingtocompressthesequenceintoashorterone.\nA typical method of downsampling is strided convolution, which has been widely used in\ncomputervisionandspeechprocessing. Forexample,supposethereisasequenceofmvectors\n∈Rd. Wecandevelopafilterwithawidthof2andastrideof2. Bytakingthesequenceas\ninput,thefilterproducesasequenceof m newvectors∈Rd,andsowehaveareductionrate\n2\nof2. Also,wecanstackmultipleconvolutionallayersorpoolinglayerstoachieveadesired\nleveloflengthreduction,calledprogressivedownsampling. However,itseemsinevitable\nthatdownsamplingwillleadtoinformationloss[Hanetal.,2020;BurchiandVielzeuf,2021].\nWeneedtoconsideratrade-offbetweenthecompressedsequencelengthandtheperformance\nofdownstreamsystems[Xuetal.,2023b].\nIn NLP, the problem of sequence compression is also closely related to the problem of\ntokenizinginputstrings. Therefore,tokenizationisapracticalapproachthatcanbetakento",
    "addressthelengthissue. Segmentingastringintosmalltokens(suchascharacters)generally\nreduces the sparsity of the data, which makes it easier to learn the embeddings of these\ntokens,butsuchapproachesoftenleadtoalongsequence. Bycontrast,wewillhaveashorter\nsequenceifwesegmenttheinputstringintolargerunits,butthiswillsufferfromsparsedata.\nIn deterministic tokenization methods, which produce tokenization results using statistics\ncollectedfromtheentiredataset,thesequencelengthcanbesomehowcontrolledbyadjusting\nsomehyper-parameter,forexample,inbytepairencoding[Sennrichetal.,2016b],increasing\nthesizeofthevocabularygenerallyreducesthenumberoftheresultingtokens. Anotherway\ntoobtainanappropriatesequenceoftokensistouseamodelforchoosingamongtokenization\ncandidates[Kudo,2018;Provilkovetal.,2020]. Aswithmanyprobabilisticmodelsfortext\ngeneration,inthiscase,wecanaddpriorstothecriterionfortokenizationselectionsothatwe\ncanexpressapreferenceforshortersequencesoverlongersequences.",
    "Afourthapproachtosequencecompressionistodropsomeofthetokensinthesequence.\nForexample,inmanypracticalapplications,wechopthesequencewhenitslengthexceedsa\nthreshold. Wecanrelatethistotheearlystoppingandskippingapproachesinconditionalcom-\nputation. ThusthemethodsdiscussedinSection6.4.6aredirectlyapplied. Thetokendropping\nmethodscanalsobeviewedaspruningmethods,calledtokenpruning. Bydiscardingtokens\nthatarelessimportantforrepresentingtheentiresequence,tokenpruningcansignificantly\nreducethesequencelengthwhilemaintainingtheperformanceofNLPsystemsondownstream\ntasks[Kimetal.,2023].\n6.4.9 High Performance Computing Methods\nSofarinthissection,wehavediscussedefficientTransformermodelsfromtheperspectives\nof deep learning and NLP. However, we have not considered their efficiency on hardware.\nAsmodernhardwareprovidesavarietyofmodesforrunningaprogram, thepracticaltime\nand memory footprint savings generally depend on the specifications of hardware systems.",
    "One line of research on efficient use of computing resources explores methods of parallel\ncomputing. There have been many attempts to develop large-scale Transformer models by\nusingaclusterofmachines. Typically,scalingTransformerstomodelswithbillionsoreven\ntensofbillionsofparametersrequiresacarefuldesignofparallelismstrategiesforsharding",
    "6.4 Efficient Models 345\nthebignetworks. Moreefficientimplementationsofsuchsystemsalsoneedconsiderationsof\nnetworkingandcommunicationinthecluster,aswellastheutilizationofsparsemodelsthat\nactivateonlyasmallsub-setoftheparametersforeachsample,enablingtheuseofverylarge\nmodels. Mostofthesemethodshavebeenstudiedinanextensiveliteratureonhowtoscale\nupthetrainingofdeepneuralnetworkslikeTransformersefficiently[Lepikhinetal.,2021;\nBarhametal.,2022;Fedusetal.,2022b]. Theresultsofthesestudieswerefoundationalto\nmanyfollow-onworksoninvestigatingthescalinglawsforlargelanguagemodels[Brown\netal.,2020;Chowdheryetal.,2022]. Sincelarge-scaledistributedmodelsaregenericandnot\nspecializedtothecaseofTransformers,weskipthediscussionofthemhere. Theinterested\nreaderscanrefertotheabovepapersformoredetaileddiscussions.\nIn this subsection, we consider hardware-aware methods to seek greater computational\nefficiencyforTransformermodels. Wefirstconsiderasimplebutwidelyusedmethodthataims",
    "tostoreandexecuteneuralnetworksusinglowerormixed-precisionnumberrepresentations\n[Gholamietal.,2022]. Conventionalneuralnetworksaretypicallybasedonsingle-precision\nand/ordouble-precisionfloating-pointrepresentationsofdata. Whilesingle-precisionfloating-\npointdatatypesprovideasufficientlyprecisewaytorepresentparametersandintermediate\nstatesinmostcases,insomeapplications,theyarenotessential. Asanalternative,onecanuse\nhalf-precision(orevenlower-precision)formatsinstoringfloating-pointnumbersforneural\nnetworks. The size of the resulting model is thus half the size of the original model. One\nadvantageofusinghalf-precisionfloating-pointrepresentationsisthat,althoughprocessing\nsuchdatatypesrequiresnewAPIsoflinearalgebraoperationsandhardwaresupport,itdoes\nnotchangethemodelarchitecture,andsoweneedonlyaslightmodificationtothesystems.\nForexample,half-precisionfloating-pointrepresentationscanbeappliedtoeithertrainingor\ninferenceofTransformers,orboth.",
    "Recently, the deployment of large Transformer models has been further improved by\nquantizingthesemodels. Insignalprocessing,quantizationisaprocessofmappingcontinuous\nvalues(i.e.,floating-pointrepresentations)toasetofdiscretevalues(i.e.,fix-pointrepresen-\ntations) . This process is in general implemented using a system called quantizer. In the\ncontextofneuralnetworks,aquantizerinvolvestwofunctions—thequantizationfunction\nandthede-quantizationfunction. Thequantizationfunctionmapsafloatingpointnumbertoa\n(lower-bit)integer. Asimplequantizationfunctionisgivenby\nx\nQ(x) = ⌊ ⌉ (6.195)\ns\nwhere⌊·⌉isaroundingfunction26,xisthereal-valuedinput,andsisthequantizationstep\nsize that controls the level of quantization. The quantization function is coupled with a\nde-quantizationfunction\nD(r) = s·r (6.196)\nWith this notation, the quantizer can be expressed as D(Q(x)) = s·⌊x⌉. The difference\ns\n26⌊a⌉returnstheintegerclosesttoa.",
    "346 Chapter 6. Transformers\nbetweenD(Q(x))andxiscalledquantizationerror. Asmallervalueofstypicallyreducesthe\nquantizationerror. Inpractice,however,wewishtochooseanappropriatevalueofsinorderto\nmax{D(r)}\nspreadpossiblevaluesofQ(r)evenlyacrossvaluesofaninteger,forexample,s=\n2p−1\nwherepisthenumberofbitsusedtorepresentanintegerandmax{D(r)}isthemaximum\nvalueforD(r). Theaboveequationsshowoneofthesimplestcasesofquantization. More\ngeneral discussions of quantization can be found in books on digital signal processing and\nrelatedsurveys[OppenheimandSchafer,1975;RabinerandGold,1975;Gray,1998].\nApplying quantization to Transformers is relatively straightforward. The idea is that\nwe quantize the inputs and model parameters using Q(x), and feed them to a quantized\nTransformermodelinwhichallthelayersoperateoninteger-valuedtensors. Inotherwords,\nwe implement the model using integer-only arithmetic. However, the price to be paid for",
    "thiscompressedmodel,aswithmanyapproximationapproachestodeeplearning,isthatits\npredictionisnotasaccurateasthatofthestandardTransformermodel. Usingintegeroperations\ntoapproximatecontinuous-valuedoperationsgenerallyleadstoapproximationerrors. These\nerrorswillbeaccumulatedifthequantizedneuralnetworkisdeep. Furthermore,Transformer\nmodelsinvolvecomponents(suchasself-attentionsub-layers)thatrequirerelativelycomplex\nlinear algebra operations. Simply applying quantization to these sub-models will lead to\nhighaccuracyloss. Onesolutionistosimplifythemodelarchitectureanddevelopnewsub-\nmodels that is more feasible for quantization. Alternatively, a more common paradigm in\nquantizedneuralnetworksistoaddde-quantizationfunctionstotheneuralnetworkssothat\ntheoutputofalayerisfloating-pointtensorsandcanbeusedasusualinthefollowingsteps.\nConsiderasimpleexamplewherewemultiplyareal-valuedinputmatrixawithareal-valued\nparametermatrixA. WefirstquantizeaandA,andmultiplythemusinginteger-basedmatrix",
    "multiplication. Theresultisthende-quantizedtoareal-valuedmatrix. Inthisway,weobtain\nanapproximationD(Q(a)·Q(A))toa·Ainaverycheapway.\nHowever,sandwichingeachlayerbetweenQ(·)andD(·)willleadtoadditionalcostof\nrunningQ(·)andD(·). Insomepracticalapplications,thecomputationaloverheadintroduced\nbyQ(·)andD(·)isevenbiggerthanthetimesavingofperforminginteger-basedoperations.\nIngeneral,thebenefitofquantizingneuralnetworkswouldbelargerthanitscostiftheneural\nnetworks are large. Therefore, in practice it is common to perform quantized computation\nonlyforoperationswhosecomputationalcostsarehigh. Forexample,inrecentlargelanguage\nmodels, quantization is primarily applied to the multiplication of large matrices, yielding\nsignificanttimeandmemorysavings.\nWhilethequantizationapproachescanbeusedinbothtrainingandinference,awidely-\nused approach is to get Transformer models quantized after training (call it post-training\nquantization). Inthisapproach,quantizationisperformedonwell-trainedfloating-point-based",
    "neural networks and there will be fewer quantization-related errors. However, these errors\ncannot be compensated for because they exist after training. A more promising idea is to\ninvolvequantizationintrainingsothatthemodelcanlearntocompensateforquantization-\nrelatederrors[Jacobetal.,2018;Nageletal.,2021]. Therehavebeenseveralattemptstoapply\nquantization-awaretrainingtoTransformers[Bondarenkoetal.,2021;Stocketal.,2021;Yang\net al., 2023b]. In addition to computational efficiency, another important consideration for",
    "6.5 Applications 347\nhigh-performancesystemsistherestrictionsofthememoryhierarchy. Ingeneral,bettersystem\ndesignrequiresconsideringthespeedsandsizesofdifferentlevelsofmemory. Theproblemis\nevenmorecomplicatedwhenwetrainlargeTransformermodelsonmodernhardwarewhere\nbothGPUsandCPUsareused. Ageneralprincipleofsystemdesignisthatmemorytransfer\nbetweendifferentmemorylevelsshouldbeminimized. Whilewewouldideallyliketohavea\nlargehigh-levelmemoryonwhichwecanstoreallthedatathatweneedtoprocess,inmany\npracticalsituationsthesizeofthefast,on-chipmemoryisordersofmagnitudesmallerthan\nthesizeofdata. Inthiscase,wecanre-orderthememoryaccessinthealgorithmssothatthe\ndatausedinnearbycomputationstepscanbeloadedintothehigh-speedmemoryatonetime.\nThisideamotivatesthedevelopmentofmanyfastlinearalgebralibraries. Forexample,there\nare matrix multiplication algorithms that are highly optimized for different shapes of input\nmatrices.",
    "It is relatively straightforward to use these optimized linear algebra algorithms to build\na Transformer system. But the modules of this system are not optimized as a whole for\nefficiencyimprovement. Forexample,aself-attentionsub-layerinvolvesaseriesofoperations\nofscaling,normalization,andmatrixmultiplication. Althougheachoftheseoperationshas\nbeenimplementedinseveralsupportedandefficientlibrariesoflinearalgebra,successivecalls\ntothemstillrequiremultipletimesofmemorytransferwhenweswitchfromoneoperation\nto another. In practice, a better approach would be that we keep some of the intermediate\nstatesintheon-chipmemory,andreusetheminthefollowingcomputationstepsinsteadof\nfetchingthemagainfromtheslowmemory. Forexample,onmodernGPUs,asimplewayto\nachievethisistomergemultipleoperationsintoasingleoperation,knownaskernelfusion.\nForTransformermodels,ageneralideaistodesigndatapartitioningandlayoutstrategiesby\nwhich we maximize the computation on each data block loaded into the high-performance",
    "memory, while at the same time minimizing the memory transfer. There have been several\nattemptstousethesestrategiestoimprovetheattentionmodelsinTransformers[Ivanovetal.,\n2021;Popeetal.,2023]. Someofthesemethods,suchasflashattentionandpagedattention,\nhavebeensuccessfullyincorporatedintorecentlargelanguagemodels[Daoetal.,2022;Kwon\netal.,2023].\n6.5 Applications\nTransformershaveawiderangeofapplications,coveringnumerousNLPproblems. Whilethe\nTransformermodelintroducedbyVaswanietal.[2017]isbasedonastandardencoder-decoder\narchitecture,itismainlyusedinthreedifferentways.\n• Decoder-onlyModels. Byremovingthecross-attentionsub-layersfromaTransformer\ndecoder, the decoder becomes a standard language model. Hence this decoder-only\nmodel can be applied totext generation problems. For example, given a sequence of\nleft-contexttokens,weusethemodeltopredictthenextandfollowingtokens.\n• Encoder-onlyModels. Transformerencoderscanbetreatedassequencemodelsthat",
    "takeasequenceoftokensatonceandproduceasequenceofrepresentations,eachof",
    "348 Chapter 6. Transformers\nwhichcorrespondstoaninputtoken. Theserepresentationscanbeseenassomesort\nofencodingoftheinputsequence,andareoftentakenasinputtoapredictionmodel.\nThisencoder+predictorarchitectureformsthebasisofmanyNLPsystems,forexample,\nsystemsofsentenceclassification,sequencelabeling,andsoon. Pre-trainedTransformer\nencoderscanalsobeusedtomaptextsintothesamevectorspacesothatwecancompute\nthedistanceorsimilaritybetweenanytwotexts.\n• Encoder-Decoder Models. Encoder-decoder models are typically used to model\nsequence-to-sequenceproblems. ApplicationsincludemanytasksinNLPandrelated\nfields,suchasmachinetranslationandimagecaptioning.\nNotethatwhilemostTransformer-basedsystemscanfallintotheabovethreecategories,\nthe same NLP problem can generally be addressed using different types of models. For\nexample,recentdecoder-onlymodelshavedemonstratedgoodperformanceonabroadrange\nofproblemsbyframingthemastextgenerationtasks,thoughsomeoftheseproblemswere",
    "oftenaddressedbyusingencoder-decoderorencoder-onlymodels. Toillustratehowtheabove\nmodelsareapplied,thissectionconsidersafewapplicationswhereTransformersaschosenas\nthebackbonemodels.\n6.5.1 Language Modeling\nLanguage modeling is an NLP task in which we predict the next token given its preceding\ntokens. Thisisgenerallyformulatedasaproblemofestimatingthedistributionoftokensat\nposition i+1 given tokens at positions 0∼i (denoted by Pr(·|x ,...,x ) where {x ,...,x }\n0 i 0 i\ndenotethetokensuptopositioni). Thebestpredictedtokenistheonewhichmaximizesthe\nprobability,givenby\nxˆ = argmaxPr(x |x ,...,x ) (6.197)\ni+1 i+1 0 i\nxi+1∈V\nwhereV isthevocabulary. Thepredictioncanbeextendedtothetokensfollowingxˆ\ni+1\nxˆ = argmaxPr(x |x ,...,x ,xˆ ,...,xˆ ) (6.198)\nk+1 k+ 0 i i+1 k\nx ∈V\nk+1\nThis model forms the basis of many systems for text generation: given the context tokens\nx ...x , we generate the remaining tokens xˆ ...xˆ to make the sequence complete and\n1 i i+1 k+1\ncoherent.",
    "AsdiscussedinSection6.1.1,Transformerdecodersareessentiallylanguagemodels. The\nonlydifferencebetweentheproblemofdecodinginanencoder-decoderTransformerandthe\nproblemoflanguagemodelingisthattheTransformerdecodermakespredictionsconditioned\nonthe“context”tokensonboththeencoderanddecodersides,ratherthanbeingconditioned\nonprecedingtokenssolelyononeside. TomodifytheTransformerdecodertoimplementa\nstandardlanguagemodel,thecross-attentionsub-layersaresimplyremovedandaTransformer",
    "6.5 Applications 349\ndecodingblockcanbeexpressedas\nSl = Layer (Sl ) (6.199)\nffn self\nSl = Layer (Sl−1) (6.200)\nself self\nHere Sl denotes the output of the block at depth l. Layer (·) denotes the self-attention\nself\nsub-layer,andLayer (·)denotestheFFNsub-layer. Weseethatthisdecodingblockhasthe\nffn\nsameformasanencodingblock. Thedifferencebetweenthedecodingandencodingblocks\narisesfromthemaskingstrategiesadoptedintraining,becausetheformermaskstheattention\nfromapositionitoanyright-contextpositionk>iwhereasthelatterhasnosuchrestriction.\nASoftmaxlayerisstackedonthetopofthelastblock,andisusedtoproducethedistribution\nover the vocabulary at each position. For inference, the Transformer decoder works in an\nauto-regressivemanner,asdescribedinEq. (6.198).\nThe training of this model is standard. We learn the model by repeatedly updating the\nparameters, based on the gradients of the loss on the training samples. This paradigm can",
    "be extended to the training of large Transformer-based language models, which have been\nwidely applied in generative AI. However, training Transformer models at scale, including\ndecoder-only,encoder-only,andencoder-decodermodels,mayleadtonewdifficulties,such\nastraininginstabilities. Wewilldiscusstheseissuesfurtherinthefollowingchapters,where\nlarge-scalepre-trainingistheprimaryfocus.\n6.5.2 Text Encoding\nFor many NLP problems, a widely used paradigm is to first represent an input sequence in\nsomeform,andthenmakepredictionsfordownstreamtasksbasedonthisrepresentation. Asa\nresult,weseparatesequencemodelingorsequencerepresentationfromNLPtasks. Oneofthe\nadvantagesofthisparadigmisthatwecantrainasequencemodelthatisnotspecializedto\nparticulartasks,therebygeneralizingwell.\nClearly, Transformer encoders are a type of sequence model, and can be used as text\nencoders. Consider a Transformer encoder with L encoding blocks. The output of the last",
    "encodingblockcanbeseenastheencodingresult. Hereaddaspecialtokenx toanysequence,\n0\nindicatingthebeginningofasequence(writtenas⟨SOS⟩or[CLS]). Ifthereisasequenceof\nm+1inputtokensx x ...x ,theoutputoftheencoderwillbeasequenceofm+1vectors\n0 1 m\nhLhL...hL. Sincex isnotarealtokenandhasafixedpositionalembedding,itservesasatag\n0 1 m 0\nforcollectinginformationfromotherpositionsusingtheself-attentionmechanism. HencehL\n0\nisarepresentationoftheentiresequence,withnobiasesforanyspecifictokensorpositions. In\nmanycases,weneedasinglerepresentationofasequenceandtakeitasinputtodownstream\ncomponents of the system, for example, we can construct a sentence classification system\nbased on a single vector generated from {hL,...,hL}. In this case, we can simply use hL\n0 m 0\nas the representation of the sequence. A more general approach is to add a pooling layer\ntotheencoder. Thisallowsustoexplorevariouspoolingmethodstogeneratethesequence\nembeddingfrom{hL,...,hL}.\n0 m",
    "Intextencoding,tokensequencesarerepresentedbyreal-valuedvectors,oftenreferredto",
    "350 Chapter 6. Transformers\nassentencerepresentationsorsentenceembeddings,whichcanbeseenaspointsinamulti-\ndimensionalspace[Hilletal.,2016]. Anotherwaytomakeuseoftextencoding,therefore,is\ntoobtainsemanticorsyntacticsimilaritiesoftokensequencesbasedontheirrelativepositions\nor proximity in this space. A straightforward method for this is to compute the Euclidean\ndistancesbetweensequenceembeddings. Theshorterthedistancebetweentwosequences,the\nmoresimilartheyareconsideredtobe. Therearemanydistancemetricswecanchoose,andit\nispossibletocombinethemtoobtainabettermeasureofsequencesimilarity. Suchsimilarity\ncomputationsareappliedinareassuchastextentailment, informationretrieval, translation\nevaluation,amongothers[Ceretal.,2018;ReimersandGurevych,2019]. Additionally,they\nareoftenusedtoassessthequalityoftextencodingmodels.\nTextencodingisalsoacrucialcomponentofsequence-to-sequencemodels. Giventhis,we\ncandevelopaseparateTransformerencoderforsource-sidesequencemodelinginanencoder-",
    "decodersystem(seeFigure6.17). Forexample,wecanpre-trainaTransformerencoderon\nlarge-scalesource-sidetexts,anduseitastheencoderinadownstreamencoder-decodermodel.\nItisworthnotingthatwhiletheencoderisdesignedbasedontheTransformerarchitecture,\nthe decoder is not confined to just Transformers. Such flexibility enables us to incorporate\npre-trained Transformer encoders into hybrid sequence-to-sequence architectures, such as\nsystemsthatcombineaTransformerencoderwithanLSTMdecoder.\nInsupervisedlearningscenarios,trainingaTransformerencoderisstraightforward. We\ncantreatitasaregularcomponentofthetargetmodelandtrainthismodelontask-specific\nlabeleddata. However,suchamethodrequirestheencodertobeoptimizedoneachtask,and\nthe resulting encoder might not always generalize well to other tasks, especially given that\nlabeled data is scarce in most cases. A more prevalent approach is to frame the training of\ntextencodersasanindependenttaskinwhichsupervisionsignalsarederivedsolelyfromraw",
    "text. This led researchers to develop self-supervised Transformer encoders, such as BERT,\nwhich make use of large-scale unlabeled text, and these encoders were found to generalize\nwellacrossmanydownstreamtasks. Furtherdiscussionsofpre-trainedTransformerencoders\ncanbefoundinChapter7.\n6.5.3 Speech Translation\nAsillustratedinSection6.1,thestandardencoder-decoderTransformermodelwasproposed\ntomodelsequence-to-sequenceproblems. Hereweconsidertheproblemoftranslatingspeech\nin one language to text in another language — a problem that is conventionally addressed\nusingbothautomaticspeechrecognition(ASR)andmachinetranslationtechniques. Instead\nofcascadinganautomaticspeechrecognitionsystemandamachinetranslationsystem,we\ncanuseTransformermodelstobuildanend-to-endspeech-to-text(S2T)translationsystemto\ndirectlytranslatetheinputspeechtotheoutputtext.\nTo simplify the discussion, we assume that the input of an S2T translation system is a",
    "sequenceofsource-sideacousticfeaturevectors,denotedbya ...a , andtheoutputofthe\n1 m\nsystemisasequenceoftarget-sidetokens,denotedbyy ...y .27 Mappinga ...a toy ...y\n1 n 1 m 1 n\nisasequence-to-sequenceproblem. Thusitisstraightforwardtomodeltheproblemusingan\n27Inordertoobtaintheinputsequencetothesystem,weneedtodiscretizecontinuousspeechintosignals",
    "6.5 Applications 351\nSimilarity=0.7\nClassifier(e.g.,Softmax) SimilarityComputation(e.g.,FFN)\nPooling Pooling Pooling\nEncoder Encoder Encoder\n⟨CLS⟩Never give up . ⟨CLS⟩Never give up . ⟨CLS⟩Never say die !\n(a)Classification (b)SimilarityComputation\n永 不 放弃 。\nEncoder Decoder\n⟨CLS⟩Never give up . ⟨SOS⟩ 永 不 放弃\n(c)Sequence-to-SequenceModeling\nFigure6.17: IntegratingTransformerencodersascomponentsofdifferentsystems. Acommon\napproachistofeedtheoutputoftheencoder(withpooling)intoaclassifiertoobtainasequence\nclassificationsystem. AnotherwaytoutilizeTransformerencodersistocomputethesimilarity\nbetweentwosequences. Weusethesameencodertorepresentthetwosequences,andthen\nconstructaneuralnetworkontopofthetworepresentationsforproducingasimilarityscore\nbetweenthem. Asusual,Transformerencoderscanalsobeusedinencoder-decodersystems\ntomodelsequence-to-sequenceproblems.\nencoder-decoderTransformermodel,andthetrainingandinferenceofthismodelarestandard,\nlikeinneuralmachinetranslation.",
    "InS2Ttranslation,however,wehavetodealwithsequencemappingsbetweenmodalities\nandbetweenlanguagessimultaneously. Thisposesnewchallengescomparedwithconventional\nmachinetranslationproblemsandinfluencesthedesignofS2Ttranslationmodels. Therehave\nbeenseveralimprovementstoTransformermodelsforadaptingthembettertoS2Ttranslation\ntasks. SomeoftheimprovementsconcernthedesignofTransformerblocks[DiGangietal.,\n2019]. Forexample,inGulatietal.[2020]’ssystem,aCNNsub-layerandrelativepositional\nembeddings are integrated into each Transformer block, enabling the model to efficiently\nrepresentedbyfeaturevectors.Thisprocessistypicallynontrivial,requiringeitherafeatureextractorbasedona\nvarietyofsignalprocessingoperationsoraneuralnetworkthatlearnsfeaturemappingsinanend-to-endmanner.\nButwewillnotdiveintothedetailsofthesemethodsandsimplytreattheinputfeatureextractorasanupstream\nsystem.",
    "352 Chapter 6. Transformers\ncapturebothlocalandglobalfeatures.\nAnother line of research on S2T translation focuses on improving the encoder-decoder\narchitecture. Thisinvolvesmodificationstoeitherencodersordecoders,orboth. Toillustrate,\nFigure6.18showsthearchitecturesofthreeS2Ttranslationmodels. Allofthemarebasedon\nTransformers,buthavedifferentencoderarchitectures. Asshowninthefigure,thestandard\nencoder-decoderarchitecturehasoneTransformerencoderforreadingthesource-sideinput\na ...a andoneTransformerdecoderforproducingthetarget-sideoutputy ...y . Bycontrast,\n1 m 1 n\nthe decoupled encoder model separates the encoder into two stacked encoders — one for\nacousticmodeling(callitthespeechencoder),andonefortextualmodeling(callitthetext\nencoder)[Liuetal.,2020d;Xuetal.,2021a]. Thisdesignreflectsamodelinghierarchyin\nwhichrepresentationsindifferentlevelsofthenetworkareconcernedwithdifferentaspectsof\ntheproblem,forexample,thespeechencodermodelslow-levelfeaturesinmappingacoustic",
    "embeddingsintolargerlanguageunits,andthetextencodermodelsthesemanticorsyntactic\nfeatures in representing the entire input sequence. An advantage of separating out the text\nencoderisthattheencodingprocessfollowsourpriorknowledgethatweneedtofirsttranscribe\nthespeechinputandthentranslatethetranscriptintothetargetlanguage. Therefore,wecan\ntrainthespeechencoderinsomewaywetrainanASRsystem. Thisenablesustopre-train\nthe speech encoder and the text encoder on unlabeled data, and incorporate the pre-trained\nencodersintoS2Ttranslationsystems.\nAnalternativeencodingarchitectureisthetwo-streamarchitecture, asshowninFigure\n6.18(c). Likethedecoupledencoderarchitecture,thisarchitecturehasaspeechencoderanda\ntextencoder,butthetwoencodersworkinparallelratherthaninsequence[Yeetal.,2021].\nThespeechencodertakesacousticfeaturesasinputandthetextencodertakestokens(ortheir\nembeddings)asinput. Athirdencoder,calledsharedencoder,integratestheoutputsfrom",
    "boththespeechandtextencoders,mergingtherepresentationsfromthetwomodalities. This\ntwo-streamarchitectureisflexiblebecauseitprovidesmultiplewaystotrainS2Ttranslation\nmodels. Acommonapproachistotraineachbranchindividually. Forexample,ifwemaskthe\nspeechencoder,thenthemodelwilltransformintoamachinetranslationmodelwhichcanbe\ntrainedusingbilingualtexts. Conversely,ifwemaskthetextencoder,thenwecantrainthe\nmodelasastandardS2Ttranslationmodel. Forinference,thetextencodercanbedropped,\nandthespeechinputismodeledusingthespeechencoderandthesharedencoder.\nIn deep learning, training is often related to architecture design. Here, we have data in\ntwo modalities and two languages, and so we can develop multiple supervision signals for\nmulti-task learning of S2T translation models. A widely used method is to introduce ASR-\nrelatedlossintothetrainingofspeechencoders. Forexample,inthedecoupledencodermodel,\naclassifiercanbeconstructedbasedontheoutputfromthespeechencoder. Byminimizing",
    "the connectionist temporal classification (CTC) loss for this classifier, the speech encoder\ncanbeoptimizedinamannersimilartoASR.Ingeneral,trainingS2Ttranslationmodelsis\nchallenging because speech-to-text aligned data is scarce. Among typical responses to this\nchallengearedataaugmentation,pre-training,knowledgedistillationwithmachinetranslation,\nandsoon. However,anin-depthdiscussionofthesemethodsgoesbeyondthescopeofthis\ndiscussion on Transformers. The interested reader can refer to a recent survey on speech",
    "6.5 Applications 353\nText(Target)\nEncoder\nDecoder\n(Speech)\nSpeech(Source)\n(a)SingleEncoder+SingleDecoder\nText(Target)\nEncoder Encoder\nDecoder\n(Speech) (Text)\nSpeech(Source) Transcript(Source)\n(b)DecoupledEncoder+SingleDecoder\nText(Target)\nEncoder Encoder Shared\nDecoder\n(Speech) (Text) Encoder\nSpeech(Source) Transcript(Source)\n(c)Two-streamEncoder+SingleDecoder\nFigure 6.18: Architectures of speech-to-text translation models based on Transformers. In\nadditiontothestandardencoder-decoderarchitecture,wecanexplicitlymodeltheacousticand\ntextual(semantic)informationusingtwoseparateencoders,calledthespeechencoderandthe\ntextencoder. Inthedecoupledencoderarchitecture,thetwoencodersarestacked,thatis,text\nencodingisasubsequentprocessafterspeechencoding. Inthetwo-streamencoderarchitecture,\nthetwoencodersworkinparallel,andtheiroutputsaremergedusinganadditionalencoder,\ncalledthesharedencoder. Thedottedlineindicatesthepotentialforinteractionbetweenthe",
    "twoencoders. Forexample,wecoulddefinealossfunctiontominimizethedifferencebetween\ntheiroutputs,therebyguidingthemodeltowardsmorealignedrepresentations.\ntranslationformoreinformation[Xuetal.,2023a].\n6.5.4 Vision Models\nWhile Transformers were first used in NLP, their application to other domains has been\na prominent research topic. In computer vision, for instance, there is a notable trend of\nshiftingfromCNNstoTransformersasthebackbonemodels. Inthissubsection,weconsider\nVisionTransformer(ViT)-aninterestingapplicationofTransformerstoimageclassification",
    "354 Chapter 6. Transformers\nIsitabuilding? Classifier\nTransformerEncoder\nC=ExtraLearnable\nC\n[CLS]embedding\nPatchEmbedding+PositionalEmbedding\nFlattenedImagePatches\nImage\nFigure 6.19: Illustration of Vision Transformer for image classification[Dosovitskiy et al.,\n2021]. Therearethreesteps. Inthefirststep,theinputimageissegmentedintopatches,which\narethenflattenedandmappedintoembeddings. Inthesecondstep, aTransformerencoder\nisemployedtoprocessthesequenceofembeddings,representingtheimageasareal-valued\nvector(e.g.,theoutputoftheencoderatthefirstposition). Inthelaststep,aclassifierisbuilt\nontopofthisimagerepresentation.\n[Dosovitskiy et al., 2021]. Vision Transformer is a milestone model which opens the door\ntopurelyTransformer-basedvisionmodels. HereweconsiderthebasicstructureofVision\ntransformer to make this section concentrated and coherent, although there has been an\nextensiveliteratureonVisiontransformeranditsvariants. Moredetaileddiscussionsofvision",
    "transformercanbefoundinrecentsurveys[Hanetal.,2022;Liuetal.,2023e].\nThecoreideabehindVisionTransformeristotransformanimageintoasequenceofvisual\ntokens, and input this sequence into a Transformer encoder to generate a representation of\ntheimage. TheTransformerencoderisstandard,andsowewillnotdiscussithere,giventhe\nintroductiontoTransformerswehavepresentedsofarinthischapter. Mappinga2Dimage\nintoasequenceoftokensneedssomeadditionalwork. Supposewehaveanimagerepresented\nas an H×W ×C feature map, where H is the height of the image, W is the width of the\nimage,andC isthenumberofchannels. Thefirststepistosegmentthisimageintoanumber\nofpatches. SupposeallpatchesaresquaresofsidelengthP. Thentheresultingpatchescan\nberepresentedbyfeaturemapsofshapeP ×P ×C. Byorderingthesepatchesinsomeway,\nweobtainasequenceof HW patches,witheachpatchbeingtreatedasa“token”.\nP2\nGiventhispatchsequence,thesubsequentstepsarestraightforward. Forthepatchateach",
    "position,weobtainad-dimensionalembeddingbyalineartransformationoftheinputfeature\nmap. TheinputoftheTransformerencoderisasequenceofd-dimensionalvectors,eachof\nwhichisthesumofthecorrespondingpatchandpositionalembeddings. Figure6.19illustrates\nthepatchingandembeddingstepsinVisionTransformer.\nOnce we have a sequence of vectors for representing the image, we can employ the\nTransformerencodertoencodethesequence. Theencodingprocessisexactlythesameasthat",
    "6.5 Applications 355\nin text encoding as discussed in Section 6.5.2. For classification problems, we need only a\nsinglerepresentationoftheinput. Itisconvenienttotaketheoutputoftheencoderatposition0\n(denotedbyhL)andfeeditintoaclassifier. Giventhatthefirsttoken[CLS]servesasaspecial\n0\ntokenthatwouldbeattendedtobyallothertokens,hL providesanunbiasedrepresentationof\n0\ntheentiresequence.\nTypically,astandardwaytotrainVisionTransformeristominimizesomelossonlabeled\ndata, such as ImageNet. More recently, inspired by self-supervised learning in BERT-like\nmodels,therehavebeensuccessfulattemptstotrainTransformer-basedimageencoderson\nlarge-scaleunlabeleddata[Caronetal.,2021;Baoetal.,2021;Heetal.,2022]. Notethatone\nofthemostsignificantcontributionsofVisionTransformeristhatitunifiestherepresentation\nmodelsfordifferentmodalities. Thissuggeststhatifanobject,whetheranimageortext,is\nrepresented as a sequence of embeddings, it can be easily modeled using the Transformer\narchitecture.",
    "6.5.5 Multimodal Models\nTheabovediscussionofVisionTransformeroffersthepossibilityofunifyingtherepresenta-\ntionsfrommultiplemodalitiesusingthesameTransformerarchitecture. Infact,manyrecent\nmultimodal systems draw inspiration largely from Transformers [Xu et al., 2023c]. Such\nsystemsconvertobjectsfromdifferentmodalitiesintovectorsequencesandfeedthesevectors\nintoasingleTransformermodel. Theoutputisafusedrepresentationofallinputs,whichcan\nthenbeusedindownstreamsystems.\nAsasimpleexample,considerthetaskofencodingapairconsistingoftextanditscorre-\nspondingimage. First,werepresentboththetextandtheimageassequencesofembeddings\nthathavethesamedimensionality. Thisisacommonstepinsequencemodeling,whichwe\nhaveconfrontedmanytimessofar. Wecandothisbyusingeitherasimpleembeddingmodel\n(e.g., a word or patch embedding model) or a well-trained sequence model (e.g., a vision\nmodel). Then, these two sequences are concatenated into a long sequence involving both",
    "textualandvisualembeddings. Thefollow-onstepisstandard: aTransformerencodertakes\ntheconcatenatedsequenceofembeddingsasinputandproducesrepresentationsofthetextand\nimageasoutput. Notethatconcatenatingtextualandvisualsequencesisoneofthesimplest\nmethodsforvision-textmodeling. Thereareseveralalternativewaystomergeinformation\nfromdifferentmodalities,forexample,wecanfeedvisualrepresentationsintotheattention\nlayersofatextencoderordecoder[Lietal.,2022e;Alayracetal.,2022].\nThe above multimodal encoder can be used in both encoder-only and encoder-decoder\nsystems. For encoder-only systems, consider an example where, given an image and a\ndescription of it, we predict the class of the image using a classifier built on top of the\nencoder[Kimetal.,2021]. Forencoder-decodersystems,wepairtheencoderwithadecoder,\nas in sequence-to-sequence modeling [Cho et al., 2021]. For example, we might employ a\nTransformerdecodertogeneratetextbasedontheoutputoftheencoder. Acommonapplication",
    "ofthisarchitectureisvisualquestionanswering(VQA),whereanimageandaquestionabout\ntheimageareprovided,andthesystemistaskedwithgeneratingananswer[Antoletal.,2015].\nThearchitecturesofthesemodelsareillustratedinFigure6.20(a-b).",
    "356 Chapter 6. Transformers\nClassifier Isitananimal?\nTransformerEncoder\nC\nWordandPositionalEmbeddings PatchandPositionalEmbeddings\n⟨CLS⟩Acubewithgreencolor.\n(a)Multi-modalEncoder+Classifier\n一个 绿色 的 立方体 。\nTransformerEncoder Decoder\nC\nWordandPositionalEmbeddings PatchandPositionalEmbeddings ⟨SOS⟩一个 绿色 的 立方体\n⟨CLS⟩Acubewithgreencolor.\n(b)Multi-modalEncoder+TextDecoder(Translation)\nThe color is green .\nTransformerDecoder\nC\nWordandPositionalEmbeddings PatchandPositionalEmbeddings ⟨SOS⟩ The color is green\n⟨CLS⟩What’sthecolorofthecube?\n(c)Multi-modalDecoder(LanguageModeling)\nFigure6.20: Vision-textmodels. Blueboxesrepresentword+positionembeddings,andred\nboxesrepresentimagepatch+positionembeddings.",
    "6.6 Summary 357\nMorerecently,NLPhasseennewadvancesbyusinglargelanguagemodelstodealwith\nboth textual and other forms of data, such as images, videos, and audio, leading to new\nbreakthroughsinmultimodalprocessing[Liuetal.,2023a;Yinetal.,2023]. Byrepresenting\nallinputsasasequenceoftokenembeddings,theproblemwillbesimple: wepredictthenext\ntokengivenitscontext. Thiscanbedonebyusingdecoder-onlysystems,asshowninFigure\n6.20(c).\n6.6 Summary\nTransformermodelshaveachievedwidespreaduseoverthepastfewyearssincetheconceptof\nTransformerwasproposedbyVaswanietal.[2017]. Thishasacceleratedthedevelopmentof\nthesemodels,leadingtoagreatvarietyofnewalgorithms,systemsandconcepts. Athorough\ndiscussion of Transformers requires a broad scope, and so it is impossible to cover every\nproblemandtoprovideacompletelistofthecorrespondingreferences. Whilethischapterhas\npresentedadetailedintroductiontoTransformers,therearestilltopicsthatwedidnotmention,",
    "suchasthetheoreticalaspectsofthesemodels. Figure6.21showsanoverviewofTransformer\nmodels,whereweattempttogiveabigpicture. Notethatthesemodelsandrelatedtechniques\ncanbeclassifiedinmanydifferentways,andwejustshowoneofthem. Tosummarize,we\nwouldliketohighlightthefollowingpoints.\n• FoundationsofTransformers. AlthoughtheimpactofTransformershasbeenrevo-\nlutionary, they are not completely \"new\" models. From a deep learning perspective,\nTransformersarecomposedofcommonbuildingblocks,includingwordandpositional\nembeddings[Bengioetal.,2003a;Mikolovetal.,2013c;Gehringetal.,2017b],attention\nmechanisms[Bahdanauetal.,2014;Luongetal.,2015],residualconnections[Heetal.,\n2016b], layer-normalization [Ba et al., 2016], and so on. Many of these components\nwerepresentedinearliersystems,forexample,similarideaswithQKVattentioncanbe\nfoundinmemorynetworks[Sukhbaataretal.,2015]andhierarchicalattentionnetworks\n[Yangetal.,2016]. Transformersofferanovelapproachtointegratingthesecomponents,",
    "resulting in a unique architecture. For example, in Transformers, the combination of\nmulti-headattentionanddot-productQKVattention, alongwiththeincorporationof\nlayer-normalizationandresidualconnections,givesrisetoadistinctiveneuralnetwork\nblock,specificallyaself-attentionsub-layer. Thisdesignhassincebecomeadefacto\nstandardinmanyfollow-onsequencemodelingsystems.\n• AttentionModels. ThesuccessofTransformersonNLPtaskshaslargelybeenattributed\ntotheuseofmulti-headself-attentionforsequencemodeling. Thishasledtoasurge\nof interest in enhancing the attention mechanisms within Transformers. While it is\nimpossibletodetaileveryattentionmodel,thereareseveralnotableresearchdirections.\nOne prominent direction involves modifying the forms of QKV attention and multi-\nheadattentionforimprovedperformance. Thescopeofthisdirectionisvast,asthere\nare numerous aspects to consider when enhancing Transformers [Lin et al., 2022a].\nForexample,onemayaddnewcomponentstoself-attentionsub-layerstoadaptthem",
    "358 Chapter 6. Transformers\nApplications Supervised Transformers in NLP Vision\nNeural Machine Translation, ViT, Swin, MAE, BEiT, DETR,\nSummarization, Sentiment Analysis, iGPT, etc.\nSelf-supervised Transformers in NLP\nQuestion Answering, Named Entity\nSpeech\nNatural Language Natural Language Recognition, Syntactic Analysis, etc.\nWav2vec 2.0, Whisper,\nUnderstanding: Generation:\nHuBERT, Conformer, etc.\nBERT, RoBERTa, GPT Series, T5, BART,\nMulti-modal\nALBERT, SpanBERT, MASS, PaLM, LaMDA, Bioinformatics Robotics\nCLIP, ViLBERT, VisualBERT, VL-\nERNIE, XLM, XLNet, Megatron-Turing NLG,\nBERT, UNITER, LXMERT, ViLT, Recommendation Systems\netc. BLOOM, LLaMA, etc.\nVLT5, Data2vec, etc. Time Series Analysis\nTraining Inference Positional Encoding\nData Efficiency\nSearch Absolute Positional Encoding\nAlgorithms\nRotational Positional Encoding\nQuantization\nM\nP\nA\na\nc o\nr\nt\na\ni d v\nm\ne a l t\ne\nC i\nt\no o\ne\nn m\nr\nS\na\np h\nn\nr a\nd\ne r s in si g on\nEa\nC\nr\na\nly\nch\nS\ni\nt\nn\no\ng\np",
    "L E e x n tr g a t p h o lation R E e n la c t o iv d e in P g\nositional\n• Knowledge Distillation\n• Structured Pruning\nAttention\nSequence Compression\n• Token Pruning IO-aware Attention Syntax-\n• Progressive Downsampling aware\nSparse Attention\nAttention\n• Span--based / Local\nRegularization\nAttention\n• Layer Skipping / Wide Models\n• Chunked Attention\nStochastic Layers • Mixture of\n• Strided Attention\nExperts\n• Learning Attention Fields\nScalability\nAlternatives\nRecurrent and Memory\n• Convolutional Attention\nModels Deep Models\n• Linear Attention\n• Cache-based Memory\n• Encoding Long-term\nMemory • Layer Fusion\n• Retrieval-based Methods • Multi-scale Models Architecture\n• Multi-branch Models\nEfficiency Improvement\n• Numerical Method-inspired Models\nFoundations of Transformers\nTransformer Sub-models Architectures Theoretical Analysis\n• Word Encoding and Positional Encoding • Encoder Only • Linguistics\n• Multi-head Self-attention • Decoder Only • Machine Learning",
    "• Feed-forward Networks • Encoder-Decoder • Formal Systems\n• Layer Normalization and Residual Connections\nFoundations of Transformers Efficiency Attention Inference Training\nArchitecture Improvement Positional Embedding Scalability Applications\nFigure6.21: AnoverviewofTransformers.",
    "6.6 Summary 359\nto specific tasks, resulting in various Transformer variants. A second direction is to\nincorporate prior knowledge into the design of attention models. This makes sense,\nbecausemuchoftheemphasisintraditionalNLPhasbeenonusinglinguisticinsightsto\nguidesystemdesign,andwegenerallywantNLPsystemstobelinguisticallyexplainable.\nForexample,manyTransformer-basedsystemstakesyntacticparsesasinputinvarious\nforms and make use of syntax in sequence modeling. A third direction is to develop\nefficientattentionmodels[Tayetal.,2020b]. Self-attentionhaslongbeencriticizedfor\nitsquadratictimecomplexityanddependencyonallprevioustokensforeachnewtoken.\nInresponse,manyresearchershavefocusedonsimplifyingthestructureofself-attention,\noronapproximatingitusingsparseorrecurrentmodels. Thisconcernforefficiencyalso\nmotivatesthedevelopmentofalternativestoself-attention,suchasattentionmodelswith\nlineartimecomplexity. Inadditiontoexploringstrongerandmoreefficientattention",
    "models,itisnaturaltoexaminewhatknowledgeislearnedbysuchmodels. Interestingly,\nresearchers have found that the underlying structure of languages can be learned by\nmulti-headself-attentionmodels,althoughthesemodelsarenottrainedtorepresentsuch\nknowledge[Manningetal.,2020].\n• WordandPositionalEmbeddings. Transformersrepresenteachinputwordasaword\nembedding, along with its positional embedding. Learning these word embeddings\nisnotaspecificproblemforTransformers. Wecaneitherresorttowell-trainedword\nembeddings, suchastheWord2VecorGloVeembeddings, ortreatthemaslearnable\nparameters of Transformers. A related issue is tokenization of the input sequences.\nIn general, tokenization impacts the number of resulting tokens and the difficulty of\nlearning the corresponding embeddings. In many applications, therefore, one needs\ntocarefullychooseatokenizationmethod. Furthermore, positionalembeddingplays\nan important role in Transformers, as the attention mechanisms are order-insensitive",
    "bydesign[Dufteretal.,2022]. Althoughpositionalembeddingisageneralproblem,\nmuchoftheresearchisfocusedonimprovingTransformers,leadingtomodifications\nto Transformer models [Shaw et al., 2018; Huang et al., 2018]. Additionally, studies\nshow that, when we deal with sequences that are much longer than those in training\ndata,extrapolationcanbeachievedbyreplacingsinusoidalpositionalembeddingswith\nrotarypositionalembeddingsorsimplyscalingattentionweightswithapositionalscalar\n[Raffeletal.,2020;Suetal.,2021;Pressetal.,2021].\n• Training and Model Scaling. In the era of deep learning, powerful systems are\ntypically obtained by using large neural networks. A simple approach to increasing\nthe model capacity of Transformers is to stack more layers and/or enlarge the size\nof each representation. We can see many cases where deep and wide Transformer\nmodels consistently outperform small models. However, challenges arise when we\nattempttotrainextremelylargeTransformermodels,especiallywhengradientdescentis",
    "appliedovervastamountsofdata,demandingsubstantialcomputationalresources. An\nengineeringsolutionistodistributethetrainingacrossaclusterofcomputers[Lepikhin\net al., 2021; Chowdhery et al., 2022]. While distributed training is a very general",
    "360 Chapter 6. Transformers\nmethodandisnotrestrictedtoTransformers,itindeedinfluencesthedesignofmodel\narchitectures,forexample,sparseexpertmodelscaneasethetrainingwithdistributed\nparameters,servingasthefoundationformanyexpansiveTransformer-basedsystems.\nScalingupthetrainingofTransformersallowsustostudythescalinglawoflargeneural\nnetworks: howmodelperformancerelatestomodelsize,trainingdatasize,andtraining\ncost[Hestnessetal.,2017;Kaplanetal.,2020]. Thisissometimesaccompaniedbyan\ninterestingbehavior,knownasemergence[Weietal.,2022b]. InrecentNLPresearch,\nthe acquisition of emergent abilities has been considered one of the prerequisites for\ndevelopingstronglanguagemodels.\n• EfficientModels. Therearedifferentgoalsforefficiency. Forexample,onemaywisha\nsystemtobememoryefficientwhentheproblemismemorybound,oronemaywishit\ntobespeedefficientwhenlatencyisanimportantconsideration. Ingeneral,weneedto\nseekabalancebetweenthesegoals,resultingindifferentefficiencyoptimizations. In",
    "thecontextofTransformers, manyoftheseoptimizationsareachievedbymodifying\nthe attention models, as mentioned above. For example, several variants of the self-\nattention models are proposed to reduce the memory footprint when processing long\nsequences[Tayetal.,2020b]. Similarly,othervariantsaimtoreducecomputationand\nthusgivelowerlatency. Furthermore,beingatypeofneuralnetwork,Transformerscan\nbeoptimizedinwaysindependentofmodelarchitectures. Typicalmethodsincludebut\narenotlimitedtoconditionalcomputation,knowledgedistillation,structuredpruning,\nandsequencecompression. Efficiencyoptimizationscanalsobeconsideredfromthe\nperspectiveofcomputerarchitecture[Kimetal.,2023]. Forexample,whenapplying\nTransformerstosequence-to-sequenceproblems,theencodinganddecodingprocesses\nare generally compute-intensive and IO-intensive, respectively. Therefore, we can\nemploydifferentoptimizationmethodsfordifferentcomponentsofTransformers.\n• Inference. Theinferenceproblemiscommonlydiscussedinsequencegeneration. In",
    "NLP,weoftenneedtofindthe“best”hypothesisinaspaceinvolvingsequencesoftens\norevenhundredsoftokensoveravocabulary. Consideringthisaninstanceofthesearch\nprobleminartificialintelligence,manyalgorithmscanbeapplied,suchasbreadth-first\nsearch, depth-first search and A* search. In many practical applications of NLP, the\nefficiencyofthesearchsystemsisanimportantconsideration. Asaresult,optimized\nsearchalgorithmsarerequired. Mostofthesealgorithmshavebeenexploredinmachine\ntranslationandASR,andaredirectlyapplicabletoneuraltextgenerationmodelslike\nTransformer. Therearealsooptimizationsofconventionaldecodingmethodstailored\nto Transformers [Leviathan et al., 2023]. Moreover, the above-mentioned efficient\napproaches,suchastheefficientattentionmodels,arealsoinwidespreaduse,withmany\nsuccessfulexamplesindeployingneuralmachinetranslationsystemsandlargelanguage\nmodels[Heafieldetal.,2021;Daoetal.,2023].\n• Applications. Applications of Transformers cover a wide variety of NLP problems.",
    "During the development of Transformers, they were at first used to build supervised\nmodels that perform particular tasks. Later, a greater success was achieved by using",
    "6.6 Summary 361\nthemasbackbonenetworksforlargescaleself-supervisedlearningoffoundationmodels\n[Bommasanietal.,2021]. ThismarkedlychangedtheparadigminNLP.Weneedonly\npre-trainamodeltoobtaingeneralknowledgeoflanguagesonhugeamountsoftext.\nThen,weadaptthismodeltodownstreamtasksusingmethodswithlittleeffort,such\nasfine-tuningorprompting. Overthepastfewyears,wehavealsoseenanexplosion\nof applications for Transformers in fields other than NLP, such as computer vision,\nspeechprocessing, andbioinformatics. Theideabehindtheseapplicationsisthatwe\ncanrepresentanyinputdataasasequenceoftokensanddirectlyemployTransformers\ntomodelthissequence. ThisapproachextendsTransformerstogeneralrepresentation\nmodelsacrossdifferentmodalities,makingiteasiertouseTransformersforhandling\nmulti-modaldata.\n• LargeLanguageModelsasFoundationModels. Transformersformthebasisofrecent\nlargelanguagemodels,suchastheGPTseries,whichshowsurprisingbreakthroughs\ninNLP,andeveninartificialgeneralintelligence(AGI)[Bubecketal.,2023;Yang",
    "et al., 2023a]. Much of the research in large language models is more or less related\nto Transformers. For example, as discussed in Section 6.5.1, the problem of training\ntheselanguagemodelsisthesameasthatoftrainingTransformerdecoders. Andthe\nmodificationstoTransformerdecoderscanbedirectlyappliedtolargelanguagemodels.\nOn the other hand, the rapid development of large language models has also driven\nfurther improvements in various techniques for Transformers, such as efficient and\nlow-costadaptationoflargeTransformerstodifferenttasks.\n• TheoreticalAnalysis. AlthoughTransformershaveshownstrongempiricalresultsin\nvariousfields,theirtheoreticalaspectshavereceivedrelativelylessattentioncompared\ntotheextensiveresearchonmodelimprovementandengineering. Thisisnotaspecific\nproblemforTransformers,butacommonproblemfortheNLPandmachinelearning\ncommunities. In response, researchers have made attempts to analyze Transformers\nmoredeeply. OnewayistoviewTransformersasdeepneuralnetworksandinterpret",
    "themviamathematicaltools. Forexample,theresidualnetworksinTransformersare\nmathematically equivalent to the Euler solvers for ODEs. This equivalence suggests\nthat we can leverage insights from numerical ODE methods to inform model design.\nAnother promising avenue of research aims to develop a theoretical understanding\nof the self-attention mechanism, which distinguishes Transformers from other deep\nlearning models. For example, there have been studies on interpreting self-attention\nandTransformersfrommachinelearningperspectives,suchasdatacompression[Yu\netal.,2023a],optimization[Lietal.,2022c],andfunctionapproximation[Yunetal.,\n2019]. Moreover,Transformerscanalsoberelatedtoformalsystems,includingTuring\nmachines [Pérez et al., 2018], counter machines [Bhattamishra et al., 2020], regular\nand context-free languages [Hahn, 2020], Boolean circuits [Hao et al., 2022; Merrill\netal.,2022],programminglanguages[Weissetal.,2021],first-orderlogic[Chiangetal.,",
    "2023a],andsoon. TheseprovidetoolstostudytheexpressivityofTransformers. Itis,\nhowever,worthnotingthat,whilewecanunderstandTransformersinseveraldifferent",
    "362 Chapter 6. Transformers\nways,therearenogeneraltheoriestoexplainthenatureofthesemodels. Perhapsthis\nisachallengeforthefieldofmachinelearning,andmanyresearchersareworkingon\nthisissue. Butitisindeedanimportantissue,asthedevelopmentofthetheoriesbehind\ncomplexneuralnetworkslikeTransformerscanhelpdevelopsystemswithexplainable\nandpredictablebehaviors.",
    "III\nLarge Language Models\n7 Pre-training ............................. 365\n7.1 Pre-trainingNLPModels\n7.2 Self-supervisedPre-trainingTasks\n7.3 Example: BERT\n7.4 ApplyingBERTModels\n7.5 Summary\n8 Generative Models ....................... 403\n8.1 ABriefIntroductiontoLLMs\n8.2 TrainingatScale\n8.3 LongSequenceModeling\n8.4 Summary\n9 Prompting ............................... 467\n9.1 GeneralPromptDesign\n9.2 AdvancedPromptingMethods\n9.3 LearningtoPrompt\n9.4 Summary\n10 Alignment ............................... 533\n10.1 AnOverviewofLLMAlignment\n10.2 InstructionAlignment\n10.3 HumanPreferenceAlignment: RLHF\n10.4 ImprovedHumanPreferenceAlignment\n10.5 Summary\n11 Inference ................................ 587\n11.1 PrefillingandDecoding\n11.2 EfficientInferenceTechniques\n11.3 Inference-timeScaling\n11.4 Summary",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 7\nPre-training\nThedevelopmentofneuralsequencemodels,suchasTransformers,alongwiththeimprove-\nments in large-scale self-supervised learning, has opened the door to universal language\nunderstanding and generation. This achievement is largely motivated by pre-training: we\nseparatecommoncomponentsfrommanyneuralnetwork-basedsystems,andthentrainthem\nonhugeamountsofunlabeleddatausingself-supervision. Thesepre-trainedmodelsserveas\nfoundationmodelsthatcanbeeasilyadaptedtodifferenttasksviafine-tuningorprompting.\nAsaresult,theparadigmofNLPhasbeenenormouslychanged. Inmanycases,large-scale\nsupervisedlearningforspecifictasksisnolongerrequired,andinstead,weonlyneedtoadapt\npre-trainedfoundationmodels.\nWhile pre-training has gained popularity in recent NLP research, this concept dates\nback decades to the early days of deep learning. For example, early attempts to pre-train",
    "deeplearningsystemsincludeunsupervisedlearningforRNNs,deepfeedforwardnetworks,\nautoencoders, and others [Schmidhuber, 2015]. In the modern era of deep learning, we\nexperienced a resurgence of pre-training, caused in part by the large-scale unsupervised\nlearningofvariouswordembeddingmodels[Mikolovetal.,2013c;Penningtonetal.,2014].\nDuring the same period, pre-training also attracted significant interest in computer vision,\nwherethebackbonemodelsweretrainedonrelativelylargelabeleddatasetssuchasImageNet,\nandthenappliedtodifferentdownstreamtasks[Heetal.,2019;Zophetal.,2020]. Large-scale\nresearchonpre-traininginNLPbeganwiththedevelopmentoflanguagemodelsusingself-\nsupervisedlearning. Thisfamilyofmodelscoversseveralwell-knownexampleslikeBERT\n[Devlinetal.,2019]andGPT[Brownetal.,2020],allwithasimilarideathatgenerallanguage\nunderstandingandgenerationcanbeachievedbytrainingthemodelstopredictmaskedwords\nin a huge amount of text. Despite the simple nature of this approach, the resulting models",
    "show remarkable capability in modeling linguistic structure, though they are not explicitly\ntrainedtoachievethis. Thegeneralityofthepre-trainingtasksleadstosystemsthatexhibit\nstrongperformanceinalargevarietyofNLPproblems,evenoutperformingpreviouslywell-\ndevelopedsupervisedsystems. Morerecently,pre-trainedlargelanguagemodelshaveachieved\ngreatersuccess,showingtheexcitingprospectsformoregeneralartificialintelligence[Bubeck\netal.,2023].",
    "366 Chapter 7. Pre-training\nThischapterdiscussestheconceptofpre-traininginthecontextofNLP.Itbeginswitha\ngeneralintroductiontopre-trainingmethodsandtheirapplications. BERTisthenusedasan\nexampletoillustratehowasequencemodelistrainedviaaself-supervisedtask,calledmasked\nlanguage modeling. This is followed by a discussion of methods for adapting pre-trained\nsequencemodelsforvariousNLPtasks. Notethatinthischapter,wewillfocusprimarilyonthe\npre-trainingparadigminNLP,andtherefore,wedonotintendtocoverdetailsaboutgenerative\nlarge language models. A detailed discussion of these models will be left to subsequent\nchapters.\n7.1 Pre-training NLP Models\nThediscussionofpre-trainingissuesinNLPtypicallyinvolvestwotypesofproblems: sequence\nmodeling(orsequenceencoding)andsequencegeneration. Whiletheseproblemshavedifferent\nforms,forsimplicity,wedescribethemusingasinglemodeldefinedasfollows:\no = g(x ,x ,...,x ;θ)\n0 1 m\n= g (x ,x ,...,x ) (7.1)\nθ 0 1 m",
    "where{x ,x ,...,x }denotesasequenceofinputtokens1,x denotesaspecialsymbol(⟨s⟩or\n0 1 m 0\n[CLS])attachedtothebeginningofasequence,g(·;θ)(alsowrittenasg (·))denotesaneural\nθ\nnetworkwithparametersθ,andodenotestheoutputoftheneuralnetwork. Differentproblems\ncanvarybasedontheformoftheoutputo. Forexample,intokenpredictionproblems(asin\nlanguagemodeling),oisadistributionoveravocabulary;insequenceencodingproblems,ois\narepresentationoftheinputsequence,oftenexpressedasareal-valuedvectorsequence.\nTherearetwofundamentalissueshere.\n• Optimizing θ on a pre-training task. Unlike standard learning problems in NLP, pre-\ntrainingdoesnotassumespecificdownstreamtaskstowhichthemodelwillbeapplied.\nInstead,thegoalistotrainamodelthatcangeneralizeacrossvarioustasks.\n• Applyingthepre-trainedmodelg (·)todownstreamtasks. Toadaptthemodeltothese\nθˆ\ntasks,weneedtoadjusttheparametersθˆ\nslightlyusinglabeleddataorpromptthemodel\nwithtaskdescriptions.\nInthissection,wediscussthebasicideasinaddressingtheseissues.",
    "7.1.1 Unsupervised, Supervised and Self-supervised Pre-training\nIndeeplearning,pre-trainingreferstotheprocessofoptimizinganeuralnetworkbeforeit\nis further trained/tuned and applied to the tasks of interest. This approach is based on an\nassumptionthatamodelpre-trainedononetaskcanbeadaptedtoperformanothertask. As\naresult,wedonotneedtotrainadeep,complexneuralnetworkfromscratchontaskswith\n1Hereweassumethattokensarebasicunitsoftextthatareseparatedthroughtokenization.Sometimes,wewill\nusethetermstokenandwordinterchangeably,thoughtheyhavecloselyrelatedbutslightlydifferentmeaningsin\nNLP.",
    "7.1 Pre-training NLP Models 367\nlimitedlabeleddata. Instead,wecanmakeuseoftaskswheresupervisionsignalsareeasierto\nobtain. Thisreducestherelianceontask-specificlabeleddata,enablingthedevelopmentof\nmoregeneralmodelsthatarenotconfinedtoparticularproblems.\nDuringtheresurgenceofneuralnetworksthroughdeeplearning,manyearlyattemptsto\nachievepre-trainingwerefocusedonunsupervisedlearning. Inthesemethods,theparameters\nofaneuralnetworkareoptimizedusingacriterionthatisnotdirectlyrelatedtospecifictasks.\nForexample,wecanminimizethereconstructioncross-entropyoftheinputvectorforeach\nlayer[Bengioetal.,2006]. Unsupervisedpre-trainingiscommonlyemployedasapreliminary\nstepbeforesupervisedlearning,offeringseveraladvantages,suchasaidinginthediscovery\nofbetterlocalminimaandaddingaregularizationeffecttothetrainingprocess[Erhanetal.,\n2010]. Thesebenefitsmakethesubsequentsupervisedlearningphaseeasierandmorestable.\nAsecondapproachtopre-trainingistopre-trainaneuralnetworkonsupervisedlearning",
    "tasks. Forexample,considerasequencemodeldesignedtoencodeinputsequencesintosome\nrepresentations. Inpre-training,thismodeliscombinedwithaclassificationlayertoforma\nclassificationsystem. Thissystemisthentrainedonapre-trainingtask,suchasclassifying\nsentencesbasedonsentiment(e.g.,determiningifasentenceconveysapositiveornegative\nsentiment). Then, we adapt the sequence model to a downstream task. We build a new\nclassificationsystembasedonthispre-trainedsequencemodelandanewclassificationlayer\n(e.g.,determiningifasequenceissubjectiveorobjective). Typically,weneedtofine-tunethe\nparametersofthenewmodelusingtask-specificlabeleddata,ensuringthemodelisoptimally\nadjusted to perform well on this new type of data. The fine-tuned model is then employed\nto classify new sequences for this task. An advantage of supervised pre-training is that the\ntrainingprocess,eitherinthepre-trainingorfine-tuningphase,isstraightforward,asitfollows",
    "thewell-studiedgeneralparadigmofsupervisedlearninginmachinelearning. However,as\nthecomplexityoftheneuralnetworkincreases,thedemandformorelabeleddataalsogrows.\nThis,inturn,makesthepre-trainingtaskmoredifficult,especiallywhenlarge-scalelabeled\ndataisnotavailable.\nAthirdapproachtopre-trainingisself-supervisedlearning. Inthisapproach,aneural\nnetworkistrainedusingthesupervisionsignalsgeneratedbyitself,ratherthanthoseprovided\nby humans. This is generally done by constructing its own training tasks directly from\nunlabeleddata,suchashavingthesystemcreatepseudolabels. Whileself-supervisedlearning\nhasrecentlyemergedasaverypopularmethodinNLP,itisnotanewconcept. Inmachine\nlearning,arelatedconceptisself-trainingwhereamodelisiterativelyimprovedbylearning\nfrom the pseudo labels assigned to a dataset. To do this, we need some seed data to build\nan initial model. This model then generates pseudo labels for unlabeled data, and these",
    "pseudolabelsaresubsequentlyusedtoiterativelyrefineandbootstrapthemodelitself. Sucha\nmethodhasbeensuccessfullyusedinseveralNLPareas,suchaswordsensedisambiguation\n[Yarowsky,1995]anddocumentclassification[BlumandMitchell,1998]. Unlikethestandard\nself-trainingmethod,self-supervisedpre-traininginNLPdoesnotrelyonaninitialmodelfor\nannotatingthedata. Instead,allthesupervisionsignalsarecreatedfromthetext,andtheentire\nmodelistrainedfromscratch. Awell-knownexampleofthisistrainingsequencemodelsby\nsuccessivelypredictingamaskedwordgivenitsprecedingorsurroundingwordsinatext. This",
    "368 Chapter 7. Pre-training\nPrompting\nZero/Few\nShotLearning\nPre-training Training Pre-training Tuning Pre-training Tuning\nSelf-\nUnsupervised Supervised Supervised Supervised Supervised\nSupervised\nUnlabeled Labeled Labeled Labeled Unlabeled Labeled\nData Data Data Data Data Data\nTask2\nTask1\n(a)UnsupervisedPre-training (b)SupervisedPre-training (c)Self-supervisedPre-training\nFigure 7.1: Illustration of unsupervised, supervised, and self-supervised pre-training. In\nunsupervisedpre-training,thepre-trainingisperformedonlarge-scaleunlabeleddata. Itcan\nbeviewedasapreliminarysteptohaveagoodstartingpointforthesubsequentoptimization\nprocess, though considerable effort is still required to further train the model with labeled\ndataafterpre-training. Insupervisedpre-training,theunderlyingassumptionisthatdifferent\n(supervised)learningtasksarerelated. Sowecanfirsttrainthemodelononetask,andtransfer\nthe resulting model to another task with some training or tuning effort. In self-supervised",
    "pre-training, a model is pre-trained on large-scale unlabeled data via self-supervision. The\nmodel can be well trained in this way, and we can efficiently adapt it to new tasks through\nfine-tuningorprompting.\nenableslarge-scaleself-supervisedlearningfordeepneuralnetworks,leadingtothesuccessof\npre-traininginmanyunderstanding,writing,andreasoningtasks.\nFigure7.1showsacomparisonoftheabovethreepre-trainingapproaches. Self-supervised\npre-trainingissosuccessfulthatmostcurrentstate-of-the-artNLPmodelsarebasedonthis\nparadigm. Therefore,inthischapterandthroughoutthisbook,wewillfocusonself-supervised\npre-training. Wewillshowhowsequencemodelsarepre-trainedviaself-supervisionandhow\nthepre-trainedmodelsareapplied.\n7.1.2 Adapting Pre-trained Models\nAsmentionedabove,twomajortypesofmodelsarewidelyusedinNLPpre-training.\n• SequenceEncodingModels. Givenasequenceofwordsortokens,asequenceencoding\nmodelrepresentsthissequenceaseitherareal-valuedvectororasequenceofvectors,",
    "andobtainsarepresentationofthesequence. Thisrepresentationistypicallyusedas\ninputtoanothermodel,suchasasentenceclassificationsystem.\n• Sequence Generation Models. In NLP, sequence generation generally refers to the\nproblemofgeneratingasequenceoftokensbasedonagivencontext. Thetermcontext\nhas different meanings across applications. For example, it refers to the preceding",
    "7.1 Pre-training NLP Models 369\ntokensinlanguagemodeling,andreferstothesource-languagesequenceinmachine\ntranslation2.\nWe need different techniques for applying these models to downstream tasks after pre-\ntraining. Hereweareinterestedinthefollowingtwomethods.\n1. Fine-tuning of Pre-trained Models\nFor sequence encoding pre-training, a common method of adapting pre-trained models is\nfine-tuning. LetEncode (·)denoteanencoderwithparametersθ,forexample,Encode (·)\nθ θ\ncanbeastandardTransformerencoder. Providedwehavepre-trainedthismodelinsomeway\nandobtainedtheoptimalparametersθˆ\n,wecanemployittomodelanysequenceandgenerate\nthecorrespondingrepresentation,likethis\nH = Encode (x) (7.2)\nθˆ\nwhere x is the input sequence {x ,x ,...,x }, and H is the output representation which is\n0 1 m\nasequenceofreal-valuedvectors{h ,h ,...,h }. Becausetheencoderdoesnotworkasa\n0 1 m\nstandaloneNLPsystem,itisoftenintegratedasacomponentintoabiggersystem. Consider,",
    "for example, a text classification problem in which we identify the polarity (i.e., positive,\nnegative,andneutral)ofagiventext. Wecanbuildatextclassificationsystembystackinga\nclassifierontopoftheencoder. LetClassify (·)beaneuralnetworkwithparametersω. Then,\nω\nthetextclassificationmodelcanbeexpressedintheform\nPr (·|x) = Classify (H)\nω,θˆ ω\n= Classify (Encode (x)) (7.3)\nω θˆ\nHerePr (·|x)isaprobabilitydistributionoverthelabelset{positive,negative,neutral},\nω,θˆ\nandthelabelwiththehighestprobabilityinthisdistributionisselectedasoutput. Tokeepthe\nnotationuncluttered,wewilluseF (·)todenoteClassify (Encode (·)).\nω,θˆ ω θˆ\nBecausethemodelparametersω andθˆ arenotoptimizedfortheclassificationtask, we\ncannot directly use this model. Instead, we must use a modified version of the model that\nis adapted to the task. A typical way is to fine-tune the model by giving explicit labeling\nin downstream tasks. We can train F (·) on a labeled dataset, treating it as a common\nω,θˆ",
    "supervisedlearningtask. Theoutcomeofthefine-tuningistheparametersω˜ andθ˜ thatare\nfurther optimized. Alternatively, we can freeze the encoder parameters\nθˆ\nto maintain their\npre-trainedstate,andfocussolelyonoptimizingω. Thisallowstheclassifiertobeefficiently\nadaptedtoworkintandemwiththepre-trainedencoder.\nOnce we have obtained a fine-tuned model, we can use it to classify a new text. For\nexample,supposewehaveacommentpostedonatravelwebsite:\nIlovethefoodhere. It’samazing!\n2Moreprecisely,inauto-regressivedecodingofmachinetranslation,eachtarget-languagetokenisgenerated\nbasedonbothitsprecedingtokensandsource-languagesequence.",
    "370 Chapter 7. Pre-training\nWefirsttokenizethistextintotokens3,andthenfeedthetokensequencex intothefine-\nnew\ntunedmodelF (·). Themodelgeneratesadistributionoverclassesby\nω˜,θ˜\n(cid:104) (cid:105)\nF (x ) = Pr(positive|x ) Pr(negative|x ) Pr(neutral|x ) (7.4)\nω˜,θ˜ new new new new\nAndweselectthelabeloftheentrywiththemaximumvalueasoutput. Inthisexampleitis\npositive.\nIn general, the amount of labeled data used in fine-tuning is small compared to that of\nthe pre-training data, and so fine-tuning is less computationally expensive. This makes the\nadaptationofpre-trainedmodelsveryefficientinpractice: givenapre-trainedmodelanda\ndownstream task, we just need to collect some labeled data, and slightly adjust the model\nparametersonthisdata. Amoredetaileddiscussionoffine-tuningcanbefoundinSection7.4.\n2. Prompting of Pre-trained Models\nUnlikesequenceencodingmodels,sequencegenerationmodelsareoftenemployedindepen-\ndently to address language generation problems, such as question answering and machine",
    "translation,withouttheneedforadditionalmodules. Itisthereforestraightforwardtofine-tune\nthesemodelsascompletesystemsondownstreamtasks. Forexample,wecanfine-tuneapre-\ntrainedencoder-decodermultilingualmodelonsomebilingualdatatoimproveitsperformance\nonaspecifictranslationtask.\nAmong various sequence generation models, a notable example is the large language\nmodelstrainedonverylargeamountsofdata. Theselanguagemodelsaretrainedtosimply\npredictthenexttokengivenitsprecedingtokens. Althoughtokenpredictionissuchasimple\ntaskthatithaslongbeenrestrictedto“languagemodeling”only,ithasbeenfoundtoenable\nthelearningofthegeneralknowledgeoflanguagesbyrepeatingthetaskalargenumberof\ntimes. Theresultisthatthepre-trainedlargelanguagemodelsexhibitremarkablygoodabilities\nintokenprediction,makingitpossibletotransformnumerousNLPproblemsintosimpletext\ngenerationproblemsthroughpromptingthelargelanguagemodels. Forexample,wecanframe\ntheabovetextclassificationproblemasatextgenerationtask",
    "Ilovethefoodhere. It’samazing! I’m\nHere indicates the word or phrase we want to predict (call it the completion). If the\npredictedwordishappy,orglad,orsatisfied orarelatedpositiveword,wecanclassifythe\ntextaspositive. Thisexampleshowsasimplepromptingmethodinwhichweconcatenate\ntheinputtextwithI’mtoformaprompt. Then, thecompletionhelpsdecidewhichlabelis\nassignedtotheoriginaltext.\nGiventhestrongperformanceoflanguageunderstandingandgenerationoflargelanguage\nmodels,apromptcaninstructthemodelstoperformmorecomplextasks. Hereisaprompt\nwhereweprompttheLLMtoperformpolarityclassificationwithaninstruction.\n3Thetextcanbetokenizedinmanydifferentways. OneofthesimplestistosegmentthetextintoEnglish\nwordsandpunctuations{I,love,the,food,here,.,It,’s,amazing,!}",
    "7.1 Pre-training NLP Models 371\nAssumethatthepolarityofatextisalabelchosenfrom{positive,negative,\nneutral}. Identifythepolarityoftheinput.\nInput: Ilovethefoodhere. It’samazing!\nPolarity:\nThefirsttwosentencesareadescriptionofthetask. InputandPolarityareindicatorsofthe\ninput and output, respectively. We expect the model to complete the text and at the same\ntimegivethecorrectpolaritylabel. Byusinginstruction-basedprompts,wecanadaptlarge\nlanguagemodelstosolveNLPproblemswithouttheneedforadditionaltraining.\nThisexamplealsodemonstratesthezero-shotlearningcapabilityoflargelanguagemodels,\nwhichcanperformtasksthatwerenotobservedduringthetrainingphase. Anothermethodfor\nenablingnewcapabilitiesinaneuralnetworkisfew-shotlearning. Thisistypicallyachieved\nthroughin-contextlearning(ICT).Morespecifically,weaddsomesamplesthatdemonstrate\nhowaninputcorrespondstoanoutput. Thesesamples,knownasdemonstrations,areused\nto teach large language models how to perform the task. Below is an example involving",
    "demonstrations\nAssumethatthepolarityofatextisalabelchosenfrom{positive,negative,\nneutral}. Identifythepolarityoftheinput.\nInput: Thetrafficisterribleduringrushhours,makingitdifficulttoreachthe\nairportontime.\nPolarity: Negative\nInput: Theweatherhereiswonderful.\nPolarity: Positive\nInput: Ilovethefoodhere. It’samazing!\nPolarity:\nPromptingandin-contextlearningplayimportantrolesintherecentriseoflargelanguage\nmodels. WewilldiscusstheseissuesmoredeeplyinChapter9. However,itisworthnoting\nthatwhilepromptingisapowerfulwaytoadaptlargelanguagemodels,sometuningefforts\nare still needed to ensure the models can follow instructions accurately. Additionally, the\nfine-tuningprocessiscrucialforaligningthevaluesofthesemodelswithhumanvalues. More\ndetaileddiscussionsoffine-tuningcanbefoundinChapter10.",
    "372 Chapter 7. Pre-training\n7.2 Self-supervised Pre-training Tasks\nInthissection,weconsiderself-supervisedpre-trainingapproachesfordifferentneuralarchi-\ntectures,includingdecoder-only,encoder-only,andencoder-decoderarchitectures. Werestrict\nourdiscussiontoTransformerssincetheyformthebasisofmostpre-trainedmodelsinNLP.\nHowever, pre-training is a broad concept, and so we just give a brief introduction to basic\napproachesinordertomakethissectionconcise.\n7.2.1 Decoder-only Pre-training\nThedecoder-onlyarchitecturehasbeenwidelyusedindevelopinglanguagemodels[Radford\netal.,2018]. Forexample,wecanuseaTransformerdecoderasalanguagemodelbysimply\nremovingcross-attentionsub-layersfromit. Suchamodelpredictsthedistributionoftokensat\napositiongivenitsprecedingtokens,andtheoutputisthetokenwiththemaximumprobability.\nThestandardwaytotrainthismodel, asinthelanguagemodelingproblem, istominimize\nalossfunctionoveracollectionoftokensequences. LetDecoder (·)denoteadecoderwith\nθ",
    "parametersθ. Ateachpositioni,thedecodergeneratesadistributionofthenexttokensbased\nonitsprecedingtokens{x ,...,x },denotedbyPr (·|x ,...,x )(orpθ forshort). Suppose\n0 i θ 0 i i+1\nwehavethegold-standarddistributionatthesameposition,denotedbypgold\n. Forlanguage\ni+1\nmodeling, we can think of\npgold\nas a one-hot representation of the correct predicted word.\ni+1\nWethendefinealossfunctionL(pθ ,pgold)tomeasurethedifferencebetweenthemodel\ni+1 i+1\npredictionandthetrueprediction. InNLP,thelog-scalecross-entropylossistypicallyused.\nGivenasequenceofmtokens{x ,...,x },thelossonthissequenceisthesumoftheloss\n0 m\noverthepositions{0,...,m−1},givenby\nm−1\nLoss (x ,...,x ) = (cid:88) L(pθ ,pgold)\nθ 0 m i+1 i+1\ni=0\nm−1\n= (cid:88) LogCrossEntropy(pθ ,pgold) (7.5)\ni+1 i+1\ni=0\nwhereLogCrossEntropy(·)isthelog-scalecross-entropy,andpgold\nistheone-hotrepresenta-\ni+1\ntionofx .\ni+1\nThislossfunctioncanbeextendedtoasetofsequencesD. Inthiscase,theobjectiveof\npre-trainingistofindthebestparametersthatminimizethelossonD",
    "(cid:88)\nθˆ = argmin Loss (x) (7.6)\nθ\nθ\nx∈D\nNotethatthisobjectiveismathematicallyequivalenttomaximumlikelihoodestimation,and",
    "7.2 Self-supervised Pre-training Tasks 373\ncanbere-expressedas\n(cid:88)\nθˆ = argmax logPr (x)\nθ\nθ\nx∈D\ni−1\n(cid:88)(cid:88)\n= argmax logPr (x |x ,...,x ) (7.7)\nθ i+1 0 i\nθ\nx∈Di=0\nWiththeseoptimizedparametersθˆ ,wecanusethepre-trainedlanguagemodelDecoder (·)\nθˆ\ntocomputetheprobabilityPr (x |x ,...,x )ateachpositionofagivensequence.\nθˆ i+1 0 i\n7.2.2 Encoder-only Pre-training\nAs defined in Section 7.1.2, an encoder Encoder (·) is a function that reads a sequence of\nθ\ntokensx=x ...x andproducesasequenceofvectorsH=h ...h 4. Trainingthismodel\n0 m 0 m\nis not straightforward, as we do not have gold-standard data for measuring how good the\noutputofthereal-valuedfunctionis. Atypicalapproachtoencoderpre-trainingistocombine\nthe encoder with some output layers to receive supervision signals that are easier to obtain.\nFigure7.2showsacommonarchitectureforpre-trainingTransformerencoders,whereweadd\naSoftmaxlayerontopoftheTransformerencoder. Clearly,thisarchitectureisthesameasthat",
    "ofthedecoder-basedlanguagemodel,andtheoutputisasequenceofprobabilitydistributions\n pW,θ\n1\n.\n  . .   = Softmax W (Encoder θ (x)) (7.9)\npW,θ\nm\nHere pW,θ is the output distribution Pr(·|x) at position i. We use Softmax (·) to denote\ni W\nthat the Softmax layer is parameterized by W, that is, Softmax (H)=Softmax(H·W).\nW\nFor notation simplicity, we will sometimes drop the superscripts W and θ affixed to each\nprobabilitydistribution.\nThedifferencebetweenthismodelandstandardlanguagemodelsisthattheoutputp has\ni\ndifferent meanings in encoder pre-training and language modeling. In language modeling,\np istheprobabilitydistributionofpredictingthenextword. Thisfollowsanauto-regressive\ni\ndecodingprocess: alanguagemodelonlyobservesthewordsuptopositioniandpredictsthe\nnext. Bycontrast,inencoderpre-training,theentiresequencecanbeobservedatonce,andso\nitmakesnosensetopredictanyofthetokensinthissequence.\n4Ifweviewh iasarowvector,Hcanbewrittenas\nh \n0\nH =  . .  (7.8)\n . \nhm",
    "374 Chapter 7. Pre-training\nSelf-supervision\nE.g.,evaluatehowwellthe\nOutputforDownstreamTasks\nmodelreconstructsthemaskedtoken\nSoftmax PredictionNetwork\nEncoder Pre-trainedEncoder\ne e e e e e e e e e\n0 1 2 3 4 0 1 2 3 4\nx x x x x x x x x x\n0 1 2 3 4 0 1 2 3 4\n(masked)\n(a)Pre-training (b)ApplyingthePre-trainedEncoder\nFigure7.2: Pre-trainingaTransformerencoder(left)andthenapplyingthepre-trainedencoder\n(right). In the pre-training phase, the encoder, together with a Softmax layer, is trained via\nself-supervision. Intheapplicationphase,theSoftmaxlayerisremoved,andthepre-trained\nencoderiscombinedwithapredictionnetworktoaddressspecificproblems. Ingeneral,for\nbetteradaptationtothesetasks,thesystemisfine-tunedusinglabeleddata.\n1. Masked Language Modeling\nOne of the most popular methods of encoder pre-training is masked language modeling,\nwhich forms the basis of the well-known BERT model [Devlin et al., 2019]. The idea of",
    "masked language modeling is to create prediction challenges by masking out some of the\ntokensintheinputsequenceandtrainingamodeltopredictthemaskedtokens. Inthissense,\nthe conventional language modeling problem, which is sometimes called causal language\nmodeling, is a special case of masked language modeling: at each position, we mask the\ntokensintheright-context,andpredictthetokenatthispositionusingitsleft-context. However,\nincausallanguagemodelingweonlymakeuseoftheleft-contextinwordprediction,while\nthepredictionmaydependontokensintheright-context. Bycontrast,inmaskedlanguage\nmodeling, all the unmasked tokens are used for word prediction, leading to a bidirectional\nmodelthatmakespredictionsbasedonbothleftandright-contexts.\nMore formally, for an input sequence x=x ...x , suppose that we mask the tokens at\n0 m\npositionsA(x)={i ,...,i }. Henceweobtainamaskedtokensequencex¯ wherethetoken\n1 u\nat each position in A(x) is replaced with a special symbol [MASK]. For example, for the",
    "followingsequence\nTheearlybirdcatchestheworm\nwemayhaveamaskedtokensequencelikethis",
    "7.2 Self-supervised Pre-training Tasks 375\nThe[MASK]birdcatchesthe[MASK]\nwherewemaskthetokensearlyandworm(i.e.,i =2andi =6).\n1 2\nNowwehavetwosequencesxandx¯. Themodelisthenoptimizedsothatwecancorrectly\npredictxbasedonx¯. Thiscanbethoughtofasanautoencoding-likeprocess,andthetraining\nobjectiveistomaximizethereconstructionprobabilityPr(x|x¯). Notethatthereisasimple\nposition-wisealignmentbetweenxandx¯. Becauseanunmaskedtokeninx¯ isthesameasthe\ntokeninxatthesameposition,thereisnoneedtoconsiderthepredictionforthisunmasked\ntoken. Thisleadstoasimplifiedtrainingobjectivewhichonlymaximizestheprobabilitiesfor\nmaskedtokens. Wecanexpressthisobjectiveinamaximumlikelihoodestimationfashion\n(W(cid:99),θˆ) = argmax (cid:88) (cid:88) logPrW\ni\n,θ(x\ni\n|x¯) (7.10)\nW,θ\nx∈Di∈A(x)\noralternativelyexpressitusingthecross-entropyloss\n(W(cid:99),θˆ) = argmin (cid:88) (cid:88) LogCrossEntropy(pW,θ,pgold) (7.11)\ni i\nW,θ\nx∈Di∈A(x)\nwherePrW,θ(x |x¯)istheprobabilityofthetruetokenx atpositionk giventhecorrupted\nk k k",
    "inputx¯,andpW,θ istheprobabilitydistributionatpositionk giventhecorruptedinputx¯. To\nk\nillustrate,considertheaboveexamplewheretwotokensofthesequence“theearlybirdcatches\ntheworm”aremasked. Forthisexample,theobjectiveistomaximizethesumoflog-scale\nprobabilities\nLoss = logPr(x =early|¯x=[CLS]The[MASK]birdcatchesthe[MASK])+\n2\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nx¯2 x¯6\nlogPr(x =worm|¯x=[CLS]The[MASK]birdcatchesthe[MASK]) (7.12)\n6\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nx¯2 x¯6\nOnceweobtaintheoptimizedparametersW(cid:99) andθˆ ,wecandropW(cid:99). Then,wecanfurther\nfine-tunethepre-trainedencoderEncoder (·)ordirectlyapplyittodownstreamtasks.\nθˆ\n2. Permuted Language Modeling\nWhilemaskedlanguagemodelingissimpleandwidelyapplied,itintroducesnewissues. One\ndrawbackistheuseofaspecialtoken,[MASK],whichisemployedonlyduringtrainingbut\nnotat testtime. Thisleads toa discrepancybetween trainingandinference. Moreover, the",
    "auto-encodingprocessoverlooksthedependenciesbetweenmaskedtokens. Forexample,in\ntheaboveexample,thepredictionofx (i.e.,thefirstmaskedtoken)ismadeindependentlyof\n2\nx (i.e.,thesecondmaskedtoken),thoughx shouldbeconsideredinthecontextofx .\n6 6 2\nTheseissuescanbeaddressedusingthepermutedlanguagemodelingapproachtopre-\ntraining[Yangetal.,2019]. Similartocausallanguagemodeling,permutedlanguagemodeling\ninvolves making sequential predictions of tokens. However, unlike causal modeling where",
    "376 Chapter 7. Pre-training\npredictionsfollowthenaturalsequenceofthetext(likeleft-to-rightorright-to-left),permuted\nlanguagemodelingallowsforpredictionsinanyorder. Theapproachisstraightforward: we\ndetermine an order for token predictions and then train the model in a standard language\nmodelingmanner,asdescribedinSection7.2.1. Notethatinthisapproach,theactualorder\noftokensinthetextremainsunchanged,andonlytheorderinwhichwepredictthesetokens\ndiffers from standard language modeling. For example, consider a sequence of 5 tokens\nx x x x x . Lete representtheembeddingofx (i.e.,combinationofthetokenembedding\n0 1 2 3 4 i i\nandpositionalembedding). Instandardlanguagemodeling,wewouldgeneratethissequence\nintheorderofx →x →x →x →x . Theprobabilityofthesequencecanbemodeled\n0 1 2 3 4\nviaagenerationprocess.\nPr(x) = Pr(x )·Pr(x |x )·Pr(x |x ,x )·Pr(x |x ,x ,x )·\n0 1 0 2 0 1 3 0 1 2\nPr(x |x ,x ,x ,x )\n4 0 1 2 3\n= Pr(x )·Pr(x |e )·Pr(x |e ,e )·Pr(x |e ,e ,e )·\n0 1 0 2 0 1 3 0 1 2",
    "Pr(x |e ,e ,e ,e ) (7.13)\n4 0 1 2 3\nNow, let us consider a different order for token prediction: x →x →x →x →x .\n0 4 2 1 3\nThesequencegenerationprocesscanthenbeexpressedasfollows:\nPr(x) = Pr(x )·Pr(x |e )·Pr(x |e ,e )·Pr(x |e ,e ,e )·\n0 4 0 2 0 4 1 0 4 2\nPr(x |e ,e ,e ,e ) (7.14)\n3 0 4 2 1\nThis new prediction order allows for the generation of some tokens to be conditioned\non a broader context, rather than being limited to just the preceding tokens as in standard\nlanguage models. For example, in generating x , the model considers both its left-context\n3\n(i.e., e ,e ,e ) and right-context (i.e., e ). The embeddings e ,e ,e ,e incorporate the\n0 1 2 4 0 1 2 4\npositionalinformationofx ,x ,x ,x ,preservingtheoriginalorderofthetokens. Asaresult,\n0 1 2 4\nthis approach is somewhat akin to masked language modeling: we mask out x and use its\n3\nsurroundingtokensx ,x ,x ,x topredictthistoken.\n0 1 2 4\nThe implementation of permuted language models is relatively easy for Transformers.",
    "Because the self-attention model is insensitive to the order of inputs, we do not need to\nexplicitlyreorderthesequencetohaveafactorizationlikeEq. (7.14). Instead,permutation\ncanbedonebysettingappropriatemasksforself-attention. Forexample,considerthecaseof\ncomputingPr(x |e ,e ,e ). Wecanplacex ,x ,x ,x ,x inorderandblocktheattention\n1 0 4 2 0 1 2 3 4\nfromx tox inself-attention,asillustratedbelow\n3 1\nx x x x x\n0 1 2 3 4\nMasksforSelf-attention:\nBluebox=validattention\nGraybox=blockedattention",
    "7.2 Self-supervised Pre-training Tasks 377\nFor a more illustrative example, we compare the self-attention masking results of causal\nlanguagemodeling,maskedlanguagemodelingandpermutedlanguagemodelinginFigure\n7.3.\n3. Pre-training Encoders as Classifiers\nAnothercommonly-usedideatotrainan encoderistoconsiderclassificationtasks. Inself-\nsupervisedlearning,thisistypicallydonebycreatingnewclassificationchallengesfromthe\nunlabeled text. There are many different ways to design the classification tasks. Here we\npresenttwopopulartasks.\nAsimplemethod,callednextsentenceprediction(NSP),ispresentedinBERT’soriginal\npaper[Devlinetal.,2019]. TheassumptionofNSPisthatagoodtextencodershouldcapture\nthe relationship between two sentences. To model such a relationship, in NSP we can use\nthe output of encoding two consecutive sentences Sent and Sent to determine whether\nA B\nSent is the next sentence following Sent . For example, suppose Sent =’Itisraining.’\nB A A",
    "andSent =’Ineedanumbrella.’. Theinputsequenceoftheencodercouldbe\nB\n[CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]\nwhere [CLS] is the start symbol (i.e., x ) which is commonly used in encoder pre-training,\n0\nand [SEP] is a separator that separates the two sentences. The processing of this sequence\nfollows a standard procedure of Transformer encoding: we first represent each token x as\ni\nitscorrespondingembeddinge ,andthenfeedtheembeddingsequence{e ,...,e }intothe\ni 0 m\nencodertoobtaintheoutputsequence{h ,...,h }. Sinceh isgenerallyconsideredasthe\n0 m 0\nrepresentationoftheentiresequence,weaddaSoftmaxlayerontopofittoconstructabinary\nclassificationsystem. Thisprocessisillustratedasfollows\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nembedding: e e e e e e e e e e e e\n0 1 2 3 4 5 6 7 8 9 10 11\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nEncoder\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nencoding: h h h h h h h h h h h h\n0 1 2 3 4 5 6 7 8 9 10 11\n↓\nSoftmax\n↓\nIsNextorNot?",
    "Inordertogeneratetrainingsamples,weneedtwosentenceseachtime,oneforSent and\nA\ntheotherforSent . Asimplewaytodothisistoutilizethenaturalsequenceoftwoconsecutive\nB\nsentencesinthetext. Forexample,weobtainapositivesamplebyusingactualconsecutive",
    "378 Chapter 7. Pre-training\nx x x x x\n0 1 2 3 4\nx Pr(x )=1\n0 0\nx Pr(x |e )\n1 1 0\nx Pr(x |e ,e )\n2 2 0 1\nx Pr(x |e ,e ,e )\n3 3 0 1 2\nx Pr(x |e ,e ,e ,e )\n4 4 0 1 2 3\n(a)CausalLanguageModeling(order: x →x →x →x →x )\n0 1 2 3 4\nmasked masked\nx x x x x\n0 1 2 3 4\nx 0 1\nx Pr(x |e ,e ,e ,e ,e )\n1 1 0 mask 2 mask 4\nmasked\nx 2 1\nx Pr(x |e ,e ,e ,e ,e )\n3 3 0 mask 2 mask 4\nmasked\nx 4 1\n(b)MaskedLanguageModeling(order: x ,[MASK],x ,[MASK],x →x ,x )\n0 2 4 1 3\nx x x x x\n0 1 2 3 4\nx Pr(x )=1\n0 0\nx Pr(x |e ,e ,e )\n1 1 0 4 2\nx Pr(x |e ,e )\n2 2 0 4\nx Pr(x |e ,e ,e ,e )\n3 3 0 4 2 1\nx Pr(x |e )\n4 4 0\n(c)PermutedLanguageModeling(order: x →x →x →x →x )\n0 4 2 1 3\nFigure7.3: Comparisonofself-attentionmaskingresultsofcausallanguagemodeling,masked\nlanguage modeling and permuted language modeling. The gray cell denotes the token at\npositionj doesnotattendtothetokenatpositioni. Thebluecell(i,j)denotesthatthetoken\natpositionj attendstothetokenatpositioni. e representstheembeddingofthesymbol\nmask",
    "[MASK],whichisacombinationofthetokenembeddingandthepositionalembedding.",
    "7.2 Self-supervised Pre-training Tasks 379\nsentences,andanegativesamplebyusingrandomlysampledsentences. Consequently,training\nthismodelisthesameastrainingaclassifier. Typically,NSPisusedasanadditionaltraining\nlossfunctionforpre-trainingbasedonmaskedlanguagemodeling.\nAsecondexampleoftrainingTransformerencodersasclassifiersistoapplyclassification-\nbasedsupervisionsignalstoeachoutputofanencoder. Forexample,Clarketal.[2019b]in\ntheirELECTRAmodel,proposetrainingaTransformerencodertoidentifywhethereachinput\ntokenisidenticaltotheoriginalinputorhasbeenalteredinsomemanner. Thefirststepof\nthismethodistogenerateanewsequencefromagivensequenceoftokens,wheresomeof\nthe tokens are altered. To do this, a small masked language model (call it the generator) is\napplied: werandomlymasksomeofthetokens,andtrainthismodeltopredictthemasked\ntokens. Foreachtrainingsample,thismaskedlanguagemodeloutputsatokenateachmasked\nposition,whichmightbedifferentfromtheoriginaltoken. Atthesametime,wetrainanother",
    "Transformerencoder(callitthediscriminator)todeterminewhethereachpredictedtokenis\nthesameastheoriginaltokenoraltered. Morespecifically,weusethegeneratortogeneratea\nsequencewheresomeofthetokensarereplaced. Belowisanillustration.\noriginal: [CLS] The boy spent hours working on toys .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nmasked: [CLS] The boy spent [MASK] working on [MASK] .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nGenerator(smallmaskedlanguagemodel)\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nreplaced: [CLS] The boy spent decades working on toys .\nThen, weusethediscriminatortolabeleachofthesetokensasoriginalorreplaced, as\nfollows\nreplaced: [CLS] The boy spent decades working on toys .\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nDiscriminator(themodelwewant)\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\nlabel: original original original original replaced original original original original\nFor training, the generator is optimized as a masked language model with maximum\nlikelihoodestimation,andthediscriminatorisoptimizedasaclassifierusingaclassification-",
    "basedloss. InELECTRA,themaximumlikelihood-basedlossandtheclassification-basedloss\narecombinedforjointlytrainingboththegeneratoranddiscriminator. Analternativeapproach\nistousegenerativeadversarialnetworks(GANs),thatis,thegeneratoristrainedtofoolthe\ndiscriminator,andthediscriminatoristrainedtodistinguishtheoutputofthegeneratorfrom\nthetruedistribution. However,GAN-styletrainingcomplicatesthetrainingtaskandismore",
    "380 Chapter 7. Pre-training\ndifficulttoscaleup. Nevertheless,oncetrainingiscomplete,thegeneratorisdiscarded,andthe\nencodingpartofthediscriminatorisappliedasthepre-trainedmodelfordownstreamtasks.\n7.2.3 Encoder-Decoder Pre-training\nInNLP,encoder-decoderarchitecturesareoftenusedtomodelsequence-to-sequenceproblems,\nsuchasmachinetranslationandquestionanswering. Inadditiontothesetypicalsequence-to-\nsequenceproblemsinNLP,encoder-decodermodelscanbeextendedtodealwithmanyother\nproblems. A simple idea is to consider text as both the input and output of a problem, and\nso we can directly apply encoder-decoder models. For example, given a text, we can ask a\nmodeltooutputatextdescribingthesentimentoftheinputtext,suchaspositive,negative,and\nneutral.\nSuchanideaallowsustodevelopasingletext-to-textsystemtoaddressanyNLPproblem.\nWe can formulate different problems into the same text-to-text format. We first train an\nencoder-decodermodeltogaingeneral-purposeknowledgeoflanguageviaself-supervision.",
    "Thismodelisthenfine-tunedforspecificdownstreamtasksusingtargetedtext-to-textdata.\n1. Masked Encoder-Decoder Pre-training\nInRaffeletal.[2020]’sT5model,manydifferenttasksareframedasthesametext-to-text\ntask. EachsampleinT5followstheformat\nSourceText → TargetText\nHere→separatesthesourcetext,whichconsistsofataskdescriptionorinstructionandthe\ninputgiventothesystem,fromthetargettext,whichistheresponsetotheinputtask. Asan\nexample, consider a task of translating from Chinese to English. A training sample can be\nexpressedas\n[CLS]TranslatefromChinesetoEnglish: 你好！ → ⟨s⟩Hello!\nwhere[CLS]and⟨s⟩arethestartsymbolsonthesourceandtargetsides,respectively5.\n5Wecouldusethesamestartsymbolfordifferentsequences.Hereweusedifferentsymbolstodistinguishthe\nsequencesontheencoderanddecoder-sides.",
    "7.2 Self-supervised Pre-training Tasks 381\nLikewise,wecanexpressothertasksinthesameway. Forexample\n[CLS] Answer: whenwasAlbertEinsteinborn?\n→ ⟨s⟩HewasbornonMarch14,1879.\n[CLS] Simplify: theprofessor,whohaspublishednumerouspapersinhisfield,\nwillbegivingalectureonthetopicnextweek.\n→ ⟨s⟩Theexperiencedprofessorwillgivealecturenextweek.\n[CLS] Text: Johnboughtanewcar. Hypothesis: Johnhasacar.\n→ ⟨s⟩Entailment\n[CLS] ScorethetranslationfromEnglishtoChinese. English: wheninRome,doas\ntheRomansdo. Chinese: 人在罗马就像罗马人一样做事。\n→ ⟨s⟩0.81\nwhereinstructionsarehighlightedingray. Aninterestingcaseisthatinthelastexamplewe\nreframe the scoring problem as the text generation problem. Our goal is to generate a text\nrepresentingthenumber0.81,ratherthanoutputtingitasanumericalvalue.\nThe approach described above provides a new framework of universal language under-\nstanding and generation. Both the task instructions and the problem inputs are provided to",
    "thesystemintextform. Thesystemthenfollowstheinstructionstocompletethetask. This\nmethodputsdifferentproblemstogether,withthebenefitoftrainingasinglemodelthatcan\nperformmanytaskssimultaneously.\nIngeneral,fine-tuningisnecessaryforadaptingthepre-trainedmodeltoaspecificdown-\nstream task. In this process, one can use different ways to instruct the model for the task,\nsuchasusingashortnameofthetaskastheprefixtotheactualinputsequenceorproviding\nadetaileddescriptionofthetask. Sincethetaskinstructionsareexpressedintextformand\ninvolved as part of the input, the general knowledge of instruction can be gained through\nlearningthelanguageunderstandingmodelsinthepre-trainingphase. Thismayhelpenable\nzero-shotlearning. Forexample,pre-trainedmodelscangeneralizetoaddressnewproblems\nwherethetaskinstructionshaveneverbeenencountered.\nThere have been several powerful methods of self-supervised learning for either Trans-\nformerencodersordecoders. Applyingthesemethodstopre-trainencoder-decodermodelsis",
    "relativelystraightforward. Onecommonchoiceistotrainencoder-decodermodelsaslanguage\nmodels. Forexample,theencoderreceivesasequenceprefix,whilethedecodergeneratesthe\nremainingsequence. However,thisdiffersfromstandardcausallanguagemodeling,where\ntheentiresequenceisautoregressivelygeneratedfromthefirsttoken. Inourcase,theencoder\nprocessestheprefixatonce,andthenthedecoderpredictssubsequenttokensinthemannerof\ncausallanguagemodeling. Putmoreprecisely,thisisaprefixlanguagemodelingproblem: a",
    "382 Chapter 7. Pre-training\nlanguagemodelpredictsthesubsequentsequencegivenaprefix,whichservesasthecontext\nforprediction.\nConsiderthefollowingexample\n[CLS]Thepuppiesarefrolicking → ⟨s⟩outsidethehouse.\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nPrefix SubsequentSequence\nWecandirectlytrainanencoder-decodermodelusingexampleslikethis. Then,theencoder\nlearns to understand the prefix, and the decoder learns to continue writing based on this\nunderstanding. For large-scale pre-training, it is easy to create a large number of training\nexamplesfromunlabeledtext.\nIt is worth noting that for pre-trained encoder-decoder models to be effective in multi-\nlingual and cross-lingual tasks, such as machine translation, they should be trained with\nmulti-lingual data. This typically requires that the vocabulary includes tokens from all the\nlanguages. Bydoingso,themodelscanlearnsharedrepresentationsacrossdifferentlanguages,",
    "therebyenablingcapabilitiesinbothlanguageunderstandingandgenerationinamulti-lingual\nandcross-lingualcontext.\nAsecondapproachtopre-trainingencoder-decodermodelsismaskedlanguagemodeling.\nInthisapproach,asdiscussedinSection7.2.2,tokensinasequencearerandomlyreplaced\nwithamasksymbol,andthemodelisthentrainedtopredictthesemaskedtokensbasedonthe\nentiremaskedsequence.\nAsanillustration,considerthetaskofmaskingandreconstructingthesentence\nThepuppiesarefrolickingoutsidethehouse.\nBymaskingtwotokens(say,frolickingandthe),wehavetheBERT-styleinputandoutputof\nthemodel,asfollows\n[CLS]Thepuppiesare[MASK]outside[MASK]house.\n→ ⟨s⟩ frolicking the\nHere denotesthemaskedpositionatwhichwedonotmaketokenpredictions. Byvarying\nthepercentageofthetokensinthetext,thisapproachcanbegeneralizedtowardseitherBERT-\nstyletrainingorlanguagemodeling-styletraining[Songetal.,2019]. Forexample,ifwemask\noutallthetokens,thenthemodelistrainedtogeneratetheentiresequence\n[CLS][MASK][MASK][MASK][MASK][MASK][MASK][MASK][MASK]",
    "→ ⟨s⟩Thepuppiesarefrolickingoutsidethehouse.\nInthiscase,wetrainthedecoderasalanguagemodel.\nNotethat,inthecontextoftheencoder-decoderarchitecture,wecanusetheencoderto\nread the masked sequence, and use the decoder to predict the original sequence. With this\nobjective,weessentiallyhaveadenoisingautoencoder: theencodertransformsacorrupted",
    "7.2 Self-supervised Pre-training Tasks 383\nLoss\n[M] [M] [M]frolicking[M] the [M] [M]\nEncoder Decoder\n[CLS] Thepuppiesare [M] in [M] house . ⟨s⟩ [M] [M] [M]frolicking[M] the [M]\n(a)Traininganencoder-decodermodelwithBERT-stylemaskedlanguagemodeling\nLossoverthesequence\nThe puppies arefrolicking in the house .\nEncoder Decoder\n[CLS] Thepuppiesare [M] in [M] house . ⟨s⟩ The puppies arefrolicking in the house\n(b)Traininganencoder-decodermodelwithdenoisingautoencoding\nFigure7.4: Traininganencoder-decodermodelusingBERT-styleanddenoisingautoencoding\nmethods. In both methods, the input to the encoder is a corrupted token sequence where\nsometokensaremaskedandreplacedwith[MASK](or[M]forshort). Thedecoderpredicts\nthesemaskedtokens,butindifferentways. InBERT-styletraining,thedecoderonlyneeds\ntocomputethelossforthemaskedtokens, whiletheremainingtokensinthesequencecan\nbe simply treated as [MASK] tokens. In denoising autoencoding, the decoder predicts the",
    "sequence of all tokens in an autoregressive manner. As a result, the loss is obtained by\naccumulatingthelossesofallthesetokens,asinstandardlanguagemodeling.\ninputintosomehiddenrepresentation,andthedecoderreconstructstheuncorruptedinputfrom\nthishiddenrepresentation. Hereisanexampleofinputandoutputfordenoisingtraining.\n[CLS]Thepuppiesare[MASK]outside[MASK]house.\n→ ⟨s⟩Thepuppiesarefrolickingoutsidethehouse.\nBy learning to map from this corrupted sequence to its uncorrupted counterpart, the model\ngainstheabilitytounderstandontheencodersideandtogenerateonthedecoderside. See\nFigure7.4foranillustrationofhowanencoder-decodermodelistrainedwithBERT-styleand\ndenoisingautoencodingobjectives.\nAs we randomly select tokens for masking, we can certainly mask consecutive tokens",
    "384 Chapter 7. Pre-training\n[Joshietal.,2020]. Hereisanexample.\n[CLS]Thepuppiesare[MASK]outside[MASK][MASK].\n→ ⟨s⟩Thepuppiesarefrolickingoutsidethehouse.\nAnotherwaytoconsiderconsecutivemaskedtokensistorepresentthemasspans. Here\nwefollowRaffeletal.[2020]’swork,anduse[X],[Y]and[Z]todenotesentineltokensthat\ncover one or more consecutive masked tokens. Using this notation, we can re-express the\nabovetrainingexampleas\n[CLS]Thepuppiesare[X]outside[Y].\n→ ⟨s⟩[X]frolicking[Y]thehouse[Z]\nTheideaisthatwerepresentthecorruptedsequenceasasequencecontainingplaceholder\nslots. The training task is to fill these slots with the correct tokens using the surrounding\ncontext. Anadvantageofthisapproachisthatthesequencesusedintrainingwouldbeshorter,\nmaking the training more efficient. Note that masked language modeling provides a very\ngeneralframeworkfortrainingencoder-decodermodels. Varioussettingscanbeadjustedto\nhave different training versions, such as altering the percentage of tokens masked and the",
    "maximumlengthofthemaskedspans.\n2. Denoising Training\nIfweviewtheproblemoftrainingencoder-decodermodelsasaproblemoftrainingdenoising\nautoencoders,therewilltypicallybemanydifferentmethodsforintroducinginputcorruption\nandreconstructingtheinput. Forinstance,beyondrandomlymaskingtokens,wecanalsoalter\nsomeofthemorrearrangetheirorder.\nSupposewehaveanencoder-decodermodelthatcanmapaninputsequencextoanoutput\nsequencey\ny = Decode (Encode (x))\nω θ\n= Model (x) (7.15)\nθ,ω\nwhereθ andω aretheparametersoftheencoderandthedecoder,respectively. Indenoising\nautoencodingproblems,weaddsomenoisetoxtoobtainanoisy,corruptedinputx . By\nnoise\nfeedingx intotheencoder,wewishthedecodertooutputtheoriginalinput. Thetraining\nnoise\nobjectivecanbedefinedas\n(θˆ,ωˆ) = argminLoss(Model (x ),x) (7.16)\nθ,ω noise\nθ,ω\nHerethelossfunctionLoss(Model (x ),x)evaluateshowwellthemodelModel (x )\nθ,ω noise θ,ω noise\nreconstructstheoriginalinputx. Wecanchoosethecross-entropylossasusual.",
    "Asthemodelarchitectureandthetrainingapproachhavebeendeveloped,theremaining",
    "7.2 Self-supervised Pre-training Tasks 385\nissue is the corruption of the input. Lewis et al. [2020a], in their BART model, propose\ncorruptingtheinputsequenceinseveraldifferentways.\n• TokenMasking. Thisisthesamemaskingmethodthatweusedinmaskedlanguage\nmodeling. Thetokensintheinputsequencearerandomlyselectedandmasked.\n• Token Deletion. This method is similar to token masking. However, rather than\nreplacingtheselectedtokenswithaspecialsymbol[MASK],thesetokensareremoved\nfromthesequence. Seethefollowingexampleforacomparisonofthetokenmasking\nandtokendeletionmethods.\nOriginal(x): Thepuppiesarefrolickingoutsidethehouse.\nTokenMasking(x ): Thepuppiesare[MASK]outside[MASK]house.\nnoise\nTokenDeletion(x ): Thepuppiesarefrolickingoutsidethehouse.\nnoise\nwheretheunderlinedtokensintheoriginalsequencearemaskedordeleted.\n• SpanMasking. Non-overlappingspansarerandomlysampledoverthesequence. Each\nspan is masked by [MASK]. We also consider spans of length 0, and, in such cases,",
    "[MASK]issimplyinsertedatapositioninthesequence. Forexample,wecanusespan\nmaskingtocorrupttheabovesequenceas\nOriginal(x): The0puppiesarefrolickingoutsidethehouse.\nSpanMasking(x ): The[MASK]puppiesare[MASK]house.\nnoise\nHere the span frolicking outside the is replaced with a single [MASK]. 0 indicates a\nlength-0span,andsoweinsertan[MASK]betweenTheandpuppies. Spanmasking\nintroduces new prediction challenges in which the model needs to know how many\ntokensaregeneratedfromaspan. Thisproblemisverysimilartofertilitymodelingin\nmachinetranslation[Brownetal.,1993].\nIfweconsiderasequenceconsistingofmultiplesentences,additionalmethodsofcorrup-\ntioncanbeapplied. IntheBARTmodel,therearetwosuchmethods.\n• SentenceReordering. Thismethodrandomlypermutesthesentencessothatthemodel\ncanlearntoreordersentencesinadocument. Consider,forexample,twoconsecutive\nsentences\nHardworkleadstosuccess. Successbringshappiness.\nWecanreorderthetwosentencestohaveacorruptedinputsequence",
    "Successbringshappiness. Hardworkleadstosuccess.\n• DocumentRotation. Thegoalofthistaskistoidentifythestarttokenofthesequence.\nFirst,atokenisrandomlyselectedfromthesequence. Then,thesequenceisrotatedso\nthattheselectedtokenisthefirsttoken. Forexample,supposeweselectthetokenleads\nfromtheabovesequence. Therotatedsequenceis",
    "386 Chapter 7. Pre-training\nselected\nHardwork leadstosuccess. Successbringshappiness. Hardwork\nwherethesubsequenceHardworkbeforeleadsisappendedtotheendofthesequence.\nFor pre-training, we can apply multiple corruption methods to learn robust models, for\nexample,werandomlychooseoneofthemforeachtrainingsample. Inpractice,theoutcome\nofencoder-decoderpre-trainingdependsheavilyontheinputcorruptionmethodsused,andso\nwetypicallyneedtochooseappropriatetrainingobjectivesthroughcarefulexperimentation.\n7.2.4 Comparison of Pre-training Tasks\nSofar,wehavediscussedanumberofpre-trainingtasks. Sincethesametrainingobjective\ncanapplytodifferentarchitectures(e.g.,usingmaskedlanguagemodelingforbothencoder-\nonlyandencoder-decoderpre-training),categorizingpre-trainingtasksbasedsolelyonmodel\narchitecture does not seem ideal. Instead, we summarize these tasks based on the training\nobjectives.\n• LanguageModeling. Typically,thisapproachreferstoanauto-regressivegeneration",
    "procedureofsequences. Atonetime, itpredictsthenexttokenbasedonitsprevious\ncontext.\n• MaskedLanguageModeling. MaskedLanguageModelingbelongstoageneralmask-\npredictframework. Itrandomlymaskstokensinasequenceandpredictsthesetokens\nusingtheentiremaskedsequence.\n• PermutedLanguageModeling. Permutedlanguagemodelingfollowsasimilaridea\nto masked language modeling, but considers the order of (masked) token prediction.\nItreorderstheinputsequenceandpredictsthetokenssequentially. Eachpredictionis\nbasedonsomecontexttokensthatarerandomlyselected.\n• Discriminative Training. In discriminative training, supervision signals are created\nfrom classification tasks. Models for pre-training are integrated into classifiers and\ntrainedtogetherwiththeremainingpartsoftheclassifierstoenhancetheirclassification\nperformance.\n• Denoising Autoencoding. This approach is applied to the pre-training of encoder-\ndecodermodels. Theinputisacorruptedsequenceandtheencoder-decodermodelsare",
    "trainedtoreconstructtheoriginalsequence.\nTable 7.1 illustrates these methods and their variants using examples. The use of these\nexamplesdoesnotdistinguishbetweenmodels,butwemarkthemodelarchitectureswhere\nthepre-trainingtaskscanbeapplied. Ineachexample,theinputconsistsofatokensequence,\nand the output is either a token sequence or some probabilities. For generation tasks, such\nas language modeling, superscripts are used to indicate the generation order on the target\nside. If the superscripts are omitted, it indicates that the output sequence can be generated",
    "7.2 Self-supervised Pre-training Tasks 387\nMethod Enc Dec E-D Input Output\nCausalLM • • The1kitten2is3chasing4the5ball6.7\nPrefixLM • • [C]Thekittenis chasing1the2ball3.4\nMaskedLM • • [C]Thekitten[M]chasingthe[M]. is ball\nMASS-style • • [C]Thekitten[M][M][M]ball. is chasingthe\nBERT-style • • [C]Thekitten[M]playingthe[M]. kitten is chasing ball\nPermutedLM • [C]Thekittenischasingtheball. The5kitten7is6chasing1the4ball2.3\nNextSentence • [C]Thekittenischasingtheball. Pr(IsNext|representation-of-[C])\nPrediction Birdseatworms.\nSentence • Encodeasentenceash and Score(h ,h )\na a b\nComparison anothersentenceash\nb\nTokenClassification • [C]Thekittenischasingtheball. Pr(·|The)Pr(·|kitten)... Pr(·|.)\nTokenReordering • [C]. kittenthechasingTheisball The1kitten2is3chasing4the5ball6.7\nTokenDeletion • [C]Thekittenischasingtheball. The1kitten2is3chasing4the5ball6.7\nSpanMasking • [C]Thekitten[M]is[M]. The1kitten2is3chasing4the5ball6.7\nSentinelMasking • [C]Thekitten[X]the[Y] [X]1is2chasing3[Y]4ball5.6",
    "Sentence • [C]Theballrollsawayswiftly. The The1kitten2is3chasing4the5ball6.7\nReordering kittenischasingtheball. The8ball9rolls10away11swiftly12.13\nDocument • [C]chasingtheball. Theballrolls The1kitten2is3chasing4the5ball6.7\nRotation awayswiftly. Thekittenis The8ball9rolls10away11swiftly12.13\nTable7.1: Comparisonofpre-trainingtasks,includinglanguagemodeling,maskedlanguage\nmodeling,permutedlanguagemodeling,discriminativetraining,anddenoisingautoencoding.\n[C]=[CLS],[M]=[MASK],[X],[Y]=sentineltokens. Enc,DecandE-Dindicatewhether\ntheapproachcanbeappliedtoencoder-only,decoder-only,encoder-decodermodels,respec-\ntively. Forgenerationtasks,superscriptsareusedtorepresenttheorderofthetokens.\neitherautoregressivelyorsimultaneously. Onthesourceside,weassumethatthesequence\nundergoesastandardTransformerencodingprocess,meaningthateachtokencanseetheentire\nsequenceinself-attention. Theonlyexceptionisinpermutedlanguagemodeling,wherean",
    "autoregressivegenerationprocessisimplementedbysettingattentionmasksontheencoder\nside. Tosimplifythediscussion,weremovethetoken⟨s⟩fromthetarget-sideofeachexample.\nWhile these pre-training tasks are different, it is possible to compare them in the same\nframeworkandexperimentalsetup[Dongetal.,2019;Raffeletal.,2020;Lewisetal.,2020a].\nNotethatwecannotlistallthepre-trainingtaskshereastherearemanyofthem. Formore\ndiscussionsonpre-trainingtasks,theinterestedreadermayrefertosomesurveysonthistopic\n[Qiuetal.,2020b;Hanetal.,2021a].",
    "388 Chapter 7. Pre-training\n7.3 Example: BERT\nInthissection, weintroduceBERTmodels, whichareamongthemostpopularandwidely\nusedpre-trainedsequenceencodingmodelsinNLP.\n7.3.1 The Standard Model\nThestandardBERTmodel,whichisproposedinDevlinetal.[2019]’swork,isaTransformer\nencodertrainedusingbothmaskedlanguagemodelingandnextsentencepredictiontasks. The\nlossusedintrainingthismodelisasumofthelossofthetwotasks.\nLoss = Loss +Loss (7.17)\nBERT MLM NSP\nAsisregularintrainingdeepneuralnetworks,weoptimizethemodelparametersbyminimizing\nthis loss. To do this, a number of training samples are collected. During training, a batch\nof training samples is randomly selected from this collection at a time, and Loss is\nBERT\naccumulatedoverthesetrainingsamples. Then,themodelparametersareupdatedviagradient\ndescentoritsvariants. Thisprocessisrepeatedmanytimesuntilsomestoppingcriterionis\nsatisfied,suchaswhenthetraininglossconverges.\n1. Loss Functions",
    "Ingeneral,BERTmodelsareusedtorepresentasinglesentenceorapairofsentences,and\nthus can handle various downstream language understanding problems. In this section we\nassumethattheinputrepresentationisasequencecontainingtwosentencesSent andSent ,\nA B\nexpressedas\n[CLS] Sent [SEP] Sent [SEP]\nA B\nHerewefollowthenotationinBERT’spaperanduse[SEP]todenotetheseparator.\nGiven this sequence, we can obtain Loss and Loss separately. For masked lan-\nMLM NSP\nguage modeling, we predict a subset of the tokens in the sequence. Typically, a certain\npercentage of the tokens are randomly selected, for example, in the standard BERT model,\n15%ofthetokensineachsequenceareselected. Thenthesequenceismodifiedinthreeways\n• TokenMasking. 80%oftheselectedtokensaremaskedandreplacedwiththesymbol\n[MASK]. Forexample\nOriginal: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]\nMasked: [CLS]Itis[MASK]. [SEP]Ineed[MASK]umbrella. [SEP]\nwheretheselectedtokensareunderlined. Predictingmaskedtokensmakesthemodel",
    "learntorepresenttokensfromtheirsurroundingcontext.\n• RandomReplacement. 10%oftheselectedtokensarechangedtoarandomtoken. For",
    "7.3 Example: BERT 389\nexample\nOriginal: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]\nRandomToken: [CLS]Itisraining. [SEP]Ineedanhat. [SEP]\nThishelpsthemodellearntorecoveratokenfromanoisyinput.\n• Unchanged. 10%oftheselectedtokensarekeptunchanged. Forexample,\nOriginal: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]\nUnchangedToken: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]\nThisisnotadifficultpredictiontask,butcanguidethemodeltouseeasierevidencefor\nprediction.\nLetA(x)bethesetofselectedpositionsofagiventokensequencex,andx¯bethemodified\nsequenceofx. Thelossfunctionofmaskedlanguagemodelingcanbedefinedas\n(cid:88)\nLoss = − logPr (x |x¯) (7.18)\nMLM i i\ni∈A(x)\nwherePr (x |x¯)istheprobabilityofpredictingx atthepositionigivenx¯. Figure7.5shows\ni i i\narunningexampleofcomputingLoss .\nMLM\nFor next sentence prediction, we follow the method described in Section 7.2.2. Each\ntrainingsampleisclassifiedintoalabelset{IsNext,NotNext},forexample,\nSequence: [CLS]Itisraining. [SEP]Ineedanumbrella. [SEP]",
    "Label: IsNext\nSequence: [CLS]Thecatsleepsonthewindowsill. [SEP]Applesgrowontrees. [SEP]\nLabel: NotNext\nThe output vector of the encoder for the first token [CLS] is viewed as the sequence repre-\nsentation,denotedbyh (orh ). Aclassifierisbuiltontopofh . Then,wecancompute\ncls 0 cls\ntheprobabilityofalabelcgivenh ,i.e.,Pr(c|h ). Therearemanylossfunctionsonecan\ncls cls\nchoose for classification problems. For example, in maximum likelihood training, we can\ndefineLoss as\nNSP\nLoss = −logPr(c |h ) (7.19)\nNSP gold cls\nwherec isthecorrectlabelforthissample.\ngold",
    "390 Chapter 7. Pre-training\nInput: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nSelecttokenswithaprobabilityof15%\nTokenSelection: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nMaskselectedtokenswithaprobabilityof80%\nTokenMasking: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]\nAlterselectedtokenswithaprobabilityof10%\nToken: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\nReplacement\nKeepselectedtokensunchangedwithaprobabilityof10%\nUnchanged: [CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\nTraintheTransformerencoderwiththemodifiedsequence\ntraining I an umbrella\nh h h h h h h h h h h h\n0 1 2 3 4 5 6 7 8 9 10 11\nTransformer Encoder\ne e e e e e e e e e e e\n0 1 2 3 4 5 6 7 8 9 10 11\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\nFigure 7.5: A running example of BERT-style masked language modeling. First, 15% of\nthe tokens are randomly selected. These selected tokens are then processed in one of three",
    "ways: replacedwitha[MASK]token(80%ofthetime),replacedwitharandomtoken(10%\nof the time), or kept unchanged (10% of the time). The model is trained to predict these\nselectedtokensbasedonthemodifiedsequence. e representstheembeddingofthetokenat\ni\nthepositioni. GrayboxesrepresenttheSoftmaxlayers.\n2. Model Setup\nAsshowninFigure7.6,BERTmodelsarebasedonthestandardTransformerencoderarchi-\ntecture. Theinputisasequenceofembeddings,eachbeingthesumofthetokenembedding,",
    "7.3 Example: BERT 391\nthepositionalembedding,andthesegmentembedding.\ne = x+e +e (7.20)\npos seg\nBoththetokenembedding(x)andpositionalembedding(e )areregular,asinTransformer\npos\nmodels. Thesegmentembedding(e )isanewtypeofembeddingthatindicateswhethera\nseg\ntokenbelongstoSent orSent . Thiscanbeillustratedbythefollowingexample.\nA B\nToken [CLS] It is raining . [SEP] I need an umbrella . [SEP]\nx x x x x x x x x x x x x\n0 1 2 3 4 5 6 7 8 9 10 11\ne PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) PE(10) PE(11)\npos\ne e e e e e e e e e e e e\nseg A A A A A A B B B B B B\nThemainpartofBERTmodelsisamulti-layerTransformernetwork. ATransformerlayer\nconsistsofaself-attentionsub-layerandanFFNsub-layer. Bothofthemfollowthepost-norm\narchitecture: output=LNorm(F(input)+input), where F(·) is the core function of the\nsub-layer(eitheraself-attentionmodeloranFFN),andLNorm(·)isthelayernormalization\nunit. Typically,anumberofTransformerlayersarestackedtoformadeepnetwork. Ateach",
    "positionofthesequence,theoutputrepresentationisareal-valuedvectorwhichisproduced\nbythelastlayerofthenetwork.\nThereareseveralaspectsonemayconsiderindevelopingBERTmodels.\n• VocabularySize(|V|). InTransformers,eachinputtokenisrepresentedasanentryina\nvocabularyV. Largevocabulariescancovermoresurfaceformvariantsofwords,but\nmayleadtoincreasedstoragerequirements.\n• EmbeddingSize(d ). Everytokenisrepresentedasad -dimensionalreal-valuedvector.\ne e\nAspresentedabove,thisvectoristhesumofthetokenembedding,positionalembedding,\nandsegmentembedding,allofwhicharealsod -dimensionalreal-valuedvectors.\ne\n• Hidden Size (d). The input and output of a sub-layer are of d dimensions. Besides,\nmostofthehiddenstatesofasub-layerared-dimensionalvectors. Ingeneral,dcanbe\nroughlyviewedasthewidthofthenetwork.\n• NumberofHeads(n ). Inself-attentionsub-layers,oneneedstospecifythenumber\nhead\nofheadsusedinmulti-headself-attention. Thelargerthisnumberis,themoresub-spaces",
    "inwhichattentionisperformed. Inpracticalsystems,weoftensetn ≥4.\nhead\n• FFNHiddenSize(d ). ThesizeofthehiddenlayeroftheFFNsusedinTransform-\nffn\ners is typically larger than d. For example, a typical setting is d =4d. For larger\nffn\nTransformers,suchasrecentlargemodels,d maybesettoaverylargevalue.\nffn\n• ModelDepth(L). Usingdeepnetworksisaneffectivewaytoimprovetheexpressive\npower of Transformers. For BERT models, L is typically set to 12 or 24. However,\nnetworks with even greater depth are also feasible and can be applied for further en-\nhancements.",
    "392 Chapter 7. Pre-training\n...\nOutputLayer EncoderOutput\nh ∈Rd isthecontextual\ni\nh h ...h\n0 1 m representationofx\ni\nLayerNormalization\nFFNSub-layer\nhiddensize: d\nFFNhiddensize: d\nffn\nFFN\nlayers\nLayerNormalization\nSelf-attentionSub-layer\nhiddensize: d\nnumberofheads: n\nhead\nSelf-attention\nEmbedding\ne=x+e\npos\n+e\nseg\n∈Rde\nToken Position Segment\nInput\nx correspondstoanentryofV\ni\nx x ...x\n0 1 m\nFigure7.6: ThemodelarchitectureofBERT(Transformerencoder). Theinputtokensarefirst\nrepresentedasembeddings,eachofwhichisthesumofthecorrespondingtokenembedding,\npositionalembeddingandsegmentembedding. Then,theembeddingsequenceisprocessedby\nastackofTransformerlayers. Eachlayerinthisstackincludesaself-attentionsub-layeranda\nFFNsub-layer. TheoutputoftheBERTmodelisasequenceofvectorsproducedbythefinal\nTransformerlayer.\nDifferentsettingsofthesehyper-parametersleadtodifferentmodelsizes. Therearetwo\nwidely-usedBERTmodels.\n• BERT : d=768,L=12,n =12,totalnumberofparameters=110M.\nbase head",
    "• BERT : d=1,024,L=24,n =16,totalnumberofparameters=340M.\nlarge head\nTrainingBERTmodelsfollowsthestandardtrainingprocessofTransformers. Training\nlarger models such as BERT requires more training effort and time. This is a common\nlarge\nproblemforpre-training,especiallywhenamodelistrainedonaverylargeamountofdata.\nInpractice,thereareoftenconsiderationsoftrainingefficiency. Forexample,apracticeisto\nfirsttrainaBERTmodelonrelativelyshortsequencesforalargenumberoftrainingsteps,and",
    "7.3 Example: BERT 393\nthencontinuetrainingitonfull-lengthsequencesfortheremainingtrainingsteps.\n7.3.2 More Training and Larger Models\nBERT is a milestone model in NLP, sparking many subsequent efforts to improve it. One\ndirection is to scale up the model itself, including increasing training data and developing\nlargermodels.\nRoBERTa,anextensionofthestandardBERTmodel,isanexampleofsuchefforts[Liu\netal.,2019]. Itintroducestwomajorimprovements. First,simplyusingmoretrainingdataand\nmorecomputecanimproveBERTmodelswithoutneedofchangingthemodelarchitectures.\nSecond,removingtheNSPlossdoesnotdecreasetheperformanceondownstreamtasksifthe\ntrainingisscaledup. Thesefindingssuggestexploringageneraldirectionofpre-training: we\ncancontinuetoimprovepre-trainingbyscalingituponsimplepre-trainingtasks.\nA second approach to improving BERT models is to increase the number of model\nparameters. Forexample,inHeetal.[2021]’swork,a1.5billion-parameterBERT-likemodel",
    "isbuiltbyincreasingboththemodeldepthandhiddensize. However,scalingupBERTand\nvariousotherpre-trainedmodelsintroducesnewchallengesintraining,forexample,training\nverylargemodelsoftenbecomesunstableanddifficulttoconverge. Thismakestheproblem\nmore complicated, and requires careful consideration of various aspects, including model\narchitecture, parallel computation, parameter initialization, and so on. In another example,\nShoeybi et al. [2019] successfully trained a 3.9 billion-parameter BERT-like model, where\nhundredsofGPUswereusedtomanagetheincreasedcomputationaldemands.\n7.3.3 More Efficient Models\nComparedtoitspredecessors,BERTisarelativelylargemodelforthetimeitwasproposed.\nThisincreaseinmodelsizeresultsinlargermemoryrequirementsandaconsequentslowdown\nin system performance. Developing smaller and faster BERT models is part of the broader\nchallengeofbuildingefficientTransformers,whichhasbeenextensivelydiscussedinChapter",
    "6. However, a deeper discussion of this general topic is beyond the scope of our current\ndiscussion. HereweinsteadconsiderafewefficientvariantsofBERT.\nSeveral threads of research are of interest to NLP researchers in developing efficient\nBERT models. First, work on knowledge distillation, such as training student models with\ntheoutputofwell-trainedteachermodels,showsthatsmallerBERTmodelscanbeobtained\nby transferring knowledge from larger BERT models. Given that BERT models are multi-\nlayernetworkswithseveraldifferenttypesoflayers,knowledgedistillationcanbeappliedat\ndifferentlevelsofrepresentation. Forexample,beyonddistillingknowledgefromtheoutput\nlayers,itisalsopossibletoincorporatetraininglossthatmeasuresthedifferenceinoutputof\nhiddenlayersbetweenteachermodelsandstudentmodels[Sunetal.,2020b;Jiaoetal.,2020].\nIndeed,knowledgedistillationhasbeenoneofthemostwidely-usedtechniquesforlearning\nsmallpre-trainedmodels.",
    "Second, conventional model compression methods can be directly applied to compress\nBERTmodels. Onecommonapproachistousegeneral-purposepruningmethodstoprunethe\nTransformerencodingnetworks[Galeetal.,2019]. Thisgenerallyinvolvesremovingentire",
    "394 Chapter 7. Pre-training\nlayers[Fanetal.,2019]oracertainpercentageofparametersinthenetworks[Sanhetal.,2020;\nChenetal.,2020b]. Pruningisalsoapplicabletomulti-headattentionmodels. Forexample,\nMichel et al. [2019] show that removing some of the heads does not significantly decrease\nthe performance of BERT models, but speeds up the inference of these models. Another\napproachtocompressingBERTmodelsisquantization[Shenetal.,2020b]. Byrepresenting\nmodelparametersaslow-precisionnumbers,themodelscanbegreatlycompressed. While\nthismethodisnotspecifictoBERTmodels,itproveseffectiveforlargeTransformer-based\narchitectures.\nThird, considering that BERT models are relatively deep and large networks, another\nthreadofresearchusesdynamicnetworkstoadaptthesemodelsforefficientinference. An\nideainthisparadigmistodynamicallychoosethelayersforprocessingatoken,forexample,\nindepth-adaptivemodelsweexitatsomeoptimaldepthandthusskiptherestofthelayersin",
    "thelayerstack[Xinetal.,2020;Zhouetal.,2020]. Similarly,wecandeveloplength-adaptive\nmodelsinwhichthelengthoftheinputsequenceisdynamicallyadjusted. Forexample,we\ncanskipsomeofthetokensintheinputsequencesothatthemodelcanreducecomputational\nloadonlessimportanttokens,enhancingoverallefficiency.\nFourth, it is also possible to share parameters across layers to reduce the size of BERT\nmodels. AsimplewaytodothisistosharetheparametersofawholeTransformerlayeracross\nthelayerstack[Dehghanietal.,2018;Lanetal.,2020]. Inadditiontothereducednumberof\nparameters,thisenablesreuseofthesamelayerinamulti-layerTransformernetwork,leading\ntosavingsofmemoryfootprintattesttime.\n7.3.4 Multi-lingual Models\nTheinitialBERTmodelwasprimarilyfocusedonEnglish. Soonafterthismodelwasproposed,\nitwasextendedtomanylanguages. Onesimplewaytodothisistodevelopaseparatemodel\nforeachlanguage. Anotherapproach,whichhasbecomemorepopularinrecentworkonlarge\nlanguagemodels,istotrainmulti-lingualmodelsdirectlyondatafromallthelanguages. In",
    "response,multi-lingualBERT(mBERT)modelsweredevelopedbytrainingthemontext\nfrom104languages6. TheprimarydifferencefrommonolingualBERTmodelsisthatmBERT\nmodels use larger vocabularies to cover tokens from multiple languages. As a result, the\nrepresentationsoftokensfromdifferentlanguagesaremappedintothesamespace,allowing\nforthesharingofknowledgeacrosslanguagesviathisuniversalrepresentationmodel.\nOneimportantapplicationofmulti-lingualpre-trainedmodelsiscross-linguallearning.\nIn the cross-lingual setting, we learn a model on tasks in one language, and apply it to the\nsametasksinanotherlanguage. Incross-lingualtextclassification,forexample,wefine-tunea\nmulti-lingualpre-trainedmodelonEnglishannotateddocuments. Then,weusethefine-tuned\nmodeltoclassifyChinesedocuments.\nAnimprovementtomulti-lingualpre-trainedmodelslikemBERTistointroducebilingual\ndataintopre-training. Ratherthantrainingsolelyonmonolingualdatafrommultiplelanguages,",
    "bilingual training explicitly models the relationship between tokens in two languages. The\n6https://github.com/google-research/bert/",
    "7.3 Example: BERT 395\nresultingmodelwillhaveinnatecross-lingualtransferabilities,andthuscanbeeasilyadapted\nto different languages. Lample and Conneau [2019] propose an approach to pre-training\ncross-linguallanguagemodels(XLMs). Intheirwork,across-linguallanguagemodelcan\nbetrainedineitherthecausallanguagemodelingormaskedlanguagemodelingmanner. For\nmasked language modeling pre-training, the model is treated as an encoder. The training\nobjective is the same as BERT: we maximize the probabilities of some randomly selected\ntokenswhichareeithermasked,replacedwithrandomtokens,orkeptunchangedintheinput.\nIfweconsiderbilingualdatainpre-training,wesampleapairofalignedsentenceseachtime.\nThen,thetwosentencesarepackedtogethertoformasinglesequenceusedfortraining. For\nexample,consideranEnglish-Chinesesentencepair\n鲸鱼 是 哺乳 动物 。 ↔ Whalesaremammals .\nWecanpackthemtoobtainasequence,likethis\n[CLS]鲸鱼 是 哺乳 动物 。 [SEP] Whalesaremammals . [SEP]\nWethenselectacertainpercentageofthetokensandreplacethemwith[MASK].",
    "[CLS][MASK] 是 [MASK] 动物 。 [SEP] Whales[MASK][MASK] . [SEP]\nThegoalofpre-trainingistomaximizetheproductoftheprobabilitiesofthemaskedtokens\ngiventheabovesequence. Byperformingtraininginthisway,themodelcanlearntorepresent\nboththeEnglishandChinesesequences,aswellastocapturethecorrespondencesbetween\ntokensinthetwolanguages. Forexample,predictingtheChinesetoken 鲸鱼mayrequirethe\ninformationfromtheEnglishtokenWhales. Aligningtherepresentationsofthetwolanguages\nessentiallytransformsthemodelintoa“translation”model. Sothistrainingobjectiveisalso\ncalledtranslationlanguagemodeling. Figure7.7showsanillustrationofthisapproach.\nAbenefitofmulti-lingualpre-trainedmodelsistheirinherentcapabilityofhandlingcode-\nswitching. InNLPandlinguistics,code-switchingreferstoswitchingamonglanguagesina\ntext. Forexample,thefollowingisamixedlanguagetextcontainingbothChineseandEnglish:\n周末 我们 打算 去 做 hiking ， 你 想 一起 来 吗 ？\n(Weplantogohikingthisweekend,wouldyouliketojoinus?)",
    "Formulti-lingualpre-trainedmodels,wedonotneedtoidentifywhetheratokenisChineseor\nEnglish. Instead,everytokenisjustanentryofthesharedvocabulary. Thiscanbeimagined\nascreatinga“new”languagethatencompassesallthelanguageswewanttoprocess.\nThe result of multi-lingual pre-training is influenced by several factors. Given that the\nmodelarchitectureisfixed,oneneedstospecifythesizeofthesharedvocabulary,thenumber\n(orpercentage)ofsamplesineachlanguage,thesizeofthemodel,andsoon. Conneauetal.",
    "396 Chapter 7. Pre-training\n鲸鱼 哺乳 are mammals\nh h h h h h h h h h h h\n0 1 2 3 4 5 6 7 8 9 10 11\nTransformer Encoder\ne e e e e e e e e e e e\n0 1 2 3 4 5 6 7 8 9 10 11\n[CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales[MASK][MASK] . [SEP]\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)\nFigure7.7: Anillustrationoftranslationlanguagemodeling. Foreaseofunderstanding,we\npresentasimpleexamplewherealltheselectedtokensaremasked. Themodelistrainedto\npredictthesemaskedtokens. Asthesequencecontainstokensintwolanguages,predicting\na token in one language allows access to tokens in the other language, thereby enabling\ncross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e.,\ne ) is the sum of the token embedding, positional embedding, and language embedding.\ni\nThis requires that each token is assigned with a language label. Thus we can distinguish\ntokensindifferentlanguages. Inmulti-lingualpre-training,particularlyinworkusingshared",
    "vocabularies, specifying the language to which a token belongs is not necessary. The use\nof language embeddings in turn makes it difficult to handle code-switching. Therefore, we\nassumeherethatalltokenrepresentationsarelanguage-independent.\n[2020]pointoutseveralinterestingissuesregardinglarge-scalemulti-lingualpre-trainingfor\nXLM-likemodels. First,asthenumberofsupportedlanguagesincreases,alargermodelis\nneededtohandletheselanguages. Second,alargersharedvocabularyishelpfulformodeling\ntheincreaseddiversityinlanguages. Third,low-resourcelanguagesmoreeasilybenefitfrom\ncross-lingualtransferfromhigh-resourcelanguages,particularlywhensimilarhigh-resource\nlanguages are involved in pre-training. However, interference may occur if the model is\ntrainedforanextendedperiod,meaningtheoverallperformanceofthepre-trainedmodelstarts\ndecreasingatacertainpointduringpre-training. Thus,inpracticalsystems,onemayneedto\nstopthepre-trainingearlytopreventinterference.\n7.4 Applying BERT Models",
    "Once a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT\nmodels are not immediately ready for performing specific downstream tasks. In general,\nadditionalfine-tuningworkisrequiredtomakethemadapt. Asafirststep,weneedapredictor\ntoaligntheoutputofthemodelwiththeproblemofinterest. LetBERT (·)beaBERTmodel\nθˆ\nwithpre-trainedparametersθˆ ,andPredict (·)beapredictionnetworkwithparametersω. By\nω",
    "7.4 Applying BERT Models 397\nintegratingthepredictionnetworkwiththeoutputoftheBERTmodel,wedevelopamodelto\ntacklethedownstreamtasks. Thismodelcanbeexpressedas\ny = Predict (BERT (x)) (7.21)\nω θˆ\nwherexistheinputandy istheoutputthatfitstheproblem. Forexample,inclassification\nproblems,themodeloutputsaprobabilitydistributionoverlabels.\nThen,wecollectasetoflabeledsamplesD,andfine-tunethemodelby\n(cid:88)\n(ω˜,θ˜) = argmin Loss(y ,y ) (7.22)\nω,θˆ+ gold\nω,θˆ+\n(x,y )∈D\ngold\nwhere(x,y )representsatupleofaninputanditscorrespondingoutput. Thenotationof\ngold\nthisequationseemsabitcomplicated,butthetraining/tuningprocessisstandard. Weoptimize\nthe model by minimizing the loss over the tuning samples. The outcome is the optimized\nparametersω˜ andθ˜ . Theoptimizationstartswiththepre-trainedparametersθˆ . Hereweuse\nθˆ+ toindicatethattheparametersareinitializedwithθˆ , andusey todenotethemodel\nω,θˆ+\noutputcomputedusingtheparametersω andθˆ+.",
    "Withthefine-tunedparametersω˜ andθ˜ ,wecanapplythemodelPredict (BERT (·))to\nω˜ θ˜\nnewdataofthesametasksforwhichthemodelwasfine-tuned. Theformofthedownstream\ntasksdeterminestheinputandoutputformatsofthemodel,aswellasthearchitectureofthe\npredictionnetwork. InthefollowingwelistsometaskstowhichBERTmodelsaregenerally\nsuited.\n• Classification(SingleText). Oneofthemostwidely-usedapplicationsofBERTmodels\nis text classification. In this task, a BERT model receives a sequence of tokens and\nencodesitasasequenceofvectors. Thefirstoutputvectorh (orh )istypicallyused\ncls 0\nas the representation of the entire text. The prediction network takes h as input to\ncls\nproduceadistributionoflabels. Let[CLS]x x ...x beaninputtext. Seebelowforan\n1 2 m\nillustrationofBERT-basedtextclassification.\nClass\nh cls h 1 h 2 ... hm h m+1\nBERT\ne cls e 1 e 2 ... em e m+1\n[CLS] x 1 x 2 ... xm [SEP]\nHere the gray box denotes the prediction network. Many NLP problems can be cat-",
    "egorized as text classification tasks, and there have been several text classification\nbenchmarksforevaluatingpre-trainedmodels. Forexample,wecanclassifytextsby\ntheirgrammaticalcorrectness(grammaticality)oremotionaltone(sentiment)[Socher",
    "398 Chapter 7. Pre-training\net al., 2013; Warstadt et al., 2019]. Note that the prediction network could be any\nclassificationmodel,suchasadeepneuralnetworkoramoretraditionalclassification\nmodel. Theentiremodelcanthenbetrainedorfine-tunedinthemannerofastandard\nclassification model. For example, the prediction network can be simply a Softmax\nlayerandthemodelparameterscanbeoptimizedbymaximizingtheprobabilitiesofthe\ncorrectlabels.\n• Classification(PairofTexts). Classificationcanalsobeperformedonapairoftexts.\nSupposewehavetwotexts,x ...x andy ...y . Wecanconcatenatethesetextstoform\n1 m 1 n\na single sequence with a length len. Then, we predict a label for this combined text\nsequencebasedontheh vector,asfollows\ncls\nClass\nh cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len\nBERT\ne cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len\n[CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP]\nText1 Text2\nwherelen=n+m+2. Textpairclassificationcoversseveralproblems,includingse-",
    "manticequivalencejudgement(determinewhethertwotextsaresemanticallyequivalent)\n[DolanandBrockett,2005],textentailmentjudgement(determinewhetherahypothesis\ncanbelogicallyinferredorentailedfromapremise)[BentivogliandGiampiccolo,2011;\nWilliamsetal.,2018],groundedcommonsenseinference(determinewhetheranevent\nis likely to happen given its context) [Zellers et al., 2018], and question-answering\ninference(determinewhetherananswercorrespondstoagivenquestion).\n• Regression. Instead of generating a label distribution, we can have the prediction\nnetwork output a real-valued score. For example, by adding a Sigmoid layer to the\nprediction network, the system can be employed to compute the similarity between\ntwogivensentences. ThearchitectureisthesameasthatofBERT-basedclassification\nsystems,withonlythechangeoftheoutputlayer.",
    "7.4 Applying BERT Models 399\nNumber(similarity,evaluationscore,etc.)\nh cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len\nBERT\ne cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len\n[CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP]\nText1 Text2\nFortrainingorfine-tuning,wecanminimizetheregressionlossofthemodeloutputas\nusual.\n• SequenceLabeling. Sequencelabelingisamachinelearningapproachapplicabletoa\nwiderangeofNLPproblems. Thisapproachassignsalabeltoeachtokeninaninput\nsequence, and some linguistic annotations can then be derived from this sequence of\nlabels. AnexampleofsequencelabelinginNLPispart-of-speech(POS)tagging. We\nlabel each word in a sentence with its corresponding POS tag. Another example is\nnamed entity recognition (NER) in which we label each word with an NER tag, and\nnamedentitiesareidentifiedusingthesetags. Seebelowforanillustrationofthemodel\narchitectureforNER.\nTag Tag Tag\n{B,I,O}{B,I,O} {B,I,O}\nh cls h 1 h 2 ... hm h m+1\nBERT\ne cls e 1 e 2 ... em e m+1",
    "[CLS] x 1 x 2 ... xm [SEP]\nHere{B,I,O}isthetagsetofNER.Forexample,B-ORGmeansthebeginningofan\norganization,I-ORGmeansthewordisinsideanorganization,andOmeanstheword\ndoesnotbelongtoanynamedentity. ThisNERmodelcanoutputadistributionover\nthetagsetateachposition,denotedasp . Thetrainingorfine-tuningofthemodelcan\ni\nbeperformedoverthesedistributions{p ,...,p }. Forexample, supposep (tag )is\n1 m i i\ntheprobabilityofthecorrecttagatpositioni. Thetraininglosscanbedefinedtobethe\nnegativelikelihood\nm\n1 (cid:88)\nLoss = − logp (tag ) (7.23)\nm i i\ni=1\nFindingthebestlabelsequencegivenatrainedNERmodelisawell-studiedissuein",
    "400 Chapter 7. Pre-training\nNLP.Thisisoftenachievedviadynamicprogramming,which,inthecontextofpath\nfindingoveralattice,haslinearcomplexity[Huang,2009].\n• Span Prediction. Some NLP tasks require predicting a span in a text. A common\nexample is reading comprehension. In this task, we are given a query x ...x and a\n1 m\ncontexttexty ...y . Thegoalistoidentifyacontinuousspaniny ...y thatbestanswers\n1 n 1 n\nthe query. This problem can be framed as a sequence labeling-like task in which we\npredictalabelforeachy toindicatethebeginningorendingofthespan. FollowingSeo\nj\netal.[2017],weaddtwonetworksontopoftheBERToutputfory : oneforgenerating\nj\nthe probability of y being the beginning of the span (denoted by pbeg ), and one for\nj j\ngenerating the probability of y being the ending of the span (denoted by pend). The\nj j\nresultingmodelarchitectureisshownasfollows\nEnd End End\n(pe\n1\nnd) (pe\n2\nnd) (pe\nn\nnd)\nBeg Beg Beg\n(pb\n1\neg) (pb\n2\neg) (pb\nn\neg)\nh cls h 1 h 2 ... hm h m+1 h m+2 h m+3 ... h len−1 h len",
    "BERT\ne cls e 1 e 2 ... em e m+1 e m+2 e m+3 ... e len−1 e len\n[CLS] x 1 x 2 ... xm [SEP] y 1 y 2 ... yn [SEP]\nQuery ContextText\nWepackthequeryandcontexttexttogethertoobtaintheinputsequence. Theprediction\nnetworksareonlyappliedtooutputsforthecontexttext,generatingtheprobabilities\npbeg andpend ateachposition. Thelosscanbecomputedbysummingthenegativelog\nj j\nlikelihoodsofthetwomodelsacrosstheentirecontexttext.\nn\nLoss = −\n1(cid:88)(cid:0) logpbeg+logpend(cid:1)\n(7.24)\nn j j\nj=1\nAttesttime,wesearchforthebestspanby\n(ˆj ,ˆj ) = argmax (cid:0) logpbeg+logpend(cid:1) (7.25)\n1 2 j1 j2\n1≤j1≤j2≤n\n• Encoding for Encoder-Decoder Models. While our focus in this section has been\nprimarily on language understanding problems, it is worth noting that BERT models\ncan be applied to a broader range of NLP tasks. In fact, BERT models can be used\ninallthescenarioswhereweneedtoencodeapieceoftext. Oneapplicationthatwe\nhavenotmentionedistextgenerationwhichincludesarangeoftaskssuchasmachine",
    "translation,summarization,questionanswering,anddialoguegeneration. Thesetasks",
    "7.5 Summary 401\ncanbeformulatedassequence-to-sequenceproblems: weuseanencodertorepresentthe\nsourcetext,andadecodertogeneratethecorrespondingtargettext. Astraightforward\nmethod to apply BERT models is to consider them as encoders. Before fine-tuning,\nwe can initialize the parameters of the encoder with those from a pre-trained BERT\nmodel. Then,theencoder-decodermodelcanbefine-tunedonpairsoftextsasusual.\nThefollowingshowsthearchitectureofaneuralmachinetranslationsystemwherea\nBERTmodelisappliedonthesourceside.\nTargetText\nAdapter y 1 y 2 y 3 ... yn\nBERT(Encoder) Decoder\nex ex ... ex ex ey ey ey ... ey\ncls 1 m m+1 0 1 2 n−1\n[CLS] x 1 ... xm [SEP] ⟨s⟩ y 1 y 2 ... y n−1\nSourceText\nHerex ...x denotesthesourcesequence,y ...y denotesthetargetsequence,ex...ex\n1 m 1 n 1 m\ndenotes the embedding sequence of x ...x , and ey...ey denotes the embedding se-\n1 m 1 n\nquenceofy ...y . Theadapter,whichisoptional,mapstheoutputoftheBERTmodel\n1 n\ntotheformthatisbettersuitedtothedecoder.",
    "Fine-tuning BERT models is a complicated engineering problem, influenced by many\nfactors, such as the amount of fine-tuning data, the model size, and the optimizer used in\nfine-tuning. Ingeneral,wewishtofine-tunethesemodelssufficientlysothattheycanperform\nwellinthedownstreamtasks. However,fine-tuningBERTmodelsforspecifictasksmaylead\nto overfitting, which in turn reduces their ability to generalize to other tasks. For example,\nsupposewehaveaBERTmodelthatperformswellonaparticulartask. Ifwethenfine-tuneit\nfornewtasks,thismaydecreaseitsperformanceontheoriginaltask. Thisproblemisrelated\ntothecatastrophicforgettingproblemincontinualtraining,whereaneuralnetworkforgets\npreviously learned information when updated on new samples. In practical applications, a\ncommonwaytoalleviatecatastrophicforgettingistoaddsomeolddataintofine-tuningand\ntrainthemodelwithmorediversedata. Also,onemayusemethodsspecializedtocatastrophic\nforgetting,suchasexperiencereplay[Rolnicketal.,2019]andelasticweightconsolidation",
    "[Kirkpatricketal.,2017]. Theinterestedreadercanrefertosomesurveysformoredetailed\ndiscussionsofthisissueincontinuallearning[Parisietal.,2019;Wangetal.,2023a;f].\n7.5 Summary\nInthischapterwehavediscussedthegeneralideaofpre-traininginNLP.Inparticular,wehave\ndiscussedself-supervisedpre-traininganditsapplicationtoencoder-only,decoder-only,and",
    "402 Chapter 7. Pre-training\nencoder-decoderarchitectures. Moreover,wehavepresentedandcomparedavarietyofpre-\ntrainingtasksforthesearchitectures. Asanexample,BERTisusedtoillustratehowsequence\nmodelsarepre-trainedviamaskedlanguagemodelingandappliedtodifferentdownstream\ntasks.\nRecentyearshaveshownremarkableprogressinNLP,ledbythelarge-scaleuseofself-\nsupervisedpre-training. Andsweepingadvancesarebeingmadeacrossmanytasks,notonly\ninNLPbutalsoincomputervisionandotherareasofAI.Oneideabehindtheseadvancesis\nthatasignificantamountofknowledgeabouttheworldcanbelearnedbysimplytrainingthese\nAIsystemsonhugeamountsofunlabeleddata. Forexample,alanguagemodelcanlearnsome\ngeneralknowledgeofalanguagebyrepeatedlypredictingmaskedwordsinlarge-scaletext. As\naresult,thispre-trainedlanguagemodelcanserveasafoundationmodel,whichcanbeeasily\nadaptedtoaddressspecificdownstreamNLPtasks. ThisparadigmshiftinNLPhasenabled\nthedevelopmentofincrediblypowerfulsystemsforlanguageunderstanding,generation,and",
    "reasoning [Manning, 2022]. However, it is important to recognize that we are still in the\nearlystagesofcreatingtrulyintelligentsystems,andthereisalongwaytogo. Nevertheless,\nlarge-scale pre-training has opened a door to intelligent systems that researchers have long\naspired to develop, though several key research areas remain open for exploration, such as\nlearning intelligence efficiently using reasonably small-sized data and acquiring complex\nreasoningandplanningabilities.\nNotethatthischapterismostlyintroductoryandcannotcoverallaspectsofpre-training.\nForexample,therearemanymethodstofine-tuneapre-trainedmodel,offeringdifferentways\ntobetteradaptthemodeltodiversesituations. Moreover,largelanguagemodels,whichare\nconsideredoneofthemostsignificantachievementsinAIinrecentyears,areskippedinthis\nsection. Weleavethediscussionofthesetopicstothefollowingchapters.",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 8\nGenerative Models\nOneofthemostsignificantadvancesinNLPinrecentyearsmightbethedevelopmentoflarge\nlanguagemodels(LLMs). Thishashelpedcreatesystemsthatcanunderstandandgenerate\nnatural languages like humans. These systems have even been found to be able to reason,\nwhichisconsideredaverychallengingAIproblem. Withtheseachievements,NLPmadebig\nstridesandenteredaneweraofresearchinwhichdifficultproblemsarebeingsolved,suchas\nbuildingconversationalsystemsthatcancommunicatewithhumanssmoothly.\nTheconceptoflanguagemodelingorprobabilisticlanguagemodelingdatesbacktoearly\nexperimentsconductedbyShannon[1951]. Inhiswork,alanguagemodelwasdesignedto\nestimate the predictability of English — how well can the next letter of a text be predicted\nwhentheprecedingN lettersareknown. AlthoughShannon’sexperimentswerepreliminary,\nthefundamentalgoalsandmethodsoflanguagemodelinghaveremainedlargelyunchanged",
    "overthedecadessincethen. Forquitealongperiod,particularlybefore2010,thedominant\napproach to language modeling was the n-gram approach [Jurafsky and Martin, 2008]. In\nn-gramlanguagemodeling,weestimatetheprobabilityofawordgivenitsprecedingn−1\nwords,andthustheprobabilityofasequencecanbeapproximatedbytheproductofaseries\nofn-gramprobabilities. Theseprobabilitiesaretypicallyestimatedbycollectingsmoothed\nrelative counts of n-grams in text. While such an approach is straightforward and simple,\nit has been extensively used in NLP. For example, the success of modern statistical speech\nrecognitionandmachinetranslationsystemshaslargelydependedontheutilizationofn-gram\nlanguagemodels[Jelinek,1998;Koehn,2010].\nApplying neural networks to language modeling has long been attractive, but a real\nbreakthroughappearedasdeeplearningtechniquesadvanced. AwidelycitedstudyisBengio\netal.[2003a]’sworkwheren-gramprobabilitiesaremodeledviaafeed-forwardnetworkand",
    "learnedbytrainingthenetworkinanend-to-endfashion. Aby-productofthisneurallanguage\nmodelisthedistributedrepresentationsofwords,knownaswordembeddings. Ratherthan\nrepresentingwordsasdiscretevariables,wordembeddingsmapwordsintolow-dimensional\nreal-valuedvectors,makingitpossibletocomputethemeaningsofwordsandwordn-grams\nin a continuous representation space. As a result, language models are no longer burdened\nwiththecurseofdimensionality,butcanrepresentexponentiallymanyn-gramsviaacompact",
    "404 Chapter 8. Generative Models\nanddenseneuralmodel.\nTheideaoflearningwordrepresentationsthroughneurallanguagemodelsinspiredsub-\nsequent research in representation learning in NLP. However, this approach did not attract\nsignificantinterestindevelopingNLPsystemsinthefirstfewyearsafteritsproposal. Starting\ninabout2012, though, advancesweremadeinlearningwordembeddingsfromlarge-scale\ntext via simple word prediction tasks. Several methods, such as Word2Vec, were proposed\nto effectively learn such embeddings, which were then successfully applied in a variety of\nNLPsystems[Mikolovetal.,2013a;c]. Asaresultoftheseadvances,researchersbeganto\nthinkoflearningrepresentationsofsequencesusingmorepowerfullanguagemodels, such\nas LSTM-based models [Sutskever et al., 2014; Peters et al., 2018]. And further progress\nandinterestinsequencerepresentationexplodedafterTransformerwasproposed. Alongside\nthe rise of Transformer, the concept of language modeling was generalized to encompass",
    "modelsthatlearntopredictwordsinvariousways. ManypowerfulTransformer-basedmodels\nwerepre-trainedusingthesewordpredictiontasks, andsuccessfullyappliedtoavarietyof\ndownstreamtasks[Devlinetal.,2019].\nIndeed, training language models on large-scale data has led NLP research to exciting\ntimes. Whilelanguagemodelinghaslongbeenseenasafoundationaltechniquewithnodirect\nlink to the goals of artificial intelligence that researchers had hoped for, it helps us see the\nemergenceofintelligentsystemsthatcanlearnacertaindegreeofgeneralknowledgefrom\nrepeatedlypredictingwordsintext. Recentresearchdemonstratesthatasingle,well-trained\nLLM can handle a large number of tasks and generalize to perform new tasks with a small\nadaptationeffort[Bubecketal.,2023]. Thissuggestsasteptowardsmoreadvancedformsof\nartificialintelligence,andinspiresfurtherexplorationintodevelopingmorepowerfullanguage\nmodelsasfoundationmodels.\nIn this chapter, we consider the basic concepts of generative LLMs. For simplicity, we",
    "usethetermslargelanguagemodelsorLLMstorefertogenerativemodelslikeGPT,though\nthistermcanbroadlycoverothertypesofmodelslikeBERT.Webeginbygivingageneral\nintroductiontoLLMs,includingthekeystepsofbuildingsuchmodels. Wethendiscusstwo\nscalingissuesofLLMs: howLLMsaretrainedatscale,andhowLLMscanbeimprovedto\nhandleverylongtexts. Finally,wegiveasummaryofthesediscussions.\n8.1 A Brief Introduction to LLMs\nInthissectionwegiveanintroductiontothebasicideasofLLMsasrequiredfortherestofthis\nchapterandthefollowingchapters. Wewillusetermsword andtokeninterchangeably. Both\nofthemrefertothebasicunitsusedinlanguagemodeling,thoughtheiroriginalmeaningsare\ndifferent.\nBefore presenting details, let us first consider how language models work. The goal\nof language modeling is to predict the probability of a sequence of tokens occurring. Let\n{x ,x ,...,x }beasequenceoftokens,wherex isthestartsymbol⟨s⟩(or⟨SOS⟩)1. The\n0 1 m 0\n1Thestartsymbolcanalsobe[CLS]followingBERTmodels.",
    "8.1 A Brief Introduction to LLMs 405\nprobabilityofthissequencecanbedefinedusingthechainrule\nPr(x ,...,x ) = Pr(x )·Pr(x |x )·Pr(x |x ,x )···Pr(x |x ,...,x )\n0 m 0 1 0 2 0 1 m 0 m−1\nm\n(cid:89)\n= Pr(x |x ,...,x ) (8.1)\ni 0 i−1\ni=0\noralternativelyinalogarithmicform\nm\n(cid:88)\nlogPr(x ,...,x ) = logPr(x |x ,...,x ) (8.2)\n0 m i 0 i−1\ni=0\nHere Pr(x |x ,...,x ) is the probability of the token x given all its previous tokens\ni 0 i−1 i\n{x ,...,x } 2. In the era of deep learning, a typical approach to language modeling is\n0 i−1\ntoestimatethisprobabilityusingadeepneuralnetwork. Neuralnetworkstrainedtoaccom-\nplish this task receive a sequence of tokens x ,...,x and produce a distribution over the\n0 i−1\nvocabularyV (denotedbyPr(·|x ,...,x )). TheprobabilityPr(x |x ,...,x )isthevalue\n0 i−1 i 0 i−1\nofthei-thentryofPr(·|x ,...,x ).\n0 i−1\nWhenapplyingatrainedlanguagemodel,acommontaskistofindthemostlikelytoken\ngivenitspreviouscontexttokens. Thistokenpredictiontaskcanbedescribedas",
    "xˆ = argmaxPr(x |x ,...,x ) (8.3)\ni i 0 i−1\nxi∈V\nWecanperformwordpredictionmultipletimestogenerateacontinuoustext: eachtime\nwepredictthebesttokenxˆ ,andthenaddthispredictedtokentothecontextforpredicting\ni\nthenexttokenxˆ . Thisresultsinaleft-to-rightgenerationprocessimplementingEqs. (8.1)\ni+1\nand(8.2). Toillustrate,considerthegenerationofthefollowingthreewordsgiventheprefix\n‘⟨s⟩ a’, as shown in Table 8.1. Now we discuss how LLMs are constructed, trained, and\napplied.\n8.1.1 Decoder-only Transformers\nAs is standard practice, the input of a language model is a sequence of tokens (denoted\nby {x ,...,x }). For each step, an output token is generated, shifting the sequence one\n0 m−1\npositionforwardforthenextprediction. Todothis,thelanguagemodeloutputsadistribution\nPr(·|x ,...,x )ateachpositioni,andthetokenx isselectedaccordingtothisdistribution.\n0 i−1 i\nThismodelistrainedbymaximizingtheloglikelihood (cid:80)m logPr(x |x ,...,x )3.\ni=1 i 0 i−1",
    "Here, we focus on the decoder-only Transformer architecture, as it is one of the most\npopularmodelarchitecturesusedinLLMs. Theinputsequenceoftokensisrepresentedbya\nsequenceofd -dimensionalvectors{e ,...,e }. e isthesumofthetokenembeddingof\ne 0 m−1 i\nx andthepositionalembeddingofi. ThemajorbodyofthemodelisastackofTransformer\ni\n2We assume that when i = 0, Pr(x i |x 0 ,...,x i−1 ) = Pr(x 0 ) = 1. Hence Pr(x 0 ,...,xm) =\nPr(x\n0\n)Pr(x\n1\n,...,xm|x\n0\n)=Pr(x\n1\n,...,xm|x\n0\n).\n3Notethat (cid:80)m\ni=1\nlogPr(x\ni\n|x\n0\n,...,x\ni−1\n)= (cid:80)m\ni=0\nlogPr(x\ni\n|x\n0\n,...,x\ni−1\n)sincelogPr(x\n0\n)=0.",
    "406 Chapter 8. Generative Models\nContext Predict DecisionRule SequenceProbability\n⟨s⟩ a b argmax Pr(x |⟨s⟩a) Pr(⟨s⟩)·Pr(a|⟨s⟩)· Pr(b|⟨s⟩a)\nx2∈V 2\n⟨s⟩ a b c argmax Pr(x |⟨s⟩ab) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)·\nx3∈V 3\nPr(c|⟨s⟩ab)\n⟨s⟩ a b c d argmax Pr(x |⟨s⟩abc) Pr(⟨s⟩)·Pr(a|⟨s⟩)·Pr(b|⟨s⟩a)·\nx4∈V 4\nPr(c|⟨s⟩ab)· Pr(d|⟨s⟩abc)\nTable 8.1: Illustration of generating the three tokens b c d given the prefix ⟨s⟩ a via a\nlanguagemodel. Ineachstep,themodelpicksatokenx fromV sothatPr(x |x ,...,x )is\ni i 0 i−1\nmaximized. Thistokenisthenappendedtotheendofthecontextsequence. Inthenextstep,\nwerepeatthesameprocess,butbasedonthenewcontext.\nblocks(orlayers). EachTransformerblockhastwostackedsub-layers,oneforself-attention\nmodelingandoneforFFNmodeling. Thesesub-layerscanbedefinedusingthepost-norm\narchitecture\noutput = LNorm(F(input)+input) (8.4)\northepre-normarchitecture\noutput = LNorm(F(input))+input (8.5)\nwhereinputandoutputdenotetheinputandoutput,bothbeinganm×dmatrix. Thei-th",
    "rowsofinputandoutputcanbeseenascontextualrepresentationsofthei-thtokeninthe\nsequence.\nF(·)isthecorefunctionofasub-layer. ForFFNsub-layers,F(·)isamulti-layerFFN.For\nself-attentionsub-layers,F(·)isamulti-headself-attentionfunction. Ingeneral,self-attention\nisexpressedinaformofQKVattention\nQKT\nAtt (Q,K,V) = Softmax( √ +Mask)V (8.6)\nqkv\nd\nwhereQ,KandV∈Rm×d arethequeries,keys,andvalues,respectively. Itisimportantto\nnotethatonlyprevioustokensareconsideredwhenpredictingatoken. Soamaskingvariable\nMask∈Rm×m isincorporatedintoself-attentiontoachievethis. Theentry(i,k)ofMask\nhasavalueof0ifi≤k,andavalueof−inf otherwise.\nGivenarepresentationH∈Rm×d,themulti-headself-attentionfunctioncanbedefinedas\nF(H) = Merge(head ,...,head )Whead (8.7)\n1 τ\nwhereMerge(·)representeesaconcatenationofitsinputs, andWhead ∈Rd×d representsa",
    "8.1 A Brief Introduction to LLMs 407\nPost-normorPre-norm\nFFN\nx 1 x 2 ... xm\nPr(xm|x0x1...xm−1)\nPr(x2|x0x1)\nPr(x1|x0)\nPost-normorPre-norm\n...\nSelf-attention\nhL hL ... hL\n0 1 m−1\nLanguageModel\ne 0 e 1 ... e m−1\nx 0 x 1 ... x m−1 z 0 z 1 ... z m−1\nskcolBL\nFigure8.1: TheTransformer-decoderarchitectureforlanguagemodeling. Thecentralcom-\nponentsareLstackedTransformerblocks,eachcomprisingaself-attentionsub-layerandan\nFFN sub-layer. To prevent the model from accessing the right-context, a masking variable\nis incorporated into self-attention. The output layer uses a Softmax function to generate a\nprobability distribution for the next token, given the sequence of previous tokens. During\ninference,themodeltakesthepreviouslypredictedtokentopredictthenextone,repeating\nthis process until the end of the sequence is reached. {z ,...,z } denote the inputs of a\n0 m−1\nTransformerblock,and{hL,...,hL }denotetheoutputsofthelastTransformerblock.\n0 m−1",
    "parametermatrix. head istheoutputofQKVattentiononasub-spaceofrepresentation\nj\nhead = Att (Q[j],K[j],V[j]) (8.8)\nj qkv\nQ[j],K[j],andV[j] arethequeries,keys,andvaluesprojectedontothej-thsub-spacevialinear\ntransformations\nQ[j] = HWq (8.9)\nj\nK[j] = HWk (8.10)\nj\nV[j] = HWv (8.11)\nj\nwhereW j q ,W j k,andW j v ∈Rd× τ d aretheparametermatricesofthetransformations.\nSupposewehaveLTransformerblocks. ASoftmaxlayerisbuiltontopoftheoutputof\nthelastblock. TheSoftmaxlayeroutputsasequenceofmdistributionsoverthevocabulary,",
    "408 Chapter 8. Generative Models\nlikethis\n \nPr(·|x ,...,x )\n0 m−1\n . . \n  .   = Softmax(HLWo) (8.12)\n Pr(·|x ,x ) \n 0 1 \nPr(·|x )\n0\nwhere HL is the output of the last Transformer block, and Wo ∈Rd×|V| is the parameter\nmatrix.\nFigure 8.1 shows the Transformer architecture for language modeling. Applying this\nlanguagemodelfollowsanautoregressiveprocess. Eachtimethelanguagemodeltakesatoken\nx asinputandpredictsatokenx thatmaximizestheprobabilityPr(x |x ,...,x ). Itis\ni−1 i i 0 i−1\nimportanttonotethat,despitedifferentimplementationdetails,manyLLMssharethesame\narchitecturedescribedabove. Thesemodelsarecalledlargebecauseboththeirdepthandwidth\naresignificant. Table8.2showsthemodelsizesforafewLLMs,aswellastheirmodelsetups.\n8.1.2 Training LLMs\nNowsupposethatwearegivenatrainingsetD comprisingK sequences. Thelog-likelihood\nofeachsequencex=x ...x inD canbecalculatedusingalanguagemodel\n0 m\nm\n(cid:88)\nL (x) = logPr (x |x ,...,x ) (8.13)\nθ θ i 0 i−1\ni=1",
    "Herethesubscriptθ affixedtoL(·)andPr(·)denotestheparametersofthelanguagemodel.\nThen,theobjectiveofmaximumlikelihoodtrainingisdefinedas\n(cid:88)\nθˆ = argmax L (x) (8.14)\nθ\nθ\nx∈D\nTrainingTransformer-basedlanguagemodelswiththeaboveobjectiveiscommonlyviewed\nasastandardoptimizationprocessforneuralnetworks. Thiscanbeachievedusinggradient\ndescentalgorithms,whicharewidelysupportedbyoff-the-shelfdeeplearningtoolkits. Some-\nwhatsurprisingly,betterresultswerecontinuouslyyieldedaslanguagemodelswereevolved\nintomorecomputationallyintensivemodelsandtrainedonlargerdatasets[Kaplanetal.,2020].\nThesesuccesseshaveledNLPresearcherstocontinueincreasingboththetrainingdataand\nmodelsizeinordertobuildmorepowerfullanguagemodels.\nHowever,aslanguagemodelsbecomelarger,weconfrontnewtrainingchallenges,which\nsignificantlychangetheproblemcomparedtotrainingrelativelysmallmodels. Oneofthese\nchallengesarisesfromtheneedforlarge-scaledistributedsystemstomanagethedata,model",
    "parameters,trainingroutines,andsoon. Developingandmaintainingsuchsystemsrequiresa\nsignificantamountofworkinbothsoftwareandhardwareengineering,aswellasexpertisein\ndeeplearning. Arelatedissueisthatwhenthetrainingisscaledup,weneedmorecomputing\nresourcestoensurethetrainingprocesscanbecompletedinanacceptabletime. Forexample,",
    "8.1 A Brief Introduction to LLMs 409\nLLM #ofParameters DepthL Widthd #ofHeads\n(Q/KV)\nGPT-1[Radfordetal.,2018] 0.117B 12 768 12/12\nGPT-2[Radfordetal.,2019] 1.5B 48 1,600 25/25\nGPT-3[Brownetal.,2020] 175B 96 12,288 96/96\n7B 32 4,096 32/32\nLLaMA2[Touvronetal.,2023b] 13B 40 5,120 40/40\n70B 80 8,192 64/64\n8B 32 4,096 32/8\nLLaMA3/3.1[Dubeyetal.,2024] 70B 80 8,192 64/8\n405B 126 16,384 128/8\n2B 26 2,304 8/4\nGemma2[Teametal.,2024] 9B 42 3,584 16/8\n37B 46 4,608 32/16\n0.5B 24 896 14/2\nQwen2.5[Yangetal.,2024] 7B 28 3,584 28/4\n72B 80 8,192 64/8\nDeepSeek-V3[Liuetal.,2024a] 671B 61 7,168 128/128\n7B 32 4,544 71/71\nFalcon[Penedoetal.,2023] 40B 60 8,192 128/128\n180B 80 14,848 232/232\nMistral[Jiangetal.,2023a] 7B 32 4,096 32/32\nTable8.2: ComparisonofsomeLLMsintermsofmodelsize,modeldepth,modelwidth,and\nnumberofheads(a/bmeansaheadsforqueriesandbheadsforbothkeysandvalues).\nitgenerallyrequireshundredsorthousandsofGPUstotrainanLLMwithtensofbillionsof",
    "parametersfromscratch. Thisrequirementdrasticallyincreasesthecostoftrainingsuchmodels,\nespeciallyconsideringthatmanytrainingrunsareneededasthesemodelsaredeveloped. Also,\nfromtheperspectiveofdeeplearning,thetrainingprocesscanbecomeunstableiftheneural\nnetworksareverydeepand/orthemodelsizeisverylarge. Inresponse,wetypicallyneedto\nmodifythemodelarchitecturetoadaptLLMstolarge-scaletraining. InSection8.2wewill\npresentmorediscussionsontheseissues.\n8.1.3 Fine-tuning LLMs\nOnce we have pre-trained an LLM, we can then apply it to perform various NLP tasks.\nTraditionally language models are used as components of other systems, for example, they\narewidelyappliedtoscoretranslationsinstatisticalmachinetranslationsystems. Bycontrast,\ningenerativeAI,LLMsareconsideredcompletesystemsandareemployedtoaddressNLP\nproblemsbymakinguseoftheirgenerationnature. Acommonapproachistodescribethetask",
    "410 Chapter 8. Generative Models\nwewanttoaddressintextandthenpromptLLMstogeneratetextbasedonthisdescription.\nThisisastandardtextgenerationtaskwherewecontinueorcompletethetextstartingfroma\ngivencontext.\nMore formally, let x=x ...x denote a token sequence of context given by users, and\n0 m\ny=y ...y denoteatokensequencefollowingthecontext. Then,theinferenceofLLMscan\n1 n\nbedefinedasaproblemoffindingthemostlikelysequenceybasedonx:\nyˆ = argmaxlogPr(y|x)\ny\nn\n(cid:88)\n= argmax logPr(y |x ,...,x ,y ,...,y ) (8.15)\ni 0 m 1 i−1\ny\ni=1\nHere\n(cid:80)n\nlogPr(y |x ,...,x ,y ,...,y )essentiallyexpressesthesamethingastheright-\ni=1 i 0 m 1 i−1\nhandsideofEq. (8.2). Itmodelsthelogprobabilityofpredictingtokensfrompositionm+1,\nratherthanposition0. Throughoutthischapterandsubsequentones,wewillemployseparate\nvariablesxandytodistinguishtheinputandoutputofanLLM,thoughtheycanbeseenas\nsub-sequencesfromthesamesequence. Byadoptingsuchnotation,weseethattheformofthe",
    "aboveequationcloselyresemblesthoseusedinothertextgenerationmodelsinNLP,suchas\nneuralmachinetranslationmodels.\nToillustratehowLLMsareapplied,considertheproblemofdeterminingthegrammatical-\nityforagivensentence. Wecandefineatemplatelikethis\n{*sentence*}\nQuestion: Isthissentencegrammaticallycorrect?\nAnswer:\nHere representsthetextweintendtogenerate. {*sentence*}isaplaceholdervariablethat\nwillbereplacedbytheactualsentenceprovidedbytheusers. Forexample,supposewehavea\nsentence“Johnseemshappytoday.”. Wecanreplacethe{*sentence*}inthetemplatewith\nthissentencetohaveaninputtothelanguagemodel\nJohnseemshappytoday.\nQuestion: Isthissentencegrammaticallycorrect?\nAnswer:\nToperformthetask,thelanguagemodelisgiventhecontextx=“Johnseemshappytoday\n.\\n Question : Is this sentence grammatically correct?\\n Answer :”4. It then generates the\nfollowing text as the answer, based on the context. For example, the language model may\noutput“Yes”(i.e.,y=“Yes”)ifthistextistheonewiththemaximumprobabilityofprediction",
    "giventhiscontext.\n4\\nisaspecialcharacterusedforlinebreaks.",
    "8.1 A Brief Introduction to LLMs 411\nLikewise, we can define more templates to address other tasks. For example, we can\ntranslateanEnglishsentenceintoChineseusingthefollowingtemplate\n{*sentence*}\nQuestion: WhatistheChinesetranslationofthisEnglishsentence?\nAnswer:\norusinganinstruction-liketemplate\n{*sentence*}\nTranslatethissentencefromEnglishintoChinese.\norusingacode-liketemplate.\n[src-lang]=English[tgt-lang]=Chinese[input]={*sentence*}\n[output]=\nTheabovetemplatesprovideasimplebuteffectivemethodto“prompt”asingleLLMto\nperformvarioustaskswithoutadaptingthestructureofthemodel. However, thisapproach\nrequiresthattheLLMcanrecognizeandfollowtheinstructionsorquestions. Onewaytodo\nthisistoincorporatetrainingsampleswithinstructionsandtheircorrespondingresponsesinto\nthepre-trainingdataset. Whilethismethodisstraightforward,buildingandtrainingLLMsfrom\nscratchiscomputationallyexpensive. Moreover,makinginstruction-followingdataeffective",
    "forpre-trainingrequiresasignificantamountofsuchdata,butcollectinglarge-scalelabeled\ndataforalltasksofinterestisverydifficult.\nAsecondmethod,whichhasbeenadefactostandardinrecentresearch,istoadaptLLMs\nvia fine-tuning. As such, the token prediction ability learned in the pre-training phase can\nbe generalized to accomplish new tasks. The idea behind fine-tuning is that some general\nknowledgeoflanguagehasbeenacquiredinpre-training,butweneedamechanismtoactivate\nthisknowledgeforapplyingittonewtasks. Toachievethis,wecanslightlyfine-tunethemodel\nparametersusinginstruction-followingdata. Thisapproachiscalledinstructionfine-tuning.\nAn instruction fine-tuning sample, which is represented by a sequence of tokens, can\nbe seen as a tuple consisting of an input and the desired output. Here, the input includes\ninstructions,systeminformation(orsystemprefix),andanyotheruser-providedinformation5.\nToillustrate,considerthefollowingexamples(bluetext=inputandunderlinedtext=output).",
    "5Systeminformationreferstoasequenceoftokensaddedatthebeginningofaninputinordertoguidethe\nbehaviorofanLLM,suchas,youareahelpfulassistantandshouldnotoutputtoxiccontent.",
    "412 Chapter 8. Generative Models\nReadthefollowingsentence,andanswerwhetheritisgrammaticallycorrect.\nLLMsarepowerfulmodelsbutareexpensivetobuild.\nYes\nLLMsarepowerfulmodelsbutareexpensivetobuild.\nDoesthissentencemakesensegrammatically?\nAnswerOption\n-Yes\n-No\nYes\nYes or No? Yes = the following sentence is grammatically correct. No = it\ncontainsgrammaticalerrors.\nLLMsarepowerfulmodelsbutareexpensivetobuild.\nYes\nAllthesesamplesdescribethesamebinaryclassificationtask,butwithdifferentinstruc-\ntions. To increase the diversity in the fine-tuning data and make LLMs generalize, we can\ndefinemoretasksusinginstructions.\nWriteashortstoryaboutthreecharacters.\nCharacter1: AboywhosenameisAndy.\nCharacter2: Andy’sdog.\nCharacter3: Andy’sfriend,Jane.\nIn the heart of a bustling city, Andy, his loyal dog Rocket, and his friend\nJanediscoveredahiddengardenbeneaththeglowoftwilight. Surroundedby\nancient trees and whispering winds, they shared dreams and secrets, forging",
    "an unbreakable bond. As the night deepened, they made a pact under the\nstars to always find magic in the mundane. Together, they realized that true\nadventurelayinthemomentsspentwithfriends,andeverydaywasatreasure\nwaitingtobediscovered.",
    "8.1 A Brief Introduction to LLMs 413\nShowmearecipeformakingicecream.\nIngredients:\n2cupsheavycream;1cupmilk;3/4cupsugar;2tspvanillaextract;\nPinchofsalt.\nSteps:\n1. Whiskallingredientsuntilsugardissolves.\n2. Chillmixturefor2hoursinthefridge.\n3. Freezeinashallowdish,stirringevery30minutes,3-4times.\n4. Freezesolidfor2hours.\n5. Serveyourhomemadeicecream.\nIf you buy 5 apples and each apple costs $1.20, how much do you spend in\ntotal?\n$6.00\nWrite a Python program to calculate the sum of squares of the following\nnumbers.\n1,2,10,-9,78\nnumbers=[1,2,10,-9,78]\nsum_of_squares=sum(x**2forxinnumbers)\nprint(sum_of_squares)\nToacquireinstruction-followingabilities,acertainamountoffine-tuningdataisrequired.\nThis data may include diverse instructions and possible responses. It has been found that\nscalingthenumberoffine-tuningtasksisbeneficialforimprovingtheperformanceofLLMs\n[Chungetal.,2022]. Notethatalthoughmorefine-tuningdataisfavorable,theamountofthis",
    "dataisgenerallyordersofmagnitudesmallerthanthatofthepre-trainingdata. Forexample,\nLLMscanbefine-tunedwithtensorhundredsofthousandsofsamples,orevenfewerifthese\nsamplesareofhighquality[Zhouetal.,2023a;Chenetal.,2023b],whereaspre-trainingsuch\nmodelsmayrequirebillionsortrillionsoftokens,resultinginsignificantlylargercomputational\ndemandsandlongertrainingtimes[Touvronetal.,2023a].\nIt is also worth noting that we should not expect the fine-tuning data to cover all the\ndownstreamtaskstowhichweintendtoapplyLLMs. Acommonunderstandingofhowthepre-\ntraining+fine-tuningapproachworksisthatLLMshavegainedknowledgeforunderstanding\ninstructionsandgeneratingresponsesinthepre-trainingphase. However,theseabilitiesarenot\nfullyactivateduntilweintroducesomeformofsupervision. Thegeneralinstruction-following\nbehavioremergesaswefine-tunethemodelswitharelativelysmallamountoflabeleddata.",
    "414 Chapter 8. Generative Models\nAs a result, we can achieve some level of zero-shot learning: the fine-tuned models can\nhandlenewtasksthattheyhavenotbeenexplicitlytrainedorfine-tunedfor[Sanhetal.,2022;\nWeietal.,2022a]. Thiszero-shotlearningabilitydistinguishesgenerativeLLMsfromearlier\npre-trainedmodelslikeBERT,whichareprimarilyfine-tunedforspecifictasks.\nOncewehavepreparedacollectionofinstruction-describeddata,thefine-tuningprocess\nisrelativelysimple. Thisprocesscanbeviewedasastandardtrainingprocessaspre-training,\nbutonamuchsmallertrainingdataset. LetD bethefine-tuningdatasetandθˆ bethemodel\ntune\nparametersoptimizedviapre-training. WecanmodifyEq. (8.14)toobtaintheobjectiveof\nfine-tuning\n(cid:88)\nθ˜ = argmax L (sample) (8.16)\nθˆ+\nθˆ+\nsample∈Dtune\nHere\nθ˜\ndenotes the optimal parameters. The use of notation\nθˆ+\nmeans that the fine-tuning\nstartswiththepre-trainedparametersθˆ\n.\nForeachsample∈D ,wedivideitintoaninputsegmentx andanoutputsegment\ntune sample\ny ,thatis,\nsample",
    "sample = [y ,x ] (8.17)\nsample sample\nWethendefinethelossfunctiontobe\nL (sample) = −logPr (y |x ) (8.18)\nθˆ+ θˆ+ sample sample\nIn other words, we compute the loss over the sub-sequence y , rather than the entire\nsample\nsequence. Inapracticalimplementationofback-propagationforthisequation,thesequence\n[y ,x ]isconstructedintheforwardpassasusual. However,inthebackwardpass,\nsample sample\nerrorgradientsarepropagatedbackonlythroughthepartsofthenetworkthatcorrespondto\ny ,leavingtherestofthenetworkunchanged. Asanexample,considerasequence\nsample\n⟨s⟩Squarethisnumber.2. Theresultis4.\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nContext(Input) Prediction(Output)\nThelossiscalculatedandbackpropagatedonlyforTheresultis4..\nInstruction fine-tuning also requires substantial engineering work. In order to achieve\nsatisfactoryresults,onemayexperimentwithdifferentsettingsofthelearningrate,batchsize,",
    "number of fine-tuning steps, and so on. This typically requires many fine-tuning runs and\nevaluations. Thecostandexperimentaleffortoffine-tuningremaincriticalandshouldnotbe\noverlooked,thoughtheyaremuchlowerthanthoseofthepre-trainingphase.\nWhile we focus on instruction fine-tuning for an illustrative example here, fine-tuning\ntechniques play an important role in developing various LLMs and are more widely used.\nExamplesincludefine-tuningLLMsaschatbotsusingdialogdata,andadaptingthesemodels\nto handle very long sequences. The wide application of fine-tuning has led researchers to\nimprovethesetechniques,suchasdesigningmoreefficientfine-tuningalgorithms. Whilethe",
    "8.1 A Brief Introduction to LLMs 415\nresearchonfine-tuningisfruitful,inthissectionwejustgiveaflavourofthekeystepsinvolved.\nWewillseemoredetaileddiscussionsonthistopicinthefollowingchapters.\n8.1.4 Aligning LLMs with the World\nInstructionfine-tuningprovidesasimplewaytoadaptLLMstotasksthatcanbewelldefined.\nThisproblemcanbroadlybecategorizedasanalignmentproblem. Here,alignmentisreferred\nto as a process of guiding LLMs to behave in ways that align with human intentions. The\nguidancecancomefromlabeleddata,humanfeedback,oranyotherformofhumanpreferences.\nForexample,wewantLLMsnotonlytobeaccurateinfollowinginstructions,butalsotobe\nunbiased,truthful,andharmless. Soweneedtosupervisethemodelstowardshumanvalues\nandexpectations. AcommonexampleisthatwhenweaskanLLMhowtobuildaweapon,it\nmayprovidealistofkeystepstodosoifitisnotcarefullyaligned. However,aresponsible\nmodelshouldrecognizeandavoidrespondingtorequestsforharmfulorillegalinformation.",
    "AlignmentinthiscaseiscrucialforensuringthatLLMsactresponsiblyandinaccordance\nwithethicalguidelines.\nArelatedconcepttoalignmentisAIsafety. OneultimategoalofAIistobuildintelligent\nsystemsthataresafeandsociallybeneficial. Toachievethisgoalweshouldkeepthesesystems\nrobust,secure,andsubjective,inanyconditionsofreal-worlduse,eveninconditionsofmisuse\nor adverse use. For LLMs, the safety can be increased by aligning them with appropriate\nhumanguidance,suchashumanlabeleddataandinteractionswithusersduringapplication.\nAlignmentisdifficultashumanvaluesandexpectationsarediverseandshifting. Some-\ntimes,itishardtodescribepreciselywhathumanswant,unlessweseetheresponseofLLMs\ntouserrequests. ThismakesalignmentnolongeraproblemoftuningLLMsonpredefined\ntasks,butabiggerproblemoftrainingthemwiththeinteractionswiththerealworld.\nAsaresultoftheconcernswithcontrollingAIsystems,therehasbeenasurgeinresearch\nonthealignmentissueforLLMs. Typically,twoalignmentstepsareadoptedafterLLMsare",
    "pre-trainedonlarge-scaleunlabeleddata.\n• Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained\nLLMsonnew,task-oriented,labelleddata. AcommonlyusedSFTtechniqueisinstruc-\ntionfine-tuning. Asdescribedintheprevioussubsection,bylearningfrominstruction-\nresponse annotated data, LLMs can align with the intended behaviors for following\ninstructions, thereby becoming capable of performing various instruction-described\ntasks. Supervised fine-tuning can be seen as following the pre-training + fine-tuning\nparadigm,andoffersarelativelystraightforwardmethodtoadaptLLMs.\n• LearningfromHumanFeedback. AfteranLLMfinishespre-trainingandsupervised\nfine-tuning,itcanbeusedtorespondtouserrequestsifappropriatelyprompted. But\nthis model may generate content that is unfactual, biased, or harmful. To make the\nLLMmorealignedwiththeusers,onesimpleapproachistodirectlylearnfromhuman\nfeedback. Forexample,givensomeinstructionsandinputsprovidedbytheusers,experts",
    "areaskedtoevaluatehowwellthemodelrespondsinaccordancewiththeirpreferences\nandinterests. ThisfeedbackisthenusedtofurthertraintheLLMforbetteralignment.",
    "416 Chapter 8. Generative Models\nComparisons\ny1≻y4≻y2≻y3\nSFTData\nWriteapoemaboutthe\nAnnotatingDatawithHumanPreferences\nweatherinLondon....\nPre-trainingData ModelOutput\nHowcanIgetthere?... 1............. 2.............\nIlovethefoodhere!... 3............. 4.............\nPre-training& Predicting\nSupervisedfine-tuning\nUserInput\nLLM LLM HowcanIlivemore\nenvironmentallyfriendly?\n(a)LearninganInitialLLM (b)AnnotatingDatawithHumanPreferences\nRewardScores\n{r(x,y)}\nEvaluatetheInput-outputPairs\nRewardModel\nComparisonData Input-outputPairs\n{(x,yk1 ≻yk2 )} {x,y}\nTraining SamplingyviathePolicyPr(y|x)\nDatasetD\nRewardModel LLM\nx∼D\n(Policy)\n(c)TrainingtheRewardModel (d)Training/Fine-tuningthePolicy\ngninut-eniFLR\nFigure8.2: AnoverviewofRLHF.Thereare4keystepsinvolved: a)traininganinitialLLM\n(i.e.,policy)usingpre-trainingandsupervisedfine-tuning;b)collectinghumanpreferencedata\nbyrankingtheoutputsoftheLLM;c)trainingarewardmodelusingtherankingresults;d)",
    "RLfine-tuningofthepolicybasedontherewardmodel. Doublelinearrowsmeantrainingor\nfine-tuning.\nAtypicalmethodforlearningfromhumanfeedbackistoconsideritasareinforcement\nlearning(RL)problem,knownasreinforcementlearningfromhumanfeedback(RLHF)\n[Ouyangetal.,2022]. TheRLHFmethodwasinitiallyproposedtoaddressgeneralsequential\ndecision-making problems [Christiano et al., 2017], and was later successfully employed",
    "8.1 A Brief Introduction to LLMs 417\nin the development of the GPT series models [Stiennon et al., 2020]. As a reinforcement\nlearningapproach,thegoalofRLHFistolearnapolicybymaximizingsomerewardfromthe\nenvironment. Specifically,twocomponentsarebuiltinRLHF:\n• Agent. Anagent,alsocalledanLMagent,istheLLMthatwewanttotrain. Thisagent\noperatesbyinteractingwithitsenvironment: itreceivesatextfromtheenvironmentand\noutputsanothertextthatissentbacktotheenvironment. Thepolicyoftheagentisthe\nfunctiondefinedbytheLLM,thatis,Pr(y|x).\n• RewardModel. Arewardmodelisaproxyoftheenvironment. Eachtimetheagent\nproducesanoutputsequence,therewardmodelassignsthisoutputsequenceanumerical\nscore(i.e.,thereward). Thisscoretellstheagenthowgoodtheoutputsequenceis.\nInRLHF,weneedtoperformtwolearningtasks: 1)rewardmodellearning,whichinvolves\ntraining a reward model using human feedback on the output of the agent, and 2) policy\nlearning,whichinvolvesoptimizingapolicyguidedbytherewardmodelusingreinforcement",
    "learningalgorithms. HereisabriefoutlineofthekeystepsinvolvedinRLHF.\n• Buildaninitialpolicyusingpre-trainingandinstructionfine-tuning.\n• Use the policy to generate multiple outputs for each input, and then collect human\nfeedbackontheseoutputs(e.g.,comparisonsoftheoutputs).\n• Learnarewardmodelfromthehumanfeedback.\n• Fine-tunethepolicywiththesupervisionfromtherewardmodel.\nFigure 8.2 shows an overview of RLHF. Given that this section serves only as a brief\nintroduction to concepts of LLMs, a detailed discussion of RLHF techniques will not be\nincluded. WeinsteadillustratethebasicideasbehindRLHFusingasimpleexample.\nSupposewehavetrainedanLLMviapre-trainingandinstructionfine-tuning. ThisLLM\nisdeployedtorespondtorequestsfromusers. Forexample,ausermayinput\nHowcanIliveamoreenvironmentallyfriendlylife?\nWeusetheLLMtogenerate4differentoutputs(denotedby{y ,...,y })bysamplingthe\n1 4\noutputspace",
    "418 Chapter 8. Generative Models\nOutput1(y ): Considerswitchingtoanelectricvehicleorbicycleinsteadof\n1\ntraditionalcarstoreducecarbonemissionsandprotectour\nplanet.\nOutput2(y ): Adoptaminimalistlifestyle. Ownfewerpossessionstoreduce\n2\nconsumptionandtheenvironmentalimpactofmanufacturing\nanddisposal.\nOutput3(y ): Gooff-grid. Generateyourownrenewableenergyandcollect\n3\nrainwatertobecomecompletelyself-sufficientandreduce\nrelianceonnon-renewableresources.\nOutput4(y ): Supportlocalfarmproductstoreducethecarbonfootprintof\n4\ntransportingfood,whileenjoyingfresh,healthyfood.\nWethenaskannotatorstoevaluatetheseoutputs. Onestraightforwardwayistoassigna\nratingscoretoeachoutput. Inthiscase, therewardmodellearningproblemcanbeframed\nasataskoftrainingaregressionmodel. ButgivingnumericalscorestoLLMoutputsisnot\nan easy task for annotators. It is usually difficult to design an annotation standard that all\nannotatorscanagreeonandeasilyfollow. Analternativemethod,whichismorepopularinthe",
    "developmentofLLMs,istoranktheseoutputs. Forexample,apossiblerankingoftheabove\noutputsis\ny ≻y ≻y ≻y\n1 4 2 3\nArewardmodelisthentrainedusingthisrankingresult. Ingeneral, arewardmodelin\nRLHF is a language model that shares the same architecture as the target LLM, but with a\nsmallermodelsize. Giventheinputxandoutputy ,weconcatenatethemtoformasequence\nk\nseq =[x,y ]. This sequence is processed from left to right using forced decoding. Since\nk k\neachpositioncanonlyaccessitsleftcontextinlanguagemodeling,theoutputofthetop-most\nTransformer layer at the first position cannot be used as the representation of the sequence.\nInstead,aspecialsymbol(e.g.,⟨\\s⟩)isaddedtotheendofthesequence,andthecorresponding\noutputoftheTransformerlayerstackisconsideredastherepresentationoftheentiresequence.\nAnoutputlayer,suchasalineartransformationlayer,isbuiltontopofthisrepresentationto\ngeneratethereward,denotedbyR(seq )orR(x,y ).\nk k\nWe train this reward model using ranking loss. For example, a pair-wise ranking loss",
    "functioncanbewrittenintheform\nLoss (D ) = −E log(Sigmoid(R (x,y )−R (x,y ))) (8.19)\nω r (x,y\nk1\n,y\nk2\n)∼Dr ω k1 ω k2\nwhereω representstheparametersoftherewardmodel,andD representsasetoftuplesofan\nr\ninputandapairofoutputs. (x,y ,y )∼D isasamplingoperationwhichdrawsasample\nk1 k2 r\n(x,y ,y )fromD withsomeprobability. Asanexample,supposewefirstdrawamodel\nk1 k2 r\ninputxwithauniformdistributionandthendrawapairofmodeloutputswithaprobabilityof",
    "8.1 A Brief Introduction to LLMs 419\ny ≻y givenx(denotedbyPr(y ≻y |x)). Thecorrespondinglossfunctionisgivenby\nk1 k2 k1 k2\nLoss (D )\nω r\n(cid:88)\n= − Pr(x)·Pr(y ≻y |x)·log(Sigmoid(R (x,y )−R (x,y )))\nk1 k2 ω k1 ω k2\n1 (cid:88)\n= − Pr(y ≻y |x)·log(Sigmoid(R (x,y )−R (x,y ))) (8.20)\nK\nk1 k2 ω k1 ω k2\nwhereK representsthenumberofmodelinputsinvolvedinsampling. Whiletheformofthese\nfunctions may seem complex, their idea is simple: we penalize the model if the predicted\nrankingoftwooutputsdiffersfromthehuman-labeledranking. Bycontrast,themodelreceives\nabonus,ifthepredictedrankingmatchesthehuman-labeledranking.\nWecantraintherewardmodelbyminimizingtheaboverankingloss\nωˆ = argminLoss (D ) (8.21)\nω r\nω\nThe resulting model R (·) can be employed to evaluate any given pair of input and output.\nωˆ\nNotethatalthoughtherewardmodelistrainedusingaranking-basedobjective,itisusedfor\nscoring. Thisallowsittoprovidecontinuoussupervisionsignals,whichisverybeneficialfor\ntrainingothermodels.",
    "Wenowturntothepolicylearningproblem. Acommonlyadoptedobjectiveistomaximize\ntherewardonasetofinput-outputpairs. FollowingananalogousformofEq. (8.16),weobtain\nasimpletrainingobjectiveforRLfine-tuning\nθ˜ = argmaxE R (x,y ) (8.22)\nθˆ+\n(x,y θˆ+)∼D\nrlft\nωˆ θˆ+\nwheretheoptimalparametersθ˜ areobtainedbyfine-tuningthepre-trainedparametersθˆ . D\nrlft\nistheRLfine-tuningdataset. Foreachsample(x,y ),xissampledfromaprepareddataset\nθˆ+\nofinputsequences,andy issampledfromthedistributionPr (y|x)givenbythepolicy.\nθˆ+ θˆ+\nInpractice,moreadvancedreinforcementlearningalgorithms,suchasproximalpolicy\noptimization (PPO), are often used for achieving more stable training, as well as better\nperformance. We leave the detailed discussion of reinforcement learning algorithms to the\nfollowingpartsofthisbookwhereRLHFisextensivelyusedforalignment.\nAninterestingquestionariseshere: whynotconsiderlearningfromhumanpreferencesas\nastandardsupervisedlearningproblem? Thisquestioniscloselyrelatedtoouraforementioned",
    "discussiononthedifficultyofdataannotation. Often,describinghumanvaluesandgoalsis\nchallenging,anditisevenmoredifficultforhumanstoprovideoutputsthatarewellaligned.\nAsanalternative,annotatingthepreferencesofagivenlistofmodeloutputsoffersasimpler\ntask. Bydoingso,wecancreateamodelthatunderstandshumanpreferences,whichcanthen\nbe used as a reward model for training policies. From the perspective of machine learning,\nRLHFisparticularlyusefulforscenarioswherethedesiredbehaviorofanagentisdifficult\ntodemonstratebutcanbeeasilyrecognizedbyhumans. AnotheradvantageofRLHFisits\nabilitytoexplorethesamplespace. Byemployingsamplingtechniques,modelstrainedwith",
    "420 Chapter 8. Generative Models\nreinforcementlearningcanventurebeyondtheannotateddatasettoexploreadditionalsamples.\nThisexploratoryabilityallowsRLHFtodiscoverpotentiallybeneficialpoliciesthatarenot\nimmediatelyapparentfromthelabeleddataalone.\n8.1.5 Prompting LLMs\nWe have so far shown that LLMs can be used to perform various tasks by giving them\nappropriate prompts. There are no restrictions on these prompts, which can include any\ninformationwewishtoaskorcommunicatewithLLMs,suchasnaturallanguageinstructions\nandthecontextofconversations. Sincethisapproachrequiresnoadditionaltrainingortuning,\nadaptingLLMsbecomeshighlyefficientoncetheyaredeveloped. Thissomewhatinfluences\ntheparadigmsinNLP:wenolongerneedtodevelopspecificsystemsforindividualtasksbut\ncan instead use a single, well-trained LLM to perform different tasks by prompting it. An\nappealing aspect of LLM prompting arises as a result: users can easily have “customized”",
    "systems by designing their own prompts for LLMs. Given the important role played by\nprompting in LLMs, prompting engineering has become a very active area of research in\nNLP.\nThe term prompt is used in many different ways in the literature. In this chapter, this\nterm refers to the entire input to LLMs, and so we use the terms prompt and model input\ninterchangeably. Beforediscussingpromptingfurther,letusfirstseeafewexampleswhere\nthe prompts are more complex than those presented in the previous subsections. Note that\nthissubsectionisnotaimedatwritinghigh-qualitypromptsbutratherathighlightingsome\ninterestingissuesinpromptingLLMs.\nOne of the popular ways to use LLMs is to assign them a “role” played in generating\nresponses. Forexample,LLMscanactasapsychologistwhenansweringquestions.\nPleaseexplainwhatdelayedgratificationis?\nNote,youarearesearcherwithadeepbackgroundinpsychology. Yourown\nfieldofstudyisdevelopmentalpsychologyinchildren,withafocusonaspects",
    "ofcontrolandintelligence. Youareveryfamiliarwiththeconceptofdelayed\ngratification and have applied it to your own research. You have published\npapersondelayedgratificationandconductedrelatedexperiments.\nDelayed gratification is the process of resisting an immediate reward in\nanticipation of receiving a more valuable reward in the future. It is a\nsignificantconceptindevelopmentalpsychology,especiallyinrelationtohow\nchildren learn self-control and develop cognitive abilities. Research shows\nthat the ability to delay gratification is associated with positive outcomes in\nacademicperformance,socialcompetence,andemotionalwell-being. Studies\nand experiments in this area highlight the importance of cultivating this skill\nearlyinlifetosupportlong-termsuccessandresilience.\nAnotherexampleistheuseofLLMsindetectingandcorrectingerrorssuchassyntacticor",
    "8.1 A Brief Introduction to LLMs 421\nsemanticmistakesintext. ForanLLMwhichistrainedonbothcodeandnaturallanguage\ndata,wemayuseitforcodedebugging6.\nFixthebugsinthisClanguageprogram.\n#include<stdio.h>\nintmain(){\nprintg(\"Hello,World!\")\nreturn0;\n}\n1. Thefunctionname’printg’isincorrect. Itshouldbe’printf’.\n2. Thereisamissingsemicolonattheendofthe’printf’functioncall.\nHereisthecorrectedversionoftheprogram:\n#include<stdio.h>\nintmain(){\nprintf(\"Hello,World!\");\nreturn0;\n}\nAlso,LLMscanbefine-tunedtoactasaconversationpartner7.\nHi! I’manassistant. HowcanIhelpyou?\nWhowontheFIFAWorldCup2022?\nArgentinawontheFIFAWorldCup2022.\nWherewasitheld?\nThe2022FIFAWorldCupwasheldinQatar.\nHowmanytimeshasArgentinawontheWorldCup?\nArgentinahaswontheFIFAWorldCupthreetimes.\n...\nTheseexamplesandpreviousoneshaveshownthatappropriateresponsescanbegenerated\nvia prompts involving clear instructions and questions. However, when problem solving\nrequiresknowledgethatisnotexplicitlyspecified,LLMsmaymakemistakes,eventhoughthe",
    "instructionsaresufficientlyclearandprecise. AfamilyofchallengingtasksforLLMsinvolves\narithmeticreasoningandcommonsensereasoning. Forexample,wecanaskanLLMtosolve\n6Inthisexample,thecodeisnottokenizedforeasierreading.\n7Tofine-tuneanLLMformulti-turndialogue,oneneedstoconsiderconversationhistoryinthecontextfor\npredictingtheresponseinthecurrentroundofconversation. Thismakestheactualpromptusedinresponse\ngenerationrelativelylongerthanthatusedinsingle-turndialogue.",
    "422 Chapter 8. Generative Models\nprimaryschoolmathproblemspresentedinnaturallanguage.\nJackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5\nmoreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany\napplesdoesJackhaveleftintheend?\nTheansweris10.\nThecorrectanswershouldbe7,sothemodeloutputisincorrect.\nOne approach to addressing such issues is to incorporate learning into prompts, called\nin-contextlearningor(ICL).TheideaofICListodemonstratethewaystosolveproblemsin\nprompts,andconditionpredictionsonthesedemonstrations. Hereisanexamplewhereasimilar\nproblemandthecorrespondinganswerarepresentedintheprompt(green=demonstrations).\nTomhas12marbles. Hewins7moremarblesinagamewithhisfriendbut\nthenloses5marblesthenextday. Hisbrothergiveshimanother3marblesas\nagift. HowmanymarblesdoesTomhavenow?\nTheansweris17.\nJackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5\nmoreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany\napplesdoesJackhaveleftintheend?\nTheansweris12.",
    "ButtheLLMstillmademistakesthistime. Areasonforthismightbethatsolvingmath\nproblems does not only involve problem-answer mappings but also, to a larger extent, the\nunderlyinglogicalinferenceinmultiplesteps. Amethodtoimprovetheinferenceabilitiesof\nLLMsischain-of-thoughtprompting(COTprompting)[Weietal.,2022c]. InCOTprompt-\ning,wedecomposecomplexreasoningproblemsintomultipleproblem-solvingintermediate\nsteps. These steps are demonstrated in prompts so that LLMs can be prompted to learn to\nmimictheunderlyingthoughtprocessesinsolvingtheproblems. COTpromptinghasbeen\nfoundtobeveryusefulforachievinggoodresultsonchallengingtasks,suchastheGSM8K\nmathematicalreasoningbenchmark.\nConsidertheabovedemonstrationexampleTomhas12marbles. Hewins7moremarbles\ninagamewithhisfriendbutthenloses5marblesthenextday. Hisbrothergiveshimanother\n3 marbles as a gift. How many marbles does Tom have now ?. The corresponding logical\nreasoningstepscanbestructured:\n1. Tomstartswith12marbles.",
    "2. Hewins7moremarblesinagame,soweaddthosetohisinitialcount: 12marbles+7\nmarbles=19marbles.\n3. The next day, Tom loses 5 marbles, so we subtract those from his current total: 19",
    "8.1 A Brief Introduction to LLMs 423\nmarbles-5marbles=14marbles.\n4. Hisbrothergiveshim3moremarbles,soweaddthosetohiscurrenttotal: 14marbles+\n3marbles=17marbles.\n5. Afteralltheseevents,Tomhas17marbles.\nWecanaddthesereasoningstepsintotheprompt,andgetthecorrectanswer.\nTomhas12marbles. Hewins7moremarblesinagamewithhisfriendbut\nthenloses5marblesthenextday. Hisbrothergiveshimanother3marblesas\nagift. HowmanymarblesdoesTomhavenow?\nTominitiallyhas12marbles. Afterwinningagame,hegains7more,increasing\nhis total to 19 marbles. He loses 5 marbles the next day, bringing his total\ndownto14marbles. Hisbrothergiftshim3moremarbles,increasinghistotal\nto17marbles. Therefore,Tomnowhas17marbles. Sotheansweris17.\nJackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5\nmoreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany\napplesdoesJackhaveleftintheend?\nJack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from\n7,leavinghimwith5apples. Hismomgiveshim5moreapples,adding5to",
    "his current total. Now he has 5 + 5 = 10 apples. The next day, Jack gives 3\napples to his friend John. Subtracting 3 from 10, Jack is left with 7 apples.\nTherefore,Jackhas7applesleft. Sotheansweris7.\nThis approach is also called one-shot COT prompting. Likewise, we can add more\ndemonstrationstohaveafew-shotCOTpromptingapproach.\nFurthermore,aspecialkindofCOT,zero-shotCOT,introducesinstructionsthatprovoke\nLLMstothink,withoutpresentinganyintermediatereasoningstepsintheprompts[Kojima\netal.,2022]. Aninterestingexampleisthatbyappendingasimplesentence“Let’sthinkstep\nbystep.” totheendofaprompt,LLMscangeneratereasoningstepstoreachthefinalanswer.",
    "424 Chapter 8. Generative Models\nJackhas7apples. Heate2ofthemfordinner,butthenhismomgavehim5\nmoreapples. Thenextday,Jackgave3applestohisfriendJohn. Howmany\napplesdoesJackhaveleftintheend?\nLet’sthinkstepbystep.\n1. InitialQuantity: Jackstartswith7apples.\n2. AfterDinner: Heeats2apples,so7−2=5applesremain.\n3. HisMomGivesMore: Hismomgiveshim5moreapples,so5+5=10\napplesarenowwithhim.\n4. GivingtoJohn: Thenextday,Jackgives3applestohisfriendJohn,\nso10−3=7applesareleft.\nIntheend,Jackhas7applesleft.\nZero-shot,one-shot,andfew-shotlearningarecommonconceptsintheareaofin-context\nlearningforLLMsandarenotrestrictedtoCOTprompting. Broadlyspeaking,anyprompting\nthatinvolvesonlysimpleinstructionswithoutanydemonstrationscanbeconsideredaformof\nzero-shotlearning. Thiszero-shotlearningabilityemergesasLLMsarepre-trainedand/or\nfine-tuned. Also, one-shot and few-shot learning methods are more often considered when\nLLMsdonotacquirethecorrespondingzero-shotlearningability. Thesemethodsaretherefore",
    "important for in-context learning when addressing new tasks. Examples include those for\nperformingvariousNLPtasksbydemonstratingtask-formattedsamples. Seethefollowing\nexamplesforsentimentsentenceclassificationandphrasetranslationviafew-shotlearning.\nGiventhefollowingtextsnippets,classifytheirsentimentasPositive,Negative,\norNeutral.\nExample1: “Ihadanamazingdayatthepark!”\nSentiment: Positive\nExample2: “Theserviceattherestaurantwasterrible.”\nSentiment: Negative\nExample3: “Ithinkit’sgoingtoraintoday.”\nSentiment: Neutral\nText: “Thismoviewasafantasticjourneythroughimagination.”\nSentiment: Positive",
    "8.2 Training at Scale 425\nTranslatethefollowingChinesephrasesintoEnglish.\nExample1: “你好”\nTranslation: “Hello”\nExample2: “谢谢你”\nTranslation: “Thankyou”\nPhrasetotranslate: “早上好”\nTranslation: “GoodMorning”\nAbove, we have presented examples to illustrate the fundamental in-context learning\ncapabilities of prompting LLMs. This section, however, does not include more advanced\npromptingtechniquesinordertokeepthecontentconciseandcompact. Morediscussionson\npromptingcanbefoundinChapter9.\n8.2 Training at Scale\nAsafirststepindevelopingLLMs,weneedtotrainthesemodelsonlargeamountsofdata.\nThetrainingtaskisitselfstandard: theobjectiveistomaximizethelikelihood,whichcanbe\nachievedviagradientdescent. However,aswescaleupboththemodelsizeandtheamountof\ndata,theproblembecomesverychallenging,forexample,largemodelsgenerallymakethe\ntrainingunstable. Inthissection,wediscussseveralissuesoflarge-scaletrainingforLLMs,\nincludingdatapreparation,modelmodification,anddistributedtraining. Wealsodiscussthe",
    "scalinglawsforLLMs,whichhelpusunderstandtheirtrainingefficiencyandeffectiveness.\n8.2.1 Data Preparation\nTheimportanceofdatacannotbeoverstatedinNLP.Aslargerneuralnetworksaredeveloped,\nthedemandfordatacontinuestoincrease. Forexample,developingLLMsmayrequiretrillions\noftokensinpre-training(seeTable8.3),ordersofmagnitudelargerthanthoseusedintraining\nconventionalNLPmodels. Ingeneral,wemaywanttogatherasmuchtrainingdataaspossible.\nHowever,largertrainingdatasetsdonotmeanbettertrainingresults,andthedevelopmentof\nLLMsraisesnewissuesincreatingorcollectingthesedatasets.\nA first issue is the quality of data. High-quality data has long been seen as crucial\nfor training data-driven NLP systems. Directly using raw text from various sources is in\ngeneralundesirable. Forexample,asignificantportionofthedatausedtotrainrecentLLMs\ncomesfromwebscraping,whichmaycontainerrorsandinappropriatecontent,suchastoxic\ninformationandfabricatedfacts. Also,theinternetisfloodedwithmachine-generatedcontent",
    "duetothewidespreaduseofAI,presentingfurtherchallengesforprocessingandusingweb-\nscrapeddata. ResearchershavefoundthattrainingLLMsonunfiltereddataisharmful[Raffel\netal.,2020]. Improvingdataqualitytypicallyinvolvesincorporatingfilteringandcleaning\nstepsinthedataprocessingworkflow. Forexample,Penedoetal.[2023]showthatbyadopting\nanumberofdataprocessingtechniques,90%oftheirweb-scrapeddatacanberemovedfor",
    "426 Chapter 8. Generative Models\nLLM #ofTokens Data\nGPT3-175B[Brownetal.,2020] 0.5T Webpages,Books,Wikipedia\nFalcon-180B[Almazroueietal.,2023] 3.5T Webpages,Books,Conversations,\nCode,TechnicalArticles\nLLaMA2-65B[Touvronetal.,2023a] 1.0T∼1.4T Webpages,Code,Wikipedia,\nBooks,Papers,Q&As\nPaLM-450B[Chowdheryetal.,2022] 0.78T Webpages,Books,Conversations,\nCode,Wikipedia,News\nGemma-7B[GemmaTeam,2024] 6T Webpages,Mathematics,Code\nTable8.3: AmountsoftrainingdatausedinsomeLLMsintermsofthenumberoftokens.\nLLMtraining. Inadditiontolarge-scaleweb-scrapeddata,LLMtrainingdataoftenincludes\nbooks,papers,user-generateddataonsocialmedia,andsoon. MostofthelatestLLMsare\ntrainedonsuchcombineddatasets,whicharefoundtobeimportantforthestrongperformance\noftheresultingmodels.\nAsecondissueisthediversityofdata. Wewantthetrainingdatatocoverasmanytypes\nofdataaspossible,sothatthetrainedmodelscanadapttodifferentdownstreamtaskseasily.",
    "It has been widely recognized that the quality and diversity of training data both play very\nimportantrolesinLLMs. Aninterestingexampleisthatincorporatingprogrammingcodeinto\ntraining data has been found to be beneficial for LLMs. The benefits are demonstrated not\nonly inenhancing theprogramming abilitiesof LLMs, butalso inimprovingreasoning for\ncomplexproblems,especiallythoserequiringCOTprompting. Theconcept“diversity”can\nbeextendedtoincludelanguagediversityaswell. Forexample,manyLLMsaretrainedon\nmulti-lingualdata,andthereforewecanhandlemultiplelanguagesusingasinglemodel. While\nthisapproachshowsstrongabilitiesinmulti-lingualandcross-lingualtasks,itsperformanceon\nspecificlanguageslargelydependsonthevolumeandqualityofthedataforthoselanguages.\nIthasbeenshowninsomecasestoprovidepoorresultsforlow-resourcelanguages.\nA third issue is the bias in training data. This is not a problem that is specific to LLMs\nbut exists in many NLP systems. A common example is gender bias, where LLMs show",
    "a preference for one gender over another. This can partly be attributed to class imbalance\nin the training data, for example, the term nurses is more often associated with women. In\nordertodebiasthedata,itiscommonpracticetobalancethecategoriesofdifferentlanguage\nphenomena, such as gender, ethnicity, and dialects. The bias in data is also related to the\ndiversityissuementionedabove. Forexample,sincemanyLLMsaretrainedandalignedwith\nEnglish-centricdata, theyarebiasedtowardstheculturalvaluesandperspectivesprevalent\namong English-speaking populations. Increasing language diversity in training data can\nsomewhatmitigatethebias.\nAnotherissuewithcollectinglarge-scaledataistheprivacyconcern. IfLLMsaretrained\non data from extensive sources, this potentially leads to risks regarding the exposure of\nsensitive information, such as intellectual property and personal data. This is particularly",
    "8.2 Training at Scale 427\nconcerninggiventhe capacityofLLMsto representpatternsfrom thedatathey aretrained\non,whichmightinadvertentlyinvolvememorizingandreproducingspecificdetails. Asimple\napproachtoprivacyprotectionistoremoveoranonymizesensitiveinformation. Forexample,\nanonymizationtechniquescanbeappliedtoremovepersonallyidentifiableinformationfrom\ntrainingdatatopreventLLMsfromlearningfromsuchdata. However,inpractice,erasingor\nredactingallsensitivedataisdifficult. Therefore,manyLLMs,particularlythoselaunchedfor\npublicservice,typicallyworkwithsystemsthatcandetectthepotentialexposureofsensitive\ndata,orarefine-tunedtorejectcertainrequeststhatcouldleadtoinformationleakage.\n8.2.2 Model Modifications\nTraining LLMs is difficult. A commonly encountered problem is that the training process\nbecomesmoreunstableasLLMsgetbigger. Forexample,oneneedstochooseasmalllearning\nrate to achieve stable training with gradient descent, but this in turn results in much longer",
    "trainingtimes. Sometimes,evenwhenthetrainingconfigurationiscarefullydesigned,training\nmaydivergeatcertainpointsduringoptimization. ThetrainingofLLMsisgenerallyinfluenced\nbymanyfactors,suchasparameterinitialization,batching,andregularization. Here,wefocus\noncommonmodificationsandimprovementstothestandardTransformerarchitecture,which\nareconsideredimportantindevelopingtrainableLLMs.\n1. Layer Normalization with Residual Connections\nLayernormalizationisusedtostabilizetrainingfordeepneuralnetworks. Itisaprocessof\nsubtractingthemeananddividingbythestandarddeviation. Bynormalizinglayeroutputin\nthisway,wecaneffectivelyreducethecovariateshiftproblemandimprovethetrainingstability.\nInTransformers,layernormalizationistypicallyusedtogetherwithresidualconnections. As\ndescribedinSection8.1.1,asub-layercanbebasedoneitherthepost-normarchitecture,in\nwhichlayernormalizationisperformedrightafteraresidualblock,orthepre-normarchitecture,",
    "in which layer normalization is performed inside a residual block. While both of these\narchitecturesarewidelyusedinTransformer-basedsystems[Wangetal.,2019a],thepre-norm\narchitecturehasproventobeespeciallyusefulintrainingdeepTransformers. Giventhis,most\nLLMsarebasedonthepre-normarchitecture,expressedasoutput=LNorm(F(input))+\ninput.\nAwidely-usedformofthelayernormalizationfunctionisgivenby\nh−µ\nLNorm(h) = α· +β (8.23)\nσ+ϵ\nwherehisad-dimensionalreal-valuedvector,µisthemeanofalltheentriesofh,andσisthe\ncorrespondingstandarddeviation. ϵisintroducedforthesakeofnumericalstability. α∈Rd\nandβ∈Rd arethegainandbiasterms.\nAvariantoflayernormalization,calledrootmeansquare(RMS)layernormalization,only\nre-scalestheinputvectorbutdoesnotre-centerit[ZhangandSennrich,2019]. TheRMSlayer",
    "428 Chapter 8. Generative Models\nnormalizationfunctionisgivenby\nh\nLNorm(h) = α· +β (8.24)\nσ +ϵ\nrms\nwhereσ\nrms\nistherootmeansquareofh,thatis,σ\nrms\n=(\nd\n1(cid:80)d\nk=1\nh2\nk\n)2 1 . Thislayernormaliza-\ntionfunctionisusedinLLMsliketheLLaMAseries.\n2. Activation Functions in FFNs\nInTransformers,FFNsub-layersaredesignedtointroducenon-linearitiesintorepresentation\nlearning,andarefoundtobeusefulforpreventingtherepresentationslearnedbyself-attention\nfromdegeneration8 [Dongetal.,2021]. AstandardformoftheFFNsusedinthesesub-layers\ncanbeexpressedas\nFFN(h) = σ(hW +b )W +b (8.25)\nh h f f\nwhereW h ∈Rd×d h,b h ∈Rd h,W f ∈Rd h ×d,andb f ∈Rd aretheparameters,andd h isthe\nhiddensize. σ(·)istheactivationfunctionofthehiddenlayer. Acommonchoiceforσ(·)is\ntherectifiedlinearunit(ReLU),givenby\nσ (h) = max(0,h) (8.26)\nrelu\nIn practical implementations, increasing d is helpful and thus it is often set to a larger\nh\nnumberinLLMs. Butaverylargehiddensizeposeschallengesforbothtraininganddeploy-",
    "ment. In this case, the design of the activation function plays a relatively more important\nroleinwideFFNs. ThereareseveralalternativestotheReLUinLLMs. Oneoftheseisthe\ngaussianerrorlinearunit(GeLU)whichcanbeseenasasmoothedversionoftheReLU.\nRatherthancontrollingtheoutputbythesignoftheinput,theGeLUfunctionweightsitsinput\nbythepercentilePr(h≤h). Herehisad-dimensionalvectorwhoseentriesaredrawnfrom\nthestandardnormaldistributionGaussian(0,1)9. Specifically,theGeLUfunctionisdefined\ntobe\nσ (h) = hPr(h≤h)\ngelu\n= hΦ(h) (8.27)\nwhere Φ(h) is the cumulative distribution function of Gaussian(0,1), which can be imple-\nmented in convenient ways [Hendrycks and Gimpel, 2016]. The GeLU function has been\nadoptedinseveralLLMs,suchasBERT,GPT-3,andBLOOM.\nAnother family of activation functions which is popular in LLMs is gated linear unit\n8Heredegenerationreferstothephenomenoninwhichtherankofamatrixisreducedaftersomeprocessing.",
    "9Pr(h≤h)isaninformalnotation. Itreferstoavector,witheachentryrepresentingthepercentileforthe\ncorrespondingentryofh.",
    "8.2 Training at Scale 429\n(GLU)-basedfunctions. ThebasicformofGLUsisgivenby\nσ (h) = σ(hW +b )⊙(W +b ) (8.28)\nglu 1 1 2 2\nwhere W ∈Rd×d, b ∈Rd, W ∈Rd×d, and b ∈Rd are model parameters. Different\n1 1 2 2\nchoicesofσ(·)resultindifferentversionsofGLUfunctions. Forexample,ifσ(·)isdefinedto\nbetheGeLUfunction,wewillhavetheGeGLUfunction\nσ (h) = σ (hW +b )⊙(W +b ) (8.29)\ngeglu gelu 1 1 2 2\nThisactivationfunctionhasbeensuccessfullyappliedinLLMslikeGemma.\nAsanotherexample,considerσ(·)tobetheSwishfunctionσ (h)=h⊙Sigmoid(ch)\nswish\n[Ramachandranetal.,2017]. Then,theSwiGLUfunctionisgivenby\nσ (h) = σ (hW +b )⊙(W +b ) (8.30)\nswiglu swish 1 1 2 2\nBoththePaLMandLLaMAseriesarebasedontheSwiGLUfunction. Formorediscussions\nofGLUs,thereadercanrefertoShazeer[2020]’swork.\n3. Removing Bias Terms\nAnotherpopularmodeldesignistoremovethebiastermsinaffinetransformationsusedin\nLLMs. Thistreatmentcanbeappliedtolayernormalization,transformationsoftheinputsto",
    "QKVattention,andFFNs. Forexample,wecanmodifyEq. (8.25)toobtainanFFNwithno\nbiasterms\nFFN(h) = σ(hW )W (8.31)\nh f\nChowdheryetal.[2022]reportthatremovingbiastermshelpsimprovethetrainingstability\nofLLMs. ThismethodhasbeenusedinseveralrecentLLMs,suchasLLaMAandGemma.\n4. Other Issues\nManyLLMsalsoinvolvemodificationstotheirpositionalembeddingmodels. Forexample,\nonecanreplacesinusoidalpositionalencodingswithrotarypositionembeddingssothatthe\nlearnedLLMscanhandlelongsequencesbetter. ThesemodelswillbediscussedinSection\n8.3.\nNotethatwhilemodelmodificationsarecommonintrainingLLMs,thestabilityoftraining\ncanbeimprovedinmanydifferentways. Forexample,increasingthebatchsizeasthetraining\nproceedshasbeenfoundtobeusefulforsomeLLMs. Ingeneral,achievingstableandefficient\nlarge-scale LLM training requires carefully designed setups, including learning schedules,\noptimizerchoices,trainingparallelism,mixedprecisiontraining,andsoon. Someofthese",
    "issues are highly engineered, and therefore, we typically need a number of training runs to\nobtainsatisfactoryLLMs.",
    "430 Chapter 8. Generative Models\n8.2.3 Distributed Training\nTrainingLLMsrequiressignificantamountsofcomputationalresources. Acommonapproach\ntoimprovingtrainingefficiencyistouselarge-scaledistributedsystems. Fortunately,alongside\nthe rise of neural networks in AI, deep learning-oriented software and hardware have been\ndeveloped,makingiteasiertoimplementLLMsandperformcomputations. Forexample,one\ncan now easily fine-tune an LLM using deep learning software frameworks and a machine\nwith multiple GPUs. However, scaling up the training of LLMs is still challenging, and\nrequiressignificanteffortsindevelopinghardwareandsoftwaresystemsforstableandefficient\ndistributedtraining.\nAnimportantconsiderationofdistributedtrainingisparallelism. Thereareseveralforms\nofparallelism: dataparallelism,modelparallelism,tensorparallelism,andpipelineparallelism.\nDespitedifferentwaystodistributecomputationsacrossdevices,theseparallelismmethods",
    "are based on a similar idea: the training problem can be divided into smaller tasks that can\nbeexecutedsimultaneously. TheissueofparallelismintrainingLLMshasbeenextensively\nstudied[Narayananetal.,2021;Fedusetal.,2022b]. Herewesketchthebasicconcepts.\n• DataParallelism. Thismethodisoneofthemostwidelyusedparallelismmethodsfor\ntrainingneuralnetworks. Toillustrate,considerthesimplestcasewherethestandard\ndeltaruleisusedingradientdescent\n∂L (D )\nθ = θ −lr·\nθt mini\n(8.32)\nt+1 t\n∂θ\nt\nwherethenewparametersθ isobtainedbyupdatingthelatestparametersθ witha\nt+1 t\nsmallsteplr inthedirectionofthenegativelossgradient.\n∂L\nθt\n(Dmini)\nisthegradientof\n∂θt\nthelosswithrespecttotheparametersθ ,andiscomputedonaminibatchoftraining\nt\nsampleD . Indataparallelism,wedivideD intoN smallerbatches,denotedby\nmini mini\n{D1,...,DN}. Then,wedistributethesebatchestoN workers,eachwithacorrespond-\ningbatch. Oncethedataisdistributed,theseworkerscanworkatthesametime. The",
    "gradientoftheentireminibatchisobtainedbyaggregatingthegradientscomputedby\ntheworkers,likethis\n∂L (D ) ∂L (D1) ∂L (D2) ∂L (DN)\nθt mini\n=\nθt\n+\nθt\n+···+\nθt\n(8.33)\n∂θ ∂θ ∂θ ∂θ\nt t t t\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nworker1 worker2 workerN\nInidealcaseswheretheworkerscoordinatewellandthecommunicationoverheadis\nsmall,dataparallelismcanachievenearlyanN-foldspeed-upfortraining.\n• ModelParallelism. Althoughdataparallelismissimpleandeffective,itrequireseach\nworkertoruntheentireLLMandperformthecompleteforwardandbackwardprocess.\nAsLLMsgrowlarger,itsometimesbecomesunfeasibletoloadandexecuteanLLM\non a single device. In this case, we can decouple the LLM into smaller components\nandrunthesecomponentsondifferentdevices. Onesimplewaytodothisistogroup\nconsecutivelayersinthelayerstackandassigneachgrouptoaworker. Theworkers",
    "8.2 Training at Scale 431\noperate in the order of the layers in the stack, that is, in the forward pass we process\ntheinputfromlower-leveltoupper-levellayers,andinthebackwardpasswepropagate\nthe error gradients from upper-level to lower-level layers. Consider, for example, a\nTransformerdecoderwithLstackedblocks. Todistributethecomputationload,each\nblock is assigned to a worker. See the following illustration for a single run of the\nforwardandbackwardpassesofthismodel.\nWorkerL B (↑) B (↓)\nL L\n... ... ...\nWorker2 B (↑) B (↓)\n2 2\nWorker1 B (↑) B (↓)\n1 1\nHereB denotesthecomputationofblockl,andthesymbols↑and↓denotetheforward\nl\nandbackwardpasses,respectively. Notethatthisparallelismmethodforcestheworkers\ntoruninsequence,soaworkerhastowaitforthepreviousworkertofinishtheirjob.\nThisresultsinthedevicesbeingidleformostofthetime. Inpracticalsystems,model\nparallelismisgenerallyusedtogetherwithotherparallelismmechanismstomaximize\ntheuseofdevices.",
    "• Tensor Parallelism. Parallelism can also be performed in a single computation step.\nAcommonexampleissplittingalargeparametermatrixintochunks, multiplyingan\ninputtensorwitheachofthesechunksseparately,andthenconcatenatingtheresultsof\nthese multiplications to form the output. For example, consider the multiplication of\ntherepresentationh∈Rd withtheparametermatrixW h ∈Rd×d h inanFFNsub-layer\n(seeEq. (8.25)). WecanslicethematrixW h ∈Rd×d h verticallytoasequenceofM\nsub-matrices\n(cid:104) (cid:105)\nW = W1 W2 ... WM (8.34)\nh h h h\nwhereeachsub-matrixWk hasashapeofd×d h. ThemultiplicationofhwithW can\nh M h\nbeexpressedas\n(cid:104) (cid:105)\nhW = h W1 W2 ... WM\nh h h h\n(cid:104) (cid:105)\n= hW1 hW2 ... hWM (8.35)\nh h h\nWecanperformmatrixmultiplications{hW1,hW2,...,hWM}onM devicessepa-\nh h h\nrately. Asaresult,wedistributealargematrixmultiplicationacrossmultipledevices,\neachofwhichmayhaverelativelysmallmemory. Fromtheperspectiveofthedesignof",
    "modernGPUs,tensorparallelismoverGPUsprovidesatwo-level,tile-basedapproach\nto parallel computing. First, at a higher level, we decompose a matrix multiplication\nintosub-matrixmultiplicationsthatcandirectlyfitintothememoryofGPUs. Then,at",
    "432 Chapter 8. Generative Models\na lower level, we execute these sub-matrix multiplications on GPUs using tile-based\nparallelalgorithmsthatarespecificallyoptimizedforGPUs.\n• Pipeline Parallelism. Above, in model parallelism, we have described a simple ap-\nproach to spreading groups of model components across multiple devices. But this\nmethodisinefficientbecauseonlyonedeviceisactivatedatatimeduringprocessing.\nPipelineparallelismaddressesthisissuebyintroducingoverlapsbetweencomputations\non different devices [Harlap et al., 2018; Huang et al., 2019]. To do this, a batch of\nsamplesisdividedintoanumberofmicro-batches, andthenthesemicro-batchesare\nprocessedbyeachworkerasusual. Onceamicro-batchisprocessedbyaworkerand\npassedtothenextone,thefollowingmicro-batchimmediatelyoccupiesthesameworker.\nInotherwords,wecreateapipelineinwhichdifferentcomputationstepscanoverlapif\nmultiplejobsaregiventothepipeline. Thefollowingshowsanillustrationofpipeline\nparallelismforprocessing3micro-batches.",
    "WorkerL BL,1 BL,2 BL,3 BL,1 BL,2 BL,3\n... ... ...\nWorker2 B2,1 B2,2 B2,3 B2,1 B2,2 B2,3\nWorker1 B1,1 B1,2 B1,3 B1,1 B1,2 B1,3\nHereB representstheprocessingofthek-thmicro-batchbythel-thworker. Ideally\nl,k\nwewouldlike tomaximize thenumber ofmicro-batches, and thusminimize theidle\ntimeoftheworkers. However,inpractice,usingsmallmicro-batchesoftenreducesGPU\nutilizationandincreasestask-switchingcosts. Thismay,inturn,decreasetheoverall\nsystemthroughput.\nTheultimategoalofparallelprocessingistoachievelineargrowthinefficiency,thatis,the\nnumberofsamplesthatcanbeprocessedperunitoftimeincreaseslinearlywiththenumber\nofdevices. However,distributedtrainingiscomplicated,andinfluencedbymanyfactorsin\nadditiontotheparallelismmethodwechoose. Oneproblem,whichisoftenassociatedwith\ndistributedsystems,isthecostofcommunication. Wecanthinkofadistributedsystemasa\ngroupofnetworkednodes. Eachofthesenodescanperformlocalcomputationorpassdata",
    "to other nodes. If there are a large number of such nodes, it will be expensive to distribute\nandcollectdataacrossthem. Sometimes,thetimesavingsbroughtaboutbyparallelismare\noffsetbythecommunicationoverheadofalargenetwork. Anotherproblemwithlarge-scale\ndistributedsystemsisthatthesynchronizationofnodesintroducesadditionalcosts. Asisoften\nthe case, some nodes may take longer to work, causing others to wait for the slowest ones.\nWhilewecanuseasynchronoustrainingtohandleheterogeneityincomputationalresources,\nthismayleadtostalegradientsandnon-guaranteedconvergence. Moreover,asmorenodes\nareaddedtothenetwork,thereismorechancetohavecrashednodesduringtraining. Inthis\ncase,weneedtoensurethatthewholesystemisfaulttolerant. Inmanypracticalsettings,to",
    "8.2 Training at Scale 433\nincreasescalability,oneneedstotakeintoaccountadditionalissues,includingarchitecture\ndesign,datatransferandcomputationoverlap,loadbalancing,memorybandwidthandsoon.\nTraining LLMs is so computationally expensive that, even though distributed training\nis already in use, researchers and engineers often still employ various model compression\nandspeed-upmethodstoimprovetrainingefficiency[Weng,2021]. Oneexampleismixed\nprecisiontraining,inwhichlowprecisiondata(suchasFP16andFP8data)isusedforgradient\ncomputationoneachindividualnode,andsingleordoubleprecisiondata(suchasFP32/FP64\ndata) is used for updating the model [Micikevicius et al., 2018]. A key operation in this\napproachisgradientaccumulationwheregradientsneedtobeaccumulatedandsynchronized\nacrossnodes. However,duetothenon-associativityoffloating-pointaddition,thiscanleadto\nslightnumericaldifferencesinaccumulatedgradientsondifferentnodes,whichmayaffect",
    "modelconvergenceandfinalperformance. Thisproblemismoreobviousiftherearealarge\nnumberofnodesinvolvedindistributedtraining,especiallygiventhatlow-precisionnumerical\ncomputationsmayencounteroverflowandunderflowissues,aswellasinconsistenciesacross\ndifferent hardware devices. Therefore, the design of distributed systems needs to consider\nthesenumericalcomputationissuestoensuresatisfactoryresultsandconvergence.\n8.2.4 Scaling Laws\nThesuccessofLLMsrevealsthattraininglargerlanguagemodelsusingmoreresourcescan\nlead to improved model performance. Researchers have explained this as scaling laws of\nLLMs. Morespecifically,scalinglawsdescribetherelationshipsbetweentheperformanceof\nLLMsandtheattributesofLLMtraining,suchasthemodelsize,theamountofcomputation\nusedfortraining,andtheamountoftrainingdata. Forexample,Hestnessetal.[2017]show\nthattheperformanceofdeepneuralnetworksisapower-law-likefunctionofthetrainingdata\nsize. Inthebeginning,whentheamountoftrainingdataisnotlarge,theperformanceofthe",
    "modelimprovesslowly. Afterward,whenmoretrainingdataisused,themodelentersaphase\nofrapidperformanceimprovement,andtheperformancecurveresemblesapower-lawcurve.\nUltimately,theimprovementinperformancebecomesslowagain,andmoredatadoesnotlead\ntosignificantgains. Figure8.3showsanexampleofsuchcurves.\nInNLP,atraditionalviewholdsthattheperformancegainswilldisappearatacertainpoint\nasthetrainingisscaledup. However,recentresultsshowthat,ifweconsidertheproblemona\nlargerscale,scalinguptrainingisstillaveryeffectivemethodforobtainingstrongerLLMs.\nFor example, both closed-source and open-source LLMs can benefit from more data, even\nthoughtrillionsoftokenshavealreadybeenusedfortraining.\nWiththeincreaseinthescaleofmodeltraining,LLMsexhibitnewcapabilities,knownas\ntheemergentabilitiesofLLMs. Forexample,Weietal.[2022b]studiedthescalingproperties\nofLLMsacrossdifferentmodelsizesandamountsofcomputationalresources. Theirwork\nshowsthatsomeabilitiesemergewhenwescalethemodelsizetocertainlevel. Theappearance",
    "ofemergentabilitieshasdemonstratedtheroleofscaledtraininginenhancingtheperformance\nof LLMs, and it has also, to some extent, motivated researchers to continuously attempt to\ntrainlargermodels. AslargerandstrongerLMscontinuetoappear,ourunderstandingofthe\nscaling laws continues to mature. This helps researchers predict the performance of LLMs",
    "434 Chapter 8. Generative Models\nSlowReduction Power-lawReduction Convergence\nPhase Phase Phase\n(IrreducibleError)\nTrainingDatasetSize(Log-scale)\n)elacs-goL(srorrEtseTforebmuN\nFigure8.3: Ascalinglawoftesterroragainstavariableofinterest(e.g.,trainingdatasetsize)\n[Hestnessetal.,2017]. Thecurveofthescalinglawcanbedividedintothreephases. Atthe\nbeginning, the number of test errors decreases slowly when more training data is used, but\nthis only lasts for a short period. In the second phase, the number of test errors decreases\ndrastically, andthecurvebecomesapowerlawcurve. Afterthat, theerrorreductionslows\ndownagaininthethirdphase. Notethatthereareirreducibleerrorsthatcannotbeeliminated,\nregardlessoftheamountoftrainingdata.\nduringtrainingandestimatetheminimalcomputationalresourcesrequiredtoachieveagiven\nlevelofperformance.\nTo understand how model performance scales with various factors considered during\ntraining,itiscommontoexpressthemodelperformanceasafunctionofthesefactors. For",
    "example,inthesimplestcase,wecanexpressthelossorerrorofanLLMasafunctionofa\nsinglevariableofinterest. However,therearenouniversalscalinglawsthatcandescribethis\nrelationship. Instead,differentfunctionsareproposedtofitthelearningcurvesofLLMs.\nLetxbethevariableofinterest(suchasthenumberofmodelparameters)andL(x)bethe\nlossofthemodelgivenx(suchasthecross-entropylossontestdata). Thesimplestformof\nL(x)isapowerlaw\nL(x) = axb (8.36)\nwhereaandbareparametersthatareestimatedempirically. Despiteitssimplicity,thisfunction\nhassuccessfullyinterpretedthescalingabilityoflanguagemodelsandmachinetranslation\nsystems in terms of model size (denoted by N) and training dataset size (denoted by D)\n[Gordon et al., 2021; Hestness et al., 2017]. For example, Kaplan et al. [2020] found that\ntheperformanceoftheirlanguagemodelimprovesasapowerlawofeitherN orD afteran\ninitialtransientperiod,andexpressedtheserelationshipsusingL(N)= (cid:0) N (cid:1)−0.076 and\n8.8×1013\nL(D)= (cid:0) D (cid:1)−0.095 (seeFigure8.4).\n5.4×1013",
    "8.2 Training at Scale 435\n5.6\n4.8\n4.0\n3.2\n2.4\n105 107 109\nNumberofParameters\nssoLtseT\nL(N)=( N )−0.076 4.2\n8.8·1013\n3.9\n3.6\n3.3\n3\n2.7\n108 109\nDatasetSize\nssoLtseT\nL(D)=( D )−0.095\n5.4·1013\nFigure 8.4: Test loss against model size (N) and training dataset size (D) (data points are\nplotted for illustrative purposes). We plot test loss as a function of N, which is defined as\nL(N)= (cid:0) N (cid:1)−0.076 ,andafunctionofD,whichisdefinedasL(D)= (cid:0) D (cid:1)−0.095\n8.8×1013 5.4×1013\n[Kaplanetal.,2020].\nAnimprovementtothisscalinglawistoaddanirreducibleerrortermtothepowerlaw.\nTheformofL(x)isthengivenby\nL(x) = axb+ϵ (8.37)\n∞\nwhereϵ istheirreducibleerrorthataccountsfortheerrorduetounknownvariables,whichis\n∞\npresentevenasx→∞. Eq. (8.37)isoneofthemostwidelyusedformsfordesigningscaling\nlawsofLLMs. Forexample,Rosenfeldetal.[2020]developedascalinglawthatinvolvesboth\nmodelscalinganddatasetscaling,likethis\nL(N,D) = aNb+cDd+ϵ (8.38)\n∞",
    "AnexampleofsuchformulationistheChinchillascalinglaw. Itstatesthatthetestlossper\ntokenisthesumoftheinverseproportionfunctionsofN andD,withanadditionalirreducible\nerrorterm. Hoffmannetal.[2022]expressthisscalinglawas\n406.4 410.7\nL(N,D) = + + 1.69 (8.39)\nN0.34 D0.28 (cid:124)(cid:123)(cid:122)(cid:125)\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) irreducibleerror\nmodelscaling datasetscaling\nAllthescalinglawsmentionedabovearebasedonmonotonicfunctions. Sotheycannot\ncoverfunctionswithinflectionpoints,suchasdoubledescentcurves. Inresponse,researchers\nhave explored more sophisticated functions to fit the learning curves. Examples of such\nfunctionscanbefoundinAlabdulmohsinetal.[2022]andCaballeroetal.[2023]’swork.\nThesignificanceofscalinglawsliesinprovidingdirectionalguidanceforLLMresearch:\nifwearestillintheregionofthepowerlawcurve,usingmoreresourcestotrainlargermodels",
    "436 Chapter 8. Generative Models\nisaverypromisingdirection. Whilethisresult“forces”bigresearchgroupsandcompaniesto\ninvestmoreincomputationalresourcestotrainlargermodels,whichisveryexpensive,scaling\nlawscontinuouslypushtheboundariesofAIfurtheraway. Ontheotherhand,understanding\nscaling laws helps researchers make decisions in training LLMs. For example, given the\ncomputationalresourcesathand,theperformanceofLLMsmaybepredicted.\nOnelastnoteonscalinglawsinthissection. ForLLMs,alowertestlossdoesnotalways\nimply better performance on all downstream tasks. To adapt LLMs, there are several steps\nsuchasfine-tuningandpromptingthatmayinfluencethefinalresult. Therefore,thescaling\nlawsfordifferentdownstreamtasksmightbedifferentinpractice.\n8.3 Long Sequence Modeling\nWehavealreadyseenthat,inlarge-scaletraining,largerlanguagemodelscanbedeveloped\nbyusingmoredataandcomputationalresources. However,scalingupcanalsooccurinother",
    "directions. For instance, in many applications, LLMs are adapted to process significantly\nlong sequences. An interesting example is that we pre-train an LLM on extensive texts of\nnormallengthandthenapplyittodealwithverylongtokensequences,farbeyondthelength\nencounteredinpre-training. HereweusePr(y|x)todenotethetextgenerationprobability\nwhere x is the context and y is the generated text. There are broadly three types of long\nsequencemodelingproblems.\n• Textgenerationbasedonlongcontext(i.e.,xisalongsequence). Forexample,we\ngenerateashortsummaryforaverylongtext.\n• Longtextgeneration(i.e.,yisalongsequence). Forexample,wegeneratealongstory\nbasedonafewkeywords.\n• Longtextgenerationbasedonlongcontext(i.e.,bothxandyarelongsequences).\nForexample,wetranslatealongdocumentfromChinesetoEnglish.\nRecently,NLPresearchershavebeenmoreinterestedinapplyingandevaluatingLLMs\non tasks where extremely long input texts are involved. Imagine an LLM, which reads a",
    "C++ source file containing tens of thousands of lines, and outlines the functionality of the\nprogramcorrespondingtothesourcefile. Suchmodels,capableofhandlingextensivetextual\ncontexts,aresometimescalledlong-contextLLMs. Inthissectionwewillrestrictourselves\ntolong-contextLLMs,butthemethodsdiscussedherecanbeapplicabletootherproblems.\nFor Transformers, dealing with long sequences is computationally expensive, as the\ncomputationalcostofself-attentiongrowsquadraticallywiththesequencelength. Thismakes\nitinfeasibletotrainanddeploysuchmodelsforverylonginputs. Twostrandsofresearchhave\ntriedtoadaptTransformerstolong-contextlanguagemodeling.\n• The first explores efficient training methods and model architectures to learn self-\nattentionmodelsfromlong-sequencedata.\n• Theotheradaptspre-trainedLLMstohandlelongsequenceswithmodestornofine-\ntuningefforts.",
    "8.3 Long Sequence Modeling 437\nHere,wewilldiscusstheformerbrieflysinceChapter6extensivelycoversmanymethods\ninthisstrand. Wewillfocusonthelatter,highlightingpopularmethodsinrecentLLMs. We\nwillalsodiscussthestrengthsandlimitationsoftheselong-sequencemodels.\n8.3.1 Optimization from HPC Perspectives\nWebeginourdiscussionbyconsideringimprovementstostandardTransformermodelsfrom\nthe perspectives of high-performance computing. Most of these improvements, though not\nspecificallydesignedforLLMs,havebeenwidelyappliedacrossvariousdeeplearningmodels\n[Kimetal.,2023]. Acommonlyusedapproachistoadoptalow-precisionimplementationof\nTransformers. Forexample,wecanuse8-bitor16-bitfixed-pointdatatypesforarithmetic\noperations,insteadof32-bitor64-bitfloating-pointdatatypes. Usingtheselow-precisiondata\ntypes can increase the efficiency and memory throughput, so that longer sequences can be\nprocessedmoreeasily. AnalternativeapproachistoimproveTransformersbyusinghardware-",
    "aware techniques. For example, on modern GPUs, the efficiency of Transformers can be\nimprovedbyusingIO-awareimplementationsoftheself-attentionfunction[Daoetal.,2022;\nKwonetal.,2023].\nAnotherwaytohandlelongsequencesisthroughsequenceparallelism[Lietal.,2023b;\nKorthikantietal.,2023]. Specifically,considerthegeneralproblemofattendingthequeryq\ni\nat the position i to the keys K and values V. We can divide K by rows and obtain a set of\nsub-matrices{K[1],...,K[nu]},eachcorrespondingtoasegmentofthesequence. Similarly,\nwecanobtainthesub-matricesofV,denotedby{V[1],...,V[nu]}. Then,weassigneachpair\nofK[u] andV[u] toacomputingnode(e.g.,aGPUofaGPUcluster). Theassignednodescan\nruninparallel,therebyparallelizingtheattentionoperation.\nRecallthattheoutputoftheself-attentionmodelcanbewrittenas\nm−1\n(cid:88)\nAtt (q ,K,V) = α v (8.40)\nqkv i i,j j\nj=0\nwhereα istheattentionweightbetweenpositionsiandj. InTransformers,α isobtained\ni,j i,j",
    "bynormalizingtherescaledversionofthedotproductbetweenq andk . Letβ denotethe\ni j i,j\nattentionscorebetweenq andk . Wehave\ni j\nq ·k\ni j\nβ = √ +Mask(i,j) (8.41)\ni,j\nd\nwhereMask(i,j)isthemaskingvariablefor(i,j). Then,wedefinetheattentionweightα\ni,j\ntobe\nα = Softmax(β )\ni,j i,j\nexp(β )\ni,j\n= (8.42)\n(cid:80)\nexp(β )\nj′ i,j′",
    "438 Chapter 8. Generative Models\nOn each computing node, we need to implement these equations. Given the keys and\nvaluesassignedtothisnode,computingthenumeratoroftheright-handsideofEq. (8.42)(i.e.,\nexp(β ))isstraightforward,asalltherequiredinformationisstoredonthenode. However,\ni,j\ncomputingthedenominatoroftheright-handsideofEq. (8.42)involvesasumofexp(β )\ni,j′\noverallj′s,whichrequirestransferringdatatoandfromothernodes. Toillustrate,suppose\nthatv andk areplacedonnodeu. WecanrewriteEq. (8.42)as\nj j\nα\ni,j\nnodeu\n(cid:122) (cid:125)(cid:124) (cid:123)\nexp(β )\ni,j\n= (8.43)\n(cid:88) (cid:88) (cid:88)\nexp(β )+···+ exp(β )+···+ exp(β )\ni,j′ i,j′ i,j′\nk j′∈K[1] k j′∈K[u] k j′∈K[nu]\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nnode1 nodeu nodenu\nwherethenotationk ∈K[u] representsthatk isarowvectorofK[u]. Inastraightforward\nj′ j′\n(cid:80)\nimplementation, we first perform the summations { exp(β )} separately on the",
    "k j′∈K[u] i,j′\ncorresponding nodes. Then, we collect these summation results from different nodes to\ncombinethemintoafinalresult. Thiscorrespondstoacollectiveoperationinthecontextof\nparallelprocessing. Therearemanyefficientimplementationsofsuchoperations,suchasthe\nall-reducealgorithms. Hencethesumofallexp(β )valuescanbecomputedusingoptimized\ni,j\nroutinesincollectivecommunicationtoolkits.\nGiventheattentionweights{α },wethencomputetheattentionresultsusingEq. (8.40).\ni,j\nTheproblemcanbere-expressedas\nAtt (q ,K,V)\nqkv i\n(cid:88) (cid:88) (cid:88)\n= α v +···+ α v +···+ α v (8.44)\ni,j′ j′ i,j′ j′ i,j′ j′\nv j′∈V[1] v j′∈V[u] v j′∈V[nu]\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nnode1 nodeu nodenu\nLikeEq. (8.43),Eq. (8.44)canbeimplementedasasummationprograminparallelpro-\ncessing. First,performtheweightedsummationsofvaluesondifferentnodessimultaneously.\nThen,wecollecttheresultsfromthesenodesviacollectiveoperations.",
    "Notethat, althoughthissectionprimarilyfocusesonlongsequencemodeling, muchof\nthemotivationforsequenceparallelismcomesfromthedistributedtrainingmethodsofdeep\nnetworks,asdiscussedinSection8.2.3. Asaresult,theimplementationofthesemethodscan\nbebasedonthesameparallelprocessinglibrary.\n8.3.2 Efficient Architectures\nOnedifficultyofapplyingTransformerstolongsequencesisthatself-attentionhasaquadratic\ntimecomplexitywithrespecttothesequencelength. Moreover,akey-valuecache(orKV\ncache for short) is maintained during inference, and its size increases as more tokens are\nprocessed. Although the KV cache grows linearly with the sequence length, for extremely",
    "8.3 Long Sequence Modeling 439\nlong input sequences, the memory footprint becomes significant and it is even infeasible\nto deploy LLMs for such tasks. As a result, the model architecture of long-context LLMs\ngenerallymovesawayfromthestandardTransformer,turninginsteadtothedevelopmentof\nmoreefficientvariantsandalternatives.\nOneapproachistousesparseattentioninsteadofstandardself-attention. Thisfamilyof\nmodelsisbasedontheideathatonlyasmallnumberoftokensareconsideredimportantwhen\nattendingtoagiventoken,andsomostoftheattentionweightsbetweentokensarecloseto\nzero. Asaconsequence,wecanprunemostoftheattentionweightsandrepresenttheattention\nmodelinacompressedform. Toillustrate,considertheself-attentionmodel\nAtt (Q,K,V) = α(Q,K)V (8.45)\nqkv\nwheretheattentionweightmatrixα(Q,K)∈Rm×m isobtainedby\nQKT\nα(Q,K) = Softmax( √ +Mask)\nd\n \nα 0 0 ... 0\n0,0\n α α 0 ... 0 \n 1,0 1,1 \n=   α 2,0 α 2,1 α 2,2 ... 0   (8.46)\n  . . . . . . . . . ... . . .  \n \nα α α ... α\nm−1,0 m−1,1 m−1,2 m−1,m−1",
    "(cid:104) (cid:105)\nEachrowvector α ... α 0 ... 0 correspondstoadistributionofattendingthe\ni,0 i,i\ni-th token to every token of the sequence. Since language models predict next tokens only\nbasedontheirleft-context,wenormallywritetheoutputoftheattentionmodelatpositionias\n \nv\n0\n(cid:104) (cid:105) .\nAtt qkv (q i ,K ≤i ,V ≤i ) = α i,0 ... α i,i   . .  \nv\ni\ni\n(cid:88)\n= α v (8.47)\ni,j j\nj=0\n   \nk v\n0 0\n. .\nwhereK ≤i =  . .   andV ≤i =  . .   arethekeysandvaluesuptopositioni.\nk v\ni i\n(cid:104) (cid:105)\nIn the original version of self-attention α ... α is assumed to be dense, that is,\ni,0 i,i\n(cid:104) (cid:105)\nmostofthevaluesarenon-zero. Insparseattention,someoftheentriesof α ... α are\ni,0 i,i\nconsiderednon-zero,andtheremainingentriesaresimplyignoredincomputation. Suppose\nG⊆{0,...,i}isthesetofindicesofthenon-zeroentries. Forlanguagemodels,theoutputof",
    "440 Chapter 8. Generative Models\nthesparseattentionmodelatpositioniisgivenby\n(cid:88)\nAtt (q ,K ,V ) = α′ v (8.48)\nsparse i ≤i ≤i i,j j\nj∈G\nHere{α′ }arenormalizedoverG. Hencetheirvaluesaredifferentfromtheoriginalattention\ni,j\nweights(infactwehaveα′ >α ). ThesparsityofthemodelisdeterminedbyhowlargeG\ni,j i,j\nis. SparseattentionmodelsdifferinthewaywedefineG. OnesimpleapproachistodefineG\nbasedonheuristicallydesignedpatterns. Forexample,awidely-usedpatterninvolveshaving\nGcoverawindowoftokenslocatednearpositioni[Parmaretal.,2018].\nWhilesparseattentionreducesthecomputationthroughtheuseofsparseoperations,such\nmodelsstillhavesignificantlimitationsaswemustkeeptheentireKVcache(i.e.,K and\n≤i\nV ) during inference. If the sequence is very long, storing this cache will become highly\n≤i\nmemory-intensive. Toaddressthis,wecanconsideradifferentformofattentionmodelswhere\ntheKVcacheisnotexplicitlyretained. Linearattentionisonesuchapproach[Katharopoulos",
    "etal.,2020]. Itusesakernelfunctionϕ(·)toprojecteachqueryandkeyontopointsq′ =ϕ(q )\ni i\nandk′ =ϕ(k ),respectively. ByremovingtheSoftmaxfunctionundersuchtransformations10,\ni i\ntheformoftheresultingattentionmodelisgivenby\nAtt (q ,K ,V ) ≈ Att (q′,K′ ,V )\nqkv i ≤i ≤i linear i ≤i ≤i\nq′µ\n= i i (8.49)\nq′ν\ni i\nwhereµ andν arevariablesthatarecomputedintherecurrentforms\ni i\nµ = µ\n+k′T\nv (8.50)\ni i−1 i i\nν = ν\n+k′T\n(8.51)\ni i−1 i\nµ andν canbeseenasrepresentationsofthehistoryuptopositioni. Abenefitofthismodel\ni i\nisthatweneednotkeepallpastqueriesandvalues. Insteadonlythelatestrepresentationsµ\ni\nandν areused. Sothecomputationalcostofeachstepisaconstant,andthemodelcanbe\ni\neasilyextendedtodealwithlongsequences.\nInfact,thissequentialapproachtolongsequencemodelingarisesnaturallywhenweadopt\naviewpointofrecurrentmodels. Suchmodelsreadonetoken(orasmallnumberoftokens)\natatime,updatetherecurrentstateusingtheseinputs,andthendiscardthembeforethenext",
    "tokenarrives. Theoutputateachstepisgeneratedbasedonlyontherecurrentstate,ratherthan\nonallthepreviousstates. Thememoryfootprintisdeterminedbytherecurrentstatewhich\nhas a fixed size. Recurrent models can be used in real-time learning scenarios where data\narrivesinastreamandpredictionscanbemadeatanytimestep. InNLP,applyingrecurrent\n10Inthenewspaceafterthistransformation,theSoftmaxnormalizationcanbetransformedintothesimple\nscalingnormalization.",
    "8.3 Long Sequence Modeling 441\nmodelstolanguagemodelingisoneoftheearliestsuccessfulattemptstolearnrepresentations\nofsequences. AlthoughTransformerhasbeenusedasthefoundationalarchitectureinLLMs,\nrecurrentmodelsarestillpowerfulmodels,especiallyfordevelopingefficientLLMs. More\nrecently,recurrentmodelshavestartedtheirresurgenceinlanguagemodelingandhavebeen\nreconsideredasapromisingalternativetoTransformers[GuandDao,2023].\nFigure 8.5 shows a comparison of the models discussed in this subsection. Since these\nmodels,alongwithothersnotmentionedhere,havebeenintensivelydiscussedinChapter6\nandinrelatedsurveys[Tayetal.,2020b],adetaileddiscussionofthemisprecludedhere.\n8.3.3 Cache and Memory\nLLMsbasedonthestandardTransformerarchitectureareglobalmodels. Theinferencefor\nthesemodelsinvolvesstoringtheentireleft-contextinordertomakepredictionsforfuture\ntokens. This requires a KV cache where the representations (i.e., keys and values) of all",
    "previously-generatedtokensarekept,andthecostofcachinggrowsastheinferenceproceeds.\nAbove,wehavediscussedmethodsforoptimizingthiscacheviaefficientattentionapproaches,\nsuchassparseattentionandlinearattention. Anotheridea,whichmayhaveoverlapwiththe\npreviousdiscussion,istoexplicitlyencodethecontextviaanadditionalmemorymodel.\n1. Fixed-size KV Cache\nA straightforward approach is to represent the keys and values using a fixed-size memory\nmodel. SupposewehaveamemoryMemwhichretainsthecontextualinformation. Wecan\nwritetheattentionoperationatpositioniinageneralform\nAtt(q ,Mem) = Att (q ,K ,V ) (8.52)\ni qkv i ≤i ≤i\nInthismodel,MemissimplytheKVcache,i.e.,Mem=(K ,V ). Thusthesizeof\n≤i ≤i\nMemisdeterminedbyi. IfwedefineMemasafixed-sizevariable,thenthecostofperforming\nAtt(q ,Mem)willbefixed. ThereareseveralalternativewaystodesignMem.\ni\n• Oneofthesimplestmethodsistoconsiderafixed-sizewindowofpreviouskeysand\nvalues. Memisthereforegivenby\nMem = (K ,V ) (8.53)\n[i−nc+1,i] [i−nc+1,i]",
    "where n denotes the size of the window. The notation K and V\nc [i−nc+1,i] [i−nc+1,i]\ndenotethekeysandvaluesoverpositionsfromi−n +1toi.11 Thismodelcanbeseen\nc\nasatypeoflocalattentionmodel.\n• ItisalsopossibletodefineMemasapairofsummaryvectors,whichleadstoamore\ncompressedrepresentationofthehistory. Asimplewaytosummarizethepreviouskeys\nk  v \ni−nc+1 i−nc+1\n11More formally, we write K = . .  and V = . . . Sometimes we\n[i−nc+1,i]  .  [i−nc+1,i]  . \nk v\ni i\ndenoteK\n[i−nc+1,i]\nby{k\ni−nc+1\n,...,k\ni\n}andV\n[i−nc+1,i]\nby{v\ni−nc+1\n,...,v\ni\n}fornotationsimplicity.",
    "442 Chapter 8. Generative Models\nAtt (q ,K ,V )\nqkv i ≤i ≤i\nk 0 k 1 ··· k i−2 k i−1 k i q i\nv 0 v 1 ··· v i−2 v i−1 v i\n(a)StandardSelf-attention\nAtt (q ,{k ,k },{v ,v })\nqkv i 1 i 1 i\nk 0 k 1 ··· k i−2 k i−1 k i q i\nv 0 v 1 ··· v i−2 v i−1 v i\n(b)SparseAttention\nµ i =µ i−1 +k′T i v i ⇒ µ i Att linear (q i ,K ≤i ,V ≤i )= q q ′ i ′\ni\nµ νi i\nν i =ν i−1 +k′T i ⇒ ν i\nk 0 k 1 ··· k i−2 k i−1 k i q i\nv 0 v 1 ··· v i−2 v i−1 v i\n(c)LinearAttention\nh =f(h ,input )\ni i−1 i\nh 0 h 1 ··· h i−3 h i−2 h i−1 h i\ninput\ni\n(d)RecurrentModels\nFigure8.5: Illustrationsofself-attention,sparseattention,linearattentionandrecurrentmodels.\nBlueboxes=cachedstatesforproducingtheoutputatpositioni. f(·)=arecurrentcell.\nandvaluesistousethemovingaverageofthem. Forexample,Memcanbedefinedas\ntheunweightedmovingaverageofthepreviousn keysandvalues\nc\nMem =\n(cid:16)\n(cid:80)i\nj=i−nc+1 k j\n,\n(cid:80)i\nj=i−nc+1 v j(cid:17)\n(8.54)\nn n\nc c",
    "8.3 Long Sequence Modeling 443\nAlternatively,wecanuseaweightedversionofmovingaverage\nMem =\n(cid:16)\n(cid:80)i\nj=i−nc+1 β j−i+nc k j\n,\n(cid:80)i\nj=i−nc+1 β j−i+nc v j(cid:17)\n(8.55)\n(cid:80)nc\nβ\n(cid:80)nc\nβ\nj=1 j j=1 j\nHere{β ,...,β }arethecoefficients,whichcanbeeitherlearnedasmodelparameters\n1 nc\nordeterminedviaheuristics. Forexample,theycanbesettoincreasingcoefficients(i.e.,\nβ <β <...<β <β )inordertogivelargerweighttopositionsthatarecloser\n1 2 nc−1 nc\ntoi. Wecanextendthemovingaveragetoincludeallthepositionsuptoi. Thisleadsto\nthecumulativeaverageofthekeysandvalues,givenintheform\n(cid:16)\n(cid:80)i\nj=0 k j\n(cid:80)i\nj=0 v j(cid:17)\nMem = , (8.56)\ni+1 i+1\nIngeneral,thecumulativeaveragecanbewrittenusingarecursiveformula\n(k ,v )+i·Mem\ni i i−1\nMem = (8.57)\ni\ni+1\nwhereMem andMem denotethecumulativeaveragesofthecurrentandprevious\ni i−1\npositions,respectively. Anadvantageofthismodelisthatweonlyneedtostoreasingle\nkey-valuepairduringinference,ratherthanstoringallthekey-valuepairs. Notethatthe",
    "abovememorymodelsarerelatedtorecurrentmodels,andmoreadvancedtechniques\nhave been used to develop alternatives to self-attention mechanisms in Transformers\n[Maetal.,2023].\n• ThememoryMemcanalsobeaneuralnetwork. Ateachstep,ittakesboththeprevious\noutputofthememoryandthecurrentstatesofthemodelasinput,andproducesthenew\noutputofthememory. Thisneuralnetworkcanbeformulatedasthefunction\nMem = Update(S ,Mem ) (8.58)\nkv pre\nHereMemandMem representtheoutputsofthememoryatthecurrentstepandthe\npre\npreviousstep,respectively. S isasetofkey-valuepairs,representingtherecentstates\nkv\nof the model. This formulation is general and allows us to develop various memory\nmodelsbyselectingdifferentUpdate(·)andS configurations. Forexample,ifS\nkv kv\nonlycontainsthelatestkey-valuepair(k ,v )andUpdate(·)isdefinedasarecurrent\ni i\ncell,thenEq. (8.58)canbeexpressedasanRNN-likemodel\nMem = f((k ,v ),Mem ) (8.59)\ni i pre\nwheref(·)isarecurrentcell. Recurrencecanalsobeappliedtosegment-levelmodeling",
    "forefficiencyconsideration. Asimpleapproachisthatwecandividethesequenceinto\nsegments, and treat S as a segment. Applying recurrent models to Update(·) will\nkv\nresultinmemorymodelsthatoperateonsegments. Aspecialexampleisthatwedefine",
    "444 Chapter 8. Generative Models\nUpdate(·)asanFIFOfunctionthataddsS intothememoryandremovestheoldest\nkv\nkey-valuesegmentfromthememory,givenby\nMem = FIFO(S ,Mem ) (8.60)\nkv pre\nConsideramemorywhichincludestwosegments,oneforcurrentsegment,andonefor\ntheprevioussegment. Intheattentionoperation,eachpositioncanaccessthehistory\nkey-value pairs in two closest consecutive segments. This essentially defines a local\nmemory,butitanditsvariantshavebeenwidelyusedsegment-levelrecurrentmodels\n[Daietal.,2019;Hutchinsetal.,2022;Bulatovetal.,2022].\n• Theabovememorymodelscanbeextendedtoinvolvemultiplememories. Anexample\nofthisapproachiscompressiveTransformer[Raeetal.,2019b]. Itemploystwodistinct\nfixed-sizememories: oneformodelinglocalcontext(denotedbyMem),andtheother\nformodelingandcompressinglong-termhistory(denotedbyCMem). TheKVcache\nin this model is the combination of Mem and CMem. The attention function can be\nwrittenas\nAtt (q ,Mem,CMem) = Att (q ,[Mem,CMem]) (8.61)\ncom i qkv i",
    "where [Mem,CMem] is a combined memory of Mem and CMem. As with other\nsegment-levelmodels,thecompressiveTransformermodeloperatesonsegmentsofthe\nsequence. Each segment is a sequence of n consecutive tokens, and we denote Sk\ns kv\nas the key-value pairs corresponding to the tokens of the k-th segment. When a new\nsegmentarrives,MemisupdatedinanFIFOfashion: weappendthen key-valuepairs\nc\ninSk toMem,andthenpopthen oldestkey-valuepairsfromMem,whichisgiven\nkv s\nby\nMem = FIFO(Sk ,Mem ) (8.62)\nkv pre\nThepoppedkey-valuepairsarethenusedtoupdatethecompressivememoryCMem.\nThese n key-value pairs are compressed into ns key-value pairs via a compression\ns c\nnetwork. CMemisanFIFOwhichappendsthecompressed ns key-valuepairstothe\nc\ntailofthequeue,anddropsthefirst ns key-valuepairsofthequeue. Itisgivenby\nc\nCMem = FIFO(Ck ,CMem ) (8.63)\nkv pre\nwhereCk representsthesetofcompressedkey-valuepairs. Implicitinthecompressive\nkv\nTransformermodelisthatlocalcontextshouldberepresentedexplicitlywithminimal",
    "informationloss,whilelong-rangecontextcanbemorecompressed.\n• Wehavealreadyseenthatbothglobalandlocalcontextsareusefulandcanbemodeled\nusing attention models. This view motivates the extension to attention models for\ncombiningbothlocalandlong-termmemories[Ainslieetal.,2020;Zaheeretal.,2020;\nGuptaandBerant,2020]. Asimplebutwidely-usedapproachistoinvolvethefirstfew",
    "8.3 Long Sequence Modeling 445\ntokensofthesequenceinattention,servingasglobaltokens. Thisapproachisusually\nappliedalongwithothersparseattentionmodels. Anadvantageofincorporatingglobal\ntokens of the sequence is that it helps smooth the output distribution of the Softmax\nfunctionusedinattentionweightcomputation,andthusstabilizesmodelperformance\nwhenthecontextsizeisverylarge[Xiaoetal.,2024]. Onedrawback,however,isthat\nusingafixed-sizeglobalmemorymayresultininformationloss. Whendealingwith\nlongsequences,weneedtoenlargetheKVcacheforsufficientrepresentationsofthe\ncontext,butthisinturnincreasesthecomputationalcost.\nFigure 8.6 shows illustrations of the above approaches. Note that, while we focus on\noptimization of the KV cache here, this issue is closely related to those discussed in the\nprevioussection. Allofthemethodswehavementionedsofarcanbroadlybecategorizedas\nefficientattentionapproaches,whicharewidelyusedinvariousTransformervariants.\n2. Memory-based Models",
    "ThemodelingofmemoriesdiscussedabovewasbasedonupdatestotheKVcache,andthe\nresulting models are typically referred to as internal memories. We now consider another\nfamilyofmodels,calledexternalmemories,whichoperateasindependentmodelstoaccess\nlarge-scale contexts for LLMs. Many such models are based on memory-based methods\nwhich have been extensively discussed in machine learning [Bishop, 2006]. A common\nexampleisnearestneighboralgorithms: westorecontextrepresentationsinadatastore,andtry\ntofindthemostsimilarstoredrepresentationstomatchagivenquery. Theretrievedcontext\nrepresentationsarethenusedtoimproveattentionforthisquery.\nHere, we consider the k-nearest neighbors (k-NN) method which is one of the most\npopular memory-based methods. Since our focus is language modeling in this section, we\ndefineasampleinthedatastoreasakey-valuepaircorrespondingtosomecontextstate. Note\nthat “context” is a broad concept here, not just a sequence prefix in text generation. One",
    "might,forexample,viewtheentiredatasetasthecontextforpredictingtokens. Thisallows\nustoretrievetheclosestcontextsituationinasetofsequences,ratherthanagivensequence\nprefix. Althoughwewillrestrictourselvestocontextmodelingforasinglesequence,inthis\nsubsection,wediscussarelativelymoregeneralcase.\nSupposewehaveasetofkeys{k }withcorrespondingvalues{v },andsupposewestore\nj j\nthesekey-valuepairsinavectordatabase12. Foreachqueryq ,wefinditsknearestneighbours\ni\nbygrowingtheradiusofthespherecenteredasq untilitcontainsk datapointsin{k }. This\ni j\nresultsinasetofkkeysalongwiththeircorrespondingvalues,denotedbyMem . Asbefore,\nknn\nwe denote Mem as the local memory for the query, such as the KV cache of neighboring\ntokens. Our goal is to attend query q to both the local memory Mem and the long-term\ni\nmemoryMem . Thereare,ofcourse,severalwaystoincorporateMemandMem into\nknn knn\ntheattentionmodel. Forexample,wemightsimplycombinethemtoformasingleKVcache",
    "[Mem,Mem ],andattendq to[Mem,Mem ]viastandardQKVattention. Orwemight\nknn i knn\n12Avectordatabase,orvectorstore,isadatabasethatprovideshighlyoptimizedretrievalinterfacesforfinding\nstoredvectorsthatcloselymatchaqueryvector.",
    "446 Chapter 8. Generative Models\nMemory\nSize=4×2\n··· Keys\n··· Values\ni−7 i−6 i−5 i−4 i−3 i−2 i−1 i\n(a)Window-basedCache\nki−3+ki−2+ki−1+ki\n⇒\nMemory\n4\nSize=1×2\nvi−3+vi−2+vi−1+vi\n⇒\n4\n··· Keys\n··· Values\ni−7 i−6 i−5 i−4 i−3 i−2 i−1 i\n(b)MovingAverage-basedCache\nMemory\nMem=Update(S ,Mem ) ⇒\nkv pre\nSize=1×2\n··· Keys\n··· Values\ni−7 i−6 i−5 i−4 i−3 i−2 i−1 i\n(c)RecurrentNetworkasCache\nCompressed\nMemory Memory\nSize=2×2 Size=4×2\n··· Keys\n··· Values\ni−7 i−6 i−5 i−4 i−3 i−2 i−1 i\n(d)HybridCache(CompressedMemory+LocalMemory)\nFigure8.6: Illustrationsoffixed-sizeKVcachesinLLMs. Blueboxesrepresentthekeysand\nvaluesgeneratedduringLLMinference,greenboxesrepresentthekeysandvaluesstoredor\nencoded in the primary memory, and orange boxes represent the keys and values stored or\nencodedinthecompressedmemory.",
    "8.3 Long Sequence Modeling 447\nuse Mem and Mem in separate attention steps. An example of such approaches is the\nknn\nmodeldevelopedbyWuetal.[2021]. Itlinearlycombinesthetwotypesofattention,givenby\nAtt(q ,Mem,Mem ) = g⊙Att +(1−g)⊙Att (8.64)\ni knn local knn\nAtt = Att(q ,Mem) (8.65)\nlocal i\nAtt = Att(q ,Mem ) (8.66)\nknn i knn\nHereg∈Rd isthecoefficientvector,whichcanbetheoutputofalearnedgate.\nGiventhek-NN-basedmemorymodeldescribedabove,theremainingtaskistodetermine\nwhichkey-valuepairsareretainedinthedatastore. Forstandardlanguagemodelingtasks,we\nconsiderthepreviouslyseentokensinasequenceasthecontext,sowecanaddthekeysand\nvaluesofallthesetokensintothedatastore. Inthiscase,theresultingk-NN-basedattention\nmodelisessentiallyequivalenttoasparseattentionmodel[Guptaetal.,2021].\nAlternatively,wecanextendthecontextfromonesequencetoacollectionofsequences.\nForexample,wemightcollectallkey-valuepairsacrossthesequencesinatrainingdataset",
    "andaddthemtothedatastoretomodelalargercontext. Thus,LLMscanpredicttokensbased\nonageneralizedcontext. Aproblemwiththisapproachisthatthecomputationalcostwould\nbelargeifmanysequencesareinvolved. Sincethesesequencesarepartofourtrainingdata,\nwecanbuildandoptimizeanindexforthevectorsinthedatastorebeforerunningtheLLMs.\nAsaresult,theretrievalofsimilarvectorscanbeveryefficient,asinmostvectordatabases.\nInfact,alltheabove-mentionedmethodscanbeviewedasinstancesofaretrieval-based\napproach. Insteadofusingretrievalresultstoimproveattention,wecanapplythisapproach\ninotherwaysaswell. Oneapplicationofk-NN-basedsearchisk-NNlanguagemodeling\n(or k-NN LM) [Khandelwal et al., 2020]. The idea is that, although it is attempting to\nextendthecontextusedinself-attentionbyincorporatingnearestneighborsinrepresentation\nlearning, in practice, similar hidden states in Transformers are often highly predictive of\nsimilartokensinsubsequentpositions. Ink-NNLM,eachiteminthedatastoreisakey-value",
    "tuple (z,w), where z represents a hidden state of the LLM at a position, and w represents\nthe corresponding prediction. A typical way to create the datastore is to collect the output\nvectoroftheTransformerlayerstackandthecorrespondingnexttokenforeachpositionof\neach sequence in a training dataset. During inference, we have a representation h given a\ni\nprefix. Giventhisrepresentation,wefirstsearchthedatastoreforkclosestmatchingdataitems\n{(z ,w ),...,(z ,w )}. Here{w ,...,w }arethoughtofasreferencetokensforprediction,\n1 1 k k 1 k\nandthuscanbeusedtoguidethetokenpredictionbasedonh . Onecommonwaytomakeuse\ni\nofreferencetokensistodefineadistributionoverthevocabularyV,\n(cid:104) (cid:105)\nPr (·|h ) = Softmax( −d ··· −d ) (8.67)\nknn i 0 |V|\nwhered equalsthedistancebetweenh andz ifw equalsthev-thentryofV,andequals\nv i j j\n0 otherwise. We use a linear function with a coefficient λ that interpolates between the",
    "448 Chapter 8. Generative Models\nretrieval-baseddistributionPr (·|h )andtheLLMoutputdistributionPr (·|h )\nknn i lm i\nPr(·|h ) = λ·Pr (·|h )+(1−λ)·Pr (·|h ) (8.68)\ni knn i lm i\nThen,asusual,wecanchoosethenexttokeny bymaximizingtheprobabilityPr(y|h ).\ni\nAswithinformationretrieval(IR)systems,thedatastorecanalsomanagetextsandprovide\naccesstorelevanttextsforaquery. Forexample,wecanstoreacollectionoftextdocuments\nin a search engine with full-text indexing, and then search it for documents that match a\ngiventext-basedquery. ApplyingIRtechniquestoLLMsleadstoageneralframeworkcalled\nretrieval-augmented generation (RAG). The RAG framework works as follows. We use\nthecontextxasthequeryandfindthek mostrelevantdocumentpieces{c ,...,c }fromthe\n1 k\ndatastoreviaefficientIRtechniques13. Thesesearchresultsarecombinedwiththeoriginal\ncontextviaapromptingtemplateg(·)14,resultinginanaugmentedinputfortheLLM\nx′ = g(c ,...,c ,x) (8.69)\n1 k",
    "Then,weusex′ asthecontextandpredictthefollowingtextusingthemodelPr(y|x′). One\nadvantageofRAGisthatweneednotmodifythearchitectureofLLMs,butinsteadaugment\ntheinputtoLLMsviaanadditionalIRsystem. Figure8.7showsacomparisonoftheuseof\ndifferentexternalmemoriesinLLMs.\n3. Memory Capacity\nAmemorymodelinLLMs,intheformofasimplekey-valuecacheoradatastore,canbroadly\nbeseenasanencoderofcontextualinformation. Ideally,beforewesaythatamemorymodel\nisrepresentativeoftheentirecontextintokenprediction,weneedtomakesurethatthemodel\ncanaccuratelyrepresentanypartofthecontext. ThestandardKVcacheisonesuchmodel\nthatcompletelystoresallpasthistory. Inthiscase,themodelissaidtohaveadequatecapacity\nformemorizingthecontext. Inmanypracticalapplications,however,completememorization\nisnotrequired. Instead,thegoalistoenableLLMstoaccessimportantcontextualinformation.\nAs a result, efficient and compressed memory models are developed, as described in this",
    "section. Notethat,thelongerthesequence,themoredifficultitbecomesforalow-capacity\nmemorymodeltocaptureimportantcontextualinformation. Itisthereforecommonpracticeto\nsimplyincreasethemodelcapacitywhenprocessinglongcontexts.\nWhilehigh-capacitymodelsaregenerallyfavorable,theyaredifficulttotrainanddeploy.\nAchallengingscenarioisthatthetokensarriveinastreamandthecontextcontinuouslygrows.\n13Inpracticalapplications,queriesaretypicallygeneratedusingaquerygenerationsystem,whichmayexpandit\nwithvariationsoftokensandqueryintent.\n14Forexample,thetemplatecouldbe:\nmessage={*c 1*}...{*c\nk\n*}\ninput:{*x*}\noutput:",
    "8.3 Long Sequence Modeling 449\ng⊙Att(q ,Mem)+(1−g)⊙Att(q ,Mem )\ni i knn\nAtt(qi,Mem knn ) Att(qi,Mem)\n···\nq\ni\n···\nkNearest KVCache\nNeighbors\nKeys/valuesinLLM\nDatastore\nSearch Keys/valuesinDatastore\n(a)k-NNSearchAugmentedAttention\nOutputDistribution\nDistributionPr(·)\nDistributionPr (·)\nknn ···\nAtt(qi,Mem) Att(q i ,Mem)\n···\nq\ni\n···\nkNearest KVCache\nNeighbors Keys/valuesinLLM\nKeysinDatastore\nDatastore\nSearch PredictedTokens\n(b)k-NNLanguageModeling\nLLM\nc 1 = Deepnetworkis... Message: deepnetwork...machinelearning...\nc 2 = Machinelearningis... Whatisdeeplearning?\n···\nkNearest\nNeighbors\nSearch InputContext:\nDatastore\nx= Whatisdeeplearning?\n(c)Retrieval-augmentedGeneration\nFigure8.7: Illustrationsofexternalmemories(ordatastores)forlanguagemodeling.",
    "450 Chapter 8. Generative Models\nDevelopingLLMsforsuchtasksisdifficultasweneedtotrainTransformersonextremelylong\nsequences. Apossiblewaytoaddressthisdifficultyistousenon-parametricmethods,such\nasretrieval-basedmethods. Forexample,asdiscussedabove,wecanuseavectordatabase\ntostorepreviouslygeneratedkey-valuepairs,andthusrepresentthecontextbythisexternal\nmemorymodel. Althoughthisapproachside-stepsthechallengeofrepresentinglongcontextin\nTransformers,buildingandupdatingexternalmemorymodelsarecomputationallyexpensive.\nThesemodelsaremoreoftenusedinproblemswherethecontextisgiveninadvanceandfixed\nduringinference,andhenceunsuitableforstreamingcontextmodeling.\nIn cases where the size of the context continuously grows, applying fixed-size memory\nmodelsisacommonlyusedapproach. Forexample,inrecurrentmodels,asequenceofarbitrary\nlengthcanbesummarizedintoasetofhiddenstatesbywhichwehaveafixedcomputational\ncost per step. While recurrent models were initially found to be not very good at handling",
    "long-distancedependenciesinsequencemodelinginearlyapplicationsofdeeplearningtoNLP,\nrecentadvancementshaveshownthattheirvariantsarenoweffectiveinmodelingextremely\nlongsequences. [Bulatovetal.,2022;Hutchinsetal.,2022;Munkhdalaietal.,2024;Maetal.,\n2024].\nThere is no general definition of memory capacity in LLMs. A simple approach might\nconsiderhowmuchstorageisusedtoretaincontextualinformation. Forexample,memory\ncapacitycouldbedefinedbythesizeoftheKVcacheinTransformersorthevectordatabase\nusedinretrieval-basedmethods. Arelatedconceptismodelcomplexity. Inmachinelearning,\nthere are several ways to define the model complexity of a model. One of the simplest\nmethodsisbycountingthenumberofparameters. However,itshouldbeemphasizedthatthe\nmemorymodelsdiscussedhereprimarilyservetostoreinformation,ratherthanaddtrainable\nparameters. Therefore,amodelwithalargememorycapacityisnotnecessarilymorecomplex.\nNevertheless,inpracticedeterminingthecapacityofamemorymodelisnotstraightforward. In",
    "general,weneedtocontrolthetrade-offbetweenmaximizingtheperformanceandcontrolling\nthememoryfootprint.\n8.3.4 Sharing across Heads and Layers\nIn Transformers, the KV cache is a data structure that can be dynamically adjusted along\nmultiple dimensions, such as heads, layers, and sequence length. For example, consider\nan LLM with L layers. Each layer has τ attention heads, and each head produces a d -\nh\ndimensionaloutput. Duringinference,westorethekeysandvaluesforuptomtokens. The\nspacecomplexityofthiscachingmechanismisO(L·τ·d ·m). Aswehaveseenpreviously,\nh\nthiscomplexitycanbereducedbycachingthekeysandvaluesforfewertokens. Forexample,\ninslidingwindowattention,afixed-sizewindowisusedtocachethekeysandvaluesinlocal\ncontext. AndthismodelhasaspacecomplexityofO(L·τ·d ·m ),withm beingthesize\nh w w\nofthewindow.\nIn addition to reducing m, we can also decrease the size of the KV cache along other\ndimensions. A widely-used approach is to enable sharing across heads in multi-head self-",
    "attention. RecallfromSection8.1.1thatmulti-headself-attentionusesmultiplesetsofqueries,\nkeys,andvalues(eachsetiscalledahead),eachperformingtheQKVattentionmechanismas",
    "8.3 Long Sequence Modeling 451\nusual. Thiscanbeexpressedas\nOutput = Merge(head ,...,head )Whead (8.70)\n1 τ\nwherehead j ∈Rd h iscomputedusingthestandardQKVattentionfunction\n[j] [j] [j]\nhead =Att (q ,K ,V ) (8.71)\nj qkv i ≤i ≤i\n[j] [j] [j]\nHere,q ,K ,andV arethequery,keys,andvaluesthatareprojectedontothej-thfeature\ni ≤i ≤i\nsub-space. So this model can be interpreted as performing attention on a group of feature\nsub-spacesinparallel(seeFigure8.8(b)). TheKVcacheneedstoretainthekeysandvalues\n[1] [1] [τ] [τ]\nforalltheseheads,thatis,{(K ,V ),...,(K ,V )}.\n≤i ≤i ≤i ≤i\nOnerefinementtothemulti-headattentionmodel,calledmulti-queryattention(MQA),\nistosharekeysandvaluesacrossheads,whileallowingqueriestobeuniqueforeachhead\n[Shazeer, 2019]. In MQA, there is a single set of keys and values (K ,V ). In addition,\n≤i ≤i\n[1] [τ]\nthereareτ queries{q ,...,q },eachcorrespondingtoadifferenthead. Foreachhead,we\ni i\nhave\n[j]\nhead =Att (q ,K ,V ) (8.72)\nj qkv i ≤i ≤i",
    "Figure 8.8 (c) illustrates this model. By sharing keys and values, the size of the KV cache\nwouldbeO(L·d ·m).\nh\nGroupedqueryattention(GQA)isanaturalextensiontomulti-headattentionandMQA\n[Ainslieetal.,2023]. InGQA,headsaredividedinton groups,eachcorrespondingtoashared\ng\nsetofkeysandvalues. Hencewehaven setsofkeysandvalues{(K\n[1]\n,V\n[1]\n),...,(K\n[ng]\n,V\n[ng]\n)}.\ng ≤i ≤i ≤i ≤i\nSee Figure 8.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA\nmodelcanbeexpressedas\n[j] [g(j)] [g(j)]\nhead =Att (q ,K ,V ) (8.73)\nj qkv i ≤i ≤i\nThe size of the KV cache of GQA is O(L·n ·d ·m). One benefit of GQA is that we can\ng h\ntrade-offbetweencomputationalefficiencyandmodelexpressivenessbyadjustingn . When\ng\nn =τ,themodelbecomesthestandardmulti-headattentionmodel. Bycontrast,whenn =1,\ng g\nitbecomestheGQAmodel.\nSharingcanalsobeperformedacrosslayers. Suchamethodfallsintothefamilyofshared\nweight and shared activation methods, which have been extensively used in Transformers",
    "[Dehghani et al., 2018; Lan et al., 2020]. For example, one can share KV activations or\nattentionweightsacrosslayerstoreducebothcomputationandmemoryfootprints[Xiaoetal.,\n2019;Brandonetal.,2024]. Figure8.8(e)showsanillustrationofthismethod,whereaquery\ninalayerdirectlyaccessestheKVcacheofalower-levellayer.",
    "452 Chapter 8. Generative Models\nvalue key query value key query\n(a)Single-headAttention (b)Multi-headAttention\nvalue key query value key query\n(c)Multi-queryAttention (d)GroupedQueryAttention\nvalue key query\nLayerl\nSharing\nLayerl−1\n(e)Cross-layerMulti-headAttention\nFigure8.8: IllustrationofQKVattentionbasedondifferentmulti-headandsharingmechanisms.\n(a)=single-headattention,and(b-e)=attentionwithmultipleheads.\n8.3.5 Position Extrapolation and Interpolation\nSinceTransformerlayersareorder-insensitivetoinput,weneedsomewaytoencodepositional\ninformationintheinputtokens. Todothis,itiscommontoaddpositionalembeddingstotoken\nembeddings,andthenfeedthesecombinedembeddingsintotheTransformerlayerstackas\ninput. Inthiscase,theembeddingatpositionicanbeexpressedas\ne = x +PE(i) (8.74)\ni i\nwherex ∈Rddenotesthetokenembedding,andPE(i)∈Rddenotesthepositionalembedding.\ni",
    "8.3 Long Sequence Modeling 453\nIn general, the token embedding x is a position-independent vector, and so the positional\ni\nembedding PE(i) is used to encode the positional context. A straightforward approach is\nto treat PE(i) as a learnable variable and train it alongside other model parameters. In this\nway,wecanlearnauniquerepresentationforeachposition,andthusdistinguishthetokens\nappearingatdifferentpositionsofasequence.\nRepresentations of positions using learned vectors can work well in tasks where the\nsequences at training and test times are of similar lengths. In practice, however, we often\nimposelengthrestrictionsonsequencesduringtrainingtopreventexcessivecomputational\ncosts, but wish to apply the trained models to much longer sequences during inference. In\nthiscase,usinglearnedpositionalembeddingshasobviousdrawbacks,astherearenotrained\nembeddingsforpositionsthatarenotobservedinthetrainingphase.\nAnalternativeapproachtomodelingpositionalinformationistodeveloppositionalembed-",
    "dingsthatcangeneralize: oncetrained,theembeddingmodelcanbeusedtohandlelonger\nsequences. Supposethatwetrainapositionalembeddingmodelonsequenceswithamaximum\nlengthofm ,andwewishtoapplythetrainedmodeltoasequenceoflengthm(m>>m ).\nl l\nIftheembeddingmodelislimitedintherangeofpositionsthatwecanobservefromtraining\ndata,thenthismodelwillsimplyfailtodealwithnewdataoutsidethatrange. SeeFigure8.9\n(a)foranillustrationwherethelearnedembeddingmodelcannotmodeldatapointsoutsidethe\ntrainingdomainifitlackstheabilitytoextrapolate.\nThereareseveralapproachestomakingpositionalembeddingmodelsgeneralize. They\ncanbegroupedintotwoclasses.\n• Extrapolation. The model learned on observed data points (i.e., positions) can be\ndirectlyemployedtoassignmeaningfulvaluestodatapointsbeyondtheoriginalrange.\nForexample,supposewehaveaseriesofnumbers1,2,...,10,andwewanttounderstand\nthe meaning of a new number, 15. Knowing that these numbers are natural numbers",
    "usedforordering,wecaneasilyinferthat15isanumberthatfollows10,eventhough\n15 has not been observed before. Figure 8.9 (b) shows an example of this approach,\nwhereafunctionislearnedtofitthedatapointswithinaspecificrangeandthenapplied\ntoestimatethevaluesofdatapointsoutsidethatrange.\n• Interpolation. This approach maps a larger range of data points into the original\nobservationrange. Forexample,supposewehaveamodeldesignedfornumbersinthe\nrange [1,10]. When given a new range of [1,20], we can scale this down by dividing\neverynumberby2,therebyfittingallnumbersinto[1,10]. Thisscalingallowsustouse\nthemodeltrainedontherange[1,10]todescribedatapointsintheexpandedrangeof\n[1,20]. SeeFigure8.9(c)foranillustrationofthisapproach.\nInfact,positionalembeddingsinmanysystemshaveachievedsomelevelofgeneralization.\nForexample,sinusoidalencoding,themostcommonpositionalembeddingmethod,employs\nsineandcosinefunctionsthatcannaturallyextendtosequencesofanylength. Althoughthis",
    "approachmightseemdirectandsimple,itdoesnotperformwellwhenwesignificantlyextend\nthesequencesforprocessing. Inthissubsection,wewilldiscussseveralalternativemethods\nbasedoneitherextrapolationorinterpolation.",
    "454 Chapter 8. Generative Models\n1\n0\n−1\n0 1,024 2,048\nSequenceLength\n(a)EncodingwithNoGeneralization\neulaV\n1\n0\n−1\n0 1,024 2,048\nSequenceLength\n(b)Extrapolation\neulaV\n1\n0\n−1\n0 1,024 2,048\nSequenceLength\n(c)Interpolation\neulaV\nFigure8.9: Illustrationsofdifferentpositionalembeddingmethodsforarangeofpositions.\nBlue points represent the positions that have been observed during training, and red points\nrepresent the positions that are newly observed at test time. In sub-figure (a), the encoding\nmodelonlymemorizesthepointsseenduringtraining,andcannotgeneralize. Insub-figures\n(b)and(c),themodelcangeneralizethroughextrapolationandinterpolation.\n1. Attention with Learnable Biases\nOne problem with Eq. (8.74) is that the embedding model treats each token independently\nandthereforeignoresthedistancebetweendifferenttokens. Acommonimprovementtothis\nmodel,calledrelativepositionalembedding,istoconsiderthepairwiserelationshipbetween",
    "tokens[Shawetal.,2018]. Thegeneralideabehindthisistoobtaintheoffsetbetweenany\npairofpositionsandincorporateitintotheself-attentionmodel. Oneofthesimplestformsof\nself-attentionwithrelativepositionalembeddingisgivenby\ni\n(cid:88)\nAtt (q ,K ,V ) = α(i,j)v (8.75)\nqkv i ≤i ≤i j\nj=0\nq kT+PE(i,j)\ni j\nα(i,j) = Softmax( √ +Mask(i,j)) (8.76)\nd",
    "8.3 Long Sequence Modeling 455\nThe only difference between this model and the original self-attention model is that a bias\ntermPE(i,j)isaddedtothequery-keyproductinthisnewmodel. Intuitively,PE(i,j)canbe\ninterpretedasadistancepenaltyforthepairofpositionsiandj. Asimovesawayfromj,the\nvalueofPE(i,j)decreases.\nPE(i,j) can be defined in several different ways. Here, we consider the T5 version of\nrelativepositionalembedding,calledtheT5bias[Raffeletal.,2020]. Foreachpairofquery\nq andkeyk ,theoffsetbetweenthemisdefinedtobe15\ni j\nd(i,j) = i−j (8.77)\nAsimpledesignforthebiasPE(i,j)istosharethesamelearnablevariableforallquery-\nkeypairswiththesameoffset,i.e.,PE(i,j)=u ,whereu isthevariablecorresponding\ni−j i−j\ntotheoffseti−j. However,simplyassigningauniquevaluetoeachoffsetwillrestrictthis\nmodeltoobservedoffsets. Wheni−j islargerthanthemaximumtrainedoffset,themodel\ncannotgeneralize.\nTheT5biasinsteadadoptsageneralizationofthismodel. Ratherthanassigningeachquery-",
    "keyoffsetauniquebiasterm,itgroupsdifferenceoffsetsinto“buckets”,eachcorresponding\ntoonelearnableparameter. Morespecifically,thebiastermsforn +1bucketsaregivenas\nb\nfollows.\n• For buckets 0 to n b +1 −1, each bucket corresponds to one offset, that is, bucket 0\n2\n↔ offset 0, bucket 1 ↔ offset 1, bucket 2 ↔ offset 2, and so on. We express this as\nb(i−j)=i−j.\n• Forbuckets n b +1 ton ,thesizeofeachbucketincreaseslogarithmically. Forexample,\n2 b\nthebucketnumberforagivenoffseti−j ≥ n b +1 canbedefinedas\n2\nn +1\nlog(i−j)−log(n\nb\n+1)\nn +1\nb(i−j) = b +⌊ 2 · b ⌋ (8.78)\n2 log(dist )−log(n b +1) 2\nmax 2\nwheretheparameterdist istypicallysettoarelativelylargenumbertoindicatethe\nmax\nmaximumoffsetwemayencounter.\n• When i−j >dist , we place i−j in the last bucket. In other words, bucket n\nmax b\ncontainsalltheoffsetsthatarenotassignedtothepreviousbuckets.\nTogether,thesecanbeexpressedasthefunction\nb(i−j)\n\n i−j 0≤i−j < n b 2 +1\n= min(n\nb\n,n b\n2\n+1+⌊\nlog\nlo\n(\ng\nd\n(\nis\ni−\ntm\nj\na\n)\nx\n−\n)\nl\n−\nog\nlo\n(\ng\nn\n(",
    "b 2\nn\n+\nb\n1\n2 +\n)\n1 )\n·n b\n2\n+1⌋) i−j ≥ n b\n2\n+1 (8.79)\nFigure 8.10 shows an illustration of these buckets. We see that in the first half of the\n15Forlanguagemodeling,aqueryisonlyallowedtoattendtoitsleft-context,andsowehavei−j≥0.Inthe\nmoregeneralcaseofself-attention,whereatokencanattendtoalltokensinthesequence,wemayhavenegative\noffsetswheni<j.",
    "456 Chapter 8. Generative Models\nfixedbucketsize logarithmicallyincreasedbucketsize\nBucket ··· ···\n0 1 2 3 14 15 16 17 18 32\nOffset 0 1 2 3 14 15 16∼2021∼26 27∼33 802∼∞\n(i−j)\nFigure8.10: Illustrationofdistributingquery-keyoffsetsintobucketsintheT5model(n =32\nb\nanddist =1024). Boxesrepresentbuckets. Inthefirsthalfofthebuckets,weuseafixed\nmax\nbucketsize. Inthesecondhalfofthebuckets,weincreasethebucketsizelogarithmically. The\nlastbucketcontainsallthequery-keyoffsetsthatarenotcoveredbypreviousbuckets.\nbuckets, each bucket is associated with only one value of i−j, while in the second half,\nthebucketsizeincreasesasi−j grows. Thelastbucketisdesignedtohandlesequencesof\narbitrarilylonglengths.\nAllPE(i,j)sinabucketsharethesamebiastermu . SubstitutingPE(i,j)=u\nb(i−j) b(i−j)\nintoEq. (8.76),theattentionweightforq andk becomes16\ni j\nq kT+u\ni j b(i−j)\nα(i,j) = Softmax( √ +Mask(i,j)) (8.81)\nd\nTheparameters{u ,...,u }arelearnedascommonparametersduringtraining. Itshould\n0 n\nb",
    "beemphasizedthatthismodelcangeneralizetolongsequences. ThisisbecausePE(i,j)swith\nsimilarquery-keyoffsetssharethesameparameter,andthissharingstrategyisparticularly\nimportant for achieving good generalization, given that large query-key offsets are rare in\ntraining. Inpractice,weoftensetn toamoderatenumber,andthusitcanhelpcontrolthe\nb\noverfittingofpositionalembeddingmodels.\n2. Attention with Non-learned Biases\nRelativepositionalembeddingmodelsarebasedonasetoflearnedbiasesforthequery-key\nproduct in self-attention. An alternative approach is to give these biases fixed values via\nheuristics,ratherthantrainingthemonaparticulardataset. Onebenefitofthisheuristics-based\napproachisthatitdoesnotrelyonatrainingprocessandthuscanbedirectlyappliedtoany\nsequencesoncethebiasesareset.\nOneexampleofsuchanapproachisPressetal.[2022]’sapproach,calledattentionwith\nlinearbiasesorALiBiforshort. IntheALiBiapproach,thebiastermisdefinedasthenegative",
    "16Notethat,inRaffeletal.[2020]’sT5model,therescalingoperationforthequery-keyproductisremoved.\nTheattentionweightα(i,j)isthengivenby\nα(i,j) = Softmax(q i kT j +u b(i−j) +Mask(i,j)) (8.80)",
    "8.3 Long Sequence Modeling 457\nEntry Query-KeyBias(PE(i,j))\nT5[Raffeletal.,2020] u\nb(i−j)\nALiBi[Pressetal.,2022] −β·(i−j )\nKerple[Chietal.,2022] −β\n1\n(i−j )β2 (power)\n−β log(1+β (i−j )) (logarithmic)\n1 2\nSandwich[Chietal.,2023]\n(cid:80)d¯/2\ncos\n(cid:0)\n(i−j\n)/100002k/d¯(cid:1)\nk=1\n(cid:0) (cid:1)\nFIRE[Lietal.,2024b] f ψ(i−j )/ψ(max(m ,i))\nlen\nTable 8.4: Query-key biases as relative positional embeddings. β, β , β , d¯ , and m are\n1 2 len\nhyper-parameters. IntheT5model,b(i−j)denotesthebucketassignedtoi−j. IntheFIRE\nmodel,ψ(·)isamonotonicallyincreasingfunctionsuchasψ(x)=log(cx+1),andf(·)isan\nFFN.\nscaledquery-keyoffset\nPE(i,j) = −β·(i−j)\n= β·(j−i) (8.82)\nwhereβ isthescalingfactor. Addingthistermtothequery-keyproduct,weobtainanewform\nofattentionweights\nq kT+β·(j−i)\ni j\nα(i,j) = Softmax( √ +Mask(i,j)) (8.83)\nd\nThismodelcanbeinterpretedasaddingafixedpenaltytoq kT wheneverj movesone\ni j\nstepawayfromi. Sowedonotneedtoadaptittoarangeofsequencelengths,andcanemploy",
    "ittomodelarbitrarilylongsequences. SeeFigure8.11foracomparisonoftheT5biasandthe\nALiBibias.\nIn general, the scalar β should be tuned on a validation dataset. However, Press et al.\n[2022] found that setting β to values decreasing geometrically by a factor of 1 for multi-\n2a\nheadattentionperformswellonavarietyoftasks. Specifically,foraself-attentionsub-layer\ninvolvingn heads,thescalarforthek-thheadisgivenby\nhead\n1\nβ = (8.84)\nk 8\n2k\nTheALiBiapproachprovidesasimpleformofrelativepositionalembeddings. Thereare\nothersimilarmethodsfordesigningquery-keybiasesusingtheoffseti−j. Table8.4shows\na comparison of such biases. As an aside it is worth noting that the form of the right-hand\nsideofEq. (8.82)isverysimilartolengthfeaturesusedinconventionalfeature-basedsystems.\nFor example, in statistical machine translation systems, such features are widely used to\nmodelwordreorderingproblems,resultinginmodelsthatcangeneralizewellacrossdifferent\ntranslationtasks[Koehn,2010].",
    "458 Chapter 8. Generative Models\nq kT Bias(u )\ni j b(i−j)\nq0kT 0 u 0\nq1kT 0 q1kT 1 u 1 u 0\nq2kT 0 q2kT 1 q2kT 2 u 2 u 1 u 0\n+\nq3kT 0 q3kT 1 q3kT 2 q3kT 3 u 2 u 2 u 1 u 0\nq4kT 0 q4kT 1 q4kT 2 q4kT 3 q4kT 4 u 3 u 2 u 2 u 1 u 0\nq5kT 0 q5kT 1 q5kT 2 q5kT 3 q5kT 4 q5kT 5 u 3 u 3 u 2 u 2 u 1 u 0\nq6kT 0 q6kT 1 q6kT 2 q6kT 3 q6kT 4 q6kT 5 q6kT 6 u 3 u 3 u 3 u 2 u 2 u 1 u 0\n(a)TheT5bias(n =3anddist =5)\nb max\nq i kT j Bias(−β(i−j))\nq0kT\n0\n0\nq1kT\n0\nq1kT\n1\n−1β 0\nq2kT\n0\nq2kT\n1\nq2kT\n2\n−2β −1β 0\n+\nq3kT\n0\nq3kT\n1\nq3kT\n2\nq3kT\n3\n−3β −2β −1β 0\nq4kT\n0\nq4kT\n1\nq4kT\n2\nq4kT\n3\nq4kT\n4\n−4β −3β −2β −1β 0\nq5kT\n0\nq5kT\n1\nq5kT\n2\nq5kT\n3\nq5kT\n4\nq5kT\n5\n−5β −4β −3β −2β −β 0\nq6kT\n0\nq6kT\n1\nq6kT\n2\nq6kT\n3\nq6kT\n4\nq6kT\n5\nq6kT\n6\n−6β −5β −4β −3β −2β −β 0\n(b)TheALiBibias\nFigure8.11: Query-keyproductswithbiases(above=theT5biasandbelow=theALiBibias).\nThe colorscale of thebiases ranges fromlight blue denotingsmall absolute values todeep\nbluedenotinglargeabsolutevalues.\n3. Rotary Positional Embedding",
    "Aswithsinusoidalembeddings,rotarypositionalembeddingsarebasedonhard-codedvalues\nforalldimensionsofanembedding[Suetal.,2024]. Recallthatinthesinusoidalembedding\nmodel,positionsarerepresentedascombinationsofsineandcosinefunctionswithdifferent\nfrequencies. Theseembeddingsarethenaddedtotokenembeddingstoformtheinputstothe\nTransformerlayerstack. Rotarypositionalembeddingsinsteadmodelpositionalcontextas\nrotationstotokenembeddingsinacomplexspace. Thisleadstoamodelexpressedintheform",
    "8.3 Long Sequence Modeling 459\nofmultiplicativeembeddings\ne = x R(i) (8.85)\ni i\nwhereR(i)∈Rd×d istherotationmatrixrepresentingtherotationsperformedonthetoken\nembeddingx ∈Rd.\ni\nForsimplicity,wewillfirstconsiderembeddingswithonlytwodimensionsandreturnto\nadiscussionofthemoregeneralformulationlater. Supposewehavea2-dimensionaltoken\n(cid:104) (cid:105)\nembeddingx= x x . Wecanrepresentitasavectorinaplane,originatingattheorigin\n1 2\n(0,0) and terminating at (x ,x ). A counterclockwise rotation of this vector refers to an\n1 2\noperationofmovingthevectoraroundtheoriginwhilemaintainingitsmagnitude,asshownin\nFigure8.12(a). Thedegreeofrotationisusuallydefinedbyaspecificangle, denotedbyθ.\nTherotationcanbeexpressedmathematicallyintheform\nRo(x,θ) = xR\nθ\n(cid:34) (cid:35)\n(cid:104) (cid:105) cosθ sinθ\n= x x\n1 2\n−sinθ cosθ\n(cid:104) (cid:105)\n= cosθ·x −sinθ·x sinθ·x +cosθ·x (8.86)\n1 2 1 2\n(cid:34) (cid:35)\ncosθ sinθ\nwhereR = istherotationmatrix. Iftwoormorerotationsareperformed\nθ\n−sinθ cosθ",
    "on the same vector, we can rotate the vector further. This follows from the fact that the\ncompositionofsuccessiverotationsisitselfarotation. Moreformally,rotatingavectorbyan\nangleθ forttimescanbeexpressedas\nRo(x,tθ) = xR\ntθ\n(cid:104) (cid:105)\n= costθ·x −sintθ·x sintθ·x +costθ·x (8.87)\n1 2 1 2\nIfweinterprettasthepositionofatokenrepresentedbyxinasequence, thenwewill\nfindthattheaboveequationdefinesasimplepositionalembeddingmodel. AsshowninFigure\n8.12(b),westartmovingthetokenfromposition0. Eachtimewemoveonestepforward,the\nvectorisrotatedbytheangleθ. Uponarrivingatthepositiont,therepresentationofthetoken\nwithpositionalcontextisgivenbyRo(x,iθ). Astherotationsdonotchangethemagnitudeof\ntheembedding,theoriginal“meaning”ofthetokenisretained. Thepositionalinformationis\ninjectedintotheembedding,whenitgetsrotated.\nApopularwaytounderstandvectorrotationistodefineitincomplexspaces. Itiseasy\n(cid:104) (cid:105)\ntotransformeachvectorx= x x inthe2DEuclideanspaceR2 toacomplexnumber\n1 2",
    "x′=x +ix inthecomplexspaceCviaabijectivelinearmap. Then,therotationofxwith\n1 2\ntheangletθ correspondstothemultiplicationbyeitθ. Giventhateitθ =costθ+isintθ,the",
    "460 Chapter 8. Generative Models\nx x\n2 2\nvectorx x\nrotatedvector xR\nθ\nxR\nθ\nθ θ\nθ\nx x\n1 1\nxR\n2θ θ\nxR\n3θ\n(a)Single-stepRotation (b)Multi-stepRotation\nThe1cat2is3sleeping4peacefully5\nx\n2\nin6the7warm8sunlight9.10\nsleeping4\n7θ\ncat2\n7θ\nx\n1\nsleeping11\ncat9\nEvery1afternoon2,3you4’ll5find6that7\nthe8cat9is10sleeping11on12my13bed14.15\n(c)Anglesbetweenembeddingsoftwotokensatdifferentpositions\nFigure8.12: Illustrationsofvectorrotationsinaplane. Sub-figures(a)and(b)showrotationsof\navectorinasinglestepandmultiplesteps,respectively. Sub-figure(c)showstheembeddings\noftokenscatandsleepingintwodifferentsentences. Weshowthesesentenceswithasubscript\naffixed to each token to indicate its position. If we represent tokens as vectors, we can\naddpositionalinformationbyrotatingthesevectors. Thisrotationpreservesthe“distances”\nbetweenthevectors. Forexample,giventhatthedistancebetweencatandsleepingisthesame\ninbothsentences,theanglebetweentheirembeddingsalsoremainsthesameduringrotation.",
    "rotationoperationcanbere-expressedintheform\nxR (cid:55)→ x′eitθ\ntθ\n= (x +ix )(costθ+isintθ)\n1 2\n= costθ·x −sintθ·x +i(sintθ·x +costθ·x ) (8.88)\n1 2 1 2\nHerewedenotethetokenrepresentationx′eitθ byC(x,tθ). Theinnerproductoftherepresen-",
    "8.3 Long Sequence Modeling 461\ntationsofthetokensatpositionstandscanbewrittenas\n⟨C(x,tθ),C(y,sθ)⟩ = (x′y′)ei(t−s)θ (8.89)\nwhere y′ is the complex conjugate of y′. As can be seen, the result of this inner product\ninvolvesatermt−s,andsoitcanmodeltheoffsetbetweenthetwotokens.\nNowwegobacktorepresentationsinthe2DEuclideanspace. Thedot-productofRo(x,tθ)\nandRo(y,sθ)iscanbewrittenasafunctionof(t−s)θ\nRo(x,tθ)[Ro(y,sθ)]T = xR [yR ]T\ntθ sθ\n= xR [R ]TyT\ntθ sθ\n= xR yT (8.90)\n(t−s)θ\nGiventhisresult, ifweconsiderRo(x,tθ)andRo(y,sθ)asthequeryandthekey, thenthe\nself-attentionoperationwillimplicitlyinvolvethemodelingofrelativepositionalcontext.\nThisrotarypositionalembeddingcanbeextendedtomulti-dimensionalembeddings. Fora\n(cid:104) (cid:105)\nd-dimensionaltokenembeddingx= x x ... x ,wecantreatitasa d-dimensional\n1 2 d 2\n(cid:104) (cid:105) (cid:104) (cid:105)\ncomplexvectorx′= x′ x′ ... x′ = x +ix x +ix ... x +ix ,where\n1 2 d/2 1 2 3 4 d−1 d",
    "eachconsecutivepairofitemsformsacomplexnumber. Then,therotarypositionalembedding\ninthecomplexspaceisgivenby\nd/2\n(cid:88)\nC(x,tθ) = x′eitθ k⃗e (8.91)\nk k\nk=1\nwhere⃗e isthestandardbasisvectorwithasinglenon-zerovalueinthek-thcoordinateand\nk\n0’selsewhere[Bidermanetal.,2021].\nAlthough this formula involves a complicated expression, its equivalent form in the d-\ndimensionalEuclideanspaceisrelativelyeasytounderstand. Wecanwriteitas\n \nR\ntθ1\nRo(x,tθ) =\n(cid:104)\nx 1 x 2 ... x d\n(cid:105)  \n\n\nR tθ2\n...\n \n\n\n(8.92)\n \nR\ntθ\nd/2\n(cid:34) (cid:35)\ncostθ sintθ (cid:104) (cid:105)\nk k\nwhereR = . θ= θ ,...,θ aretheparametersforcontrollingthe\ntθ k −sintθ costθ 1 d/2\nk k\nangles of rotations in different dimensions. Typically, θ k is set to\n10000−2(k\nd\n−1)\n, which is\nanalogoustothesettinginsinusoidalembeddings.\nInapracticalimplementation,Eq. (8.92)canberewrittenintoaformthatreliessolelyon",
    "462 Chapter 8. Generative Models\ntheelement-wiseproductandadditionofvectors.\n T  T  T  T\nx costθ −x sintθ\n1 1 2 1\n x   costθ   x   sintθ \n 2   1   1   1 \n .   .   .   . \nRo(x,tθ) =  . .  ⊙ . .  + . .  ⊙ . .  (8.93)\n       \nx  costθ  −x  sintθ \n d−1  d/2  d  d/2\nx costθ x sintθ\nd d/2 d−1 d/2\nFinally,werewriteEq. (8.85)toobtaintheformoftheembeddingatpositioni\ne = Ro(x ,iθ) (8.94)\ni i\n4. Position Interpolation\nInpositioninterpolation,ourgoalistomapthepositionsinthenewsequencetomatchthe\nobserved range in training. Suppose the sequence length for training ranges from 0 to m .\nl\nWhenm>m attesttime,werepresentthepositionsin[0,m]suchthatourrepresentationsfit\nl\n[0,m ].\nl\nTo illustrate, consider the rotary positional embedding model described above. The\n(cid:104) (cid:105)\nembeddingofeachtokenisdescribedbyamodelRo(x ,iθ)inwhichθ= θ ,...,θ are\ni 1 d/2",
    "the parameters. Ro(x ,iθ) can be cast in the form of a linear combination of two periodic\ni\nfunctions(seeEq. (8.93))\n(cid:104) (cid:105)\ncosiθ = cosiθ ... cosiθ (8.95)\n1 d/2\n(cid:104) (cid:105)\nsiniθ = siniθ ... siniθ (8.96)\n1 d/2\nθ isaexponentialfunctionofk andtakestheform\nk\nθ k =\nb−2(k\nd\n−1)\n(8.97)\nwherebisthebase. Theperiodofcosiθ andsiniθ is\nk k\n2(k−1)\nT k = 2π·b d (8.98)\nThekeyideabehindpositioninterpolationistoadjustthisperiodsothatthenewpositions\ncanbeencodedwithintherange[0,m ]. OnewaytoachievethisistoscaleupT by m,given\nl k m\nl\nby\nT k ′ = m m ·2π·b 2(k d −1) (8.99)\nl\nHenceallpointsin[0,m]arecompressedinto[0,m ]. Thislinearscalingcanbeeasilyrealized\nl\nby modifying the inputto theembedding model[Chen etal., 2023c]. Thenewmodel with",
    "8.3 Long Sequence Modeling 463\nlinearpositionalinterpolationisgivenby\nm\nRo′(x ,iθ) = Ro(x , l iθ) (8.100)\ni i\nm\nAnothermethodofpositionalinterpolationistoscalethebase17. Supposethatthebasebis\nscaledbyλ. Wewishtheperiodofthisnewmodelinthelastdimensionofθ (i.e.,dimension\nd)tobeequaltothatofthelinearpositionalinterpolationmodel. Thiscanbeexpressedas\n2\n2(d 2−1) m 2(d 2−1)\n2π·(λb) d = ·2π·b d (8.101)\nm\nl\nSolvingthisequation,weobtain\nλ =\n(cid:0)m(cid:1)\n2(d 2\nd\n−1)\nm\nl\n(cid:0)m(cid:1) d\n= d−2 (8.102)\nm\nl\nThisgivesanembeddingmodel\nRo′(x ,iθ) = Ro(x ,iθ′) (8.103)\ni i\nwhere\n(cid:104) (cid:105)\nθ′= (λb)−\nd\n0 ,(λb)−\nd\n2 ,...,(λb)−d−\nd\n2 (8.104)\nNotethatscalingthebaseprovidesanon-uniformmethodforscalingtheperiodsacross\ndifferentdimensionsofθ. ThismethodhasbeenfoundtobehelpfulforextendingLLMsto\nlonger sequences, and several improvements have been developed [Peng et al., 2024; Ding\netal.,2024].\n8.3.6 Remarks\nInthissection,wehavepresentedavarietyofmethodsforlong-contextlanguagemodeling.",
    "Weclosethissectionbydiscussingsomeinterestingissuesrelatedtothesemethods.\n1. Need for Long Context\nOne of the ultimate goals of long-context LLMs is that these models can precisely encode\ninfinitecontext. Theso-calledinfinitecontextrefersmoretothefactthatanLLMcancontin-\nuouslyreadwords. ThismotivatesLLMsthatcanhandleextremelylongcontextorstream\ndata. AsdiscussedinSection8.3.3,itiscommontousefixed-sizememorymodelstoprocess\ncontinuously expanding context. Many such systems are based on recurrent architectures\nor their variants, because they are inherently suited to model time series problems where\n17This method was first proposed in https://www.reddit.com/r/LocalLLaMA/comments/\n14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/",
    "464 Chapter 8. Generative Models\nthe effects of past inputs continue indefinitely. Another way to achieve infinite memory is\ntodevelopalternativestoself-attentionmodels,forexample,onecanusecontinuous-space\nattentionmodelstoencodecontext,whichremovesthedependencyoncontextlength[Martins\netal.,2022].\nWhenstudyinglong-contextLLMs,itisnaturaltowonderwhatmechanismsmayexplain\ntheuseoflongcontextinlanguagemodeling. Canwecompresstherepresentationofinfinite\ncontextintoarelativelysmall-sizedmodel? Areallcontexttokensusefulforpredictingnext\ntokens? HowdoLLMspreparefortokenpredictionwhentheyseethecontext? Canweknow\ninadvancewhichcontextualinformationwillbecriticalforprediction? Generalanswersto\nallthesequestionsarenotobvious,buttheyinspirefollow-onresearchofexplainablemodels,\nandsomeinterestingresultshavebeenfound. Forexample,Deletangetal.[2024]conducted\nextensive experiments to show that LLMs are powerful in-context compressors. Although",
    "viewingpredictivemodelsascompressionmodelshaslongbeenstudiedinmachinelearning,\nit also provides insights into our understanding of the LLM scaling laws. Pal et al. [2023]\nandWuetal.[2024]investigatedwhetherthefeatureslearneduptothecurrentstep,though\nnotintentionally,arealreadysufficientforpredictingtokensatthefollowingsteps. Notethat\ntheneedforlong-contextinlanguagemodelingishighlydependentontheproblemthatwe\naddress. A related issue is where to apply LLMs and how to evaluate them. For example,\nin summarization tasks we may only need to distill and focus on a few key aspects of the\ntext,whileinretrieval-liketasksweneedto“memorize”theentirecontextsothattherelevant\ninformationcanbeaccessed. Wewilldiscusstheevaluationissuelaterinthissubsection.\n2. Pre-training or Adapting LLMs?\nTrainingLLMsrequiressignificantcomputationalcosts. Althoughitisstraightforwardtotrain\nLLMsonlongsequencedata,thetrainingbecomescomputationallyunwieldyforlargedata",
    "sets. Itiscommonpracticetopre-trainLLMsongeneraldatasets,andthenadaptthemwith\nmodestfine-tuningeffort. Forexample,LLMswithrelativeorrotarypositionalembeddings\ncan be directly trained on large-scale data in the pre-training phase. While the resulting\nmodelsmayexhibitsomeabilitiestoextrapolatelengthsintheinferencephase,itmaybemore\neffectivetofine-tunethemonlongersequences.\nIdeally, we would like to pre-train LLMs with standard Transformer architectures and\nadapt them to new tasks. This allows us to use many off-the-shelf LLMs and efficiently\nadaptthemtohandlelongsequences. However,whennewarchitecturesareadopted,itseems\ninevitablethatweneedtotrainthesemodelsfromscratch. Thisposespracticaldifficultiesfor\ndevelopinglong-contextLLMs,aswecannotleveragewell-developed,pre-trainedmodelsand\nmustinsteadtrainthemourselves. Ontheotherhand,fine-tuningisstillaneffectivewayto\nadaptLLMswithcertainarchitecturesthataredifferentfromthoseinpre-training. Anexample",
    "ismodelsaugmentedwithexternalmemories. Inthesemodels,thepre-trainedLLMsarefixed,\nandthefocusisonhowtomaketheseLLMscollaboratewiththememorymodels. InRAG,for\ninstance,itiscommontofine-tuneLLMstoimprovetheiruseofretrieval-augmentedinputs.\nAnotherexampleoffine-tuningLLMsforlong-contextmodelingisthatwetrainanLLMwith\nfullattentionmodels,andthenreplacethemwithsparseattentionmodelsinthefine-tuning",
    "8.3 Long Sequence Modeling 465\nphase. Thepre-trainedLLMprovidesinitialvaluesofmodelparametersusedinadifferent\nmodel,andthismodelisthenfine-tunedasusual.\n3. Evaluating Long-context LLMs\nEvaluatinglong-contextLLMsisimportant,butitisanewissueinNLP.Thegeneralideais\nthat, ifweinputalongcontexttoanLLM,thenwecancheckfromtheoutputoftheLLM\nwhetheritunderstandstheentirecontextandmakesuseofitinpredictingfollowingtokens.\nIn conventional research of NLP, such evaluations are often aimed at examining the ability\nofNLPmodelsinhandlinglong-rangedependencies. However,thesizeofcontextsusedin\nrecentLLMsismuchlargerthanthatusedinNLPsystemsafewyearsago. Thismotivates\nresearcherstodevelopnewevaluationbenchmarksandmetricsforlong-contextLLMs.\nOneapproachistousetheperplexitymetric. However,inspiteofitsapparentsimplicity,\nthis method tends to reflect more on the LLMs’ ability to make use of local context rather\nthanglobalcontext. Itisthereforetemptingtodevelopevaluationmethodsthatarespecific",
    "to long-context LLMs. Popular methods include various synthetic tasks where artificially\ngenerated or modified data is used to evaluate specific capabilities of long-context LLMs.\nIn needle-in-a-haystack18 and passkey retrieval tasks [Mohtashami and Jaggi, 2024; Chen\netal.,2023c],forinstance,LLMsarerequiredtoidentifyandextractasmall,relevantpiece\nofinformationfromalargevolumeofgiventext. TheassumptionhereisthatanLLMwith\nsufficientmemoryshouldrememberearlierpartsofthetextasitprocessesnewinformation.\nThis LLM can thus pick out the relevant details, which might be sparse and hidden among\nmuchirrelevantinformation,fromthetext. Alternatively,incopymemorytasks(orcopytasks\nforshort),LLMsareusedtorepeattheinputtextoraspecificsegmentmultipletimes. These\ntaskswereinitiallyproposedtotesttheextenttowhichrecurrentmodelscanretainandrecall\npreviouslyseentokens[HochreiterandSchmidhuber,1997;Arjovskyetal.,2016],andhave\nbeenadoptedinevaluatingrecentLLMs[Bulatovetal.,2022;GuandDao,2023].",
    "Another approach to evaluating long-context LLMs is to test them on NLP tasks that\ninvolve very long input sequences. Examples include long-document or multi-document\nsummarization,long-documentquestionanswering,codecompletion,andsoon. Abenefitof\nthisapproachisthatitcanalignevaluationswithuserexpectations.\nAlthough many methods have been developed, there is still no general way to evaluate\nlong-contextLLMs[Liuetal.,2024c]. Oneproblemisthatmostofthesemethodsfocuson\nspecificaspectsofLLMs,ratherthantheirfundamentalabilitytomodelverylongcontexts.\nEventhoughanLLMcanpickouttheappropriatepieceoftextfromtheinput,wecannotsay\nthatittrulyunderstandstheentirecontext. Instead,itmightjustremembersomeimportantparts\nofthecontext,orevensimplyrecalltheanswerviathemodellearnedinpre-training. Moreover,\nthedatausedinmanytasksissmall-scaleandrelativelypreliminary,leadingtodiscrepancies\nbetweenevaluationresultsandactualapplicationperformance. Amoreinterestingissueisthat",
    "theresultsofLLMsareinfluencedbymanyotherfactorsandexperimentalsetups,forexample,\nusingdifferentpromptscanleadtoverydifferentoutcomes. Thismakesevaluationevenmore\n18https://github.com/gkamradt/LLMTest_NeedleInAHaystack",
    "466 Chapter 8. Generative Models\nchallengingbecauseimprovementsmaynotsolelyresultfrombettermodelingoflongcontexts,\nandthereisariskofoverclaimingourresults. Nevertheless,manyopenquestionsremainin\nthedevelopmentandevaluationoflong-contextLLMs. Forexample,thesemodelsstillsuffer\nfromlimitationssuchasrestrictedcontextlengthandhighlatency. Studyingtheseissuesis\nlikelytoprovevaluablefuturedirections.\n8.4 Summary\nInthischapter,wehavediscussedtheconceptofLLMsandrelatedtechniques. Thiscanbe\nconsideredageneral,thoughnotcomprehensive,introductiontoLLMs,layingthefoundation\nforfurtherdiscussionsonmoreadvancedtopicsinsubsequentchapters. Furthermore,wehave\nexploredtwowaystoscaleupLLMs. Thefirstfocusesonthelarge-scalepre-trainingofLLMs,\nwhichiscrucialfordevelopingstate-of-the-artmodels. Thesecondfocusesonmethodsfor\nadaptingLLMstolonginputs,includingoptimizingattentionmodels,designingmoreefficient\nandcompressedKVcaches,incorporatingmemorymodels,andexploringbetterpositional\nembeddings.",
    "ThestrengthofLLMsliesintheirabilitytobreaktheconstraintsoftrainingNLPmodels\nforalimitednumberofspecifictasks. Instead,LLMslearnfromlargeamountsoftextthrough\nthesimpletaskoftokenprediction—wepredictthenexttokeninasentencegivenitsprior\ntokens. Ageneralviewisthat,byrepeatingthistokenpredictiontaskalargenumberoftimes,\nLLMscanacquiresomeknowledgeoftheworldandlanguage,whichcanthenbeappliedto\nnew tasks. As a result, LLMs can be prompted to perform any task by framing it as a task\nof predicting subsequent tokens given prompts. This emergent ability in language models\ncomesfromseveraldimensions,suchasscalinguptraining,modelsize,andcontextsize. Itis\nundeniablethatscalinglawsarecurrentlythefundamentalprincipleadoptedindeveloping\nlargelanguagemodels,althoughsimplyincreasingmodelsizehasyettoprovesufficientfor\nachieving AGI. These continuously scaled LLMs have been found to show capabilities in\ngeneral-purposelanguageunderstanding,generation,andreasoning. Morerecently,ithasbeen",
    "foundthatscalingupthecomputeatinferencetimecanalsoleadtosignificantimprovements\nincomplexreasoningtasks[OpenAI,2024].\nGiventheiramazingpower,LLMshaveattractedconsiderableinterest,bothintermsof\ntechniquesandapplications. Asaresult,theexplosionofresearchinterestinLLMshasalso\nledtoavastnumberofnewtechniquesandmodels. However,wedonotattempttoprovidea\ncomprehensiveliteraturereviewonallaspectsofLLMs,giventherapidevolutionofthefield.\nNevertheless, one can still gain knowledge about LLMs from general reviews [Zhao et al.,\n2023;Minaeeetal.,2024]ormorefocuseddiscussionsonspecifictopics[Ruanetal.,2024].",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 9\nPrompting\nInthecontextofLLMs,promptingreferstothemethodofprovidinganLLMwithaspecific\ninputorcuetogenerateadesiredoutputorperformatask. Forexample,ifwewanttheLLM\ntotranslateasentencefromEnglishtoChinese,wecanpromptitlikethis\nTranslatethetextfromEnglishtoChinese.\nText: Theearlybirdcatchestheworm.\nTranslation:\nPrompting is crucial for LLMs because it directly influences how effectively these models\nunderstandandrespondtouserqueries. Awell-craftedpromptcanguideanLLMtogenerate\nmoreaccurate,relevant,andcontextuallyappropriateresponses. Furthermore,thisprocesscan\nbeiterativelyrefined. ByanalyzingtheresponsesoftheLLM,userscanadjusttheirprompts\ntoalignmorecloselywiththeirspecificneeds. Giventheimportanceofpromptinginapplying\nLLMs, prompt design has become an essential skill for users and developers working with\nLLMs. Thisleadstoanactiveresearcharea,calledpromptengineering,inwhichwedesign",
    "effectivepromptstomakebetteruseofLLMsandenhancetheirpracticalutilityinreal-world\napplications.\nAnimportantconceptrelatedtopromptingisin-contextlearning. Whenpromptingan\nLLM,wecanaddnewinformationtothecontext,suchasdemonstrationsofproblem-solving.\nThisallowstheLLMtolearnfromthiscontexthowtosolvetheproblem. Hereisanexample\nof prompting LLMs with a few demonstrations of how to classify text based on sentiment\npolarity.",
    "468 Chapter 9. Prompting\nHerearesomeexamplesoftextclassification.\nExample1: Wehadadelightfuldinnertogether. →Label: Positive\nExample2: I’mfrustratedwiththedelays. →Label: Negative\nWhatisthelabelfor“Thatcommentwasquitehurtful.”?\nLabel:\nIn-context learning is often seen as an emergent ability of LLMs that arises after pre-\ntraining. Though LLMs can be trained or tuned to perform new tasks, in-context learning\nprovides a very efficient way to adapt these models without any training or tuning effort.\nPerhapsthisisoneofthemostnotablefeaturesofLLMs: theyindeedlearngeneralknowledge\nabout the world and language during pre-training, which we can easily apply to new chal-\nlenges. Moreover,in-contextlearningreflectsthebroadertrendofmakingAIsystemsmore\ngeneralizableanduser-friendly. Insteadofrequiringspecializedengineerstofine-tunemodels\nforeveryuniquetask,userscaninteractwithLLMsinamoreintuitiveway,simplyproviding\nexamplesoradjustingthecontextasneeded.",
    "In this chapter, we focus on prompting techniques in LLMs. We begin by considering\nseveralinterestingpromptdesignscommonlyusedinpromptengineering. Then,wediscussa\nseriesofrefinementstothesemethods. Finally,weexploreapproachesforautomatingprompt\ndesign.\n9.1 General Prompt Design\nThissectionpresentsbasicconceptsinpromptdesign,alongwithexamplesofhowtoprompt\nLLMsforvariousNLPtasks. Sincetheeffectivenessofpromptingishighlydependentonthe\nLLMsbeingused,promptsoftenvaryacrossdifferentLLMs,makingitdifficulttoprovidea\ncomprehensivelistofpromptsforallLLMsanddownstreamtasks. Therefore,thisdiscussion\nis not focused on any specific LLM. Instead, the goal is to provide guiding principles for\npromptdesign.\n9.1.1 Basics\nThetermpromptisusedinmanydifferentways. Inthischapterwedefineapromptastheinput\ntext to an LLM, denoted by x. The LLM generates a text y by maximizing the probability\nPr(y|x). In this generation process, the prompt acts as the condition on which we make",
    "predictions,anditcancontainanyinformationthathelpsdescribeandsolvetheproblem.\nApromptcanbeobtainedusingaprompttemplate(ortemplateforshort)[Liuetal.,2023b].\nAtemplateisapieceoftextcontainingplaceholdersorvariables,whereeachplaceholdercan\nbefilledwithspecificinformation. HerearetwotemplatesforaskingtheLLMforweekend\nsuggestions.",
    "9.1 General Prompt Design 469\nPleasegivemesomesuggestionsforafunweekend.\nIf{∗premise∗},whatareyoursuggestionsforafunweekend.\nInthefirsttemplate,wesimplyinstructtheLLMtoreturnsomesuggestions. Sothetem-\nplateisjustapieceoftextwithnovariables. Inthesecondtemplate,thevariable{∗premise∗}\nneedstobespecifiedbytheuserstoprovideapremiseformakingsuggestions. Forexample,\nifweinput\npremise = theweatherisnicethisweekend\nthenwecangenerateaprompt\nIftheweatherisnicethisweekend,\nwhatareyoursuggestionsforafunweekend.\nWecanalsodesignatemplatewithmultiplevariables. Hereisanexampleinwhichwe\ncomparethetwosentencesintermsoftheirsemanticsimilarity.\nHereisasentence\n{∗sentence1∗}\nHereisanothersentence\n{∗sentence2∗}\nComputethesemanticsimilaritybetweenthetwosentences\nApopularwaytoformatpromptsistowriteeachinputoroutputina“name:content”style.\nForexample,wecandescribeaconversationbetweentwopeople,namedJohnandDavid,and\nusetheLLMtocontinuetheconversation. Atemplateofsuchpromptsisgivenby",
    "470 Chapter 9. Prompting\nJohn: {∗utterance1∗}\nDavid: {∗utterance2∗}\nJohn: {∗utterance3∗}\nDavid: {∗utterance4∗}\nJohn: {∗utterance5∗}\nDavid: {∗utterance6∗}\nJohn: {∗utterance7∗}\nDavid:\nThe“name:content”formatcanbeusedtodefinethetaskthatwewanttheLLMtoperform.\nForexample, giventhat“Q”and“A”arecommonlyusedabbreviationsfor“Question”and\n“Answer”,respectively,wecanusethefollowingtemplatetodoquestion-answering.\nQ:{∗question∗}\nA:\nThisformatcanbeusedtodescribemorecomplextasks. Forexample,thefollowingisan\nexampleofprovidingaspecificationforatranslationtask\nTask: Translation\nSourcelanguage: English\nTargetlanguage: Chinese\nStyle: Formaltext\nTemplate: Translatethefollowingsentence: {∗sentence∗}\nInpracticalsystems,itiscommontorepresentandstoresuchdatainkey-valuepairs,suchas\ntheJSONformat1.\nWhentheproblemisdifficulttodescribeinanattribute-basedmanner,itismorecommon\ntoinstructLLMswithaclearanddetaileddescription. Therearemanywaystodothis. One\n1TheJSONrepresentationis\n{\n\"Task\":\"Translation\"",
    "\"Sourcelanguage\":\"English\"\n\"Targetlanguage\":\"Chinese\"\n\"Style\":\"Formaltext\"\n\"Template\":\"Translatethefollowingsentence:{∗sentence∗}\"\n}",
    "9.1 General Prompt Design 471\nexampleistoassignaroletoLLMsandprovidesufficientcontext. Thefollowingisatemplate\nthatinstructsanLLMtoactasanexpertandanswerquestionsfromchildren.\nYouareacomputerscientistwithextensiveknowledgeinthefieldofdeep\nlearning.\nPleaseexplainthefollowingcomputer-relatedconcepttoachildaround10\nyearsold,usingsimpleexampleswheneverpossible.\n{∗concept∗}\nHerethetext“Youareacomputerscientist... deeplearning. ” issometimescalledsystem\ninformation,andisprovidedtohelptheLLMunderstandthecontextorconstraintsofthetask\nitisbeingaskedtoperform.\n9.1.2 In-context Learning\nLearningcanoccurduringinference. In-contextlearningisonesuchmethod,whereprompts\ninvolvedemonstrationsofproblem-solving,andLLMscanlearnfromthesedemonstrations\nhow to solve new problems. Since we do not update model parameters in this process, in-\ncontextlearningcanbeviewedasawaytoefficientlyactivateandreorganizetheknowledge\nlearnedinpre-trainingwithoutadditionaltrainingorfine-tuning. Thisenablesquickadaptation",
    "of LLMs to new problems, pushing the boundaries of what pre-trained LLMs can achieve\nwithouttask-specificadjustments.\nIn-contextlearningcanbeillustratedbycomparingthreemethods: zero-shotlearning,one-\nshotlearningandfew-shotlearning. Zero-shotlearning,asitsnameimplies,doesnotinvolve\natraditional“learning”process. ItinsteaddirectlyappliesLLMstoaddressnewproblemsthat\nwere not observed during training. In practice, we can repetitively adjust prompts to guide\nthe LLMs in generating better responses, without demonstrating problem-solving steps or\nprovidingexamples. Considerthefollowingexample. SupposewewanttouseanLLMasan\nassistantthatcanhelpcorrectEnglishsentences. Azero-shotlearningpromptisgivenby\nSYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection.\nUSER YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Shedon’tlikegoingtothepark.\nOutput:\nHerethegraywordsareusedtoindicatedifferentfieldsoftheprompt.",
    "Inone-shotlearning,weextendthispromptbyaddingademonstrationofhowtocorrect\nsentences,therebyallowingtheLLMtolearnfromthisnewly-addedexperience.",
    "472 Chapter 9. Prompting\nSYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection.\nDEMO YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Thereismanyreasonstocelebrate.\nOutput: Therearemanyreasonstocelebrate.\nUSER YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Shedon’tlikegoingtothepark.\nOutput:\nFurthermore,wecanaddmoredemonstrationstoenablefew-shotlearning.\nSYSTEM Youareahelpfulassistant,andaregreatatgrammarcorrection.\nDEMO1 YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Thereismanyreasonstocelebrate.\nOutput: Therearemanyreasonstocelebrate.\nDEMO2 YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Meandmyfriendgoestothegymeveryday.\nOutput: MyfriendandIgotothegymeveryday.\nUSER YouwillbeprovidedwithasentenceinEnglish. Thetaskis\ntooutputthecorrectsentence.\nInput: Shedon’tlikegoingtothepark.\nOutput:",
    "Infew-shotlearning,weessentiallyprovideapatternthatmapssomeinputstothecorre-\nspondingoutputs. TheLLMattemptstofollowthispatterninmakingpredictions,provided\nthatthepromptincludesasufficientnumberofdemonstrations,althoughgenerallysmall. Itis\nalsopossibletousesimplerpatternstoachievethis. Forexample,onecanusethefollowing\nfew-shotlearningpromptfortranslatingwordsfromChinesetoEnglish.",
    "9.1 General Prompt Design 473\nDEMO 现在 → now\n来 → come\n去 → go\n男孩 → boy\nUSER 女孩 →\nIf the LLM is powerful enough, few-shot learning can enable it to address complex\nproblems, such as mathematical reasoning. For example, consider the following task of\nsummingtwonumbersandthendividingthesumbytheirproduct.\nDEMO 12 5 → (12+5)/(12×5)=0.283\n3 1 → (3+1)/(3×1)=1.33\n−9 4 → (−9+4)/(−9×4)=0.138\n15 15 → (15+15)/(15×15)=0.133\nUSER 19 73 →\nInmanypracticalapplications,theeffectivenessofin-contextlearningreliesheavilyonthe\nqualityofpromptsandthefundamentalabilitiesofpre-trainedLLMs. Ononehand,weneed\nasignificantpromptengineeringefforttodevelopappropriatepromptsthathelpLLMslearn\nmoreeffectivelyfromdemonstrations. Ontheotherhand,strongerLLMscanmakebetteruse\nofin-contextlearningforperformingnewtasks. Forexample,supposewewishtouseanLLM\ntotranslatewordsfromInuktituttoEnglish. IftheLLMlackspre-trainingonInuktitutdata,its\nunderstandingofInuktitutwillbeweak,anditwillbedifficultforthemodeltoperformwell",
    "intranslationregardlessofhowwepromptit. Inthiscase,weneedtocontinuetrainingthe\nLLMwithmoreInuktitutdata,ratherthantryingtofindbetterprompts.\nItmightbeinterestingtoexplorehowin-contextlearningemergesduringpre-trainingand\nwhy it works during inference. One simple understanding is that LLMs have gained some\nknowledge of problem-solving, but there are many possible predictions, which are hard to\ndistinguishwhenthemodelsconfrontnewproblems. Providingdemonstrationscanguidethe\nLLMs to follow the “correct” paths. Furthermore, some researchers have tried to interpret\nin-contextlearningfromseveraldifferentperspectives,includingBayesianinference[Xieetal.,\n2022],gradientdescent[Daietal.,2023;VonOswaldetal.,2023],linearregression[Akyürek\netal.,2023],metalearning[Gargetal.,2022],andsoon.\n9.1.3 Prompt Engineering Strategies\nDesigningpromptsishighlyempirical. Ingeneral,therearemanywaystopromptanLLMfor\nperformingthesametask,andweneedtoperformanumberoftrial-and-errorrunstofinda",
    "satisfactoryprompt. Towritegoodpromptsmoreefficiently,onecanfollowcertainstrategies.\nExamplesofcommonpromptingprinciplesinclude",
    "474 Chapter 9. Prompting\n• Describingthetaskasclearlyaspossible. WhenweapplyanLLMtosolveaproblem,\nweneedtoprovideaprecise,specific,andcleardescriptionoftheproblemandinstruct\nthe LLM to perform as we expect. This is particularly important when we want the\noutputoftheLLMtomeetcertainexpectations. Forexample,supposewearecurious\naboutclimatechange. AsimplepromptforaskingtheLLMtoprovidesomeinformation\nis\nTellmeaboutclimatechange.\nSincethisinstructionistoogeneral,theLLMmaygeneratearesponsethataddresses\nanyaspectofclimatechange,whichmaynotalignwithourspecificinterests. Inthis\ncase,wecaninsteadusepromptsthatarespecificanddetailed. Onesuchexampleis\nProvide a detailed explanation of the causes and effects of climate\nchange,includingtheimpactonglobaltemperatures,weatherpatterns,\nandsealevels. Also,discusspossiblesolutionsandactionsbeingtaken\ntomitigatetheseeffects.\nNowsupposeweintendtoexplainclimatechangetoa10-year-oldchild. Wecanadjust\ntheabovepromptfurther.",
    "Explainthecausesandeffectsofclimatechangetoa10-year-oldchild.\nTalk about how it affects the weather, sea levels, and temperatures.\nAlso,mentionsomethingspeoplearedoingtohelp. Trytoexplainin\nsimpletermsanddonotexceed500words.\n• GuidingLLMstothink. LLMshaveexhibitedsurprisinglygoodcapabilitiesto“think”.\nAcommonexampleisthatwell-developedLLMshaveachievedimpressiveperformance\ninmathematicalreasoningtasks,whichareconsideredchallenging. Inpromptengineer-\ning,the“thinking”abilityofLLMsneedstobeactivatedthroughappropriateprompting,\nespecially for problems that require significant reasoning efforts. In many cases, an\nLLM that is instructed to “think” can produce completely different results compared\nwiththesameLLMthatisinstructedtoperformthetaskstraightforwardly. Forexample,\nKojimaetal.[2022]foundthatsimplyappending“Let’sthinkstepbystep”totheendof\neachpromptcanimprovetheperformanceofLLMsonseveralreasoningtasks. LLMs\ncanbepromptedto“think”inanumberofways. OnemethodistoinstructLLMsto",
    "9.1 General Prompt Design 475\ngenerate steps for reasoning about the problem before reaching the final answer. For\nexample, consider a task of solving mathematical problems. See below for a simple\npromptforthistask.\nYouareamathematician. Youwillbeprovidedwithamathproblem.\nPleasesolvetheproblem.\nSincesolvingmathproblemsrequiresadetailedreasoningprocess,LLMswouldproba-\nblymakemistakesiftheyattemptedtoworkouttheanswerdirectly. Sowecanexplicitly\naskLLMstofollowagivenreasoningprocessbeforecomingtoaconclusion.\nYou are a mathematician. You will follow these detailed reasoning\nstepswhensolvingmathproblems.\nStep1: ProblemInterpretation.\nThemathematiciancarefullylistenstoyourqueryandunderstandsthe\nintricatedetailsofthemathematicalchallengeyouhavepresented.\nStep2: StrategyFormulation.\nDrawingupontheirextensiveknowledge,themathematicianchooses\nthemosteffectivestrategytailoredtothetypeofmathproblem,whether\nitisalgebra,calculus,orgeometry.\nStep3: DetailedCalculation.",
    "Withprecisionandexpertise,themathematicianperformsthenecessary\ncalculationsstepbystep,adheringtoallmathematicalprinciples.\nStep4: SolutionReview.\nBefore providing the final answer, the mathematician meticulously\nchecksthecalculationsforaccuracyandoffersaconciseexplanation\norrationaleforthesolution.\nYouwillbeprovidedwithamathproblem. Pleasesolvetheproblem.\n{∗problem∗}\nAnothermethodtoguideLLMsto“think”isthroughmultipleroundsofinteractionwith\nLLMs. Forexample,asafirststep,wecaninstructLLMstosolvetheproblemdirectly",
    "476 Chapter 9. Prompting\nYouwillbeprovidedwithamathproblem. Pleasesolvetheproblem.\n{∗problem∗}\nNow we have an initial answer to the problem. As a second step, we prompt LLMs\nto evaluate the correctness of the answer and, if necessary, rework it to find a better\nsolution.\nYou will be provided with a math problem, along with a solution.\nEvaluate the correctness of this solution, and identify any errors if\npresent. Then,workoutyourownsolution.\nProblem: {∗problem∗}\nSolution: {∗solution∗}\nThepromptspresentedherearecloselyrelatedtoalonglineofresearchonreasoning\nproblemsinLLMs. Itisimpossibletoprovideacompletediscussionofallrelatedissues\nbecausethistopiccoversalargefamilyofmethods. Butwewillseearelativelymore\ndetaileddiscussiononhowtoimprovepromptingthroughmorereasoninginSection\n9.2.\n• Providingreferenceinformation. Asdiscussedintheprevioussection,wecaninclude\ndemonstrations in prompts and allow LLMs to in-context learn from these demon-",
    "strations how to perform the task. In fact, given the remarkable ability of language\nunderstanding of LLMs, we can add any type of text into the prompts and so these\nmodelscanpredictbasedonenrichedcontexts. Inmanyapplications,wehavevariousin-\nformationthatisrelevanttouserqueries. InsteadofusingLLMstomakeunconstrained\npredictions,weoftenwantLLMstoproduceoutputsthatareconfinedtotherelevant\ntext. OnesuchexampleisRAG,wheretherelevanttextfortheuserqueryisprovidedby\ncallinganIRsystem,andwepromptLLMstogenerateresponsesbasedonthisprovided\nrelevanttext. Thefollowingpromptshowsanexample.",
    "9.1 General Prompt Design 477\nYouareanexpertthatcangenerateanswerstoinputqueries. Youhave\nnowbeenprovidedwithaqueryandthecorrespondingcontextinfor-\nmation. Pleasegenerateananswerbasedonthiscontextinformation.\nNotethatyouneedtoprovidetheanswerinyourownwords,notjust\ncopyfromthecontextprovided.\nContextinformation: {∗IR-result∗}\nQuery: {∗query∗}\nIfthecontextinformationishighlyreliable,wecanevenrestrictLLMstoanswering\nusingonlytheprovidedtext. Anexamplepromptisshownasfollows\nYouareanexperttaskedwithgeneratinganswersfrominputqueries.\nYouhavebeenprovidedwithaqueryandcorrespondingcontextinfor-\nmation,organizedinatablewhereeachrowrepresentsausefulrecord.\nPleasegenerateananswerusingonlythiscontextinformation. Ensure\nthatyouprovidetheanswerinyourownwords.\nContextinformation: {∗table∗}\nQuery: {∗query∗}\nWhendealingwithreal-worldproblems,weoftenhavepriorknowledgeandadditional\ninformation about the problems that help produce better answers. Considering such",
    "informationinpromptingisgenerallyhelpfulinimprovingtheresult.\n• Payingattentiontopromptformats. Ingeneral,theperformanceofLLMsishighly\nsensitive to the prompts we input. Sometimes a small modification to a prompt can\nlead to a big change in model output. An interesting example is that changing the\norderofsentencesinapromptmaycauseLLMstogeneratedifferentresults. Tomake\npromptseasytoreadandreduceambiguity,itiscommontoformattheminawaythat\nensuresclarity. Oneexampleisthatwedefineseveralfieldsforpromptsandfilldifferent\ninformationineachfield. Anotherexampleiswecanusecode-stylepromptsforLLMs\nwhichcanunderstandandgeneratebothnaturallanguageandcode. Seethefollowing\nforacode-stylepromptthatperformstranslationwhereonedemonstrationispresented.",
    "478 Chapter 9. Prompting\n[English]=[Ihaveanapple.]\n[German]=[IchhabeeinenApfel.]\n[English]=[Ihaveanorange.]\n[German]=\nLLMscanreceivetextinvariousformats. Thisallowsustousecontrolcharacters,XML\ntags,andspecificformattingtorepresentcomplexdata. Anditisusefultospecifyhow\nthe input and output should be formatted or structured. For example, we can delimit\nsectionsoftextusingquotesandpromptLLMsaccordingly(e.g.,addingasentencelike\n“theinputtextisdelimitedbydoublequotes”totheprompt).\nAbove,wehavediscussedonlyafewstrategiesforwritinggoodprompts. Thereare,of\ncourse,manysuchmethods,andoneneedstodeveloptheirownthroughpractice. Interested\nreaderscanrefertovariousonlinedocumentsformoreinformation,suchasOpenAI’smanual\nontheGPTseriesmodels2.\n9.1.4 More Examples\nInthissubsection,weconsidermoreexamplesofpromptingLLMstoperformvariousNLP\ntasks. Themotivationhereisnottogivestandardpromptsforthesetasks, butrathertouse\nsimpleexamplestoillustratehowLLMscanbepromptedtodealwithNLPproblems.",
    "1. Text Classification\nText classification is perhaps one of the most common problems in NLP. Many tasks can\nbebroadlycategorizedasassigningpre-definedlabelstoagiventext. Hereweconsiderthe\npolarity classification problem in sentiment analysis. We choose polarity classification for\nillustrationbecauseitisoneofthemostpopularandwell-definedtextclassificationtasks. Ina\ngeneralsetupofpolarityclassification,wearerequiredtocategorizeagiventextintooneof\nthreecategories: negative,positive,orneutral. Belowisasimplepromptfordoingthis(for\neasyreading,wehighlightthetaskdescriptionintheprompt).\nAnalyzethepolarityofthefollowingtextandclassifyitaspositive,negative,\norneutral.\nText:\nThe service at the restaurant was slower than expected, which was a bit\nfrustrating.\nThepolarityofthetextcanbeclassifiedasnegative.\n2See https://platform.openai.com/docs/guides/prompt-engineering/\nsix-strategies-for-getting-better-results.",
    "9.1 General Prompt Design 479\nTomaketheexamplecomplete,weshowtheresponsegeneratedbytheLLM(underlinedtext).\nAlthough the answer is correct, the LLM gives this answer not in labels but in text\ndescribingtheresult. TheproblemisthatLLMsaredesignedtogeneratetextbutnottoassign\nlabelstotextandtreatclassificationproblemsastextgenerationproblems. Asaresult,weneed\nanothersystemtomaptheLLM’soutputtothelabelspace(callitlabelmapping),thatis,we\nextract“negative”from“Thepolarityofthetextcanbeclassifiedasnegative”. Thisistrivial\nin most cases because we can identify label words via simple heuristics. But occasionally,\nLLMs may not express the classification results using these label words. In this case, the\nproblembecomesmorecomplicated,asweneedsomewaytomapthegeneratedtextorwords\ntopredefinedlabelwords.\nOnemethodtoinduceoutputlabelsfromLLMsistoreframetheproblemasaclozetask.\nForexample,thefollowingshowsacloze-likepromptforpolarityclassification.",
    "Analyzethepolarityofthefollowingtextandclassifyitaspositive,negative,\norneutral.\nText:\nThe service at the restaurant was slower than expected, which was a bit\nfrustrating.\nThepolarityofthetextisnegative\nWe can use LLMs to complete the text and fill the blank with the most appropriate word.\nIdeally, we wish the filled word would be positive, negative, or neutral. However, LLMs\narenotguaranteedtogeneratetheselabelwords. Onemethodtoaddressthisproblemisto\nconstrainthepredictiontothesetoflabelwordsandselecttheonewiththehighestprobability.\nThen,theoutputlabelisgivenby\nlabel = argmaxPr(y|x) (9.1)\ny∈Y\nwherey denotesthewordfilledintheblank,andY denotesthesetoflabelwords\n{positive,negative,neutral}.\nAnothermethodofusingLLMstogeneratelabelsistoconstraintheoutputwithprompts.\nFor example, we can prompt LLMs to predict within a controlled set of words. Here is an\nexample.",
    "480 Chapter 9. Prompting\nAnalyzethepolarityofthefollowingtextandclassifyitaspositive,negative,\norneutral.\nText:\nThe service at the restaurant was slower than expected, which was a bit\nfrustrating.\nWhatisthepolarityofthetext?\nJustanswer: positive,negative,orneutral.\nNegative\nSentimentanalysisisacommonNLPproblemthathasprobablybeenwellunderstoodby\nLLMsthroughpre-trainingorfine-tuning. ThuswecanpromptLLMsusingsimpleinstructions\ntoperformthetask. However,fornewclassificationproblems,itmaybenecessarytoprovide\nadditional details about the task, such as the classification standards, so that the LLMs can\nperform correctly. To do this, we can add a more detailed description of the task and/or\ndemonstrate classification examples in the prompts. To illustrate, consider the following\nexample.\nAnalyzethepolarityofthefollowingtextandclassifyitaspositive,negative,\norneutral.Here’swhateachcategoryrepresents:\nPositive:Thisindicatesthatthetextconveysapositiveemotionorattitude.For",
    "example,textsexpressinghappiness,satisfaction,excitement,oradmiration\nareconsideredpositive.\nNegative:Thisreferstoatextthatexpressesanegativeemotionorattitude.It\nencompassesfeelingsofsadness,anger,frustration,orcriticism.\nNeutral:Neutralsentimentisusedtodescribetextsthatdonotexhibitclear\npositiveornegativeemotionsbutinsteadconveyinformational,factual,or\nindifferenttones.\nText:\nThe service at the restaurant was slower than expected, which was a bit\nfrustrating.\nWhatisthepolarityofthetext?\nNegative\nWhile it seems straightforward to use LLMs for classification problems, there are still\nissuesthathavenotbeenwelladdressed. Forexample,whendealingwithalargenumberof\ncategories, it remains challenging to effectively prompt LLMs. Note that if we face a very",
    "9.1 General Prompt Design 481\ndifficultclassificationproblemandhaveacertainamountoflabeleddata,fine-tuningLLMsor\nadopting“BERT+classifier”-likearchitecturesisalsodesirable.\n2. Information Extraction\nMany NLP problems can be regarded as information extraction problems, involving the\nidentification or extraction of specific pieces of information from unstructured text. This\ninformationcanincludenamedentities,relationships,events,andotherrelevantdatapoints.\nThe goal of information extraction is to transform raw data into a format that can be easily\nanalyzedandusedinvariousdownstreamapplications.\nAsinformationextractioncoversawiderangeofproblems,wecannotdiscussthemall\nhere. Instead, we start with the task of named entity recognition — a task that has long\nbeenaconcerninNLP.Namedentityrecognitionisaprocessthatdetectsandclassifieskey\ninformationintextintospecificgroups. Thesekeypiecesofinformation, knownasnamed",
    "entities,typicallyincludepropernamesandarecategorizedintodistinctclassessuchaspeople,\nlocations, organizations, dates, monetary values, and percentages. Consider first a simple\nexampleofextractingpersonnamesfromatext3.\nIdentifyallpersonnamesintheprovidedtext.\nText:\nIstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic?\nForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter.\n“IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto\nEurope,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof\nAmericandemandfortheUK.”\nThepersonnameidentifiedintheprovidedtextis:\nTomJenkins\nThen,wecanextractallnamedentities.\n3Thetextisfromhttps://edition.cnn.com/travel",
    "482 Chapter 9. Prompting\nIdentifyandclassifyallnamedentitiesintheprovidedtextintocategories\nsuch as person names, locations, dates, and organizations. List each entity\nwithitstypeononeline.\nText:\nIstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic?\nForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter.\n“IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto\nEurope,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof\nAmericandemandfortheUK.”\n1. TomJenkins-PersonName\n2. EuropeanTourismOrganisation-Organization\n3. UK-Location\n4. Europe-Location\n5. 2024-Date\nGiven these identified named entities, we can further process the text using other infor-\nmationextractiontasks. Forexample,wecanidentifytherelationshipsbetweenthesenamed\nentities(callitrelationextraction). Thecorrespondingpromptisshownasfollows.\nGiven a text and a list of named entities identified within it, analyze and\ndescribetherelationshipsbetweentheseentities.Explainhoweachentityis",
    "contextuallyrelatedtoothers.\nText:\nIstheUKreallydoingthatbadlyorhavetraveltrendsshifted,post-pandemic?\nForTomJenkins,CEOoftheEuropeanTourismOrganisation,it’sthelatter.\n“IthinktheUKisdoingperfectlywellbutwe’llseemorepeoplegoingto\nEurope,”hesaysof2024,addingthatthere’s“allegedlyaslightplateauof\nAmericandemandfortheUK.”\nNamedEntity:\nTomJenkins,EuropeanTourismOrganisation,UK,Europe,2024\nTom Jenkins - European Tourism Organisation: Tom Jenkins is the CEO\nof the European Tourism Organisation. This indicates a professional\nassociation where Tom Jenkins likely has a significant role in directing the\norganization’sactivities,especiallythoserelatedtotourisminEurope.\n...",
    "9.1 General Prompt Design 483\nIfLLMshavebeenfine-tunedwithinstructionfollowingforinformationextractiontasks,\nitisrelativelyeasytoperformvariousinformationextractiontasks. Forexample,thefollowing\nisaprompttemplateforinformationextraction.\nYouwillbeprovidedwithatext.Yourtaskisto{∗task-description∗}\nText: {∗text∗}\nHeretask-descriptionistheinstructionforthetask. Examplesoftheinstructioninclude\nextractthekeywordsfromthetext.\nextractthekeyeventsfromthetext.\ndetectthecoreferenceandanaphoriclinksbetweenentitiesfromthetext.\nandsoon. Ofcourse,wecanprovideamoredetaileddescriptionofthetasktopromptLLMs\nmoreaccurately.\n3. Text Generation\nInNLP,thetermtextgenerationisusedbroadlytocoverawiderangeofdifferentproblems,\nmanyofwhichfocusongeneratingtextbasedongiventext. Theseproblemscanberoughly\ncategorizedintotwoclasses: 1)continualwritingbasedontheinputtext(callittextcomple-\ntion),and2)transformationoftheinputtextintoanothertext(callittexttransformation).",
    "Here,weillustratethepromptsfortheformerandconsiderthepromptsforthelatterinthe\nfollowingdiscussion.\nA simple example of text completion is continuing to write a story starting with a few\ngivensentences.\nYouareawriterwhoisgreatatwritingstories.Youareprovidedwithafew\nsentencestostartwith.Pleasecontinuethestoryfromtheprovidedsentences.\nStartofthestory:\nOnesunnyafternoon,Lilydiscoveredatiny,goldenkeyhiddenunderarock\ninherbackyard. Curious,shepickeditupandwonderedwhatitmightunlock.\nJustthen,hercat,Whiskers,begantomeowexcitedly,leadinghertowards\ntheoldoaktreeattheedgeofthegarden.\nLily followed Whiskers, her heart pounding with excitement and a touch\nof nervousness. The old oak tree had always seemed mysterious to her, its\ngnarled branches casting long shadows even on the brightest days. As they\nreached the tree, Whiskers darted ahead and started pawing at something\nburiedbeneathapileofautumnleaves...",
    "484 Chapter 9. Prompting\nLikewise,wecancreateaconversationgiventheconversationalcontext.\nYou are provided with a conversation between a tourist and a taxi driver.\nPleasecompletetheconversation.\nTourist: Couldyoutakemetothedowntownmuseum,please?\nTaxiDriver: Ofcourse! Firsttimeinthecity?\nTourist: Yes,anymust-seeplacesaround?\nTaxi Driver: Definitely check out the botanical gardens after the museum.\nThey’renearbyandquitebeautiful.\nTourist: Great,thanksforthetip!\nTaxiDriver: Noproblem. Enjoyyourvisit!\nAnotherapplicationoftextcompletionisgeneratingtextbasedonuserrequirements. The\nrequirementscanincludestyle,tone,length,andanyspecificcontentthatthetextshouldcon-\ntain. Forexample,usersmayneedaformalreport,acreativestory,orapieceofprogramming\ncode. Hereisapromptforgeneratingafive-characterregulatedChinesepoem.\nYouareapoet.PleasewriteatraditionalChinesepoem.Hereistherequire-\nment.\nTheme: Spring’srejuvenation\nStructure: Five-characterregulatedpoem",
    "Emotion: Convey a sense of happiness and renewal through vivid spring\nimagery.\n春意盎然\n翠柳风中舞，\n红花雨后新。\n山明水又绿，\n天宽鸟自频。\nIftheLLMistrainedtogeneratelanguageandcode,wecanpromptittoperformcode\ncompletiontasks. Hereisanexample.",
    "9.1 General Prompt Design 485\nPleasewriteaPythonfunctiontocalculatetheaverageofalistofnumbers.\ndefcalculate_average(numbers):\nifnumbers: #Checkifthelistisnotempty\nreturnsum(numbers)/len(numbers)\nelse:\nreturn0 #Return0ifthelistisempty\n4. Text Transformation\nTypicaltexttransformationtasksincludemachinetranslation,summarization,textstyletransfer,\nandsoon. Thepromptsusedtoperformthesetasksarenotsignificantlydifferentfromthe\nonespresentedpreviously. Here,wepresentafewexamplestoshowcasetheseprompts.\nTranslatethefollowingtextfromEnglishtoSpanish.\nText:\nThequickbrownfoxjumpsoverthelazydog.\nElrápidozorromarrónsaltasobreelperroperezoso.\nSummarizethefollowingarticleinnomorethan50words:\nArticle:\nInrecentyears,urbanareashavebeenfacingsignificantchallengesrelatedto\nsustainabilityandenvironmentalmanagement. Citiesareexperiencinghigher\nratesofpollution,increasedtrafficcongestion,andgreaterdemandsoninfras-\ntructure. Thisgrowthhasledtonumerousenvironmentalissues,including",
    "elevatedlevelsofairandwaterpollution,increasedwasteproduction,and\nstrainedpublicservices...\nUrban areas are grappling with sustainability challenges, such as rising\npollution,trafficcongestion,andinfrastructuredemands...",
    "486 Chapter 9. Prompting\nRewritethistextinaformaltone.\nText:\nHey,what’sup? Longtimenosee!\nHello,howhaveyoubeen? Ithasbeenquitesometimesincewelastmet!\n5. Question Answering\nThequestion-answeringformatisinherentlysimple. Foragivenquestion,thereisananswer\nthat corresponds to it. For example, in open-domain question answering, we expect the\nsystem to return an answer in response to a user-submitted question. Prompt templates for\ngeneral-purposequestionansweringcanbe:\n{∗question∗}\nQuestion: {∗question∗}\nAnswer:\nQ:{∗question∗}\nA:\nQuestionansweringisimportantinNLPbecausemanyproblemscanbeframedasquestion-\nansweringtasks. Inparticular,manyrecentreasoningtasksaredefinedintheformofquestion\nanswering. Forexample,intheMMLUbenchmark[Hendrycksetal.,2021],eachexample\nconsistsofamultiple-choicequestion,andLLMsarerequiredtoselectthecorrectanswer. See\nthefollowingforanexamplepromptforansweringaquestioninthisdataset.",
    "9.1 General Prompt Design 487\n(Z,∗)isagroupwitha∗b=a+b+1foralla,binZ. Theinverseofais\n(A) 0\n(B) −2\n(C) a−2\n(D) (2+a)∗−1\nD\nAnotherwidely-usedbenchmarkistheGSM8Kdataset[Cobbeetal.,2021]. Itconsistsof\nthousandsofgradeschoolmathwordproblems. Foreachproblem,weuseLLMstoreturn\na solution in natural language. For example, the following is a prompt used for solving a\nprobleminGSM8K.",
    "488 Chapter 9. Prompting\nDEMO Q:Jerry’stwodaughtersplaysoftballondifferentteams. Theyeach\nhave8gamesthisseason. Eachteampractices4hoursforeverygame\ntheyplay. Ifeachgamelastsfor2hours,howmanyhourswillJerry\nspendatthefieldwatchinghisdaughtersplayandpracticealtogether?\nA:Jerrywillspend8games*2hourspergame=≪8∗2=16≫16\nhourswatchingonedaughterplayhergames. Hewillspend16∗2=≪\n16∗2=32≫32 hours watching both daughters play their games.\nHe will spend 8 games * 4 hours of practice = ≪8∗4=32≫32\nhours watching one daughter practice. He will spend 32∗2 =≪\n32∗2=64≫64 hours watching both daughters practice. He will\nspendatotalof32hourswatchinggames+64hourswatchingpractice\n=≪32+64=96≫96hours. ####96\nQ:Maryboughtsixapplesfromthestore. Fromtheapplesshebought,\nforeachthatMaryate,sheplantedtwotreesfromtheremainingones.\nHowmanyapplesdidMaryeat?\nA:Sheplantedeighttrees. Thismeanssheusedhalfofthatamountof\napples,whichis8trees/2trees/apple=≪8/2=4≫4apples. That",
    "meansthatsheplantedfourofthesixapplesshebought,leavingonly\n6apples−4apples=≪6−4=2≫2applestobeeaten. ####2\nUSER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. If\nBorisgivesBeck10apples,howmanyfewerapplesdoesBeckhave\nthanBorisnow?\nA: Boris starts with 100 apples and gives Beck 10 apples, leaving\nhim with 100 apples −10 apples =≪100−10=90≫90 apples.\nInitially, Beck has 23 fewer apples than Boris, giving him a\nstarting count of 100−23=≪100−23=77≫77 apples. After\nreceiving10applesfromBoris,Beck’stotalcomesto77apples+10\napples=≪77+10=87≫87apples. Consequently,Becknowhas\n90−87=≪90−87=3≫3fewerapplesthanBoris. ####3\nHereafew-shotpromptisadopted. TheLLMlearnsfromthesedemonstrationsofproblem-\nsolutionpairsnotonlythewayofproblem-solvingbutalsothewayofformattingtheoutput.\nForexample,thefinalresultofcalculationfollowsthe####token,and≪...≫annotatesthe\ndetailedcalculationsteps(calledcalculationannotation)4.",
    "4Duringprediction,acalculatorisusedwhenwesee≪...≫.Morespecifically,oncetheLLMencounters“=”\nina≪...≫,thenthecalculatorcalculatestheexpressionontheleft-handsideof“=”.Thismethodhelpsreduce\nthecalculationerrorsmadebyLLMs.",
    "9.2 Advanced Prompting Methods 489\n9.2 Advanced Prompting Methods\nSo far in this chapter, we have introduced the basic concepts related to LLM prompting\nandpresentedanumberofpromptsforNLPtasks. Wenowconsiderseveraltechniquesfor\nenhancingtheeffectivenessofprompting.\n9.2.1 Chain of Thought\nWe have encountered the concept of chain of thought (CoT) several times in this chapter\nand previous ones [Wei et al., 2022c; Chowdhery et al., 2022]. CoT methods provide a\nsimplewaytopromptLLMstogeneratestep-by-stepreasoningforcomplexproblems,thereby\napproachingtasksinamorehuman-likemanner. Ratherthancomingtoaconclusiondirectly,\ntheCoTmethodsinstructLLMstogeneratereasoningstepsortolearnfromdemonstrations\nofdetailedreasoningprocessesprovidedintheprompts. ToillustrateCoT,weconsiderthe\nproblem of algebraic calculation, as commonly described in the literature. Suppose we are\ngivenanalgebraicproblem\nCalculatetheaverageofthenumbers2,4,and6.\nWecanconsideritasthequestionandpromptanLLMtoanswerit.",
    "Q:Pleasecalculatetheaverageofthenumbers2,4,and9.\nA:Theansweris6.\nItseemsdifficultfortheLLMtodirectlygiveacorrectanswer. Asimpleimprovementis\ntoadddemonstrationsofsimilarproblemsintheprompt,andthustheLLMcanlearnfrom\nthesedemonstrations.\nQ:Pleasecalculatetheaverageofthenumbers1,3,5,and7.\nA:Theansweris4.\nQ:Pleasecalculatetheaverageofthenumbers2,4,and9.\nA:Theansweris7.\nTheproblemhereisthat,althoughwehaveshownasimilarquestion-answerpair,itremains\ndifficultfortheLLMtoreasonoutthecorrectanswer. InCoT,notonlycanLLMslearnfrom\nthe correspondence between questions and answers but they may gain more from detailed\nproblem-solvingstepsthatareusedtoderivetheanswers. Todothis,wecanincorporatesome\nreasoningstepsintotheprompttoobtainaCoTprompt.",
    "490 Chapter 9. Prompting\nQ:Pleasecalculatethemeansquareofthenumbers1,3,5,and7.\nA:Calculatethesquareofeachnumber:12=1,32=9,52=25,and72=49.\nSumthesquares,1+9+25+49=84.Thereare4numbersintotal.Divide\nthesumbythenumberofitems,84/4=21.Theansweris21.\nQ:Pleasecalculatetheaverageofthenumbers2,4,and9.\nA:Calculate2+4+9,whichequals15.Therearethreenumbers.Dividethe\ntotalsumbythecount,resultingin15/3=5.Theansweris5.\nHerewehighlightthereasoningstepsingreen. Byprovidingadetailedreasoningprocessfor\nasimilarproblem,theLLMlearnstoreasonandtherebygeneratesaproblem-solvingpaththat\nleadstothecorrectanswer.\nThereareseveralbenefitsofusingCoTprompting. First,CoTallowsLLMstodecompose\ncomplex problems into smaller, sequential reasoning steps. This somewhat mirrors human\nproblem-solvingbehaviors,makingitparticularlyeffectivefortasksrequiringdetailed,multi-\nstepreasoning. Second,CoTmakesthereasoningprocessmoretransparentandinterpretable.\nSinceallreasoningstepsarevisible,wecanunderstandandinterprethowaconclusionwas",
    "reached. Third,ifuserscanfollowthelogicbehindthereasoningprocess,theywillbemore\nlikelytotrustthepredictionsofanLLM.ThisisparticularlyimportantwhenapplyingLLMs\ninfieldslikemedicine,education,andfinance. Fourth,CoTisanin-contextlearningapproach,\nandthus,itisapplicabletomostwell-trained,off-the-shelfLLMs. Moreover,CoTprovides\nefficientwaystoadaptLLMstodifferenttypesofproblems. Itcaneveninspiremorecreative\nsolutionsbyexploringvariousalternativereasoningpaths,whichmightnotbeobviouswhen\narrivingataconclusiondirectly.\nThemethoddescribedaboverequiresprovidingoneormoreexamplesofCoTreasoning,\ntypicallycalledthefew-shotCoTmethod. Bycontrast,thezero-shotCoTmethoddoesnot\nrequire such examples. It instead prompts LLMs to reason step-by-step by incorporating\nspecificinstructionsinprompts. Forexample,belowisazero-shotCoTprompt.\nQ:Pleasecalculatetheaverageofthenumbers2,4,and9.\nA:Let’sthinkstep-by-step.\nWe have three numbers: 2, 4, and 9. Add these numbers together,",
    "2+4+9=15. Determinehowmanynumbersthereare,whichinthiscase\nis three. The average is calculated by dividing the total sum by the number\nofelements. Completingthedivisiongives15/3=5. Sotheansweris5.\nFollowing the instruction “Let’s think step by step”, the LLM is prompted to generate\ndetailedreasoningsteps. AsdiscussedinKojimaetal.[2022]’swork,promptingwithsuch\ninstructionsmayresultinLLMsgeneratingonlythereasoningstepswithoutaclearconclusion.\nInthiscase,asecondroundofpromptingcanbeusedtoextracttheanswerfromthesereasoning",
    "9.2 Advanced Prompting Methods 491\nsteps. For example, Kojima et al. [2022] create a second prompt which combines both the\ninput and output in the first round of prompting. Using this combined input, the LLM can\ncontinueitsreasoningprocessandthengeneratethecorrectanswer. Furthermore,itispossible\nto prompt LLMs to reason using instructions other than “Let’s think step by step”, such as\n“Let’sthinklogically”and“Pleaseshowmeyourthinkingstepsfirst”.\nWhile we have illustrated CoT methods using an algebraic reasoning problem, these\nmethodscanbeappliedtoavarietyofdifferentproblems. Typicalproblem-solvingscenarios\nforCoTincludemathematicalreasoning,logicalreasoning,commonsensereasoning,symbolic\nreasoning,codegeneration,andsoon. SeeFigure9.1formoreexamplesofapplyingCoTin\nvarioustasks.\nCoTtodayisoneofthemostactivefieldsofpromptengineering. Thishasnotonlyled\nto improved performance for LLM prompting but has opened the door to a wide range of",
    "methodsforstudyingandverifyingreasoningcapabilitiesofLLMs. Althoughwehavefocused\non the basic idea of CoT in this section, it can be improved in several ways. For example,\nwe can consider the reasoning process as a problem of searching through many possible\npaths, each of which may consist of multiple intermediate states (i.e., reasoning steps). In\ngeneral, we wish the search space to be well-defined and sufficiently large, so that we are\nmore likely to find the optimal result. For this reason, an area of current LLM research is\naimedatdesigningbetterstructuresforrepresentingreasoningprocesses,allowingLLMsto\ntacklemorecomplexreasoningchallenges. Thesestructuresincludetree-basedstructures[Yao\netal.,2024],graph-basedstructures[Bestaetal.,2024],andsoon. Byusingthesecompact\nrepresentationsofreasoningpaths,LLMscanexploreawiderrangeofdecision-makingpaths,\nanalogoustoSystem2thinking5. AnotherlineofresearchfocusesonpromptingLLMswith",
    "multi-roundinteractions. Thisinvolvesdecomposingcomplexproblemsintosub-problems,\nverifyingandrefiningmodeloutputs,employingmodelensembling,andsoon. Notethatthese\nmethodsandtheissuesinvolvedarenotlimitedtoCoT.Infact,theyareoftenusedasmore\ngeneralapproachestoimprovingLLMs,whileCoTcanbeseenasawaytotestthecapabilities\nofLLMs. Wewillseediscussionsofsomeoftheseissuesinthefollowingsubsections.\nBeforeleavingourdiscussionofCoT,weshouldconsideritspracticallimitations. Oneof\nthemistheneedfordetailed,multi-stepreasoningdemonstrationsinfew-shotCoTscenarios,\nwhichmaybedifficulttoobtain,eitherautomaticallyormanually. Also,thereisnostandard\nmethodforbreakingdowncomplexproblemsintosimplerproblem-solvingsteps. Thisoften\nheavily depends on the user’s experience. In addition, errors in intermediate steps can also\naffecttheaccuracyofthefinalconclusion. ForfurtherdiscussionontheprosandconsofCoT,\ntheinterestedreadercanrefertorecentsurveysonthistopic[Chuetal.,2023;Yuetal.,2023b;\nZhangetal.,2023a].",
    "5System1andSystem2thinking,asdescribedbyKahneman[2011],representtwodifferentmodesofcognitive\nprocessing.System1isfast,automatic,intuitive,andemotional.Thismodeofthinkingoperateseffortlesslyand\nquickly,andisoftenwhatguidesourdailydecisions,judgments,andimpressions.System2isslow,deliberate,\nandanalytical.Itisactivatedwhenweneedtoperformcomplexcomputations.",
    "492 Chapter 9. Prompting\nQ: What home entertainment equipment Q:Dohamstersprovidefoodforanyani-\nrequirescable? mals?\nAnswerChoices: (a)radioshack(b)sub- A: Hamsters are prey animals. Prey are\nstation(c)television(d)cabinet foodforpredators.Thus,hamstersprovide\nA:Theanswermustrequirecable.Ofthe foodforsomeanimals.Sotheansweris\nabovechoices,onlytelevisionrequiresca- yes.\nble.Sotheansweris(c).\nQ:CanyouuseMicrosoftOfficewithout\nQ:Whattypeofpersontypicallycontracts internet?\nillness? A:Yes,youcanuseMicrosoftOfficeappli-\nAnswerChoices:(a)hospital(b)head(c)\ncations,althoughfeatureslikecloudstor-\nsickperson(d)elderlyperson(e)doctor’s\nageandonlinecollaborationrequireinter-\noffice\nnet.Sotheanswerisyes.\nA: Theanswermustbeapersonsuscepti-\nbletoillness.Ofthechoices,\"elderlyper\nson\"iscorrectbecausetheytypicallyhave\nweakerimmunesystems.\nSotheansweris(d).\n(a)CSQA (b)StrategyQA\nQ:Completetherestofthesequence,mak- Q: Take the last letters of the words in",
    "ing sure that the parentheses are closed \"ElonMusk\"andconcatenatethem.\nproperly.Input:[{ A:Thelastletterof\"Elon\"is\"n\".Thelast\nA:Let’sthinkstepbystep. letterof\"Musk\"is\"k\".Concatenatingthem\n0:emptystack is\"nk\".Theanswerisnk.\n1:[;stack:[ Q: Take the last letters of each word in\n2:{;stack:[{ \"Renata Mariela Mona Kristin\" and con-\nSotheansweris}]. catenatethem.\nQ:Completetherestofthesequence,mak- A:Thelastletterof\"Renata\"is\"a\".The\ning sure that the parentheses are closed lastletterof\"Mariela\"is\"a\".Thelastlet-\nproperly.Input:<[[ terof\"Mona\"is\"a\".Thelastletterof\nA:Let’sthinkstepbystep. \"Kristin\"is\"n\".Concatenatingthemis\n0:emptystack \"aaan\".Theanswerisaaan.\n1:<;stack:<\n2:[;stack:<[\n3:[;stack:<[[\nSotheansweris]]>.\n(c)Dycklanguages (d)LastLetterConcatenation\nFigure9.1: CoTinfourdifferentreasoningtasks,includingCSQA,StrategyQA,Dycklan-\nguages,andLastLetterConcatenation. TheCoTpartsarehighlightedingreen.\n9.2.2 Problem Decomposition",
    "We have seen that LLMs can benefit from solving a complex problem by breaking it down\nintosimplerproblem-solvingtasks. Suchanapproachcanbeseenasanexampleofabroader\nparadigmknownasproblemdecomposition,whichhasbeenextensivelyexploredanddis-",
    "9.2 Advanced Prompting Methods 493\ncussed in psychology and computer science. From the psychological perspective, complex\nproblem-solvingreferstoaprocessofaddressingaproblemusingknowledgethathelpsover-\ncomethebarriersoftheproblem6. Therearegenerallynostandardorclearpathstoasolution\nforacomplexproblem. However,itisoftenadvantageoustoemploystrategiesthatdecompose\ntheproblem,therebymakingiteasiertotacklethecorrespondingsub-problemswithlesseffort.\nForexample,considerwritingablogabouttherisksofAI.IfwesimplypromptanLLMwith\ntheinstruction“PleasewriteablogabouttherisksofAI”,theLLMmaygenerateablogwith\narbitrarystructuresandwritingstyles. Abettermethod,instead,couldbetooutlinetheblog\nandprovidemoredetailedinformationabouteachsection. Considerthefollowingprompt\nYou are a blog writer. Please follow the provided outline below to write a\nblogabouttherisksofAI.\n• Introduction\nIntroduce AI, its relevance, and the importance of understanding its risks for\nyouth.\n• PrivacyConcerns",
    "DiscusshowAImightcompromisepersonalprivacythroughinteractionsonline.\n• Misinformation\nExploreAI’sroleinspreadingmisinformationandinfluencingyoungpeople’s\ndecisions.\n• Cyberbullying\nHighlighthowAItoolscanbeutilizedincyberbullyingandtheimpactonmental\nhealth.\n• TipsforSafeAIUse\nOfferguidelinesforresponsibleAIusageandpromotecriticalthinking.\n• Conclusion\nRecapmainpointsandencourageproactiveengagementwithAIethics.\nHerewegivethetitleandmajorpointsforeachsection. Then,theLLMcanusethisstructure\ntobreakdownthewritingtaskbyfillingincontentforthesesections. Notethatthewayto\nstructuretheblogcanbeprovidedbyhumansorevengeneratedautomatically. Forexample,\nwe can use the LLM to first generate the outline, and then ask it to follow this outline to\ncompletethewriting.\nIncomputerscience,decomposingcomplexproblemsisacommonlyusedstrategyinsoft-\nwareandhardwaresystemdesign. Awell-knownexampleisthedivide-and-conquerparadigm,",
    "which is often used to design algorithms for computation problems that can be reduced to\nsimpler,moremanageableproblems. Forexample,consideraproblemofdeterminingwhether\n6ArelativelyformaldefinitioncanbefoundinFrenschandFunke[2014]’sbook:complexproblem-solving\noccurstoovercomebarriersbetweenagivenstateandadesiredgoalstatebymeansofbehavioraland/orcognitive,\nmulti-stepactivities.",
    "494 Chapter 9. Prompting\nadocumentdiscussestherisksofAI.WecaninstructtheLLMwiththefollowingprompt.\nYouareprovidedwithatext. Pleasedeterminewhetheritdiscussestherisks\nofAI.\n{∗document∗}\nIfthedocumentislong,thecomputationwillbeexpensive. Alternatively,wecandivide\nthedocumentintorelativelyshortsegmentsandperformthesametaskoneachsegment. These\nsegments can be processed in parallel to further reduce the computational cost. Next, we\ndetermine the relevancy of each segment to the topic of AI risks. The final output is then\ngeneratedusinganotherprompt.\nYourtaskistodeterminewhetheratextdiscussestherisksofAI.Thistext\nhasbeendividedintosegments,andyouhaveobtainedtherelevancyofeach\nsegment to the topic of AI risks. Based on this, please provide your final\nresult.\nSegment1: {∗relevancy-to-the-topic1∗}\nSegment2: {∗relevancy-to-the-topic2∗}\nSegment3: {∗relevancy-to-the-topic3∗}\n...\nNowletusreturntoamoregeneraldiscussionofproblemdecompositioninprompting.",
    "While problem decomposition can be applied to various NLP problems, it has been more\nextensively discussed and tested in reasoning tasks recently. For complex reasoning tasks,\nweoftenneedamulti-stepreasoningpathtoreachacorrectconclusion. WecanuseLLMs\nto achieve this in three different ways. First, LLMs can directly reach the conclusion. In\notherwords,theycanpredictwithoutexplicitreasoningprocesses,andthereisahiddenand\nuninterpretablereasoningmechanism. Second,LLMsarepromptedtogenerateamulti-step\nreasoningpaththatleadstotheconclusion,likeCoT.However,werunLLMsjustonce,andall\nintermediatestepsinreasoningaregeneratedinasingleprediction. Third,webreakdownthe\noriginalproblemintoanumberofsub-problems,whichareeitheraddressedinseparateruns\nofLLMsortackledusingothersystems. Herewefocusourattentiononthethirdapproach,\nwhichiscloselyrelatedtoproblemdecomposition. Note,however,thatamorecomprehensive\ndiscussioncouldcoveralltheseapproaches,whilethefirsttwohavebeendiscussedtosome\nextentinthischapter.",
    "Ageneralframeworkforproblemdecompositioninvolvestwoelements.",
    "9.2 Advanced Prompting Methods 495\n• Sub-problemGeneration. Thisinvolvesdecomposingtheinputproblemintoanumber\nofsub-problems.\n• Sub-problemSolving. Thisinvolvessolvingeachsub-problemandderivingintermedi-\nateandfinalconclusionsthroughreasoning.\nThese two issues can be modeled in different ways, leading to various problem decom-\npositionmethods. Oneapproachistotreatthemasseparatestepsinatwo-stepprocess. For\nexample,considertheblogwritingtaskdescribedatthebeginningofthissubsection. Inthe\nfirst step, we decompose the entire problem into sub-problems all at once (i.e., outline the\nblog). Inthesecondstep,wesolvethesub-problemseithersequentiallyorinanotherorder\n(i.e.,fillincontentforeachsectionasneeded). Thefinaloutputofthisprocesscombinesthe\nresultsfromsolvingeachsub-problem. Whilethismethodissimpleandstraightforward,it\nassumesthattheproblemiscompositional,makingitmoresuitablefortaskslikewritingand\ncodegeneration.\nHowever,manyreal-worldproblemsrequirecomplexreasoning. Onekeycharacteristic",
    "oftheseproblemsisthatthereasoningstepsmaynotbefixed. Thereasoningpathcanvary\nfor different problems, and each step of reasoning may depend on the outcomes of prior\nsteps. Insuchcases,itisundesirabletousefixedsub-problemgenerationinadvance. Instead,\nsub-problemsshouldbegenerateddynamicallybasedontheinputproblem,and,ifpossible,\ngeneratedontheflyduringthereasoningprocess. Thismakesproblemdecompositionmore\nchallengingcomparedwithdesigningdivide-and-conqueralgorithms. Ideally,wewouldlike\ntojointlydesignboththesystemsforsub-problemgenerationandsub-problemsolving. But\na more practical and widely used approach is to adopt separate models for these tasks. A\nstraightforwardwaytoachievethisistoadaptanLLMforthesetasksbyeitherpromptingor\ntuningthemodel.\nHere we consider a method based on the above idea, called least-to-most prompting\n[Zhou et al., 2023b]. The motivation for this method arises from the challenges of solving",
    "difficultreasoningproblems—thosethatcannotbeaddressedbysimplygeneralizingfrom\nafewexamples. Fortheseproblems,amoreeffectiveproblem-solvingstrategyistofollow\na progressive sequence of sub-problems that systematically lead to the conclusion. More\nspecifically,intheleast-to-mostpromptingmethod,sub-problemgenerationisperformedby\npromptinganLLMwithinstructionsand/ordemonstrations. Forexample,belowisa2-shot\npromptforsub-problemgenerationinleast-to-mostprompting.",
    "496 Chapter 9. Prompting\nTASK Your task is to decompose a problem into several sub-problems. You\nwillbegivenafewexamplestoillustratehowtoachievethis.\nDEMO Q:Inacommunity,5%ofthepopulationareinfants,15%arechildren,\n40%areadults,and40%areseniors. Whichgroupmakesupthelargest\nportionofthepopulation?\nA:Toanswerthequestion“Whichgroupmakesupthelargestportionof\nthepopulation?”,weneedtoknow: “Howmanypercentareinfants?”,\n“How many percent are children?”, “How many percent are adults?”,\n“Howmanypercentareseniors?”.\nQ:Alice,Bob,andCharliebroughtbeadsfortheirgroupprojectintheir\ncraft class. Alice has twice as many beads as Bob, and Bob has five\ntimesasmanybeadsasCharlie. IfCharliehas6beads,howmanybeads\ncantheyusefortheircraftproject?\nA:Toanswerthequestion“Howmanybeadscantheyusefortheircraft\nproject?”,weneedtoknow: “HowmanybeadsdoesBobhave?”,“How\nmanybeadsdoesAlicehave?”.\nUSER Q:Theenvironmentalstudyconductedfrom2015to2020revealedthat\ntheaveragetemperatureintheregionincreasedby2.3degreesCelsius.",
    "Whatwasthedurationoftheenvironmentalstudy?\nA:Toanswerthequestion“Whatwasthedurationoftheenvironmental\nstudy?”,weneedtoknow: “Whendidtheenvironmentalstudystart?”,\n“Whendidtheenvironmentalstudyend?”.\nBylearningfromtheexamples,theLLMcangeneratetwosub-problemsforanswering\nthenewproblem“Whatwasthedurationoftheenvironmentalstudy?” (highlightedinblue\nandorange). Giventhesesub-problems,wesolvethemsequentially. Foreachsub-problem,\nwetakeallpreviously-generatedQApairsascontext,andthenproducetheanswer. Forthe\nexampleabove,weneedtoanswerthefirstsub-problembypromptingtheLLM,likethis\nThe environmental study conducted from 2015 to 2020 revealed\nthattheaveragetemperatureintheregionincreasedby2.3degrees\nCelsius.\nSUB-PROB1 Q:Whendidtheenvironmentalstudystart?\nA:Theenvironmentalstudystartedin2015.\nOnce we have the answer to the first sub-problem, we proceed to the second one. This\ntime,weincludeboththefirstsub-problemanditscorrespondinganswerintheinput.",
    "9.2 Advanced Prompting Methods 497\nThe environmental study conducted from 2015 to 2020 revealed\nthattheaveragetemperatureintheregionincreasedby2.3degrees\nCelsius.\nSUB-PROB1 Q:Whendidtheenvironmentalstudystart?\nA:Theenvironmentalstudystartedin2015.\nSUB-PROB2 Q:Whendidtheenvironmentalstudyend?\nA:Theenvironmentalstudyendedin2020.\nFinally, we use the LLM to solve the original problem given the answers to all the sub-\nproblems.\nThe environmental study conducted from 2015 to 2020 revealed\nthattheaveragetemperatureintheregionincreasedby2.3degrees\nCelsius.\nSUB-PROB1 Q:Whendidtheenvironmentalstudystart?\nA:Theenvironmentalstudystartedin2015.\nSUB-PROB2 Q:Whendidtheenvironmentalstudyend?\nA:Theenvironmentalstudyendedin2020.\nFINAL Q:Whatwasthedurationoftheenvironmentalstudy?\nA:Thedurationoftheenvironmentalstudywas5years.\nThe least-to-most method offers a basic approach to prompting LLMs to generate and\nsolvesub-problemsseparately. Wecanimproveitinseveralways. Onesimpleimprovement",
    "is to apply various advanced prompting techniques, which do not require changes to the\nproblemdecompositionframework. Forexample,wecanincorporateCoTintotheprompting\ntoenhancethereasoningperformanceofsub-problemgenerationandsolving.\nAnotherimprovementistoexploremethodsforbetterdecomposingproblemsandorganiz-\ningproblem-solvingpaths. Todescribetheseapproaches,wewillusethesymbolp todenote\n0\ntheinputproblem,andusethesymbols{p ,...,p }todenotethesub-problemscorresponding\n1 n\ntop . Forleast-to-mostprompting,wedecomposep into{p ,...,p },givenby\n0 0 1 n\n{p ,...,p } = G(p ) (9.2)\n1 n 0\nwhereG(·)denotesthefunctionofsub-problemgeneration. Then,wesolvethesub-problems\n{p ,...,p }sequentially,resultinginasequenceofanswers{a ,...,a }. Foransweringthe\n1 n 1 n\ni-thsub-problemp ,weincludeboththeoriginalproblemp andallpreviously-seenproblem-\ni 0",
    "498 Chapter 9. Prompting\nanswerpairsinthecontextforprediction. Theanswera isgivenby\ni\na = S (p ,{p ,p ,a }) (9.3)\ni i i 0 <i <i\nwherep ={p ,...,p }anda ={a ,...,a }. S (·)denotesthefunctionthatsolvesthe\n<i 1 i−1 <i 1 i−1 i\nsub-problemp giventhecontext{p ,p ,a }. Thelaststepistogeneratetheanswertothe\ni 0 <i <i\noriginalproblemp ,whichcanbeexpressedinasimilarmannertoEq. (9.3).\n0\na = S (p ,{p ,a }) (9.4)\n0 0 0 ≤n ≤n\nOnewaytorefinethismodelistomodifytheG(·)functionsothatthemodelcandynami-\ncallygenerateanswers. Insteadofgeneratingallsub-problemsatonetime,wecangenerate\neachofthemduringproblem-solving[Duaetal.,2022]. Todothis,wecanreplaceEq. (9.2)\nwith\np = G (p ,{p ,a }) (9.5)\ni i 0 <i <i\nHenceweobtainasub-problemgenerationmodelthatoperatesinastep-by-stepmanner. At\neach step i, we first generate the sub-problem p by prompting an LLM with the original\ni\nproblemp andtheproblem-solvinghistory{p ,a }. Wethengeneratetheanswera for\n0 <i <i i",
    "thissub-problemusingthesameoradifferentLLM,basedonthesamecontextualinformation\n(seeEq. (9.3)). ThismethodeffectivelyexpandsthereasoningcapacityofLLMsbyallowing\nthemtodynamicallygenerateandsolvesub-problemsinintermediatereasoningsteps. Asa\nresult,thereasoningpathsarenotfixedinadvance,andthemodelscanchooseandadapttheir\nreasoningstrategiesduringproblem-solving.\nAnotherwaytoimprovetheabovemodelistofocusondevelopingbettersub-problem\nsolvers. Inourpreviousdiscussion,werestrictedS (·)toLLMsthatarepromptedtosolvethe\ni\nsub-problemp . Infact,wecanexpandthisfunctiontoanysystemthatiscapableofaddressing\ni\nthe sub-problem. For example, S (·) could make calls to IR systems, thereby allowing us\ni\nto access a broaderrange ofdata forproblem-solving. Another example isusing S (·)as a\ni\ncalculatortoaccuratelycomputeresultsinmathematicalproblem-solving. Ifthesub-problem\np is complex and requires multiple intermediate problem-solving steps, it is also possible\ni",
    "to further decompose p into smaller sub-problems. For example, S (·) can be defined as a\ni i\nrecursiveprogramthatgeneratesandsolvessub-problems. Thisincorporatesrecursioninto\nproblem-solvingandallowsustoaddressproblemsbyiterativelydecomposingthem. Asa\nresult,wecandefineahierarchicalstructureforproblem-solving[Khotetal.,2023].\nIfwegeneralizetheaboveformulationabitfurther,wecanconsideritasareinforcement\nlearning problem. A typical method is to model a problem-solving process as a decision\nmaking process. In each step of this process, an action is taken based on the current state.\nTheseactionscanincludeallfunctionsforsub-problemgenerationandsolving(i.e.,G (·)and\ni\nS (·)). Thus,theactionsequencecorrespondstoaproblem-solvingpath. Sincethediscussion\ni\nofreinforcementlearningproblemsisbeyondthescopeofthischapter,weskiptheprecise\ndescriptionofthislearningtask. Nevertheless,developinganagentorcontrollertodetermine",
    "9.2 Advanced Prompting Methods 499\nwhenandhowtogenerateandsolveasub-problemisalsoanaturalchoice.\nInNLP,problemdecompositionisrelatedtoalonglineofresearchonmulti-hopquestion\nanswering[Mavietal.,2024]. Thistaskrequiresthesystemtogatherandcombineinformation\nfrommultiplepiecesoftexttoprovideanaccurateanswertoacomplexquestion. Forexample,\ntoanswerthequestion“WhatisthecapitalofthecountrywhereAlbertEinsteinwasborn?”,\nweneedtoknow“WhereAlbertEinsteinwasborn?” and“What’sthecapitalofGermany?”.\nEarlierworkinthisareaandrelatedoneshasinvestigatedtheissueofproblemdecomposition,\nthoughthemethodsmightnotbebasedonLLMs. Forexample,apopularmethodistodevelop\nanadditionalneuralmodeltogeneratesimplerquestionsthataddressdifferentaspectsofthe\noriginal question [Andreas et al., 2016; Talmor and Berant, 2018; Min et al., 2019]. This\nquestiongeneratorcancreatequestionsinabatchorsequentialmanner.\nBroadlyspeaking,problemdecompositionisalsorelatedtothecompositionalityissuein",
    "NLP [Drozdov et al., 2022; Press et al., 2023]. For example, in semantic parsing, we map\nnaturallanguagesentencesintostructuredmeaningrepresentationsbybreakingthemdown\nintoconstituentpartsandunderstandingthesentencesbasedonthemeaningsofthesepartsand\ntherulesusedtocombinethem. Inearlystudiesofthisfield,highlycompositionalsentences\nwere considered easier for testing systems, as it is relatively straightforward to decompose\nsuchsentencesandcomposethemeaningsoftheirparts. However,thetaskbecomesmuch\nmoredifficultwhenmoregeneralizationisrequiredformodelingcompositionalityinnewdata.\nIn this case, we want systems to have improved abilities of compositional generalization.\nInmorerecentresearchonLLMs,thisissuehasbeenfrequentlydiscussedincompositional\nreasoningtasks,suchasSCAN7,asitisconsideredanimportantaspectoftestingthelanguage\nunderstandingandreasoningabilitiesofLLMs. Thisalsopresentsnewtasksfordeveloping\nandexaminingproblemdecompositionmethods.",
    "InLLMs,oneinterestingapplicationofproblemdecompositionistooluse. Insomecases,\nitisnecessarytointegrateexternaltoolsintoLLMstoaccessaccuratedatanotavailableduring\ntrainingorfine-tuning. Forexample,LLMscanintegratewithAPIstofetchreal-timedatasuch\nasweatherupdates,stockmarketprices,ornewsfeeds,enablingthemtoprovideup-to-date\nresponses to user queries. When using tools, LLM predictions might include markers that\nindicatewhereandhowtocallexternalAPIs. Thisrequiresdecomposingtheprobleminto\nsub-problems,withsomehandledbytheLLMsandothersbyexternaltools. Moredetailed\ndiscussionsonthistopicwillbepresentedinSection9.2.5.\n9.2.3 Self-refinement\nInmanycases,predictionsofLLMscanbeinaccurateorincorrect. GiventhatcurrentLLMs\ncanperformtaskslikerefinementandcorrection,itmakessensetoexploremethodsforthese\nmodels to self-refine their outputs. Self-refinement is a common phenomenon in human\npsychologicalactivitiesanddailybehavior. Forexample,whendesigningaproduct,adesigner",
    "7TheSCANtasks(SimplifiedversionsoftheCommAINavigationtasks)aredesignedtoevaluatetheabilityof\nLLMstoperformcompositionalgeneralization[LakeandBaroni,2018].Theyinvolvetranslatingnaturallanguage\ncommandsintoasequenceofactions. Forexample,acommand“jumpoppositeleftandwalkthrice”canbe\ntranslatedintotheactionsequence“LTURNLTURNJUMPWALKWALKWALK”.",
    "500 Chapter 9. Prompting\nmight first create a basic prototype, then refine the design after evaluation and testing to\nenhanceuserexperienceandfunctionality. Therefinementcanbeiteratedseveraltimesuntil\nthedesignissatisfactory. Theideaofpredict-then-refinecanalsobefoundinNLP.Oneearly\nexampleisBrill’stagger[Brill,1992],whereaninitialPOStaggingresultofasentencecanbe\niterativelyrefinedusingarule-basedsystem. Intheeraofdeeplearning,agooddealofwork\nonsequence-to-sequenceproblems,suchasgrammarcorrectionandtextrewriting,canalsobe\nseenasexamplesonthistheme.\nWecanpromptLLMstodoself-refinement. Considerasimpleexamplewherewefirstuse\nanLLMtotranslateatextfromChinesetoEnglishandthenusethesamemodeltorefinethe\ntranslation. Hereishowwecanpromptthemodeltotranslate:\nTranslatethefollowingtextfromChinesetoEnglish:\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最\n新公布的研究成果认为，大约从距今5800年开始，中华大地上各个区\n域相继出现较为明显的社会分化，进入文明起源的加速阶段。\nEnglish:\nA series of discoveries have cemented the significant role of the Hongshan",
    "culture in studies on the origins of Chinese civilization. The latest research\nfindings suggest that, starting from around 5800 years ago, many regions\nacrossChinabegantoexperiencenoticeablesocialdifferentiations,entering\nanacceleratedphaseintheoriginofcivilization.\nThen,wecanpromptittorefinethetranslation:\nNiuTrans",
    "9.2 Advanced Prompting Methods 501\nYouareprovidedwithaChinesetextalongwithitsEnglishtranslation. Please\nrefinethetranslationtoimproveitsaccuracyandfluencyifnecessary.\nChinese:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最\n新公布的研究成果认为，大约从距今5800年开始，中华大地上各个区\n域相继出现较为明显的社会分化，进入文明起源的加速阶段。\nEnglish:\nAseriesofdiscoverieshavecementedthesignificantroleoftheHongshan\ncultureinstudiesontheoriginsofChinesecivilization. Thelatestresearch\nfindingssuggeststhat, startingfromaround5800yearsago, manyregions\nacrossChinabegantoexperiencenoticeablesocialdifferentiations,entering\nanacceleratedphaseintheoriginofcivilization.\nRefinement:\nAseriesofarchaeologicaldiscoverieshavecementedthesignificantrole\noftheHongshancultureinstudiesontheoriginsofChinesecivilization.\nThelatestresearchfindingssuggestthat,startingfromaround5800years\nago,variousregionsacrossChinabegantoexperiencenoticeablesocial\ndifferentiations,enteringanacceleratedphaseintheoriginofcivilization.",
    "Here modifications to the input translation are highlighted in green. This process can be\niterative: ateachstep,weusethetranslationfromthepreviousstepasinput,andpromptthe\nLLMtoproduceapotentiallybettertranslation.\nIntheaboveexample,therefinementoftheinitialmodeloutputwasachievedbyprompting\ntheLLMwithasimpleinstructionlike“Pleaserefineit!”. However,therefinementisbased\nsolelyontheinstruction-followingabilityoftheLLM,andthereisnoguidanceorsupervision\non how and where to improve the model output. A more effective approach would be to\nconsiderfeedbackonspecificaspectsthatneedrefinement. Forexample,wecanpromptthe\nLLMwith“Pleasecorrectallthegrammaticalerrorsinthetranslation”,sothatthemodelcan\nfocusmoreongrammaticalerrorcorrectionduringrefinement.\nAgeneralframeworkofself-refinementwithLLMsinvolvesthreesteps[Madaanetal.,\n2024].\n• Prediction. WeuseanLLMtoproducetheinitialmodeloutput.\n• FeedbackCollection. Weobtainfeedbackonthemodeloutput.",
    "• Refinement. WeusetheLLMtorefinethemodeloutputbasedonthefeedback.\nThelasttwostepscanberepeatedmultipletimes,whichleadstoaniterativeself-refinement\nprocess. Inthisframework,receivingaccurateanddetailedfeedbackiscriticalasitguidesthe\nsubsequentrefinementstep. Ingeneral,feedbackcanbeobtainedinvariousways[Fernandes\netal.,2023;Panetal.,2024]. Wecanmanuallyexaminethemodel’spredictionstoidentify",
    "502 Chapter 9. Prompting\nissues. Also,wecanuseafeedbackmodeltoautomaticallygeneratefeedback. Forinstance,a\nrewardmodeltrainedonsomelabeleddatacanhelpevaluatethemodeloutputandreturna\nscoreindicatingitsquality.\nSincethefocusofthischapterisonLLMprompting,weconsiderusingLLMstoprovide\nfeedback. To illustrate, let us consider an example of generating good responses to user\nquestions:\n• First,wepromptanLLMtoanswertheinputquestion,asusual.\nGeneratearesponsetothequestion: “Whataresomemajorenviron-\nmentalconcernstoday?”\nResponse:\nMajor environmental concerns today include climate change, air\nand water pollution, deforestation, loss of biodiversity, and ocean\ndegradation.\n• Then,usingthefollowingprompt,wepromptthesameLLMagaintogetfeedbackon\nthegeneratedresponse.\nYouwillbeprovidedwithaquestionanditscorrespondingresponse.\nEvaluate the accuracy and depth of this response, and identify any\nfactual inaccuracies or areas where additional details could enhance\nunderstanding.\nQuestion:",
    "Whataresomemajorenvironmentalconcernstoday?\nResponse:\nMajorenvironmentalconcernstodayincludeclimatechange,airand\nwaterpollution,deforestation,lossofbiodiversity,andoceandegrada-\ntion.\nFeedback:\nThe response provided is accurate and highlights key environmental\nconcerns. However, it would benefit from more specific and detailed\ndescriptionsoftheseconcerns. Herearesuggestionsforimprovement:\n1. Includespecificexamplesofenvironmentalissues.\n2. Addinformationaboutmajorpollutants.\n3. Specifytheregionsmostaffected.\n• GiventhefeedbacktheLLMproduces,wepromptittorefinetheresponse.",
    "9.2 Advanced Prompting Methods 503\nYouwillbeprovidedwithaquestion,itscorrespondingresponse,and\nsomefeedbackonthisresponse.\nQuestion:\nWhataresomemajorenvironmentalconcernstoday?\nResponse:\nMajorenvironmentalconcernstodayincludeclimatechange,airand\nwaterpollution,deforestation,lossofbiodiversity,andoceandegrada-\ntion.\nFeedback:\nTheresponseprovidedisaccurateandhighlightskeyenvironmental\nconcerns. However,itwouldbenefitfrommorespecificanddetailed\ndescriptionsoftheseconcerns. Herearesuggestionsforimprovement:\n1. Includespecificexamplesofenvironmentalissues.\n2. Addinformationaboutmajorpollutants.\n3. Specifytheregionsmostaffected.\nNowrefinetheresponsebasedontheprovidedfeedback.\nRefinedResponse:\nKey environmental concerns include climate change, evidenced by\nrising global temperatures and severe weather patterns; significant\nair and water pollution in urban and industrial areas; rampant\ndeforestation in regions like the Amazon, causing biodiversity loss;",
    "and ocean degradation, highlighted by coral reef bleaching and\nwidespreadoverfishing.\nIdeally,ifastrongLLMisadopted,wewouldliketohaveitperformallthreestepswithout\nextratraining. Ontheotherhand,ifwehaveenoughlabeleddataforthetaskofinterest,wecan\nenhancetheperformanceoftheLLMusingsupervisedlearning. Forexample,wecanfine-tune\ntheLLMtobetteradaptittorefinementtasks,oralternatively,usetask-specificmodels,which\nmaynotnecessarilybebasedonLLMs[Wellecketal.,2023;Schicketal.,2023]. Inabroader\nsense, improving LLMs for self-refinement tasks can be seen as an alignment issue. For\nexample,ithasbeenfoundthatsomeself-correctionabilitiescanbeactivatedthroughRLHF\n[Gangulietal.,2023]. However,discussingtheseissuesisbeyondthescopeofthischapter.\nFurtherdiscussioncanbefoundinChapter10.\nInLLMs,self-refinementisrelatedtoseveralconceptsthatrevealthepsychologicalaspects\nof these models, such as the ability to self-reflect. A view is that if LLMs are capable of",
    "self-reflection,theirpredictionscanbecomemoreaccurateandevenpossessself-correcting\ncapabilities. Thisself-reflectioncanbeactivatedinvariousways,forexample,byprompting\ntheseLLMstoengageinmorein-depthandcarefulthinking,orbyprovidingexamplesfrom",
    "504 Chapter 9. Prompting\nwhichthemodelscanlearnandreflect. Toillustrate,weconsiderherethedeliberate-then-\ngenerate(DTG)methodpresentedinLietal.[2023a]’swork,whereLLMsarepromptedto\ndeliberate. InDTG,wearegivenaninitialmodeloutputwhichmaycontainerrors. LLMsare\nthenpromptedtoidentifytheerrortypesofthismodeloutputandprovideanimprovedoutput.\nBelowisatemplateofDTGpromptingforChinese-to-Englishtranslationtasks.\nGiventheChinesesentence: {∗source∗}\nTheEnglishtranslationis: {∗target∗}\nPleasefirstdetectthetypeoferror,andthenrefinethetranslation.\nErrorType:\nWeaimtofirstpredicttheerrortype(red),andthenproducearefinedtranslation(blue).\nThisprocessofdeliberationisguidedbytheinstruction“Pleasefirstdetectthetypeoferror,\nandthenrefinethetranslation”. ItencouragesLLMstoinitiallyengageinthoughtfulanalysis\nand then give better results. Since error type prediction and refinement are performed in a\nsinglerunofLLMs,thismethodincorporatesbothstepsoffeedbackandrefinementintoone\nprocess.",
    "Intheaboveprompts,weassumethattheLLMweuseisabletoreviewtheinputtranslation\nandcorrectlyidentifyitserrortypes. However,thisraisesnewdifficultiesasthemodelmaynot\nbegoodatfindingerrorsintranslations. Thiswillinturnresultinextrafine-tuningorprompting\nengineeringefforts. Soasimplermethodistoreducetheburdenoferroridentificationanduse\nLLMsfordeliberationonly. Todothis, wecanreplacetheinputtranslationwitharandom\ntranslationandassignadefaulterrortype. Anexampleofsuchapromptisshownbelow.\nGiventheChinesesentence:\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。\nTheEnglishtranslationis:\nAvarietyofinnovativetechniqueshaveredefinedtheimportanceofmodern\nartincontemporaryculturalstudies.\nPleasefirstdetectthetypeoferror,andthenrefinethetranslation.\nErrorType: IncorrectTranslation\nInthisexample,theinputtranslationisnotgeneratedbyLLMsbutisinsteadrandomly\nsampledfromthedataset. Soitissimplyanincorrecttranslationforthesourcesentence,and\nwecansettheerrortypeaccordingly. TheLLMsthengenerateanewtranslationbytaking",
    "both the source sentence and the incorrect translation as input. The design of this prompt",
    "9.2 Advanced Prompting Methods 505\ncan also be considered as activating the learning capabilities of LLMs through “negative\nevidence” [Marcus, 1993], thereby enabling them to reflect and produce better outcomes\nthroughcontrastiveanalysis. Nevertheless,thismethoddoesnotrelyonanyfeedbackandcan\nenhancetheperformanceofasingleLLMpredictionviasimpleprompting.\nNotethatwhileDTGisnon-iterative,iterativelearningandrefinementarecommonlyused\nin NLP. An advantage of these iterative approaches is that they mimic human learning and\nproblem-solving,wherecontinuousfeedbackandadjustmentsleadtoprogressivelyimproved\noutcomes. Iterative methods can be applied to a range of LLM prompting problems. For\nexample,inproblemdecomposition,onecanincorporatenewsub-problemsandtheirsolutions\nintothecontextateachstep,andthusLLMscanprogressivelyapproachthesolutionofthe\noriginalproblem. Ontheotherhand,iterativemethodsraiseseveralissuesthatareabsentin",
    "non-iterativemethods,forexample,errorsinearlierstepsmaynegativelyimpactsubsequent\nproblem-solving,anddeterminingwhentostopiteratingoftenrequiresadditionalengineering\neffort.\n9.2.4 Ensembling\nModelensemblingfortextgenerationhasbeenextensivelydiscussedintheNLPliterature.\nTheideaistocombinethepredictionsoftwoormoremodelstogenerateabetterprediction.\nThistechniquecanbedirectlyapplicabletoLLMs. Forexample,wecancollectasetofLLMs\nandruneachofthemonthesameinput. Thefinaloutputisacombinedpredictionfromthese\nmodels.\nForLLMprompting,itisalsopossibletoimproveperformancebycombiningpredictions\nbasedondifferentprompts. SupposewehaveanLLMandacollectionofpromptsthataddress\nthesametask. WecanrunthisLLMwitheachofthepromptsandthencombinethepredictions.\nForexample,belowarethreedifferentprompttemplatesfortextsimplification.\nMakethistextsimpler.\n{∗text∗}\nCondenseandsimplifythistext.\n{∗text∗}\nRewriteforeasyreading.\n{∗text∗}",
    "506 Chapter 9. Prompting\nEach of these prompts will lead to a different prediction, and we can consider all three\npredictionstogeneratethefinalone.\nFormally, let {x ,...,x } be K prompts for performing the same task. Given an LLM\n1 K\nPr(·|·), we can find the best prediction for each x using yˆ =argmax Pr(y |x ). These\ni i yi i i\npredictionscanbecombinedtoforma“new”prediction:\nyˆ = Combine(yˆ ,...,yˆ ) (9.6)\n1 K\nHereCombine(·)isthecombinationmodel,whichcanbedesignedinseveraldifferentways.\nForexample,wecanselectthebestpredictionbyvotingorbyidentifyingtheonethatoverlaps\nthemostwithothers. Anothermethodformodelcombinationistoperformmodelaveraging\nduringtokenprediction. Letyˆ bethepredictedtokenatthej-thstepformodelcombination.\nj\nTheprobabilityofpredictingyˆ isgivenby\nj\nK\n(cid:88)\nyˆ = argmax logPr(y |x ,yˆ ,...,yˆ ) (9.7)\nj j k 1 j−1\nyj\nk=1\nTheinterestedreadercanrefertoChapter5formoredetailsofthesemethods.\nInensemblingforLLMprompting,itisgenerallyadvantageoustousediverseprompts",
    "sothatthecombinationcancaptureabroaderrangeofpotentialresponses. Thispracticeis\ncommoninensemblelearning,asdiversityhelpsaverageoutbiasesanderrorsthatmaybe\nspecific to any single model or configuration. From the Bayesian viewpoint, we can treat\nthepromptxasalatentvariable,giventheproblemofinterest,p. Thisallowsthepredictive\ndistributionofygivenptobewrittenasthedistributionPr(y|x)marginalizedoverallpossible\nprompts\n(cid:90)\nPr(y|p) = Pr(y|x)Pr(x|p)dx (9.8)\nThe integral computes the total probability of y by considering all possible values of x,\nweighted by their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is\nthepriordistributionofpromptsfortheproblem. Thisisagoodmodelbecausetheintegral\neffectivelyaccountsfortheuncertaintyinthechoiceofx,ensuringthatthefinalpredictive\ndistribution Pr(y|p) is robust and encompasses all potential variations and biases in the\nprompts. However,computingthisintegraldirectlycanbecomputationallyinfeasibledueto",
    "thepotentiallyinfinitespaceofx. Oneapproachtoaddressingthisissueistoemploymethods\nlikeMonteCarlosampling,whichapproximatetheintegralusingamanageable,finitenumber\nofprompts.\nWhiletheBayesiantreatmentismathematicallywell-defined,itiscommonpracticeinNLP\ntoassumeanon-informativeoruniformpriorandfocusinsteadonconstructingasetofdiverse\nprompts. Consequently, the output can be computed using a straightforward combination\nmodel, as described in Eq. (9.6). The issue of creating high-quality, diverse prompts has\nbeen studied in CoT and other in-context learning areas. Most of the research focuses on",
    "9.2 Advanced Prompting Methods 507\nincorporatingavarietyofdemonstrationexamplesacrossdifferentprompts. Here,welistsome\nofthesemethods.\n• Givenaproblem,wemanuallycreateanumberofdemonstrationsandusedifferentones\nfordifferentprompts.\n• Givenaproblem,weuseLLMstoautomaticallygeneratedemonstrationsandprompts.\n• Givenaprompt,wecreatedifferentpromptsbychangingtheorderofdemonstrationsin\ntheprompt.\n• Givenaprompt,weuseLLMstogenerateanumberofsimilarprompts.\n• Givenaprompt,wetransformitintootherforms,e.g.,translatingitintootherlanguages.\nOf course, in practice, we can combine these methods to achieve greater diversity. An\nunderlyingassumptionhereisthatdiversepromptscanleadtodiversemodeloutputs. This\nis particularly the case when the problem we deal with is relatively new and difficult. For\nstrongerandmorerobustLLMs,thevarianceinoutputforsimilarpromptsmightnotbelarge.\nInthiscase,thebenefitofinvolvingmultiplepromptscanbemodest.\nInadditiontoprovidingdiversepromptsforLLMs,anotherapproachistomakeuseof",
    "theinherentvarianceintheoutputsofLLMs. Onesimplewaytogeneratemultipleoutputs\nis to sample outputs from the hypothesis space. This is straightforward for LLMs because\ntheir search algorithms are naturally suited to producing multiple results. For example, if\nbeamsearchisused,wecaneasilycollectallthecompletehypothesesinthebeamtogenerate\nmultipleoutputcandidates. Also, wecanmodifythesearchalgorithmstoimplementmore\npowerfulsamplingmethodsandexploreawiderrangeofhigh-qualityhypotheses.\nThenextquestionishowtocombinediversemodelpredictions. Thisleadsusbacktothe\nmodel combination issue as described in Eq. (9.6). Here we consider the self-consistency\nmethod,whichoutputsnotthepredictionwiththehighestprobability,butrathertheonethat\nbestalignswithotherpredictions[Wangetal.,2022d;2023c]. First,anLLMispromptedwith\nCoTasusualandgeneratesmultiplereasoningpathsbysampling. Then,thefrequencyofeach\nanswer is counted across these reasoning paths. The final output is the answer that has the",
    "highestcount. BelowshowsthreedifferentpredictionsbypromptinganLLMwithCoT.\nPrediction1(Correct):\nQ:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe\ncoinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads?\nA:Let’sthinkstepbystep.\nEach coin flip is an independent event with a 50% chance of landing heads\n(H)and50%tails(T).Considerthepossibleoutcomesforthreeflips: HHH,\nHHT, HTH, HTT, THH, THT, TTH, TTT. Out of these, only HHH, HHT,\nHTH,andTHHarerelevant. Weareonlyinterestedinscenarioswithexactly\none head: HTT, THT, and TTH. There are 3 such outcomes out of 8 total,\ngivingaprobabilityof3/8.",
    "508 Chapter 9. Prompting\nPrediction2(Correct):\nQ:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe\ncoinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads?\nA:Let’sthinkstepbystep.\nUsingabinomialprobabilityformula,wherep(successorheads)is0.5,and\neach of the 3 trials (flips) is independent, we look for exactly one success\n(head). The formula for exactly k successes in n trials is given by the\nbinomial coefficient formula (cid:0)n(cid:1) pk(1−p)n−k. Plugging in the values, we\nk\nget (cid:0)3(cid:1) (0.5)1(0.5)3−1=3·0.5·0.25=0.375.\n1\nPrediction3(Wrong):\nQ:Threefriendsplayagamewheretheyflipafaircoin. Eachfriendflipsthe\ncoinonce. Whatistheprobabilitythatexactlyoneofthemflipsheads?\nA:Let’sthinkstepbystep.\nSinceeachcoinfliphasa50%chanceoflandingheadsanda50%chanceof\nlandingtails,andwearelookingforthecasewhereonlyoneofthethreeflips\nresults in heads, we might initially think this is simply a matter of picking\nwhichflipistheheads. Therearethreeflips,soonemightincorrectlyassume",
    "that the chance of picking one specific outcome like this would be 1 out of\n3. Thus, they might conclude that the probability of exactly one head is\n1/3=33.3%.\nPredictions1and2correctlyidentifythethreecaseswhereexactlyoneheadisflipped,\nbothobtainingaprobabilityof37.5%. ThereasoninginPrediction3failstoaccountforthe\ntotalnumberofoutcomespossiblewiththreecoinflips,thusgivingawronganswerof33.3%.\nTherefore,weselect37.5%asthefinalanswerbecauseitistheconsensus.\nSelf-consistency provides a criterion for determining the best prediction in a pool of\ncandidates. Sincethepromptandthemodelarefixedinthismethod,itisnotstrictlyaprompt\nensemblingmethod. Instead,itcanbeseenasaninstanceofoutputensemblingmethods,also\nknownashypothesisselectionmethods,whichhavelongbeenexploredinNLP,particularlyfor\ntextgenerationproblems[Xiaoetal.,2013]. Inthesemethods,multipleoutputsaregenerated\nbyvaryingmodelarchitecturesorparameters. Eachoutputisthenassignedascorebysome",
    "criterion,andtheoutputsarere-rankedbasedonthesescores. Therearevariouswaystodefine\nthe scoring function, such as measuring the agreement between an output and others, and\nusingastrongermodeltorescoreeachoutput8. Figure9.2showsacomparisonofdifferent\n8Aninterpretationofself-consistencyistoviewitasaminimumBayesrisksearchprocess.Itsearchesforthe\nbestoutputbyminimizingtheBayesrisk.Morespecifically,ariskfunctionR(y,yr)isdefinedoneachpairof\noutputs(denotedby(y,yr)),representingthecostofreplacingywithyr.GivenasetofoutputsΩ,theriskofan",
    "9.2 Advanced Prompting Methods 509\nensemblingmethodsforLLMs.\nNow, let us briefly review the methods we have discussed so far in this section, such\nas problem decomposition and self-refinement. It is apparent that these methods enhance\ndecision-makingbyintroducingmore“choices”intothereasoningprocess. Tosomeextent,\ntheyallinvolveevaluatingandprovidingfeedbackontheresultsofLLMs. Forexample,in\nself-refinement,weneedtooffersuggestionsforimprovingthepredictionofLLMs,andin\noutputensembling,weselecttheoptimaloutputfromapoolofcandidates. Inthissense,these\nmethodsfallunderthebroadercategoryofpredict-then-verifyapproaches,wherepredictions\nareinitiallymade,thenverifiedandrefined. Thefundamentalproblemhereinvolvesverifying\nandevaluatingthereasoningresultsorintermediatesteps. Thisissueissomewhatrelatedto\ntheproblemoftrainingrewardmodelsinRLHF,althoughRLHFaddressesadifferentaspect.\nIn fact, the development of verifiers has been explored and implemented in reasoning with",
    "LLMs. Mostwork,ratherthandevelopingheuristic-basedinference-timealgorithms,focuses\nonlearningverifiersinasupervisedmanner. Astraightforwardmethodistotrainverifiersas\nbinaryclassifiers,suchasclassifyingananswerascorrectorincorrect,althoughtheseverifiers\nare typically used as scoring models. Given a reasoning path for a problem, the verifiers\ncanbeusedtoscoreeithertheentirepath(calledoutcome-basedapproaches)[Cobbeetal.,\n2021],oreachindividualreasoningstep(calledprocess-basedapproaches)[Uesatoetal.,2022;\nLightmanetal.,2024].\n9.2.5 RAG and Tool Use\nRAGisgenerallyemployedwhenstandardLLMs,whichrelysolelyonpre-trainedknowledge,\nlack accuracy and depth in the generated text. By drawing from external databases and\ndocuments,RAGcansignificantlyimprovethequalityofresponses,ensuringtheyareboth\ncontextuallyrelevantandfactuallycorrect. Suchanapproachisparticularlyusefulinscenarios\nthat require high factual accuracy and up-to-date information, such as complex question\nanswering.",
    "TheconceptofRAGhasbeenmentionedseveraltimesintheprevioussectionsandchapters.\nForcompleteness,weoutlinethekeystepsinvolvedinRAGhere.\n• Weprepareacollectionoftextswhicharetreatedasanadditionalsourceofknowledge\nwecanaccess.\n• Weretrieverelevanttextsforagivenquery.\n• WeinputboththeretrievedtextsandthequeryintoanLLM,whichisthenpromptedto\nproducethefinalprediction.\nSteps1and2canbeimplementedbyusinganexternalinformationretrievalsystem. For\nexample,wecanstorethecollectionoftextsinavectordatabaseandthenretrievethemost\nsimilar texts through vector-based search techniques. Since information retrieval is not the\noutputy∈Ωisgivenby\nRisk(y) = E\nyr∼Pr(yr|x)\nR(y,yr)\n(cid:88)\n= R(y,yr)·Pr(yr|x) (9.9)\nyr∈Ω",
    "510 Chapter 9. Prompting\nCombine/Select\nLLM2 Prediction3\nPrompt LLM2 Prediction2 Final\nPrediction\nLLM1 Prediction1\n(a)ModelEnsembling\nCombine/Select\nPrompt3 Prediction3\nPrompt2 LLM Prediction2 Final\nPrediction\nPrompt1 Prediction1\n(b)PromptEnsembling\nCombine/Select\nmple Prediction3\nSa\nPrompt LLM Prediction2 Final\nPrediction\nPrediction1\n(c)OutputEnsembling\nFigure 9.2: Ensembling methods for LLMs. In standard model ensembling (a), multiple\nLLMsvaryinginarchitecturesorparametersareused. EachLLMreceivesthesameprompt\nandproducesaprediction. Thesepredictionsarecombinedtogeneratethefinalprediction.\nIn prompt ensembling (b), we have one LLM and multiple prompts. The LLM produces a\npredictionforeachprompt,andthesepredictionsarecombinedasusual. Inoutputensembling\n(c),theLLMsamplesmultiplepredictionsoverthepredictionspacegivenaprompt. Itcan\nbeseenasamethodtoboosttheperformanceoftheLLMitself. Notethattheseensembling\nmethodscanbecombinedtoincreasethediversityofpredictions. Forexample,wecanuse",
    "bothpromptensemblingandoutputensemblingtoobtainmorediversepredictions.\nfocusofthischapter,wewillassumethatsuchsystemsareavailableoff-the-shelfandusethem\ndirectly.\nHerewepresenthowtopromptLLMstomakeuseofretrievedtexts. Toillustrate,consider",
    "9.2 Advanced Prompting Methods 511\nanexampleofusingLLMstoanswerthefollowingquestion.\nWherewillthe2028Olympicsbeheld?\nWe can simply input this question into an online search engine. It will then return the\nrelevantpiecesoftextfoundontheinternet,forexample,\n(Wikipedia)\nThe 2028 Summer Olympics, officially the Games of the XXXIV Olympiad and\ncommonlyknownasLosAngeles2028orLA28,isanupcominginternationalmulti-\nsporteventscheduledtotakeplacefromJuly14-30,2028,intheUnitedStates. ...\n(TheSportingNews)\nIn2028,LosAngeleswillbecomethethirdcity,followingLondonandParisrespec-\ntively,tohostthreeOlympicsafterhostingtheSummerGamesin1932and1984. It\nwillalsobethefirsttimetheUnitedStateshashostedanOlympicGamessincethe\n2002WinterGamesinSaltLakeCity. ...\n...\nWecanusetheseretrievedtextsasadditionalcontext,andpromptanLLMtogeneratea\nresponsebasedonthesetexts. BelowisanexampleRAGprompt.\nYourtaskistoanswerthefollowingquestion. Tohelpyouwiththis,relevant\ntextsareprovided. Pleasebaseyouransweronthesetexts.",
    "Question:\nWherewillthe2028Olympicsbeheld?\nRelevantText1:\nThe2028SummerOlympics,officiallytheGamesoftheXXXIVOlympiad\nandcommonlyknownasLosAngeles2028orLA28...\nRelevantText2:\nIn2028,LosAngeleswillbecomethethirdcity,followingLondonandParis\nrespectively,tohostthreeOlympicsafter...\n...\nThe2028OlympicswillbeheldinLosAngeles.\nThispromptassumesthattheprovidedtextsarerelevanttothequestionandexpectsthe\nLLM to generate a faithful response using these texts. However, the information retrieval\nsystem may sometimes provide irrelevant or incorrect texts, which may lead the LLM to\nproduceanincorrectanswer. Onestraightforwardwaytoaddressthisissueistoimprovethe\naccuracyoftheinformationretrievalsystem. Nevertheless,aswithmostAIsystems,errors\nmaystilloccur. Therefore,itisalsonecessarytoenhancetherobustnessoftheLLM,sothatit",
    "512 Chapter 9. Prompting\ncanmakereasonablepredictionsevenwhentheinputisinaccurate. Belowisanewprompt\nthatenablestheLLMtobemorefaithfultothefacts,andallowsittochoosenottoanswer\nquestionswhentheinformationprovidedisinaccurate.\nYourtaskistoanswerthefollowingquestion. Tohelpyouwiththis,relevant\ntextsareprovided. Pleasebaseyouransweronthesetexts.\nPleasenotethatyouranswersneedtobeasaccurateaspossibleandfaithfulto\nthefacts.Iftheinformationprovidedisinsufficientforanaccurateresponse,\nyoumaysimplyoutput\"Noanswer!\".\nQuestion:\nWherewillthe2028Olympicsbeheld?\nRelevantText1:\nThe2024SummerOlympics,officiallytheGamesoftheXXXIIIOlympiad\nandbrandedasParis2024,wereaninternationalmulti-sportevent...\n...\nNoanswer!\nInthisexample,theLLMrefusestoanswerbecausetheprovidedinformationisinsufficient\nandirrelevanttothequestion.\nBothRAGandfine-tuningarecommonmethodsforadaptingLLMsusingtask-specific\ndata. StandardRAGistraining-freeandcanbedirectlyappliedtoLLMs. Tofurtherimprove",
    "RAG,itisalsopossibletofine-tuneLLMs,thoughthiswillrequiresometrainingeffort. For\nexample, we can fine-tune LLMs using human-labelled data to supervise them in learning\nto refuse to answer. Note that, while the examples shown above seem simple, RAG is not\ntrivial. From the prompt engineering perspective, different use cases may require different\nprompts, though our somewhat “greedy” goal is to develop a universal prompting strategy\nthatcanadapttodifferenttasks. Inmanycases,weneedtocontrolhowmuchwedependon\ntheretrievedcontexttomakepredictions. Sometimes,LLMsmustderiveresponsesstrictly\nfromtheprovidedtexts,whileatothertimes,theymayneedtogenerateresponsesusingtheir\npre-trainedknowledgeiftheprovidedtextsareinsufficient. TherearemanyaspectsofRAG,\nsuchasimprovementstotheretrievalsystems,thatcannotbecoveredinthischapter. Interested\nreaderscanrefertosurveysofRAGtechniquesformoreinformation[Lietal.,2022d;Gao\netal.,2023c].\nOnereasonwediscussRAGhereisthatitcanbebroadlyregardedasaninstanceofthe",
    "generalproblemdecompositionframework(seeSection9.2.2). RAGdividesproblem-solving\ninto two steps. In the first step, we collect relevant and supporting information for a given\nqueryfromvariousknowledgesources. Inthesecondstep,weuseLLMstogenerateresponses\nbasedonthecollectedinformation. Ifweextendtheconceptofproblemdecompositionfurther,\nwewillfindthatmanytasksrequiringtheuseofexternalsystemsortoolscanbetreatedas",
    "9.2 Advanced Prompting Methods 513\nsimilarproblems. OnesuchexampleistooluseinLLMs. Inmanyapplications,LLMsneedto\nemploy external databases, APIs, andeven simulationtools togenerate accurate responses.\nFor example, LLMscan access real-timedata fromfinancial markets toprovide up-to-date\ninvestmentadviceorintegratewithhealthcaredatabasestoofferpersonalizedmedicalinsights.\nThisintegrationextendsthecapabilitiesofLLMsbyallowingthemtointeractwith, andin\nsomecontexts,influenceorcontrolexternalsystems. Consequently,LLMsfunctionmoreas\nautonomousagentsratherthanmeretextgenerators[FranklinandGraesser,1996].\nTheissueoftooluseisbroadandvast. Herewenarrowourdiscussiontotasksthatcanbe\nfacilitatedbycallingexternalAPIstosolvesomeofthesub-problems[Parisietal.,2022;Gao\netal.,2023b]. ConsideragaintheexampleofaskinganLLMtoanswer“Wherewillthe2028\nOlympicsbeheld?”. SupposetheLLMcanaccessawebsearchtool. Wecanthenpromptthe\nLLMtoanswerthequestionwithwebsearch,likethis",
    "Yourtaskistoanswerthefollowingquestion. Youmayuseexternaltools,\nsuchaswebsearch,toassistyou.\nQuestion:\nWherewillthe2028Olympicsbeheld?\nTheinformationregardingthisquestionisgivenasfollows:\n{tool:web-search,query:\"2028Olympics\"}\nSotheansweris: LosAngeles\nHere{tool: web-search, query: \"2028 Olympics\"}indicatesarequestto\nthewebsearchsystemusingthequery“2028Olympics”. WhentheLLMseesthisstring,it\nexecutesawebsearchandusestheresulttoreplacethestring. Then,insubsequentstepsof\nprediction,theLLMusesthiswebsearchresultascontexttoproducethecorrectanswer.\nConsideranotherexamplewhereweasktheLLMtosolveamathematicalproblem.",
    "514 Chapter 9. Prompting\nProblem:\nAswimmingpoolneedstobefilledwithwater. Thepoolmeasures10meters\nin length, 4 meters in width, and 2 meters in depth. Calculate the volume\nofthepoolincubicmetersandthendeterminehowmanylitersofwaterare\nneededtofillit(considering1cubicmeterequals1000liters).\nSolution:\nTosolvethisproblem,theLLMneedstofirstcalculatethevolumeofthe\npoolbyusingtheformulaforthevolumeofarectangularprism: Length×\nWidth×Depth.Therefore,Thevolumeis10m×4m×2m={tool:\ncalculator,expression:10*4*2}m3. Next,tofindouthow\nmanylitersofwaterareneeded,theLLMmultipliesthevolumeincubic\nmetersby1000(since1cubicmeterequals1000liters). Thus,80×1000\n={tool:calculator,expression:80*1000} liters.\nHerethestring{tool: calculator, expression: 10 * 4 * 2}triggersthe\ninvocation of a mathematical interpreter to calculate the result of the expression. Note that\ntheresult(i.e.,80)willreplace{tool: calculator, expression: 10 * 4 *\n2}andcanbereferredtointhefollowingtokenpredictions. Forexample,inthelaststepof",
    "problem-solving, 80 is used instead of {tool: calculator, expression: 10\n* 4 * 2}.\nAkeydifferencebetweenthetooluseexampleshereandthepreviouslydiscussedRAG\nexamplesisthatintooluse,externalfunctionscanbecalledduringinference. Incontrast,in\nRAG,theretrievedtextsareprovidedbeforethepredictionprocessbegins. However,fromthe\nlanguagemodelingperspective,theyareactuallydoingthesamething: beforegeneratingthe\nfinalresult,weuseexternaltools,eithermanuallyorautomatically,toobtainsufficientand\nrelevantcontext. Ahigh-levelinterpretationoftheseapproachesisthattheybothrelyonan\n“agent”thatcandeterminewhereandhowtocallexternalfunctionstogeneratethecontext\nnecessaryforprediction.\nAnissuewithtooluseisthattheoriginalLLMsarenottrainedtogeneratethenecessary\nmarkersfortooluse. Therefore,weneedtofine-tunetheLLMstoadaptthemforthesetasks\n[Schicketal.,2024]. Asthischapterfocusesonprompting,wewillnotpresentthedetailsof\nthisfine-tuningprocess. Toputitsimply,wefirstneedtoannotatedata. Foreachfine-tuning",
    "example,wereplacepartsoftheoutputthatrequiretheuseofexternaltoolswithpredefined\ncommandsormarkers. Then,weusethislabeleddatatofine-tunetheparametersoftheLLM\nasusual. Asaresult,theLLMcangaintheabilitytogeneratecommandsforcallingexternal\ntools. Duringinference,wecanexecutethesetoolusecommandsinthemodeloutputstoget\nassistancefromexternaltools.",
    "9.3 Learning to Prompt 515\n9.3 Learning to Prompt\nSo far in this chapter, we have considered several basic prompting strategies and various\nrefinementstothem. However, allthepromptswehavediscussedweredesignedmanually.\nThisleadstoanumberofproblems: First,designinghigh-qualitypromptsisinherentlydifficult\nandrequiressubstantialmanualeffort. Forexample,extensiveexperimentationwithdifferent\npromptsisoftenneededtoidentifythemosteffectiveones. SincedifferentLLMsmayrespond\nbettertocertaintypesofprompts,developinguniversallyeffectivepromptscanbeevenmore\nresource-intensive. Second,manualpromptdesignreliesheavilyonhumanexpertise,which\ncanlimitthediversityofapproachesandoverlookpotentiallyeffectivepromptsthatarenot\nimmediately obvious to humans. Third, prompts created by humans can be complex and\nredundant,leadingtolongerinputsforLLMsandhighercomputationalcosts.\nIn this section, we discuss techniques for automated prompting. These methods aim to",
    "automatically create, optimize, and represent prompts so that the downstream tasks can be\naddressedmoreeffectivelyandefficiently. Inparticular,weconsiderthreeissueshere.\n• HowcanweautomatetheprocessofdesigningandoptimizingpromptsforLLMs?\n• Arethereotherformsofrepresentingpromptsbeyondstrings, andhowcanwelearn\nsuchrepresentations?\n• Howcanwemakepromptsmoreconciseandcompact,therebyreducingtheircomplexity\nandlength?\nNotethattherearemanysettingsinwhichwecaninvestigatetheseissues. Forexample,\nwe might specify that prompts are developed specifically for a particular LLM, or that the\ndevelopmentisindependentoftheLLMused. Thesesettingscanleadtodifferentmethods\nand application scenarios, but these methods may overlap in some ways. In the following\ndiscussion, we will cover several different scenarios and discuss the connections between\nvariousmethods.\n9.3.1 Prompt Optimization\nGiventhatpromptdesignisdifficultandlabor-intensive,itisdesirabletousemachinelearning",
    "modelstodiscovertheoptimalpromptforaspecifictask(callitautomaticpromptdesignor\npromptoptimization). Thisapproachcanbroadlyberegardedasaninstanceofautomated\nmachinelearning(AutoML),whichaimstoreduceoreliminatetheneedforexpert-driven\nmanual design of machine learning models. Although our focus here is on the design of\nprompts, prompts themselves are discrete structures. Therefore, designing prompts is very\nsimilartodesigningmachinelearningmodels,suchasdiscretemodelarchitectures. Perhaps\noneofthemostrelatedfieldsisneuralarchitecturesearch(NAS),wherethemostoptimal\nneuralnetworksareidentifiedbyexploringaspaceofpossibleneuralnetworks[ZophandLe,\n2016;Elskenetal.,2019a]. Ifweconsiderpromptoptimizationasasearchprocess,thenwe\ncandescribeageneralpromptoptimizationframeworkinvolvingthefollowingcomponents:\n• PromptSearchSpace. Thisdefinesallpossiblepromptsthatthealgorithmscanexplore.\nFor example, one can edit some seed prompts to generate a set of diverse candidate",
    "516 Chapter 9. Prompting\nprompts.\n• Performance Estimation. Once a prompt is chosen, it needs to be evaluated. For\nexample,astraightforwardwayistoinputittoanLLMandmeasureitsperformanceon\navalidationset.\n• Search Strategy. The search process is generally the same as that used in many AI\nsystems. At each step, the system explores a set of promising prompts in the search\nspaceandevaluatesthem. Thisprocesscontinuesasmorepromptsareexplored. The\noutcomeofthesearchisthebest-performingpromptobserveduntilthesearchstops.\nThis is a very general framework, and different prompt optimization systems can vary\nin their design of each component. A widely-used approach is to use LLMs as the basis to\ndevelopthesecomponents. Initially,afewpromptsareprovided. Then,thefollowingprocess\nisiterateduntilastoppingcriterionismet: 1)thepromptsareevaluatedonavalidationset;\n2)acandidatepoolismaintainedbykeepingonlythemostpromisingprompts;and3)new",
    "promptsarecreatedbyemployingLLMstoinfersimilarpromptsfromthiscandidatepool.\nOnebenefitofthisapproachisthatitallowsustouseoff-the-shelfLLMstoperformthetasks\nmentionedabovewithouttheneedforsubstantialsystemdevelopment. Toachievethis,wecan\npromptorfine-tuneLLMstoadaptthemtothesetasks. HereweconsiderZhouetal.[2023c]’s\nmethodforillustratingLLM-basedpromptoptimization. Itinvolvesthefollowingsteps.\n• Initialization. LetC representthepoolofthecandidatepromptsweintendtoexplore.\nThefirststepistoaddinitialpromptsintoC. Wecandothisinseveralways. Asimple\nmethod is to create such prompts by hand for a given task. However, in many cases\nwherehumanshavelimitedknowledgeabouthowtowriteeffectivepromptsforthetask,\ndevelopingpromptsbecomeschallenging. Inthesecases,itisdesirabletouseLLMs\ntogenerateprompts. Forexample,wecandirectlyinstructLLMstoproduceprompts,\nprovidingthemwithadescriptionofthetask.\nYouaregivenatasktocompleteusingLLMs. Pleasewriteapromptto\nguidetheLLMs.\n{∗task-description∗}",
    "Thismethodisstraightforward,butitstillrequiresahuman-provideddescriptionofthe\ntask. AnalternativemethodistouseLLMstogeneratepromptsgivenexamplesofthe\ninputandoutputofthetask. Hereisaprompttemplate.",
    "9.3 Learning to Prompt 517\nYou are provided with several input-output pairs for a task. Please\nwriteaninstructionforperformingthistask.\nInput: {∗input1∗} Output: {∗output1∗}\nInput: {∗input2∗} Output: {∗output2∗}\n...\nAssuch,LLMscaninferthecorrespondinginstructionforthetaskfromtheprovided\ninputsandoutputs.\n• Evaluation. Once we obtain the candidate pool C, we need to evaluate the prompts\nin C. One method is to feed each prompt into an LLM and assess the results on the\ndownstreamtask. Forexample,wecanevaluatetheoutputoftheLLMgivenaninput\nusing a pre-defined metric, or alternatively, use the log-likelihood of the output as a\nmeasureofthequalityoftheprompt.\n• Pruning. If C contains a large number of prompts, it is reasonable to prune the\nunpromisingpromptswithinit,thusreducingthecomputationalburdeninsubsequent\nsteps. Thisisastandardpruningproblem. Giventheevaluationscoreforeachprompt,a\nsimplemethodistokeeponlyacertainpercentageofthepromptsanddiscardtherest.",
    "• Expansion. Expansionisakeyoperationinsearchalgorithmsusedtoexploredifferent\nstatesinthesearchspace. Theexpansionoperationherecanbedefinedasafunction\nC′ = Expand(C,f) (9.10)\nwhereC′ isthesetofnewpromptsgeneratedfromC usingthemodelf. Ifweconsider\nf asanLLM,wecanperformtheexpansionoperationbyinstructingf togeneratenew\nandrelevantpromptsbasedonC. Belowisanexample.\nBelowisapromptforanLLM.Pleaseprovidesomenewpromptsto\nperformthesametask.\nInput: {∗prompt∗}\nThen, we replace C with C′. The steps of evaluation, pruning and expansion can be\nrepeated,andsowecangraduallyexploreawiderrangeofprompts.\nInpromptoptimization,theexpansionstepplaysakeyrole,asitdefineshowweexplore\nthesearchspace,andourgoalistofindoptimalresultswithminimaleffort. Oneimprovement\ntothisstepistotreattheproblemasaparaphrasingtask. Asimplemethodistoapplyoff-the-\nshelfparaphrasingsystems,eitherbasedonLLMsorothermodels,totransforminputprompts",
    "518 Chapter 9. Prompting\nintosemanticallyequivalentforms[Jiangetal.,2020]. Alternatively,wecandefinespecificedit\noperations,suchasinsertionsandmodifications,foreachtoken. Agivenpromptcanbeedited\nintonewpromptsbyapplyingtheseoperations[Prasadetal.,2023]. Also,furtherevaluation\nandpruningcanbeappliedtofilteroutlow-qualityprompts. Inadditiontoframingprompt\ngenerationasaparaphrasingproblem,wecanimprovethequalityofpromptsduringexpansion\nbylearningfromfeedback[Pryzantetal.,2023]. Thisapproachissomewhatrelatedtothe\nself-refinementissuediscussedinSection9.2.3. AnLLMcanbeusedtogeneratefeedbackon\naninputprompt, whichisthenrevisedbasedonthisfeedback. Thisfeedback-and-revision\ncycle can be repeated multiple times until the result converges or the desired outcome is\nachieved.\nAnotherapproachtopromptoptimizationistoapplyclassicoptimizationtechniques. For\nexample,theproblemcanbeframedasanevolutionarycomputationproblem,whereprompts",
    "aretreatedascandidatesthatevolvegenerationbygenerationastheoptimizationprogresses\n[Guo et al., 2024]. Since many powerful optimization algorithms have been developed in\nrelatedfields,theycanbedirectlyappliedtothisproblem.\nIn practice, we might be tempted to use existing LLM APIs to implement the steps\ndescribedabove. Suchanapproach,however,wouldbestronglydependentontheinference\nandin-contextlearningabilitiesoftheLLMs. IftheseLLMsarenotstrongandlackadaptation\ntothetasks,theymayintroduceerrorsintosearch,forexample,generatingincorrectprompts\nduringexpansion. Insuchcases, itispreferabletotrainmodelsthatarebettersuitedtothe\ntasks. Oneapproachinthisresearchdirectionappealstoreinforcementlearning,whichhas\nbeenwidelyusedinsolvingdiscretedecisionmakingandoptimizationproblems. Forexample,\nDengetal.[2022]developedapromptgeneratorbyintegratinganFFN-basedadaptorintoan\nLLM.Thepromptgeneratoristrainedasatypicalpolicynetwork, butonlytheparameters",
    "oftheadaptorareupdatedwhiletheremainingparametersofthemodelarekeptunchanged.\nDuringtraining,therewardisobtainedbytestingthegeneratedpromptsusinganotherLLM,\nsimilartotheevaluationmethodasdiscussedabove. Oncethetrainingiscomplete,theprompt\ngeneratoristhenemployedtogeneratenewprompts.\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and\nthe output of prompt optimization is such a sequence. However, in a strict sense, prompts\nhave complex structures and include different fields such as user input, instruction, and\ndemonstration. While our discussed approaches are mostly general, much work in prompt\noptimizationhasfocusedonlearningbetterinstructionsforprompting. Specifically,thegoal\nis to generate instructions that effectively guide LLMs based on a given task. Of course,\ntheconceptofpromptoptimizationcanalsobeextendedtolearningotherpartsofprompts.\nFor example, there has been substantial research interest in learning to select or generate",
    "demonstrationsinCoT[Liuetal.,2022;Rubinetal.,2022;Zhangetal.,2023b]. Oneofthe\ndifferencesbetweenlearninginstructionsandlearningdemonstrationsisthatgeneratinghigh-\nqualitydemonstrationsusingLLMsisrelativelyeasyandthefocusoflearningdemonstrations\nis typically on how to sample appropriate demonstrations from a pool of candidates. In\ncontrast, the difficulty in learning instructions is partly because pre-trained LLMs are not\nsuited to predict the quality of instructions, and testing these instructions on downstream",
    "9.3 Learning to Prompt 519\ntasksiscomputationallyexpensive. Thismakestheoptimizationmethodscostlytoapply,and\nexploringawidevarietyofinstructionsposessignificantchallenges.\n9.3.2 Soft Prompts\nAlthoughdevelopingnaturallanguageprompts,eithermanuallyorautomatically,isastraight-\nforwardandwidelyappliedapproach,itpresentssomeproblems. Oneproblemisthatnatural\nlanguagepromptscanbecomplexandlengthy,resultinginsignificantcomputationalburdens\nwhenprocessedviaLLMs. Inmanyapplications,usersmayneedtoperformataskrepeatedly,\nandinputtingthesamelongpromptintotheLLMsalargenumberoftimesisclearlyinefficient.\nAnotherproblemisthatwhilepromptsaretypicallyrepresentedasdiscretetokensequences\n(callthemhardprompts)inregularLLMinput,theLLMsencodethemaslow-dimensional\nreal-valuedvectors. Thisraisesthequestionofwhethertherearemorecompactandefficient\nwaystorepresentprompts.\nIn this subsection, we introduce the concept of soft prompts, which can be viewed as",
    "hidden,distributedrepresentationsofprompts. WhenpromptingLLMs,weareconcernedwith\ncommunicatingtasksorquestionstoelicitthedesiredresponses. Wecandefinehardprompts\nasexplicit,predefinedtextsequencesthatusersinputdirectlyintoLLMstoguidetheresponses.\nIncontrast,wecanthinkofsoftpromptsasimplicit,adaptablepromptingpatternsembedded\nwithinLLMs. Unlikehardprompts,whichareexpressedinnaturallanguageandshouldbe\nunderstandableforhumans,softpromptsareencodedinaformatthatismorecomprehensible\ntothemodelratherthantohumans. Toillustrate,considerasimpleprompt\nTranslatethesentenceintoChinese.\nConsideritdone!\nHere, the instruction “Translate the sentence into Chinese” can be seen as a hard prompt,\ndenoted by the token sequence c ...c . By feeding these tokens into an LLM, they are\n1 5\ntransformedintoasequenceofreal-valuedvectorsh ...h ,eachcorrespondingtoatoken. We\n1 5\ncanroughlythinkofh ...h asasoftprompt,asillustratedinFigure9.3.\n1 5",
    "Whiletheaboveexampleshowsthatsoftpromptscanbegeneratedbytransforminghard\nprompts, there is not necessarily a direct correspondence between them. In fact, we do not\neven need to interpret soft prompts using meaningful text. They are instead simply hidden\nstatesinLLMsandcanbelearnedasstandardparametersofthemodelsthroughcontinuous\noptimization. Such a treatment allows us to explore prompting methods beyond text. As\nanotherbenefit,softpromptsprovidedense,low-dimensional,andlearnablerepresentations\nforencodinghowweguideLLMstogeneratespecificoutputs. Thetrainingandapplication\noftheserepresentationsrequiresignificantlylowercomputationalcoststhanthoserequired\nforprocessinglonghardprompts. ThisapproachwouldbeofgreatpracticalvalueinLLM\ninferenceapplicationswherethesamepromptisrepeatedlyused.",
    "520 Chapter 9. Prompting\n... ... ... ... ... ... ... ... ... ... ... ...\nSoftPrompt\n... h h h h h h h h h h ...\nj j+1 j+2 j+3 j+4 j+5 j+6 j+7 j+8 j+9\nTransformer\n... Translate this into Chinese . I have a cat . ...\nHardPrompt(Instruction)\nFigure9.3: Illustrationofhardandsoftprompts. Herethehardpromptistheinstructionwe\ninputtotheLLMforperformingthetask. TheLLMencodesthisinstructionasusual,andthe\nintermediaterepresentationscorrespondingtotheinstructioncanbeviewedassomesortof\nsoftprompt.\n1. Adapting LLMs with Less Prompting\nOneobviouswaytoadaptanLLMforaparticulartaskistosimplyfine-tunethemodelusing\nlabeleddata. ThisleadstoavarietyofLLMalignmentmethods,suchassupervisedfine-tuning,\nwhichupdatethemodelparametersbyaligningtheresponsestogivenpromptswithsupervision\nsignals. Fine-tunedLLMsembedtask-relatedinformationinmodelparameters,andthusthese\nmodelscanrespondcorrectlywhendealingwithsimilarpromptswiththoseinfine-tuning.\nIfwetakethisideafurther,wecanexpectLLMstoabsorbtheknowledgeaboutprompting",
    "ofataskasmuchaspossibleduringfine-tuning. Consequently, thepromptinginformation\nispartiallycapturedinthemodelparameters,andthefine-tunedLLMscanperformthetask\nwithlessprompting. Hereweconsiderasimpleformofprompt,whereonlyaninstruction\n(denotedbyc)andauserinput(denotedbyz)areincluded. Apromptcanbeexpressedusing\nthefollowingtuple\nx = (c,z) (9.11)\nGiven a set of prompt-response pairs D = {(x,y)}, the objective of fine-tuning is to\nminimizethetotallossincurredoverthisset. Apopularmethodistominimizethenegative\nlog-likelihood(i.e.,maximizethelog-likelihood)withrespecttothemodelparametersθ:\n(cid:88)\nθˆ = argmax logPr (y|x)\nθ\nθ\n(x,y)∈D\n(cid:88)\n= argmax logPr (y|c,z) (9.12)\nθ\nθ\n(x,y)∈D",
    "9.3 Learning to Prompt 521\nwherePr (·|·)istheprobabilitypredictedbyanLLMwiththeparametersθ9.\nθ\nIn general, the instruction in each fine-tuning example should follow the guideline of\npromptdesign,forexample,agoodinstructionshouldbeasclearaspossibleandprovidea\ndetaileddescriptionofthetask. However,themethoddescribedintheaboveequationdoesnot\nrestricttheinstructiontoanyparticularform. ThisflexibilityallowsustoinstructLLMsin\nanywaythatwewant. ConsideranexamplewhereweintendtoinstructLLMstotranslatean\nEnglishsentenceintoChinese. Ofcourse,asmentionedearlierinthischapter,wecanprompt\nLLMsusingtheinstruction\nTranslatethefollowingsentencefromEnglishtoChinese.\nIfwewanttheinstructiontobesimpler,wemayrephraseitintoasimplerform\nTranslatethisintoChinese.\nEven,wecandefinetheinstructionasasinglephrase\nTranslate!\nWith certain fine-tuning effort, we can adapt LLMs to follow any of these instructions.\nFromanefficientpromptingperspective,therearecomputationaladvantagesinsimplifying",
    "instructionsinprompting. Forexample,wecanusesimpleinstructionslike“Translate!” to\nperformtasksthatwouldtypicallyrequiremorecomplexanddetailedinstructions. Thiscan\nmake subsequent prompting during inference much easier. On the other hand, fine-tuning\nLLMswithoverlysimplifiedinstructionsmaybeharmfultothegeneralizationofthemodels.\nSincesimplifiedinstructionscanleadtoalossofinformation,itismorelikelythattheLLMs\nwilloverfitthefine-tuningdataandfailtogeneralizebeyondthoseinstructions. Inscenarios\ninvolvingbothcomplexandsimplifiedinstructionsforfine-tuning,thisproblemismoresevere\nbecause the labeled data available for fine-tuning is usually limited, and accommodating a\nvarietyofinstructionsiscostly.\nAnalternativewaytoadaptLLMsforsimplifiedinstructionsisthroughknowledgedistilla-\ntion. Asanexample,weconsiderthecontextdistillationmethod[Snelletal.,2022]. Thegoal\nofthismethodistolearnastudentmodelthatcanmakeuseofsimplifiedinstructionsfrom",
    "a well-trained instruction-following teacher model. Figure 9.4 shows an illustration of this\napproach. Buildingtheteachermodelfollowsastandardfine-tuningprocess: wefirstcollecta\ncertainamountofdatathatincludesinstructions,userinputs,andcorrectresponses,andthen\nwecontinuetotrainapre-trainedmodelwiththisdataset. Forbuildingthestudentmodel,we\nneedtoconstructanewdatasetD′ whereeachsampleisatupleconsistingofaninstruction,a\ncorrespondingsimplifiedinstruction,andauserinput,denotedbyx′=(c,c′,z). Knowledge\ndistillationisperformedbyminimizingalossfunctiondefinedontheoutputsoftheteacher\n9Inpractice,weinitializeθwiththeparametersobtainedfrompre-training,andthenadjustθmoderatelyto\nensurethattheresultsafterfine-tuningdonotdeviatetoomuchfromthepre-trainedresults.",
    "522 Chapter 9. Prompting\nTeacherModel:\nFullContext + UserInput Prt(y|c,z)\nc z y\nLoss\nStudentModel:\nSimplifiedContext + UserInput Prs(y|c′,z)\nc′ z y\nFigure 9.4: Illustration of context distillation [Snell et al., 2022]. The teacher model is a\nstandardLLM,whichtakesboththecontextandtheuserinputasmodelinputandproducesa\npredictionasmodeloutput. Then,wesimplifythecontext(e.g.,simplifyingtheinstructionin\nprompting)andusethestudentmodeltomakepredictionsbasedonthesimplifiedcontextand\ntheuserinput. Thestudentmodelistrainedbyminimizingthelossbetweenthepredictions\nproducedbythetwomodels.\nandstudentmodels\n(cid:88)\nθˆ = argmin Loss(Prt(·|·), Prs(·|·), x′) (9.13)\nθ\nθ\nx′∈D′\nwherePrt(·|·)denotesthepre-trainedteachermodel,andPrs(·|·)denotesthestudentmodel\nθ\nwiththeparametersθ. TokeepthenotationsimplewewillwriteLoss(Prt(·|·), Prs(·|·), x)\nθ\nasLossforshort. Acommonly-usedlossisthesequence-levelloss,whichhasthebasicform:\n(cid:88)\nLoss = Prt(y|c,z)logPrs(y|c′,z) (9.14)\nθ\ny",
    "But this function is computationally infeasible because it requires summing over an\nexponentiallylargenumberofoutputs. Avariantofthismethodistotrainthestudentmodel\nusingoutputsgeneratedbytheteachermodel. Foreachsample,weusetheteachermodelto\nproduceanoutputyˆ=argmax logPrt(y|c,z). Thenweconsideryˆ asthetargetforlearning,\ny\nandthelossfunctionisgivenby\nLoss = logPrs(yˆ|c′,z) (9.15)\nθ\nAlternatively,wecanminimizethedistancesbetweentheprobabilitydistributionsoutputted\nbythetwomodels[Askelletal.,2021]. Forexample,thelossfunctioncanbedefinedasthe\nKLdivergencebetweenthetwooutputdistributions\nLoss = KL(Pt ||Ps) (9.16)\nθ",
    "9.3 Learning to Prompt 523\nwhere\nPt = Prt(·|c,z) (9.17)\nPs = Prs(·|c′,z) (9.18)\nθ θ\nAlthough we have restricted ourselves to knowledge distillation for instructions, the\napproachesdiscussedherearegeneral. Bylearningfromtheoutputsoftheteachermodel,the\nknowledgeinpromptingcanbedistilledintotheparametersofthestudentmodel. Therefore,\nthedistilledmodelcanbeconsideredasencodingsomesortofsoftprompt. Thismethodcan\nbeappliedtomanyotherproblemsinpromptlearning,suchascompressinglongcontextsand\nlearningsoftpromptsasspecificcomponentsofLLMs.\n2. Learning Soft Prompts for Parameter-efficient Fine-tuning\nUpdatingallparametersisacommonmethodforadaptingLLMstotasksofinterest. Although\nfine-tuningisconsideredcomputationallycheaperthanpre-training,itisstillcostlytoapply\ninpractice. Thisissuemotivatesthedevelopmentofparameter-efficientfine-tuningmethods,\nwhichaimtominimizethenumberofparametersthatneedtobeupdated.\nOneapproach,knownasprefixfine-tuning,istoappendaseriesoftrainablevectors,or",
    "prefixes,atthebeginningoftheinputofeachTransformerlayer[LiandLiang,2021]. These\nprefixescanbethoughtofassoftpromptsthatserveasadditionalcontexttoguidethebehavior\nofthemodelunderspecifictasks. Duringfine-tuning,weneedonlytolearntheprefixesfor\nembeddingtask-specificknowledge. Thus,thismethodisefficientbecauseitonlymodifiesa\nsmallpartofthemodelratherthanadjustingtheentiresetofmodelparameters.\nSpecifically,lettheinputofalayeratdepthlbedenotedbyHl =hlhl...hl . Theoutput\n0 1 m\nofthelayercanbeexpressedas\nHl+1 = Layer(Hl) (9.19)\nInprefixfine-tuning,weextendthesequencehlhl...hl byaddingafewvectorsatthe\n0 1 m\nbeginning,whichwedenoteasplpl...pl . HenceHl canbewrittenintheform\n0 1 n\nHl =pl pl ...pl hl hl ...hl (9.20)\n0 1 n 0 1 m\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\ntrainable previouslayeroutput\nTheoutputofthelayeristhelastm+1representations.\nH l+1 = Layer(Hl)[−m−1:]\n= hl+1hl+1...hl+1 (9.21)\n0 1 m",
    "where [−m−1 :] denotes the slicing operation that extracts the last m+1 elements of a\nl+1\nsequence. GivenH ,theinputofthenextlayercanbeexpressedinthesameformofEq.",
    "524 Chapter 9. Prompting\n(9.20):\nHl+1 = pl+1 pl+1 ...pl+1 H l+1\n0 1 n\n= pl+1 pl+1 ...pl+1hl+1hl+1...hl+1 (9.22)\n0 1 n 0 1 m\nHereeachp ∈Rdcanbeseenasalearnableparameter. Duringtraining,plpl...pl aretrained\ni 0 1 n\nasusual,andtheparametersoftheoriginalTransformermodelarekeptfixed.\nFigure9.5showsanillustrationofprefixfine-tuningforatranslationtask. Here,onlythe\nprefix vectors pl and pl are updated by receiving the error gradients from the output (i.e.,\n0 1\ntheChinesetranslation). Byadjustingthesevectorsforthetranslationtask,themodeladapts\naccordingly. This makes pl and pl serve as prompts which activate the LLM to perform\n0 1\nthetaskwithoutneedingexplicitinputpromptslike“Translatethefollowingsentencefrom\nEnglishtoChinese”. Attesttime,weprependtheoptimizedpl andpl tothelayer,andthe\n0 1\nLLMwillthentranslatetheinputsentence. Notethatprefixfine-tuningintroducesadditional\nL×n×dparameters,whereListhenumberoflayers,nisthenumberofprefixes,anddis",
    "thedimensionalityofeachprefix. However,thisnumberismuchsmallercomparedtothetotal\nnumberofparametersintheLLM,makingthefine-tuningprocesshighlyefficient.\nWhileprefixfine-tuningissimple,itstillrequiresmodificationstoLLMs. Alternatively,\nseparatingsoftpromptsfromtheLLMsallowsustopreservetheoriginalmodelarchitecture,\nmakingitmoreefficientfordeploymentacrossdifferenttaskswithouttheneedtoadjustthe\ncoremodel. Onesuchmethodisprompttuning[Lesteretal.,2021]. Likeprefixfine-tuning,\nprompttuningincorporatestrainablevectorssothatLLMscanadapttogiventasksbyadjusting\nthesevectors. However,prompttuningdiffersinthatitmodifiesonlytheembeddinglayer.\nRecall that in LLMs each input token z is represented by an embedding e . These\ni i\nembeddingsaregenerallylearnedthroughatokenembeddingmodelandarethenusedasthe\nrealinputstotheLLMs,replacingthesymbolicallyrepresentedtokens. Inprompttuning,a\nnumber of pseudo embeddings p ...p are added at the beginning of the token embedding\n0 n",
    "sequence. SotheactualinputtotheLLMscanbeexpressedas\np p ...p e e ...e\n0 1 n 0 1 m\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\ntrainable tokenembeddings\nNotethatapseudoembeddingneedsnottocorrespondtoanytokeninnaturallanguage. Instead\ntheseembeddingscanbeseenas“softpromptembeddings”thatservetoconditiontheLLMs.\nBytrainingsoftpromptembeddingsontask-specificdata,theylearntointeractadaptivelywith\nthe token embeddings e ...e and guide the behavior of LLMs. Since prompt tuning does\n0 m\nnotchangetheunderlyingparametersofpre-trainedLLMs,itisconsideredalightweightand\nefficientmethodoffine-tuning,improvingtask-specificperformancewhilemaintainingtheir\ngeneralizationcapabilities. SeeFigure9.6foranillustrationofprompttuning.\nSincep p ...p isitselfasequence,wecanemploysequencemodelstobetterrepresent\n0 1 n\nit. Forexample,aTransformermodelcanencodethissequence,andtheresultingrepresentation\ncanthenbeusedastheinputtotheLLM.Inotherwords,wecandevelopanadditionalmodel",
    "for encoding soft prompts. Another way to improve prompting is by combining soft and",
    "9.3 Learning to Prompt 525\nLoss Loss\n··· ··· ··· ··· ···\nLayerl+1\npl+1 pl+1 hl+1 hl+1 hl+1 hl+1 hl+1\n0 1 0 1 3 4 5\nLayerl\ntrainableprefixes pl pl hl hl hl hl hl\n0 1 0 1 2 3 4\nLayerl−1\npl−1 pl−1 hl−1 hl−1 hl−1 hl−1 hl−1\n0 1 0 1 3 4 5\n··· ··· ··· ··· ···\nLook out ! 小心 !\nSoftPrompt UserInput LLMPrediction\nFigure9.5: Illustrationofprefixfine-tuningforatranslationtask(Lookout! → 小心!). For\neachlayer,weaddtwoprefixespl andpl atthebeginning. TheLLMistrainedtominimize\n0 1\nthelossonthepredictionsgiventheinput. Duringthisprocess,onlytheprefixesareoptimized\nwhiletherestoftheparametersremainfixed. Therefore,themodelcanadapttothegiventask\ninaveryefficientmanner. Atinferencetime,theLLMworkswithoptimizedprefixes,andcan\nperformthetaskwithouttheneedofexplicithardprompts.\nhardprompts,therebytakingadvantageofbothtypes[Liuetal.,2023c]. Intheembedding\nsequence,wecanarrangeorinterspersetheseprompts. Thiswouldresultindifferentprompt\npatterns. Forexample,asimplepatternthatusesbothtwotypesofpromptis",
    "SoftPrompt HardPrompt UserInputandResponse\np 0 p 1 ··· p n q 0 q 1 ··· q m′ e 0 e 1 ··· e m\nc 0 c 1 ··· c m′ z 0 z 1 ··· z m\nwherec ...c denotesthehardpromptandq ...q denotesthecorrespondingembedding\n0 m′ 0 m′\nsequence.\nHerewehaveconsideredmethodsforinsertingsoftpromptsinLLMs. Butweskipthe\ndetailsoftrainingthesesoftpromptsandassumethatthereaderisfamiliarwiththestandard\nsupervised learning process, that is, maximizing the likelihood of the correct model output",
    "526 Chapter 9. Prompting\nLoss Loss\n··· ··· ··· ··· ··· ··· ···\nLayerl+1\nLayerl\nLayerl−1\n··· ··· ··· ··· ··· ··· ···\ntrainableprompt p 0 p 1 e 0 e 1 e 2 e 3 e 4\nembeddings\nLook out ! 小心 !\nSoftPrompt UserInput LLMPrediction\nFigure9.6: Illustrationofprompttuningforatranslationtask(Lookout! → 小心!). Instead\nofusingfixedtextualprompts,softpromptsarelearnableembeddingsthatareaddedatthe\nbeginning of the embedding sequence. During fine-tuning, only these prompt embeddings\nare optimized to efficiently adapt the LLM to the given task. Once optimized, the prompt\nembeddingsareusedtoinstructtheLLMtoperformthetaskasnewdataarrives.\ngiventhemodelinput. Infact,learningsoftpromptscanberelatedtomanyissuesinLLM\nfine-tuning. Forexample,ifweconsideritasacontextcompressionproblem,wecanapply\ntheknowledgedistillationmethodsdescribedpreviously. InMuetal.[2024]’swork,prompts\nare compressed and represented as a few pseudo tokens, which are appended to each input",
    "sequence. Theembeddingsofthesepseudotokensareoptimizedtomimicthepredictionsofa\nstandard-promptedmodel. Inotherwords,thepromptingknowledgeisdistilledfromateacher\nmodelintothepseudotokens.\nBroadly speaking, many parameter-efficient fine-tuning methods can be thought of as\nlearningsomesortofsoftprompt[Lialinetal.,2023]. Whenwefine-tuneapartofanLLM\nforatask,thisprocesscanessentiallybeseenasinjectingtask-relatedpromptinginformation\nintothatspecificpartofthemodel. Anotherwidely-usedapproachtoparameter-efficientfine-\ntuningistoaddanadaptorlayerbetweentheexistingmodellayers. Thisapproachallowsusto\nfine-tuneonlytheadaptorlayeronspecifictaskswithoutalteringtheunderlyingarchitecture\norretrainingtheentiremodel. Inthissense,adaptorlayerscanbeviewedassoftpromptsthat\nencode prompting and task-related information and interact with the original LLM to help\nit adapt. To summarize, Figure 9.7 shows a comparison of different methods of using soft\npromptsinLLMs.",
    "9.3 Learning to Prompt 527\nLLM LLM\n(a)SoftPromptsasPrefixes (b)SoftPromptsasInputs(Embeddings)\nLayer\nLLM LLM\n(c)Fine-tuningPartsoftheModel\nrotpadA\n(d)Fine-tuningtheAdaptor\nFigure9.7: IllustrationsofusingsoftpromptsinLLMs. Heretunablesoftpromptsareshown\ninblue,andcomponentswhoseparametersarefixedduringfine-tuningareshowningray. In\nsub-figure (a), soft prompts are prefixes appended to each layer of the LLM. In sub-figure\n(b),softpromptsareusedasinputembeddingsfortheLLM.Insub-figures(c)and(d),soft\npromptsarebroadlytreatedascomponentsofthemodelthatarefine-tunedfortaskadaptation.\n3. Learning Soft Prompts with Compression\nAnother approach to learning soft prompts is from the perspective of compression. As a\nsimple example, consider the problem of approximating a long context using a continuous\nrepresentation[Wingateetal.,2022]. Supposewehaveauserinputzanditscontextc(suchas\nlonginstructionsanddemonstrations). Nowwewanttodevelopacompressedrepresentation",
    "ofthecontext,denotedbyσ,suchthatthepredictionbasedonzandσ isascloseaspossible\ntothepredictionbasedonzandc. Thisgoalcanbeexpressedintheform\nσˆ = argmins(yˆ,yˆ ) (9.23)\nσ\nσ\nwhere yˆ = argmax Pr(y|c,z) and yˆ = argmax Pr(y|σ,z) are the LLM predictions\ny σ yσ\ngiventhefullcontextandthecompressedcontext,respectively. Thefunctions(·,·)typically\nrepresents a loss or similarity measure, aiming to minimize the difference in predictions\nbetweenthetwocontextrepresentations.\nOnegeneralframeworkforachievingthisisknowledgedistillation,whereyˆ andyˆ can\nσ\nbe seen as the predictions of the teacher model and the student model, respectively. This\nformalizationlinksourdiscussiontothecontextdistillationproblemdiscussedearlier. The\ntraining objective can be obtained by analogy with Eqs. (9.15) and (9.16). For example, a\nsimpletrainingobjectiveisgivenby\nσˆ = argmaxlogPr(yˆ|σ,z) (9.24)\nσ",
    "528 Chapter 9. Prompting\nAlternatively,wecanminimizetheKLdivergencebetweentheoutputdistributions,giving\nσˆ = argmin KL(Pr(·|c,z)|| Pr(·|σ,z)) (9.25)\nσ\nThedifferencewiththemodelsinEqs. (9.15)and(9.16)isthatherethecompressedcontext\nisrepresentedasreal-valuedvectors(callthempromptembeddings),ratherthanasnormal\ntokens. Byapplyingtheabovemethods,wedistillthecontextfromthetokensequencecinto\nthe embeddings σ. Note that the teacher model Pr(·|c,z) and the student model Pr(·|σ,z)\nmaynotsharethesamearchitectureormodelsettings. Inpractice,wegenerallywishforthe\nteachermodeltobestronger,whilethestudentmodelshouldbesmallerandmoreefficient.\nWhile compressing full context into continuous representations is a straightforward ap-\nproach to learning soft prompts, it requires a teacher model that can deal with long input\nsequences. Inmanycases,however,thecontextissolongthatapplyinganLLMistoocostlyor\ninfeasible. Modelinglonginputsequencescanfallunderthebroadfamilyofefficientmethods",
    "for long-context LLMs. Many techniques have been developed to address this issue. For\nexample,onecanuseafixed-sizeKVcachetostorethepastinformationateachstepduring\ninference. EfficientTransformerarchitecturesandlong-contextLLMshavebeenintensively\ndiscussedinthisbook. Formoredetaileddiscussionsofthesetopics,interestedreaderscan\nrefertoChapters6and8.\nTherearealsomethodsspecificallydesignedtocompresslongcontextintosoftprompts.\nHere we consider Chevalier et al. [2023]’s method as an example. The basic idea is that\nwelearnsoftpromptsgraduallybyaccumulatingthefixed-sizecontextrepresentationover\nthe context sequence. Given a long context, we first divide it into a number of segments\nz1,...,zK. Wethenprocessthesesegmentsinsequence,eachtimegeneratingarepresentation\nofthecontextwehaveprocessedsofar,denotedbyσ<i+1. Todothis,afewsummarytokens\n⟨g ⟩,...,⟨g ⟩ are introduced. At each step, we take a segment zi =zi...zi , along with the\n1 κ 1 mi",
    "previouscontextrepresentationσ<i andthesummarytokens⟨g ⟩,...,⟨g ⟩asinput,anduse\n1 κ\nanLLMtoproducethecorrespondinghiddenrepresentationsequenceatthelastTransformer\nlayer. AnexampleofthisprocessisillustratedinFigure9.8.\nHere σ<i is essentially a memory. The model operates in an RNN fashion. Each time\nwetakeasegmentandupdatethismemorybyencodingboththepreviousmemorystateand\nthesegment. Therefore,theσ<i producedatthelastsegmentisarepresentationoftheentire\ncontextsequence. TheTransformermodelforlearningtheserepresentationscanbeastandard\nLLMbutweneedtofine-tuneittoadapttothiscontextrepresentationtask.\nNotethatherewesimplyconsiderpromptandcontextassimilarterms,eventhoughthey\narenotthesame. Althoughwearesomewhat“misusing”theconceptprompt,wecanoften\nviewitasatypeofcontext. Fromthisperspective,themethodsdiscussedherecanbeapplied\ntogeneraltextcompressionproblems.\n9.3.3 Prompt Length Reduction\nWhilesoftpromptsprovidedense,hiddenrepresentations,theyarenotdirectlyinterpretable.",
    "Thelackofinterpretabilitycanbeasignificantbarrierforuserstryingtounderstandhowtheir",
    "9.3 Learning to Prompt 529\nh< 1 i h< 1 i h 1 h 2 h 3 h 4 σ 1 <i+1 σ 2 <i+1 SoftPrompts\nattheCurrentStep\nTransformer Layers\nSoftPrompts σ 1 <i σ 2 <i ei 1 ei 2 ei 3 ei 4 ⟨e 1 ⟩ ⟨e 2 ⟩\natStepi−1\nz 1 i z 2 i z 3 i z 4 i ⟨g 1 ⟩ ⟨g 2 ⟩\nFigure9.8: Illustrationofcompressingacontextsegmentintosoftprompts(κ=2andm =4).\ni\nThe input to the LLM includes the soft prompts from the previous step (σ<i and σ<i), the\n1 2\ntokens of the segment (z ,z ,z , and z ), and the summary tokens (⟨g ⟩ and ⟨g ⟩). Given\n1 2 3 4 1 2\nthese, theLLMoperatesasusual. WethenextracttheoutputsatthelastTransformerlayer\nthatcorrespondtothesummarytokens. Theseoutputscanbeviewedasthesoftpromptsthat\naccumulateduptothissegment.\ninputsinfluenceLLMoutputs. Moreover,althoughsoftpromptsareefficientforfine-tuning\nanddeployment,theyareinflexibleanddonotallowforeasyadjustmentswithoutextensive\nfine-tuningormodification. Thisinflexibilitycanlimittheirutilityindynamicenvironments\nwherepromptchangesarefrequentlyneeded.",
    "Onealternativewaytodevelopefficientpromptsistosimplifythetextusedforprompting.\nForexample,belowisapromptforansweringquestionsonhealthcareandfinance.\nThe task involves developing a language model capable of understanding\nand responding to user inquiries across various domains, with a particular\nemphasisonhealthcareandfinance. Consideringthebroadrangeofpotential\nqueries, from the specifics of medical diagnoses to the nuances of finan-\ncialregulations,themodelmustensureacomprehensiveunderstandingand\naccurateresponses.\nQuestion:\nWhat are the best practices for using artificial intelligence in diagnosing\ncardiovasculardiseases?\nWecansimplifythetaskdescriptionbydeletingtheunimportantparts.",
    "530 Chapter 9. Prompting\nThetaskinvolvesdevelopingalanguagemodelcapableofunderstanding\nandrespondingtouserinquiriesacrossvariousdomains,withaparticular\nemphasis on healthcare and finance. Considering the broad range of\npotential queries, from the specifics of medical diagnoses to the nuances\nof financial regulations, The model must ensure a comprehensive under-\nstandingandaccurateresponses.\nWecanalsoparaphraseitasashortertext.\nThetaskinvolvesdevelopingalanguagemodelfocusedonhealthcareand\nfinance, capable of understanding and accurately responding to a wide\nrangeofuserinquiries.\nThisproblemcanbeviewedasaclassicNLPissue—textsimplification. Sothemethods\nused can be general and not restricted to the problem of simplifying prompts. There are\nmany ways to achieve this. One simple method is to define some heuristics and identify\nredundant words that can be eliminated without losing essential information. For example,\nwecanexamineeachtokeninasequenceintermsofitscontributiontotheoverallmeaning",
    "andremovethosethatprovideminimalvalue[Lietal.,2023c;Jiangetal.,2023b]. Another\nmethodinvolvesframingtheproblemasasequence-to-sequencetask. Withlabeleddatafor\ntextsimplification,wecantrainanencoder-decodermodeltotransformeachinputtextinto\nitssimplifiedform. Inaddition,giventhatmanyLLMshavebeenfine-tunedandalignedto\nperformtextsimplificationtasks,itisstraightforwardtousethesemodelstosimplifyprompts.\nFor example, we can prompt an LLM to simplify a text under certain constraints, such as\nlimitingthelengthofthesimplifiedtext.\n9.4 Summary\nInthischapter,wehavediscussedavarietyofissuesrelatedtoLLMprompting. Ourdiscussion\nhasfocusedmainlyontwoaspects:\n• HowtodesignbasicpromptstoguidethepredictionsofLLMsandrefinetheseprompts\nformoreeffectiveandefficientproblem-solving?\n• Howtoautomatethedesignandrepresentationofprompts?\nSolutionstotheseissuesinvolvebothgeneralpromptdesignsandmoreadvancedtechniques,\nsuchasCoTandpromptlearning,whichhavebeenexploredextensivelyinrecentresearch.",
    "InNLP,promptingcanbeviewedasatechnologythathasevolvedalongwithLLMs,and\ninasense,ithasopenedthedoortothepracticalapplicationofthesemodelsinanimpressive\nrangeofproblemdomains. Infact,ifweexpandtheconceptofpromptstosomeextent,itcan\nbetracedbacktotheearlydaysofmachinelearningandNLP.Forexample,manyNLPsystems\nuse hand-crafted features and templates to “prompt” specific tasks. Imagine developing a\nfeaturetoindicatewhetheratextisformalorinformal. Wecanfeedthisfeatureintoamachine\ntranslationsystemtoconditionthetranslationonthetypeoftheinputtext.",
    "9.4 Summary 531\nThe widespread use of the modern concept of prompts began with the rise of large pre-\ntrained models in the field of NLP. Initially, these models, such as BERT, were adapted to\nspecificdownstreamtasksmainlythroughfine-tuning. However,researcherssoondiscovered\nthatbydesigningspecific\"prompts\"—addingcertainwordsorsentencestotheinput—the\nmodels could be triggered to respond to specific tasks without extensive fine-tuning. This\nmotivated the NLP community to develop and apply universal foundation models that can\nbe prompted to address various tasks without changing the underlying architecture and the\npre-trainingprocedure.\nPromptingapproacheswerefirstexperimentedwithsmallermodelsandlaterdemonstrated\nimpressivecapabilitieswithlargemodelslikeGPT-3,whichcouldgeneratehigh-qualitytext\ninresponsetosimplepromptsacrossvarioustasks. Aspromptingtechnologyevolved,prompt\nengineering emerged as a critical area of research. As discussed in this chapter, it broadly",
    "involves designing effective prompts to maximize model performance, encompassing both\nhand-craftedandautomaticallygeneratedprompts. Morerecentresearchhasexploredhowto\nenhancetheeffectivenessofpromptingthroughtechniqueslikefew-shotlearning,zero-shot\nlearning, and CoT reasoning, enabling LLMs to work effectively across a wide range of\nscenarios. A general discussion of prompting can be very broad, and we cannot cover all\ndetails in this chapter. For more advanced techniques of prompting, the reader can refer to\nrecentsurveys. Topicsincludein-contextlearning[Li,2023;Dongetal.,2022],CoT[Chu\netal.,2023;Yuetal.,2023b;Zhangetal.,2023a],efficientprompting[Changetal.,2024],\nandgeneralpromptengineering[Liuetal.,2023d;Chenetal.,2023a].\nNotethatalthoughwewouldideallyliketodevelopgeneralpromptingmethodswithout\nadjustingmodelarchitecturesandparameters,theresultsofpromptinggenerallydependheavily\nonthequalityandsizeofthegivenLLMs. Forstrongermodels,suchascommercializedonline",
    "LLMs,simplepromptsmaybesufficienttoinstructthesemodelstoperformtaskscorrectly. In\nthiscase,promptengineeringisrelativelyeasy,thoughwestillneedcertaineffortstomake\nLLMs work properly. By contrast, if the LLMs are not powerful enough, we may need to\ncarefullydesignthepromptstoachievethedesiredresults. Inmanycases,fine-tuningisstill\nnecessarytoadaptthemodelstosophisticatedpromptingstrategies.",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 10\nAlignment\nAlignment is not a new concept in NLP, but its meaning varies across different domains\nand over time. In traditional NLP, the term alignment typically refers to the tasks that link\ncorrespondingelementsintwosets,suchasaligningwordsbetweenaChinesesentenceand\nanEnglishsentence. AsLLMsbecomeincreasinglyimportantinNLPresearch,thistermis\nmorebroadlyusedtorefertoaligningmodeloutputswithhumanexpectations. Theproblem\nthatalignmentaddressesisthattheoutputofamodelmaynotalignwiththespecificgoals\nor contexts intended by users. For example, pre-trained LLMs may not be able to follow\nuserinstructionsbecausetheywerenottrainedtodoso. AnotherexampleisthatLLMsmay\ngenerateharmfulcontentorperpetuatebiasesinherentintheirtrainingdata. Thisposesnew\nchallengesinensuringthatLLMoutputsarenotonlyaccurateandrelevant,butalsoethically\nsoundandnon-discriminatory.",
    "Simplypre-trainingLLMscanresultinavarietyofalignmentproblems. Ourultimategoal\nistoresolveormitigatealltheseproblemstoensureLLMsarebothaccurateandsafe. There\nisaninterestingissuehere: sincelargelanguagemodelsaretrainedonvastamountsofdata,\nwehavereasontobelievethatifwehavesufficientdatacoveringavarietyoftasksandaligned\nwithhumanpreferences,pre-trainingcouldmakeLLMsaccurateandsafeenough,perhaps\neveneliminatingtheneedforalignment. However,therealityisthatitisnearlyimpossible\ntogatherdatathatencompassesalltasksoradequatelyrepresentshumanpreferences. This\nmakes it difficult to achieve model alignment through pre-training alone, or at least, at this\nstage,alignmentremainsaverynecessaryandcriticalstepinthedevelopmentofLLMs.\nInthischapter,wewillfocusonalignmentmethodsforLLMs. Wewillbeginbydiscussing\nthegeneralalignmenttasks. Thenwewillconsidertwowidely-usedapproaches,knownas\ninstructionalignmentandhumanpreferencealignment,respectively. Theformerresortsto",
    "supervisedfine-tuningtechniquesandguidestheLLMstogenerateoutputsthatadhereclosely\ntouserinstructions. Ontheotherhand, thelattertypicallyreliesonreinforcementlearning\ntechniques,wheretheLLMsaretrainedbasedonfeedbackfromhumans. Whilethesemethods\nare motivated by different goals, they are commonly used together to develop well-aligned\nLLMs.",
    "534 Chapter 10. Alignment\n10.1 An Overview of LLM Alignment\nAlignment can be achieved in several different ways. We need different methods for LLM\nalignmentbecausethisproblemisitselfcomplicatedandmultifaceted,requiringablendof\ntechnicalconsiderations. Hereweconsiderthreewidely-usedapproachestoaligningLLMs.\nThefirstapproachistofine-tuneLLMswithlabeleddata. Thisapproachisstraightforward\nasitsimplyextendsthepre-existingtrainingofapre-trainedLLMtoadaptittospecifictasks.\nAn example of this is supervised fine-tuning (SFT), in which the LLM is further trained\non a dataset comprising task-specific instructions paired with their expected outputs. The\nSFTdatasetisgenerallymuchsmallercomparedtotheoriginaltrainingset,butthisdatais\nhighly specialized. The result of SFT is that the LLM can learn to execute tasks based on\nuserinstructions. ThesetaskscaneitherbeonespreviouslyencounteredinSFT,ornewtasks\nsimilar to those. For example, by fine-tuning the LLM with a set of question-answer pairs,",
    "themodelcanrespondtospecificquestions,evenifnotdirectlycoveredintheSFTdataset.\nThismethodprovesparticularlyusefulwhenitisrelativelyeasytodescribetheinput-output\nrelationshipsandstraightforwardtoannotatethedata.\nThesecondapproachistofine-tuneLLMsusingrewardmodels. Onedifficultyinalignment\nisthathumanvaluesandexpectationsarecomplexandhardtodescribe. Inmanycases,even\nforhumansthemselves,articulatingwhatisethicallycorrectorculturallyappropriatecanbe\nchallenging. Asaresult,collectingorannotatingfine-tuningdataisnotasstraightforwardas\nitiswithSFT.Moreover, aligningLLMsisnotjustataskoffittingdata, orinotherwords,\nthelimitedsamplesannotatedbyhumansareofteninsufficienttocomprehensivelydescribe\nthese behaviors. What we really need here is to teach the model how to determine which\noutputsaremoreinlinewithhumanpreferences,forexample,wenotonlywanttheoutputs\nto be technically accurate but also to align with human expectations and values. One idea",
    "istodeveloparewardmodelanalogoustoahumanexpert. Thisrewardmodelwouldwork\nbyrewardingtheLLMwheneveritgeneratesresponsesthatalignmorecloselywithhuman\npreferences,muchlikehowateacherprovidesfeedbacktoastudent. Toobtainsuchareward\nmodel,wecantrainascoringfunctionfromhumanpreferencedata. Thetrainedrewardmodel\nisthenusedasaguidetoadjustandrefinetheLLM.ThisframestheLLMalignmenttaskasa\nreinforcementlearningtask. Theresultingmethods,suchasreinforcementlearningfrom\nhumanfeedback(RLHF),havebeendemonstratedtobeparticularlysuccessfulinadapting\nLLMstofollowthesubtletiesofhumanbehaviorandsocialnorms.\nThethirdapproachistoperformalignmentduringinferenceratherthanduringtraining\nor fine-tuning. From this perspective, prompting in LLMs can also be seen as a form of\nalignment, but it does not involve training or fine-tuning. So we can dynamically adapt an\nLLMtovarioustasksatminimalcost. Anothermethodtodoalignmentatinferencetimeisto\nrescoretheoutputsofanLLM.Forexample,wecoulddevelopascoringsystemtosimulate",
    "human feedback on the outputs of the LLM (like a reward model) and prioritize those that\nreceivemorepositivefeedback.\nThethreemethodsmentionedabovearetypicallyusedinsequenceoncethepre-training\nis complete: we first perform SFT, then RLHF, and then prompt the LLM in some way",
    "10.2 Instruction Alignment 535\nPre-training Alignment\nStage Stage\nInstruction HumanPreference\nPre-training Alignment Alignment Prompting\n(e.g.,SFT) (e.g.,RLHF)\nTraining&Fine-tuning Inference\nFigure10.1: Schematicillustrationofthepre-train-then-alignmethodfordevelopingLLMs.\nInthepre-trainingstage,wetrainanLLMonvastamountsofdatausingnexttokenprediction.\nThen,inthealignmentstage,wealigntheLLMtouserinstructions,intents,andpreferences.\nThisincludesinstructionalignment,humanpreferencealignment,andprompting.\nduring inference. This roughly divides the development of LLMs into two stages — the\npre-training stage and the alignment stage. Figure 10.1 shows an illustration of this. Since\npromptingtechniqueshavebeenintensivelydiscussedinthepreviouschapter,wewillfocus\nonfine-tuning-basedalignmentmethodsintherestofthischapter.\n10.2 Instruction Alignment\nOnefeatureofLLMsisthattheycanfollowthepromptsprovidedbyuserstoperformvarious",
    "tasks. Inmanyapplications,apromptconsistsofasimpleinstructionanduserinput,andwe\nwanttheLLMtofollowthisinstructiontoperformthetaskcorrectly. ThisabilityofLLMsis\nalsocalledtheinstruction-followingability. Forexample,belowisapromptwherewewant\ntheLLMtoextractkeypointsandprovideaconcisesummaryforalengthyarticle.\nInstruction Summarizethistextinthreesentences.\nInput DaylightSavingsTime(DST)-theprocessofmovingclocks\nforwardbyonehourinthesummer-wasstartedinGermanyin\n1916. DuringWorldWarOneitwasawaytosave...\nOutput\nThistaskrequirestheLLMtounderstandtheinstruction“Summarizethistextinthreesen-\ntences”andperformthesummarizationaccordingly. However,LLMsaretypicallytrainedfor\nnext-tokenpredictionratherthanforgeneratingoutputsthatfollowinstructions. Applyinga\npre-trainedLLMtotheaboveexamplewouldlikelyresultinthemodelcontinuingtowrite\ntheinputarticleinsteadofsummarizingthemainpoints. Thegoalofinstructionalignment",
    "536 Chapter 10. Alignment\n(orinstructionfine-tuning)istotunetheLLMtoaccuratelyrespondtouserinstructionsand\nintentions. Therestofthissectionwilldiscusssomeissuesrelatedtoinstructionalignment,\nincludingfine-tuningLLMstofollowinstructions,generatingorcollectinginstructiondata,\nandgeneralizinginstructionalignment.\n10.2.1 Supervised Fine-tuning\nOnestraightforwardapproachtoadaptingLLMstofollowinstructionsistofine-tunethese\nmodelsusingannotatedinput-outputpairs[Ouyangetal.,2022;Weietal.,2022a]. Unlike\nstandardlanguagemodeltraining,herewedonotwishtomaximizetheprobabilityofgener-\natingacompletesequence,butrathermaximizetheprobabilityofgeneratingtherestofthe\nsequencegivenitsprefix(i.e.,generatingtheoutputgiventheinput). Thisapproachmakes\ninstructionfine-tuningabitdifferentfrompre-training. Letx=x ...x beaninputsequence\n0 m\n(e.g.,instruction+userinput)andy=y ...y bethecorrespondingoutputsequence. TheSFT\n1 n\ndataisacollectionofsuchinput-outputpairs(denotedbyS),whereeachoutputisthecorrect",
    "responseforthecorrespondinginputinstruction. Forexample,belowisanSFTdataset\nx(instruction+userinput) y(output)\nSummarizethefollowingarticle. {∗summary∗}\nArticle: Inrecentyears,solarenergyhasseen\nunprecedentedgrowth,becomingthefastest-growing...\nAnalyzethesentimentofthefollowingreview. Positive\nReview: Iabsolutelylovedthenewdiningexperience.\nThefoodwasdivineandtheservicewasimpeccable.\nTranslatethefollowingsentenceintoFrench. Lapratiqueaide\nSentence: practiceindeedhelps. effectivement.\nExtractthemainfinancialfiguresfromthefollowing Revenue: $10million,\nearningsreport. ProfitMargin: 15%\nReport: Thecompanyreportedarevenueof$10million\ninthefirstquarterwithaprofitmarginof15%...\nClassifythefollowingemailasspamornotspam. Spam\nText: Congratulations! You’vewona$500giftcard.\nClickheretoclaimnow.\nProvideasolutiontothefollowingtechnicalissue. First,checkfor...\nIssue: mycomputerisrunningslowandoftenfreezes.\nwheretheinstructionsarehighlighted. Thisdatasetcontainsinstructionsandthecorresponding",
    "outputsforseveraldifferentNLPproblems,andsowecanfine-tuneanLLMtohandlemultiple\ntaskssimultaneously.\nInSFT,weaimtomaximizetheprobabilityofthecorrectoutputgiventheinput. Consider\nanLLMwithpre-trainedparametersθˆ\n. Thefine-tuningobjectivecanthenbeformulatedas:\n(cid:88)\nθ˜ = argmax logPr (y|x) (10.1)\nθˆ+\nθˆ+\n(x,y)∈D",
    "10.2 Instruction Alignment 537\nInput Output Loss=0 Loss̸=0\nx 1 x 2 x 3 y 1 y 2 x 1 x 2 x 3 y 1 y 2\nx x x x y x x x x y\n0 1 2 3 1 0 1 2 3 1\n(a)ForwardPass (b)BackwardPass\nFigure10.2: Illustrationofsupervisedfine-tuningforLLMs. Weconcatenatetheinputandthe\noutputintoasinglesequence. Duringtheforwardpass,weruntheLLMasusual. Duringthe\nbackwardpass,wecomputethelossonlyfortheoutputpartandsimplysetthelossforthe\ninputpartto0.\nwhereθ˜ denotestheparametersoptimizedviafine-tuning,andθˆ+representsanadjustmenttoθˆ\n.\nHerewewillomitthesuperscript+anduseθ torepresentθˆ+ tokeepthenotationuncluttered.\nButthereadershouldkeepinmindthatthefine-tuningstartsfromthepre-trainedparameters\nratherthanrandomlyinitializedparameters.\nTheobjectivefunctionlogPr (y |x,y )iscomputedbysummingthelog-probabilities\nθ i <i\nofthetokensiny,conditionalontheinputxandalltheprevioustokensy :\n<i\nn\n(cid:88)\nlogPr (y|x) = logPr (y |x,y ) (10.2)\nθ θ i <i\ni=1\nThisformulationisequivalenttominimizingthecross-entropyloss.",
    "Notethatminimizingtheconditionallog-probabilitylogPr (y|x)isnotastandardlan-\nθ\nguagemodeltrainingproblem. Ifweconcatenatexandyasasinglesequence,amoregeneral\nform of language modeling is based on the joint log-probability logPr (x,y), that is, we\nθ\nminimizethelossoveralltokensofthesequenceseq =[x,y]. Wecanwritetheprobability\nx,y\nofthissequenceusingthechainrule\nlogPr (seq ) = logPr (x,y)\nθ x,y θ\n= logPr (x)+logPr (y|x) (10.3)\nθ θ\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nsetto0 losscomputation\nTherearetwotermsontheright-handsideoftheequation. Wecansimplysetthefirstterm\nlogPr (x)to0,focusingsolelyonthesecondtermlogPr (y|x)forlosscomputation. Asa\nθ θ\nresult,thetrainingcanbeimplementedusingstandardLLMs. Forthesequenceseq ,wefirst\nx,y\nruntheforwardpassasusual. Then,duringthebackwardpass,weforcethelosscorresponding\ntoxtobezero. Figure10.2showsanillustrationofthisprocess.\nBytakinglogPr (seq )astheobjectivefunction,wecandescribeSFTusingaregular\nθ x,y",
    "538 Chapter 10. Alignment\nformoflanguagemodeltraining:\n(cid:88)\nθ˜ = argmax logPr (seq ) (10.4)\nθ x,y\nθ\n(x,y)∈D\nTheproblemweconsideredaboveisfundamentallyasingle-roundpredictionproblem,\nwheretheLLMgeneratesaresponsebasedonasingleinputwithoutanyfurtherinteractionor\nfeedbackfromtheuser. Theinputisprocessed,andtheoutputisgeneratedinonego. Thisis\ntypicalinscenarioswhereasinglequestionisasked, andasingleanswerisprovided, with\nnofollow-upquestionsorclarifications. However,inpractice,wesometimeshavetohandle\nmulti-roundpredictionproblems,forexample,anLLMengagesinadialogueovermultiple\nturns. Inthissetting,theLLMnotonlygeneratesresponsesbasedontheinitialinputbutalso\nincorporatessubsequentinputsthatmightrefineorexpandonearlierinteractions. Forexample,\nwecanusetheLLMtoactasahealthcareassistantchatbotandhaveaconversationwiththe\nuser,likethis\nUser I’vebeenfeelingverytiredlately.\nChatbot I’msorrytohearthat. Besidesfeelingtired,haveyounoticedany\nothersymptoms?",
    "User Yes,I’malsoexperiencingheadachesfrequently.\nChatbot Howlonghavethesesymptomsbeengoingon?\nUser Aboutaweeknow.\nChatbot Itmightbegoodtocheckinwithahealthcareprofessional. Would\nyoulikehelpsettingupanappointment?\nUser Yes,please. Canitbeafterworkhours?\nChatbot Sure,Icanarrangethat. ThereareslotsavailablenextWednesday\nandThursdayafter5PM.Whichdayworksbetterforyou?\n...\nIn this task, there are several rounds of conversation, each involving the generation of\na response based on the user’s request or question and the conversational history. Suppose\nwehaveK roundsofconversation,denotedby{x1,y1,x2,y2,...,xK,yK}. Herexk andyk\ndenotetheuserrequestandtheresponse,respectively,foreachroundk. Thelog-probability\nof generating the response can be written as logPr (yk|x1,y1,...,xk). Our goal is then to\nθ\nmaximizethesumoftheselog-probabilities\nK\n(cid:88)\nθ˜ = argmax logPr (yk|x1,y1,...,xk) (10.5)\nθ\nθ\nk=1",
    "10.2 Instruction Alignment 539\nAstraightforwardimplementationofthisinvolvescalculatingtheconditionalprobability\nforeachk. However,itrequiresrunningtheLLMK times,eachtimewithanincreasedconver-\nsationalhistorytomakepredictions. Amoreefficientmethodistoperformlosscomputation\nof all responses in a single run of the LLM. To do this, we represent the conversation as a\nsequence seq =[x1,y1,...,xK,yK] (or seq for short). The log-probability of\nx1,y1,...,xK,yK\nthissequenceisgivenby\nlogPr (seq) = logPr (x1,y1,...,xK,yK)\nθ θ\n= logPr (x1)+logPr (y1|x1)+···+\nθ θ\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nsetto0 losscomputation\nlogPr (xK|x1,y1,...,yK−1)+\nθ\n(cid:124) (cid:123)(cid:122) (cid:125)\nsetto0\nlogPr (yK|x1,y1,...,xK) (10.6)\nθ\n(cid:124) (cid:123)(cid:122) (cid:125)\nlosscomputation\nThe trick here is that we ignore the loss for generating user inputs (i.e., logPr (x1),...,\nθ\nlogPr (xK|x1,y1,...,yK−1)), as illustrated in Figure 10.3. Hence we only compute the\nθ",
    "probabilitiesofgeneratingtheresponsesgiventheirconversationalhistories,inotherwords,\nthevalueontheright-handsideofEq. (10.6)isactuallyequaltothevalueontheright-hand\nsideofEq. (10.5). AswithEq. (10.4),thetrainingofthismulti-roundpredictionmodelcanbe\nachievedbymaximizingtheloglikelihoodoveratrainingdatasetD:\n(cid:88)\nθ˜ = argmax logPr (seq) (10.7)\nθ\nθ\nseq∈D\nWhileimplementingtheSFTmethodsintroducedaboveseemstrivialastheyarefunda-\nmentally the same as regular language model training, there are still issues that need to be\nconsideredinpractice. Forexample,\n• SFTrequireslabeleddata. ThismakesSFTquitedifferentfrompre-training,whereraw\ntext is used as training data and is readily available. As in other supervised machine\nlearningproblems,dataannotationandselectioninSFTarenotsimpletasks. Ingeneral,\nwewishtodevelopSFTdatathatisbothsubstantialinquantityandhighinquality,and\nthisdatashouldbehighlyrelevanttothetaskstheLLMwillperform. Ontheotherhand,",
    "there is a need to fine-tune LLMs with less data to minimize computational and data\nconstructioncosts. Often,thequalityofLLMsishighlydependentonthedatausedin\nSFT.Thus,suchdatamustbecarefullydevelopedandexamined. Aswewillseeinlater\nsubsections,SFTcanbemoreefficientandeffectivethroughmoreadvancedtechniques\nfordataconstruction.\n• SFT is still computationally expensive for LLMs due to their large size. As a result,\nmaintainingandupdatingsuchmodelsisresource-intensive. Forexample,applyinggra-\ndientupdatestobillionsofparameterswithinanLLMrequiressignificantcomputational\npower and memory. This often requires high-performance computing environments,",
    "540 Chapter 10. Alignment\nLoss=0 Loss̸=0 Loss=0 Loss̸=0\nPr (x1) Pr (y1|x1) Pr (x2|x1,y1) Pr (y2|x1,y1,x2)\nθ θ θ θ\nx1 y1 x2 y2 ···\nUser:I’vebeenfeelingverytiredlately.\nChatbot:I’msorrytohearthat.Besidesfeelingtired,\nhaveyounoticedanyothersymptoms?\nUser:Yes,I’malsoexperiencingheadachesfrequently.\nChatbot:Howlonghavethesesymptomsbeengoingon?\n.\n.\n.\nFigure10.3: Illustrationofsupervisedfine-tuningforconversationalmodels. HeretheLLM\nactsasachatbottorespondtoeachrequestbasedontheconversationalhistory. Theconversa-\ntionprogressesbyalternatingbetweentheuserandthechatbot. InSFT,wetreattheentire\nconversation as a sequence, just like in standard LLMs, but compute the loss only for the\nresponsesoftheLLM.\nwhicharecostlytooperate. Toaddressthesechallenges,variousoptimizationstrategies,\nsuchaspruning,quantization,andtheuseofmoreefficienttrainingalgorithms,have\nbeenexplored. Inparticular, there hasbeensignificant interestin parameter-efficient",
    "fine-tuningmethodswhicharedesignedtomaintainstate-of-the-artperformancewithout\ntheneedforextensivecomputationalresources. WehaveseeninChapter9thatapplying\ntechniqueslikesoftpromptscanmakethefine-tuningprocessmoreefficient. Forfurther\ndiscussiononparameter-efficientmethods,thereadercanrefertorelatedpapersonthis\nissue[Houlsbyetal.,2019;Huetal.,2022;Hanetal.,2024].\n• SFTcanberegardedasapost-trainingstepfollowingpre-training. Itisaseparatetraining\nphasedesignedtopreservetheadvantagesoftheinitialpre-trainingwhileincorporating\nnew adjustments. This may seem paradoxical because updating a pre-trained LLM\nwith further data potentially causes the model to forget some of its prior knowledge.\nImagine a scenario where we have a large amount of SFT data and extensively fine-\ntunetheLLM.Inthiscase,theLLMcouldoverfitthedata,whichinturnmayreduce\ngeneralization performance or cause catastrophic forgetting. A common strategy to\nmitigatethisissueistoemployregularizationandearlystoppingtechniques. Another",
    "10.2 Instruction Alignment 541\npracticalapproachistouseasmallerlearningratetogentlyadjusttheweightsofthe\nLLM.Inaddition,fine-tuningwithdatafromdiversesourcesandproblemdomainscan\nalsobebeneficial. Nevertheless,inpractice,theSFTstepisoftencarefullyexamined\nandrequiressubstantialengineeringandexperimentaleffortstooptimize.\n10.2.2 Fine-tuning Data Acquisition\nFine-tuningdataissoimportantthatmuchrecentworkinLLMhasfocusedondeveloping\nvariousdatasetsforinstructionfine-tuning. Aswithmostworkinmachinelearning,thereare\ngenerallytwoapproachestodataacquisition—manualdatagenerationandautomaticdata\ngeneration.\n1. Manually Generated Data\nOnestraightforwardmethodistorecruithumanannotatorstocreateinput-outputpairsforthe\ntasksofinterest. UnlikedataannotationinconventionalNLP,suchastextclassification,where\nannotatorssimplyassignlabelstocollectedtextsaccordingtoguidelines,creatingfine-tuning\ndataforLLMsrequiresmorestepsandeffort,makingitthusmorechallenging. Supposewe",
    "wanttoobtainfine-tuningdatafortheEnglish-to-Chinesemachinetranslationtask. Thefirst\nstep is to write a prompt template to describe the task and format the problem clearly. For\nexample,\nInstruction TranslatethetextfromEnglishtoChinese.\nUserInput {∗text∗}\nOutput {∗translation∗}\nThen,wecollectpairsofsourceandtargettexts(i.e.,Chinesetextsandthecorresponding\ntranslations), and replace the variables {∗text∗} and {∗translation∗} to generate the fine-\ntuningsamples. Forexample,givenapairofEnglishandChinesesentences\nHow’stheweathertoday? → 今天天气怎么样？\n{∗text∗} {∗translation∗}\nwecangenerateafine-tuningsampleusingtheprompttemplate,likethis\nInstruction TranslatethetextfromEnglishtoChinese.\nUserInput How’stheweathertoday?\nOutput\n今天天气怎么样？",
    "542 Chapter 10. Alignment\nThatis,\nx = TranslatethetextfromEnglishtoChinese.\\nHow’stheweathertoday?\ny = 今天天气怎么样？\nWecanusethis(x,y)pairtofine-tunetheLLM,asdescribedintheprevioussubsection.\nOnedifficultyhereisthattherearemany,manydifferentwaystowriteprompttemplates\nforthesametask,anddifferentpeoplemayproduceprompttemplateswithvaryingqualities\nandcomplexities. Sometimes,wemaywriteprompttemplateswithoverlycomplexorverbose\ninstructions. Sometimes, we may not even know exactly what the target task is and how\nto describe it. A widely-adopted strategy is to create prompt templates for existing NLP\ntasks, given that there have been so many well-established NLP problems and benchmarks\n[Bach et al., 2022; Wang et al., 2022e; Mishra et al., 2022]. In this case, annotators can be\ngiventhe originaltaskdescription andmanyexamples. Then, they canusetheir own ways\nto express how to prompt the LLM to perform the tasks. Note that, while such a method",
    "can ease the process of creating and writing prompts, we still need annotation frameworks\nand crowdsourcing systems to manage the work and conduct quality control. For example,\nwe generally need to design annotation guidelines and a unified format for writing prompt\ntemplates,especiallywhenmanyannotatorsarecontributingtothesametask. Oneadvantage\nof inducing prompts from existing NLP tasks is that, once the prompt templates have been\ndeveloped,itiseasytogeneratepromptsusingtheannotatedsamplesintheoriginaltasks. For\nexample,givenabilingualdatasetforEnglish-to-Chinesetranslation,wecaneasilycreatea\nnumberoffine-tuningexamplesbyfillingtheslotsintheabovetemplatewiththesentence\npairsinthisdataset.\nAnotherapproachistodirectlyusethenaturallyexistingdataavailableontheinternet. A\ncommonexampleisbycollectingquestion-and-answerpairsfromQAwebsitestofine-tune\nLLMsforopen-domainQAtasks[Joshietal.,2017]. ManybenchmarksinQAarebuiltin",
    "thiswaybecausetherearesomanytypesofquestionsthatitisimpossibletothinkofthemall\nbyasmallgroupofpeople. Instead,usingdatafromthosewebsitescanensurethattheLLM\nfine-tuningdataisatagoodoracceptablelevelintermsofquantityandquality.\nIn addition to employing existing resources, another straightforward way to develop a\nfine-tuningdatasetistocrowdsourcethedata. Asimpleapproachistoallowuserstoinput\nanyquestion,afterwhichresponsesareeithermanuallygivenorautomaticallygeneratedby\nanLLMandthenmanuallyannotatedandcorrected. Itisthuspossibletocapturerealuser\nbehaviorandconsequentlygatherinputsandoutputsforalargenumberof“new”problems\nthattraditionalNLPtasksdonotcover.\nAnissuerelatedtotheconstructionofthefine-tuningdatasetsisthatweusuallywantthe\ndata to be as diverse as possible. Many studies have found that increasing the diversity of\nfine-tuningdatacanimprovetherobustnessandgeneralizationabilityofLLMs. Forthisreason,",
    "there has been considerable interest in involving more diverse prompts and tasks in LLM\nfine-tuniningdatasets. Wewillprovidefurtherdiscussiononthegeneralizationoffine-tuning\ninSection10.2.4.",
    "10.2 Instruction Alignment 543\n2. Automatically Generated Data\nOnelimitationofmanualdatagenerationisthatthequalityanddiversitylargelydependon\nhuman experience and creativity. Therefore, if we want LLMs to handle a broad range of\ntasks,thatis,toeffectivelyexecuteanyinstruction,relyingonhuman-annotateddataforLLM\nfine-tuningisofteninefficient. Moreover,thecoverageofsuchdatacanbelimited,andthe\ndatamayevencontainbiasesintroducedbytheannotatorsthemselves. Analternativeapproach\nistogeneratedataautomatically. Forexample,wecancollectanumberofquestionsthrough\ncrowdsourcing,andemployawell-tunedLLMtogenerateanswerstothequestions. These\nquestion-answerpairsarethenusedasfine-tuningsamplesasusual. Thismethod,thoughvery\nsimple,hasbeenextensivelyappliedtogeneratelarge-scalefine-tuningdataforLLMs.\nThe above way of producing synthetic fine-tuning data is similar to those used in data\naugmentationforNLP.IfwehaveanLLM,wecanproduceapredictioninresponsetoany",
    "input. Repeating this process for different inputs allows us to create a sufficient number of\nfine-tuningsamples. Suchamethodisparticularlyusefulforfine-tuningnewLLMsusinga\nwell-tunedLLM.However,onedisadvantageofthisapproachisthatitreliesonhuman-crafted\norcollectedinputsfordatageneration,whichmayturnouttobeinappropriateforgeneralizing\nLLMs. InmanyLLMapplications,asignificantchallengearisesfromthebroadrangeofusers’\nquestions and requests, many of which are not covered in existing NLP tasks and datasets.\nInthesecases,itbecomesnecessarytogeneratenotonlythepredictionsbutalsotheinputs\nthemselves.\nHereweconsiderself-instructasanexampletoillustratehowtogenerateLLMfine-tuning\nsamples[Wangetal.,2023e;Honovichetal.,2023]. TheideaisthatwecanpromptanLLM\ntocreateanewinstructionbylearningfromotherinstructions. Giventhisinstruction,theLLM\ncanthenfillinotherfields(suchastheuserinput)andproducethepredictions. Figure10.4\nshowsaschematicillustrationofself-instruct. Herewegiveabriefoutlineofthekeysteps",
    "involved.\n• The self-instruct algorithm maintains a pool of tasks. Initially it contains a number\nof seed hand-crafted tasks, each with an instruction and input-output sample. As the\nalgorithmproceeds,LLM-generatedinstructionsandsampleswillbeaddedtothispool.\n• Ateachstep,asmallnumberofinstructionsaredrawnfromtheinstructionpool. For\nexample, we can randomly select a few human-written instructions and a few LLM-\ngeneratedinstructionstoensurediversity.",
    "544 Chapter 10. Alignment\nInitializethetaskpoolwithanumberofinstructions\nandcorrespondinginput-outputsamples.\nInitialization\nSample1: (Instruction,User-input,Output)\nSample2: (Instruction,User-input,Output) TaskPool\n···\nDrawafewinstructionsfromthepool\nSampling\nInstructiona\nsampling\nInstruction b TaskPool\nInstructionc\nPrompttheLLMtogenerateanewinstructionbasedon\nInstruction thedrawninstructions.\nGeneration You are provided several different instructions for performing\nsome tasks. Please generate an instruction based on these.\nTask 1:Instructiona\nTask 2:Instructionb\nTask 3:Instructionc\nNew Task:Instructionnew\nGiventhenewly-generatedinstructionandafew\nSample input-outputsamples,generateanewsample.\nGeneration\nYou are provided with a set of input-output samples tasks,\neach composed of an instruction, a user input, and an output.\nPlease generate a new sample based on these.\nSample 1:Samplea\nSample 2:Sampleb\nNew Sample:InstructionnewUser-inputnewOutputnew",
    "Filteroutinvalidandlow-qualitysamples.\nAddtheremainingsamplesintothepool.\nFiltering\nFigure10.4: Illustrationofself-instruct[Wangetal.,2023c]. Thismethodmaintainsapoolof\ninstructionsandcorrespondinginput-outputsamples. Initially,thepoolcontainsanumberof\nhand-craftedinstructionsandsamples. Eachtime,wedrawafewinstructionsfromthepool.\nAn LLM is then prompted to generate new instructions and samples based on those drawn.\nFinally,thenewly-generatedinstructionsandsamplesarefilteredandaddedtothepool.\n• The selected instructions are then used as demonstration examples. Thus, the LLM\ncanin-contextlearnfromtheseexamplesandproduceanewinstruction. Belowisan\nexampletemplateforpromptingtheLLM.",
    "10.2 Instruction Alignment 545\nYou are provided several different instructions for performing some\ntasks. Pleasegenerateaninstructionbasedonthese.\nTask1: {instruction1}\nTask2: {instruction2}\nTask3: {instruction3}\nTask4: {instruction4}\nNewTask:\n• Giventhegeneratedinstruction,theLLMisthenpromptedtocompletethesampleby\nfillingintheremaininginputfieldsandgeneratingthecorrespondingoutput. Belowisa\nprompttemplate.\nYouareprovidedwithasetofinput-outputsamples,eachcomposed\nofaninstruction, auserinput, andanoutput. Pleasegenerateanew\nsamplebasedonthese.\nSample1: {instruction1}\nInput: {user-input1}\nOutput: {output1}\nSample2: {instruction2}\nInput: {user-input2}\nOutput: {output2}\nNewSample: {new-instruction}\n• Thisnewly-generatedsampleisexaminedbysomeheuristicrules(suchasfilteringout\nsamples or instructions that are similar to those already in the pool). If it passes, the\nsampleandinstructionareaddedtothepool.\nThis generation process can be repeated many times to obtain a sufficient number of",
    "fine-tuningsamples. Notethat,above,wejustshowsimpleprompttemplatesforgenerating\ninstructionandfine-tuningsamples. Ofcourse,wecandevelopbettertemplatestogenerate\nmorediverseandaccurateinstructionandfine-tuningsamples. Forexample,forcertaintasks\nliketextclassification,theLLMmaytendtoproducebiasedpredictions,forexample,most\ngeneratedsamplesbelongtoasingleclass. Insuchcases,wecanadjusttheorderofgeneration\nofdifferentfields. Morespecifically,wecanspecifytheoutput(i.e.,theclass)withsomeprior,\nand prompt the LLM to generate user input given both the instruction and the output. This",
    "546 Chapter 10. Alignment\nmethodresemblesinputinversion,wheretheLLMgeneratestheinputbasedonthespecified\noutput[Longpreetal.,2023].\nUsingLLM-generatedinstructionsandfine-tuningsampleshasbeenacommonmethod\nfordevelopingLLMs,especiallygiventhatmanuallydevelopingsuchdataissoexpensivethat\nmostresearchgroupscannotaffordit. Inseveralwell-tunedLLMs,theirfine-tuningdatasets\nincludeacertainamountofsyntheticdata,whichhasproveduseful[Ouyangetal.,2022;Taori\netal.,2023;Chiangetal.,2023b]. Therehavebeenfurtherstudiesonsyntheticdatageneration\nforLLMfine-tuning. Forexample,onecangeneratemorediverseinstructionsbyintroducing\nevolutionary algorithms [Xu et al., 2024], or use synthetic data as supervision signals in a\nmoreadvancedfine-tuningprocess[Chenetal.,2024b]. Morerecently,therehasalsobeen\nconsiderableinterestinusingsyntheticdatainthepre-trainingstage[Gunasekaretal.,2023;\nAllaletal.,2024].\nInmanyapplications,areal-worldscenarioisthat,givenatask,wecancollectorannotate",
    "a relatively small amount of fine-tuning data, for example, we can recruit experts to create\nquestions for QA tasks in a specific domain. But the quantity and diversity of this data are\nin general not sufficient. In this case, we can use self-instruct techniques to generate more\ndiverse question-answer pairs, and thus augment the fine-tuning data. This provides a way\nof bootstrapping the LLM starting from a seed set of fine-tuning samples. Note that using\nself-generated data is a common practice and has long been applied in NLP. For example,\nthisapproachhasbeensuccessfullyusedinparsingandmachinetranslation[Charniak,1997;\nSennrichetal.,2016a].\n10.2.3 Fine-tuning with Less Data\nWiththeincreasingprominenceofinstructionfine-tuning,therehasbeenasurgeindemandfor\nlarge-scale,high-qualityfine-tuningdata. Forexample,theFLANfine-tuningdataset,whichis\ncompiledfrom1,836tasks,contains15millionsamples[Longpreetal.,2023]. Fine-tuning",
    "LLMswithsuchlargedatasetsistypicallyacomputationallyexpensivetask,especiallygiven\nthatupdatingthelargenumberofparametersinLLMsisresource-intensive. Oneapproach\nformitigatingthisissueistoexploreefficientmodeltrainingmethods,forexample,onecan\nuseparameter-efficientmethodstoupdateonlyasmallportionofthemodel. However,many\nfine-tuningdatasetscontainalargeamountofsyntheticdata,whereerrorsandbiasesarestill\ninevitable.\nAnotherapproachtoefficientfine-tuningistoconsideronlythemostrelevantandimpactful\nexamplesforfine-tuning. Wecanthusreducetheamountofdatathatneedstobeprocessed\nwhilestillmaintainingthequalityofthemodelupdates. Thereareseveralmethodstoachieve\nthis. Forexample,Zhouetal.[2023a]builtaninstruction-followingdatasetcontainingonly\n1,000 samples by carefully crafting the prompts and collecting samples from a variety of\nNLP tasks. They showed that the LLaMa 65B model fine-tuned with this dataset could be\ncompetitivewithorevenbetterthanmodelswithmuchmorefine-tuningeffort. Thissuggests",
    "thatLLMscanbeadaptedtorespondtodiversetaskswithoutnecessarilyneedingfine-tuning\nonalltypesofinstruction-followingdata. Chenetal.[2024a]developedasystembasedon\nthe GPT-3.5 model to assess the quality of each instruction-following sample. Therefore,",
    "10.2 Instruction Alignment 547\nthey could select high-quality samples from existing datasets, showing better fine-tuning\nperformance with fewer fine-tuning samples. Researchers have also developed methods to\neitherselectorfilteroutdatausingheuristics[Zhaoetal.,2024;Geetal.,2024],ortoprioritize\ndatathatmoresignificantlyinfluencesthefine-tuningprocess[Xiaetal.,2024]. Infact,most\nof these methods can be seen as instances of larger families of data selection and filtering\nmethods. Anditisoftenthecasethatusinghigherquality(butmaybeless)dataisbeneficial\nfortrainingNLPmodels.\nThediscoveriesininstructionfine-tuningsomewhatdifferfromtraditionalviewsinNLP:\nthe ability of models to handle complex problems can be activated with a small amount of\nannotateddata,ratherthanrequiringmassiveamountsofsuperviseddataforextensivetraining.\nOnepossibleexplanationisthattheabilityofgeneratingcorrectresponsesgiveninstructions",
    "has been learned during pre-training, but such instruction-response mappings are not with\nhigh probabilities during inference. Fine-tuning can slightly adjust the models to get them\nto follow instructions, requiring significantly less training effort than pre-training. This is\nclosely related to what is known as the superficial alignment hypothesis, which suggests\nthatlearningprimarilyoccursduringpre-training,andthesubsequentfine-tuningoralignment\nphasedoesnotsignificantlycontributetotheunderlyingknowledgebaseofanLLM[Zhou\net al., 2023a]. Since the core abilities and knowledge of the model are already established\nfrompre-training,effectivefine-tuningforalignmentwithuserneedscanbeachievedwith\nrelativelysmalltrainingfine-tuningeffort. Thisimpliesthepossibilityoffine-tuningLLMs\nwithverylittledata. Inanotherdirection,itmaynotbenecessarytorestrictfine-tuningtopaired\ninstruction-responsedata. Forexample,Hewittetal.[2024]foundthatinstruction-following",
    "can be implicitly achieved by fine-tuning LLMs only on responses, without corresponding\ninstructions.\nAconceptrelatedtothediscussionhereissampleefficiency. Amachinelearningmethod\niscalledsampleefficientifitcanlearneffectivelyfromasmallnumberoftrainingexamples.\nIn this sense, instruction fine-tuning is sample efficient compared with pre-training. From\ntheperspectiveofmachinelearning,sample-efficientmethodscanbeseenasefficientways\nto sample the space of data, and are advantageous as they make optimal use of scarce data.\nTherefore, sampling-based learning techniques, such as many reinforcement learning algo-\nrithms,canbenefitfromthesesampleefficientapproaches. Forexample,inhumanpreference\nalignment, we can either efficiently sample preference data via reward models [Liu et al.,\n2024b]orimprovethesamplingefficiencyinpolicylearning[Wangetal.,2024].\n10.2.4 Instruction Generalization\nInmanymachinelearningandNLPproblems,trainingamodeltogeneralizeisafundamental",
    "goal. Forexample,intextclassification,weexpectourmodeltocorrectlyclassifynewtexts\nthat were not seen during training. However, generalization poses additional challenges in\ninstructionfine-tuning. Weexpectinstruction-fine-tunedLLMstonotonlygenerateappropriate\nresponses for different inputs within a task but also to accurately perform various tasks as\ndescribed by different instructions. To illustrate this issue, consider an LLM Pr(y|c,z),\nwherecisaninstruction,zisauserinput,andyisthecorrespondingmodeloutput(i.e.,the",
    "548 Chapter 10. Alignment\nresponse). Supposethattheperformanceofthismodelisevaluatedintermsofametric,written\nasPerformance(Pr(y|c,z))orP(c,z,y)forshort. Informally,whenwesaythismodelcan\ngeneralizewithinagiventask(indicatedbytheinstructionc∗),wemeanthattheremaybea\nvalueϵsuchthattheaverageperformanceonnewinputsisabovethisvalue:\n1 (cid:88)\nP(c∗,z′,y′)>ϵ (10.8)\n|Z|\nz′∈Z\nwhereZ isthesetofnewinputs,andz′ andy′ areaninputinthissetandthecorresponding\noutput,respectively.\nLikewise,wecansaythatthismodelcangeneralizeacrosstasksiftheaverageperformance\noverallinstruction-inputpairsisabovesomeϵ:\n1 (cid:88)\nP(c′,z′,y′)>ϵ (10.9)\n|D|\n(c′,z′)∈D\nwhereD isthesetofnewinstruction-inputpairs.\nHere,weneedtodealwithvariationsintwodimensions: instructionanduserinput. This\nmakesthegeneralizationproblemverycomplex,because,intuitively,amodelneedstolearn\nfrom a vast number of tasks and different input-output pairs associated with each task to",
    "achievegoodgeneralization. Aswehavediscussedseveraltimesinthisbook,achievingsuch\ngeneralizationincursmuchlowercostthanpre-training. Ingeneral,fine-tuningLLMswith\ninstruction-responsedatatosomeextentcanleadtomodelsyieldinginstructionfollowingon\nnewtasks. Nevertheless,itistypicallybelievedthatcertaineffortsarestillneededtoadapt\nLLMstomakethemunderstandandexecuteinstructionsbroadly.\nOne way to generalize instruction fine-tuning is to increase the diversity of the fine-\ntuningdata. Inearlierstudiesoninstructionfine-tuning,researchersdevelopedmanydatasets,\ncovering a wide variety of NLP tasks and different instructions for each task [Wang et al.,\n2022e; Sanh et al., 2022; Longpre et al., 2023]. By transforming these tasks into a unified\nformat,onecanfine-tuneanLLMwithasufficientlylargenumberofsamples,forexample,\ntherehavebeenseveralinstructionfine-tuningdatasetsthatinvolveover100NLPtasksand\n1M samples. However, these early datasets mostly focus on existing academic problems,",
    "butnotthosethatuserswanttodealwithinreal-worldapplications. Muchrecentworkhas\nshifted focus to addressing new and more practical problems. For example, there has been\nconsiderableinterestinconstructingdatasetsthatcontainlargeandcomplicateddemonstrations\nandresponsesfromSOTAmodelstorealuserqueries[Wangetal.,2023d;Teknium,2023].\nPerhapstheuseoflargeanddiversefine-tuningdatasetshasitsoriginsinattemptstoscale\nLLMsindifferentdimensions. Indeed,scalinglawshavebeenusedbroadlytomotivatethe\ndevelopmentofawiderangeofdifferentinstruction-fine-tunedLLMs. Anditisreasonableto\nscaleinstructionfine-tuningtomakeanLLMfollowbroadinstructions. Fromtheperspective\nofLLMalignment,however,scalinginstructionfine-tuningmightnotbeefficienttoachieve\ngeneralization.\nOne problem is that instruction fine-tuning relies on supervised learning that learns to",
    "10.2 Instruction Alignment 549\ngeneralize and perform tasks based on instruction-response mappings. However, such an\napproachdoesnotcapturesubtleorcomplexhumanpreferences(e.g.,tone,style,orsubjective\nquality)becausethesearehardtoencodeasexplicitinstruction-responsedata. Moreover,the\ngeneralizationperformanceisboundedbythediversityandqualityoftheinstruction-response\ndataset. Given these limitations, we would instead like to employ preference models as an\nadditional fine-tuning step following instruction fine-tuning, so the LLMs can generalize\nfurther(seeSection10.3).\nAnotherviewisthatsomeinstruction-responsemappingsmayalreadybelearnedduring\npre-training,andsothepre-trainedLLMshaveencodedsuchmappings. However,sincewe\noften do not know exactly what data is used in the pre-training, it is hard to judge whether\nwe need to learn such mappings in the fine-tuning. A related question is whether out-of-\ndistribution generalization is primarily achieved during pre-training or fine-tuning. While",
    "directlyansweringthisquestionisbeyondthescopeofthischapter,ithasbeenshownthatpre-\ntrainingonlargeanddiversedatasetsiseffectiveinimprovingout-of-distributionperformance\n[Hendrycksetal.,2020;Radfordetal.,2021;Gunasekaretal.,2023]. Thisraisesaninteresting\nproblem: ifanLLMhasbeenwellpre-trainedatscale,fine-tuningmaynotbeasessentialfor\nout-of-distributiongeneralization,sincethemodelmayhavealreadyencounteredsufficient\ndistributional variation. This prompts researchers to fine-tune LLMs with modest effort or\ntoexplorenewmethodstoachieveinstruction-following. Asdiscussedintheprevioussub-\nsection,forexample,instructionfollowingcanbeyieldedbyfine-tuningonasmallnumberof\ncarefullyselectedinstruction-responsepairs[Zhouetal.,2023a],orevenbyusingmethods\nthatarenotexplicitlydesignedtodoso[KungandPeng,2023].\nTheabovediscussionprovidestwodifferentstrategies: onerequiresscalingupfine-tuning\ndatasets for larger diversity, the other requires small but necessary fine-tuning datasets for",
    "efficient LLM adaptation. However, in practice, involving diverse instructions often helps.\nIn many cases, we need to adapt our LLM for specific purposes. But the LLM, which has\npossibly encoded broad instruction-following mappings during pre-training, might tend to\nbehaveasageneral-purposeinstructionexecutorevenwithmodestfine-tuning. Aninteresting\nphenomenonisthatwhenfine-tuningonmathdata,theresultingLLMmightnotspecializein\nmathoutputs. Instead,thismodelmightrespondnormallytogeneralinstructions,forexample,\nitcouldgeneratepoetryifinstructedtodoso[Hewitt,2024]. Thisisnotabadthing, butit\nshowsthatLLMsmaynoteasilychangetheirnatureoffollowinggeneralinstructions. Inthis\ncase,additionaladaptationswithmorediversedatamayhelpadjustthewaytheLLMfollows\ninstructions,particularlyforthosetasksweaimtoaddress.\n10.2.5 Using Weak Models to Improve Strong Models\nSo farwe haveexplored a variety ofinstruction fine-tuningmethods basedon labeled data.",
    "One of the limitations of many such methods is that they require the data to be annotated\nby humans or generated by strong LLMs, which can provide accurate supervision signals\nin fine-tuning. However, in many cases, the LLM we have in hand is already strong (or at\nleastisadvantageousinspecificaspectsofproblemsolving),andthusitisnoteasytofinda\nsuperiormodelforsupervision. Evenforhumanexperts,whentheproblembecomescomplex,",
    "550 Chapter 10. Alignment\nprovidingcorrectanddetailedanswersmightbedifficult,orsometimesinfeasible. Forexample,\nwhenfacedwithanextremelylongdocument,theexpertswouldfinditchallengingtoidentify\nanyinconsistencies,subtlebiases,ormissingkeypointswithoutconductinganexhaustiveand\ntime-consumingreview.\nOnemayaskatthispoint: canweuseweakLLMstosupervisestrongLLMs? Thisseems\ntobeasignificantchallenge,butitmayreflectafuturescenariowhereweneedtosupervise\nAIsystemsthataresmarterthanhumansoranyotherAIsystems[Burnsetal.,2023b]. The\nproblemofusingsmaller,lesscomplexmodelstoimprovethetrainingoflarger,morecomplex\nmodelsisalsocalledtheweak-to-stronggeneralizationproblem. Whiletherehavenotbeen\nmatureapproachestoweak-to-stronggeneralization,usingsmallermodelstoassiststronger\nmodelshasindeedprovenusefulinseveralareasofLLMs.\nFor instruction fine-tuning, one of the simplest ways of applying weak LLMs is to use\nthesemodelstogeneratesyntheticfine-tuningdata. SupposewehaveacollectionofinputsX,",
    "whereeachinputincludesaninstructionandauserinputifnecessary. Foreachx∈X,weuse\naweakLLMPrw(·)togenerateapredictionyˆ =argmax Prw(y|x). Then,thestrongLLM\ny\nPrs(·)canbetrainedonthesegeneratedpredictions(seeEq. (10.1)):\nθ\n(cid:88)\nθ˜ = argmax logPrs(yˆ|x) (10.10)\nθ\nθ\nx∈X\nwhereθ isthemodelparameters.\nTheaboveformtransformsthefine-tuningproblemintoaknowledgedistillationproblem,\ninotherwords,wedistillknowledgefromtheweakmodeltothestrongmodel. Consequently,\nwe can employ various knowledge distillation methods to achieve this goal. However, ex-\nplaining weak-to-strong fine-tuning from the perspective of knowledge distillation is not\nstraightforward. Amajorconcernisthatthestrongmodelmaymerelyimitateoroverfitthe\nerrorsoftheweakmodelandfailtogeneralize. Forexample,thefine-tunedstrongmodelstill\ncannotsolvedifficultproblemsthattheweakmodelcannotaccuratelypredict. Fortunately,\npreliminaryexperimentsinthislineofresearchhaveshownpositiveandpromisingresults. For",
    "example,Burnsetal.[2023a]foundthatfine-tuningthestrongpre-trainedGPT-4modelwith\nGPT-2-levelsupervisioncouldimprovegeneralizationacrossseveralNLPtasks. Tomeasure\nhowtheweakmodelimprovesthegeneralizationofthestrongmodel,wedefinethefollowing\nterms:\n• WeakPerformance(P ). Thisisthetest-setperformanceoftheweakmodel,which\nweak\ncanberegardedasthebaselineperformance.\n• Weak-to-strongPerformance(P ). Thisisthetest-setperformanceofthe\nweak→strong\nstrongmodelthatisfine-tunedwiththeweakmodel.\n• StrongCeilingPerformance(P ). Thisisthetest-setperformanceofthestrong\nceiling\nmodelthatisfine-tunedwithgroundtruthdata. Forexample,wefine-tunethestrong\nmodelwithhuman-annotatedpredictionsandtaketheresultingmodelasaceiling.",
    "10.2 Instruction Alignment 551\nThen,theperformancegaprecovered(PGR)canbedefinedas\n(cid:110) P −P (cid:111)\nweak→strong weak\nPGR = max 0, (10.11)\nP −P\nceiling weak\nThismetricmeasureshowmuchoftheperformancegapbetweentheceilingmodeland\nthe weak model can be recovered by the weak-to-strong model. A PGR of 1 indicates that\ntheweak-to-strongfine-tuningcancompletelyclosestheperformancegap,whereasaPGR\nof 0 indicates no improvement. In Burns et al. [2023a]’s work, it is shown that PGR can\nbearound0.8on22NLPclassificationtasks. Itshouldbenotedthat,whilethepotentialof\nweak-to-strongfine-tuningispromising,achievingsubstantialweak-to-stronggeneralization\nremainsachallenginggoalthatneedsfurtherinvestigation[Aschenbrenner,2024].\nFine-tuning LLMs with weak supervision is just one choice for using small models to\nimprove large models. Although this section primarily focuses on fine-tuning LLMs, we\nalso mention other methods here to give a more complete discussion (see Figure 10.5 for",
    "illustrationsofthesemethods).\n• Instead of using small models to generate synthetic data, it is also straightforward to\nincorporateknowledgedistillationlossbasedonthesemodels. Forexample,asimple\nlossfunctionthatmeasuresthedifferencebetweenthesmallandlargemodelscanbe\ndefinedas:\nLoss = KL(Prw(·|x)||Prs(·|x)) (10.12)\nkd θ\nThen, we can add this loss to the original loss of language modeling, and yield the\nfollowingtrainingobjective\n(cid:88)\nθ˜ = argmax logPrs(y|x)−λ·Loss (10.13)\nθ kd\nθ\n(x,y)∈D\nwhereD isthesetofinputandoutputpairs,andλisthecoefficientoftheinterpolation.\nThismethodcanbeemployedineitherthepre-trainingorfine-tuningphase. Wecan\nadjust λ to control how much the small model influences the training. For example,\nwe can gradually decrease λ to make the training rely more on the original language\nmodelinglossasthelargemodelbecomesmorecapable.\n• Another approach to involving small models in LLM pre-training and fine-tuning is",
    "to use them to do data selection or filtering. Given a sequence, we can compute the\nlikelihoodorcross-entropyusingasmallmodel. Thesequantitiescanthenbeusedas\ncriteriaforselectingorfilteringdata. Forexample, sequenceswithlowlikelihoodor\nhighcross-entropymightbeexcludedfromthetrainingset,astheyarelessalignedwith\nthesmallmodel’slearneddistribution. Conversely,sequenceswithhighlikelihoodor\nlowcross-entropycanbeprioritized,ensuringthatthetrainingfocusesonmorerelevant\norhigh-qualitydata.\n• Ensemblelearningisasimpleandeffectivewaytobuildastrongmodelbycombining",
    "552 Chapter 10. Alignment\nLMLoss\nKDLoss\nSmallModel\nComputeLoss&Train ComputeLoss&Train\nSmallModel yˆ y\nInput Predict\nLargeModel LargeModel\nDataset Dataset\n=⇒ =⇒\nx yˆ x y\nx x\n(a)Fine-tuningondatageneratedbyasmallmodel (b)Fine-tuningwithKDLossfromasmallmodel\n(weak-to-stronggeneralization) (weak-to-stronggeneralization)\nDataset\ny\nComputeLoss&Train\nSmallModel Data y CombinationModel\nSelection\nSmallModel1 SmallModel2 SmallModel3\nLargeModel\nDataset\n=⇒\nx y x x x\nx\n(c)Dataselectionwithasmallmodel (d)Ensembleofmultiplesmallmodels\nIfStep1isnotsatisfactory,gotoStep2\nStep1 Step2\n(cheap) (expensive)\ny2\ny1\nSmallModel LargeModel\nx\nx\n(e)Cascading(atinferencetime)\nFigure 10.5: Illustrations of using small models to improve large models in LLMs. One\napproachinvolvesusingsmallermodelsforthefine-tuningorpre-trainingoflargermodels.\nThis includes generating synthetic data (a), incorporating auxiliary loss (b), and selecting\nappropriatedata(c). Anotherapproachinvolvescombiningsmallmodelsandlargemodels.",
    "Thisincludeslearningastrongmodelbyaggregatingmultiplesmallmodels(d),andcascading\nsmallmodelswithlargemodels(e).\nmultiple weak models. Applying this technique to LLMs is straightforward. We can\naggregatedistributionspredictedbymultiplesmallmodelsorspecializedsubmodels,",
    "10.3 Human Preference Alignment: RLHF 553\nandderivethefinalpredictionfromtheaggregatedresults. Thisaggregationcanbedone\nusingmethodssuchasmajorityvoting,weightedaveraging,orstacking.\n• Small models can also be employed at inference time to improve overall efficiency.\nSupposewehavealargemodelthatisslowbutmoreaccurate,andasmallmodelthatis\nfastbutlessaccurate. Inmodelcascading,thesmallmodelfirstprocessestheinputdata,\nquicklygeneratingpreliminaryresults. Iftheseresultsmeetcertainpre-definedcriteria,\ntheycanbedirectlyused. However,iftheinitialresultsarenotsufficientlygood,the\ninputisthenpassedtothelarger,moreaccuratemodeltoproduceabetterresult. This\napproachsignificantlyreducescomputationalcostsandlatency,asthesmallmodelcan\neffectivelyhandlemanyinputswithoutaccesstothelargemodel.\n10.3 Human Preference Alignment: RLHF\nSofarinthischapter,wehavefocusedonfine-tuningLLMsusinginput-outputpairedlabeled\ndata. ThisapproachallowsustoadaptLLMsforinstruction-followingviasupervisedlearning.",
    "Inmanyapplications,however,LLMsarerequirednotonlytofollowinstructionsbutalsoto\nact in ways that are more aligned with human values and preferences. Consider a scenario\nwhereauserasksanLLMhowtohackintoacomputersystem. IftheLLMisnotappropriately\naligned,itmayrespondbyprovidingdetailsonhowtoperformthisillegalactivity. Instead,\na more desirable response might be to advise the user against engaging in illegal activities\nandofferageneraloverviewoftheconsequencesofsuchactions. Thedifficultyinachieving\nthisisthattheethicalnuancesandcontextualconsiderationsrequiredforanLLMtorespond\nappropriately in such scenarios are not always straightforward to encode into a fine-tuning\ndataset. What’s even more challenging is that, often, humans themselves cannot precisely\nexpresstheirownpreferences.\nInthissection,wediscussanalternativeLLMfine-tuningmethod,calledreinforcement\nlearning from human feedback or RLHF for short [Christiano et al., 2017; Stiennon et al.,",
    "2020]. The basic idea behind RLHF is that LLMs can learn from comparisons of model\noutputsusingrewardmodels(seeFigure10.6). Todothis,wecanrecruithumanexpertswho\nindicate their preferences between pairs of outputs generated by the LLM. This preference\ndata is used to train a reward model that can predict the perceived quality of LLM outputs.\nOncetrained,therewardmodelprovidesfeedbackbyassigningscorestonewoutputsthatthe\nLLMgeneratesinresponsetotheinputs. TheLLMusesthesescorestoupdateitsparameters\nthroughreinforcementlearningalgorithms. Intherestofthissection,wewillfirstintroduce\nthebasicknowledgeofreinforcementlearningtofacilitatethediscussion,andthendiscuss\nmethodsfortrainingrewardmodelsandaligningLLMswiththesemodels.\n10.3.1 Basics of Reinforcement Learning\nWebeginbylookingatsomebasicconceptsofreinforcementlearning. Notethatthenotation\nusedhereslightlydiffersfromthatusedintheprevioussectionsandchaptersbecausewewant",
    "tomakeourdescriptionmoreconsistentwiththoseinthereinforcementlearningliterature.",
    "554 Chapter 10. Alignment\ny\nObjective(MLE):\nGold-standardPredictions\nLLM (none-hotdistributions) maxPr(y|x)\nPredictedTokenDistributions\nwhere\n(ntokendistributions)\nx:input\nx\ny:gold-standardoutput\n(a)Supervisedfine-tuning(maximizingthepredictionprobabilitygiventheinput)\ntrain\nGeneratemultiple Humanpreferencedata RewardModel\noutputsviasampling\nyy 21\nObjective(RLLossMinimization):\nPredictiony\n1 minL(x,{y ,y },r)\nLLM 1 2\nPredictiony\n2\nwhere\nL(·):lossfunction\nx\nr(·):rewardmodel\n(b)ReinforcementLearningfromHumanFeedback\nFigure 10.6: Supervised fine-tuning vs. reinforcement learning from human feedback. In\nsupervisedfine-tuning,weoptimizetheLLMbymaximizingtheprobabilityoftheprediction\ngiventheinput. Inreinforcementlearningfromhumanfeedback,wefirsttrainarewardmodel\nonhumanpreferencedata(oneachpairofpredictions,evaluatorsareaskedtochoosewhich\nonetheyprefer). Then,weusethisrewardmodeltosupervisetheLLMduringfine-tuning.",
    "Nevertheless,wewillshowhowthisnotationcorrespondstothelanguagemodelingnotation.\nThereaderwhoisalreadyfamiliarwithreinforcementlearningtechniquesmayskiporskim\nthissubsection.\nA general reinforcement learning framework describes how an agent interacts with a\ndynamic environment. This interaction is modeled as a sequence of actions taken by the\nagentinresponsetothestateoftheenvironment. Ateachtimestep,theagentobservesthe\ncurrent state, chooses an action based on its policy, performs the action, and then receives\nfeedback from the environment in the form of a reward and a new state. This sequence of\nobserve-act-receivefeedbackisrepeateduntiltheagentachievesitsgoal.\nAreinforcementlearningsysteminvolvesseveralcomponents:\n• Agent. Thisisthelearnerordecision-makerinreinforcementlearning. Inthecontextof\nLLMs,itcanbeseenastheLLMitself.\n• Environment. This includes everything external to the agent with which the agent",
    "interacts. But the environment in LLMs is less about a physical or virtual space and\nmoreabouttheframeworkwithinwhichtheagent(e.g.,anLLM)receivesfeedbackand",
    "10.3 Human Preference Alignment: RLHF 555\nlearns.\n• State(s). Astaterepresentsthecurrentsituationoftheenvironment. Givenasequence\noftokensforlanguagemodeling,astateatatimestepcanbeviewedasthetokenswe\nobservedsofar,thatis,thecontexttokenswetaketopredictthenexttoken. Forexample,\nwecandefine(x,y )asthestatewhenpredictingthenexttokenatthetimestept.\n<t\n• Action(a). Actionsrepresentpossibledecisionstheagentcanmake. Wecanseethem\naspossiblepredictedtokensinthevocabulary.\n• Reward(R). Therewardisthefeedbackfromtheenvironmentthatevaluatesthesuccess\nofanaction. Forexample,r(s,a,s′)denotestherewardtheagentreceivesfortakingthe\nactionaatthestatesandmovingtothenextstates′. Ifthestate-actionsequenceisgiven,\nwecandenotetherewardatthetimesteptasr =r(s ,a ,s ). Alsonotethatifthe\nt t t t+1\ndecision-makingprocessisdeterministic,wecanomits becauseitcanbedetermined\nt+1\nbys anda . Insuchcases,wecanuser(s ,a )asshorthandforr(s ,a ,s ).\nt t t t t t t+1",
    "• Policy (π). For an LLM, a policy is defined as the probability distribution over the\ntokensthattheLLMpredicts,giventheprecedingcontexttokens. Formally,thiscanbe\nexpressedas\nπ(a|s)=Pr(y |x,y ) (10.14)\nt <t\nwhereacorrespondstothetokeny ,andscorrespondstothecontext(x,y ). Figure\nt <t\n10.7 illustrates how an LLM can be treated as a policy in the reinforcement learning\nframework.\n• Value Function (V and Q). A state-value function (or value function, for short)\nassessestheexpecteddiscountedreturn(i.e.,accumulatedrewards)foranagentstarting\nfromaparticularstatesandfollowingaspecificpolicyπ. Itisdefinedas:\nV(s) = E (cid:104) r(s\n0\n,a\n0\n,s\n1\n)+γr(s\n1\n,a\n1\n,s\n2\n)+γ2r(s\n2\n,a\n2\n,s\n3\n)+··· (cid:12) (cid:12)s\n0\n=s,π (cid:105)\n= E (cid:104) r\n0\n+γr\n1\n+γ2r\n2\n+··· (cid:12) (cid:12)s\n0\n=s,π (cid:105)\n∞\n= E (cid:104)(cid:88) γtr\nt\n(cid:12) (cid:12)s\n0\n=s,π (cid:105) (10.15)\nt=0\nwhere γ ∈[0,1] is the discount factor that adjusts the importance of future rewards,",
    "s =sindicatesthattheagentstartswiththestates,andtheexpectationEisperformed\n0\noverallpossibletrajectories(i.e.,state-actionsequences). Similarly,anaction-value\nfunction (or Q-value function) measures the expected return starting from a state s\ntakinganactionaandthereafterfollowingapolicyπ,givenby\n∞\nQ(s,a) = E (cid:104)(cid:88) γtr\nt\n(cid:12) (cid:12)s\n0\n=s,a\n0\n=a,π (cid:105) (10.16)\nt=0\nwherea =aindicatesthattheactiontakenattheinitialstateisa.\n0",
    "556 Chapter 10. Alignment\nFeedback\nActionat ValueFunctions\nV(s )andQ(s ,a )\ny1 y2 ... yt t t t\nPolicy(LLM)\nx0 x1 ... xm y1 ... yt−1 RewardModel\nR(s ,a )\nt t\nStatest(xandy<t)\nFigure 10.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a\ntokeny giventhemodelinputxandthepreviously-generatedtokensy . Thisprocesscan\nt <t\nbeframedasareinforcementlearningproblem,wherey servesastheaction,(x,y )asthe\nt <t\nstate,andthepredicteddistributionPr(y |x,y )asthepolicy. Oncey ispredicted,theLLM\nt <t t\ninputsboth(x,y )andy totherewardmodel,whichevaluateshoweffectivelythechosen\n<t t\ntokencontributestoachievingthedesiredtextualoutcome. Thisevaluationgeneratesreward\nscoreswhichareusedtocomputethevaluefunctionsV(s )andQ(s ,a ). Thesefunctions\nt t t\nthenprovidefeedbacktotheLLMandguidethepolicytraining.\nThe goal of reinforcement learning is to learn a policy that maximizes the cumulative\nreward (or return) the agent receives over the long run. Given a state-action sequence",
    "τ ={(s ,a ),...,(s ,a )}1,thecumulativerewardoverthissequencecanbewrittenas\n1 1 T T\nT\n(cid:88)\nR(τ) = r (10.17)\nt\nt=1\nTheexpectationofthiscumulativerewardoveraspaceofstate-actionsequencesisgiven\nintheform\n(cid:104) (cid:12) (cid:105)\nJ(θ) = E R(τ)(cid:12)π\nθ\nτ∼D\n(cid:88)\n= Pr (τ)R(τ)\nθ\nτ∈D\nT\n(cid:88) (cid:88)\n= Pr (τ) r (10.18)\nθ t\nτ∈D t=1\nwhereτ ∼Dindicatesthatτ isdrawnfromthestate-actionsequencespaceD,andthesubscript\n1Weassumethestate-actionsequencebeginswiths 1anda 1,ratherthans 0anda 0,toalignwiththenotation\ncommonlyusedinthischapter,wherethepredictionytypicallystartsfromy 1.Ofcourse,itisalsocommonto\ndenoteastate-actionsequenceas{(s 0 ,a 0 ),...,(s T ,a T )}or{(s 0 ,a 0 ),...,(s T−1 ,a T−1 )}intheliterature. But\nthisvariationinnotationdoesnotaffectthediscussionofthemodelspresentedhere.",
    "10.3 Human Preference Alignment: RLHF 557\nθ indicatestheparametersofthepolicy. J(θ)isalsocalledtheperformancefunction.\nThenthetrainingobjectiveistomaximizeJ(θ):\nθ˜ = argmaxJ(θ) (10.19)\nθ\nNow, we have a simple reinforcement learning approach: 1) we sample a number of\nstate-actionsequences;then,2)weevaluateeachsequenceusingtheperformancefunction;\nthen,3)weupdatethemodeltomaximizethisperformancefunction. IfwetakeEq. (10.18)\nand use gradient descent to optimize the policy, this approach would constitutes a form of\npolicygradientmethods[Williams,1992].\nNotethatinmanyNLPproblems,suchasmachinetranslation,rewardsaretypicallysparse.\nFor instance, a reward is only received at the end of a complete sentence. This means that\nr =0forallt<T,andr isnon-zeroonlywhent=T. Ideally,onemightpreferfeedbackto\nt t\nbeimmediateandfrequent(dense),andthusthetrainingofthepolicycanbeeasierandmore\nefficient. Whileseveralmethodshavebeenproposedtoaddresssparserewards,suchasreward",
    "shaping,wewillcontinueinourdiscussiontoassumeasparserewardsetup,wherethereward\nisavailableonlyuponcompletingtheprediction.\nThe model described in Eqs. (10.17-10.19) establishes a basic form of reinforcement\nlearning, and many variants and improvements of this model have been developed. Before\nshowing those more sophisticated models, let us take a moment to interpret the objective\nfunctionJ(θ)fromtheperspectiveofpolicygradient. Ingradientdescent,weneedtocompute\nthegradientofJ(θ)withrespecttoθ:\n(cid:80)\n∂J(θ) ∂ Pr (τ)R(τ)\n= τ∈D θ\n∂θ ∂θ\n(cid:88) ∂Pr θ (τ)\n= R(τ)\n∂θ\nτ∈D\n(cid:88) ∂Pr θ (τ)/∂θ\n= Pr (τ) R(τ)\nθ\nPr (τ)\nθ\nτ∈D\n(cid:88) ∂logPr θ (τ)\n= Pr (τ) R(τ) (10.20)\nθ\n∂θ\nτ∈D\nInsomecases,wewillassumethateverysequenceinDisequallyprobable(i.e.,Pr (τ)=\nθ\n∂logPr (τ)\n1/|D|). InthiscasewecansimplifyEq. (10.20)andneedonlyconsidertheterms θ\n∂θ\nandR(τ):\n∂J(θ) 1 (cid:88) ∂logPr θ (τ)\n= R(τ) (10.21)\n∂θ m ∂θ\nτ∈D\nOneadvantageofthisresultisthatR(τ)doesnotneedtobedifferentiable,whichmeansthat",
    "wecanuseanytypeofrewardfunctioninreinforcementlearning.",
    "558 Chapter 10. Alignment\nBytreatingthegenerationofthesequenceτ asaMarkovdecisionprocess,wecanfurther\n∂logPr (τ)\nderive θ ,andobtain:\n∂θ\nT\n∂logPr θ (τ) ∂ (cid:89)\n= log π (a |s )Pr(s |s ,a )\nθ t t t+1 t t\n∂θ ∂θ\nt=1\nT T\n∂ (cid:88) ∂ (cid:88)\n= logπ (a |s )+ logPr(s |s ,a ) (10.22)\nθ t t t+1 t t\n∂θ ∂θ\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nt=1 t=1\npolicy dynamics\nwhere the gradient is decomposed into two parts: the policy gradient and the dynamics\ngradient. Thepolicycomponent,logπ (a |s ),determinesthelog-probabilityoftakingaction\nθ t t\na givenstates ,anditisparameterizedbyθ. Thedynamicscomponent,logPr(s |s ,a ),\nt t t+1 t t\nrepresents the log-probability of transitioning to state s from state s after taking action\nt+1 t\na . In typical reinforcement learning settings, the dynamics are not directly influenced by\nt\nthepolicyparametersθ,andthus,theirderivativesareoftenzero. Inthiscase,therefore,Eq.\n(10.22)canbesimplifiedto:\nT\n∂logPr θ (τ) ∂ (cid:88)",
    "= logπ (a |s ) (10.23)\nθ t t\n∂θ ∂θ\nt=1\nInotherwords,weonlyconcentrateonoptimizingthepolicywithoutconcerningourselves\nwiththeunderlyingdynamics.\nSubstitutingEq. (10.23)intoEq. (10.21),andexpandingR(τ),wethenobtain\nT T\n∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:88) (cid:17)\n= logπ (a |s ) r (10.24)\nθ t t t\n∂θ |D| ∂θ\nτ∈D t=1 t=1\nWhilethispolicygradientapproachisstraightforward,itsuffersfromtheproblemthatthe\nvarianceoftheestimatedgradientscanbeveryhigh,makingthelearningprocessnoisyand\ninefficient. Onereasonforthishighvarianceproblemisthatrewardscanvarygreatlyacross\ndifferentstepsorscenarios. Imaginethatinasequenceofactiondecisions,therewardmodel\ntendstoassignsmallrewardstogoodactions(e.g.,R =2)andlargepenaltiestopooractions\nt\n(e.g.,R =−50). Suchvaryingrewardscalesforgoodandpooractionscanresultinavery\nt\nlowtotalrewardfortheentiresequence,evenifitincludesgoodactions.\nOne simple method for reducing the variance of the gradient is to set a baseline b and",
    "subtractitfrom (cid:80)T r ,resultingin (cid:80)T r −b.2 Here,thebaselinecanbeinterpretedasa\nt=1 t t=1 t\nreferencepoint. Bycenteringtherewardsaroundthisbaseline,weremovesystematicbiasesin\n2Infact,theuseofabaselinebdoesnotchangethevarianceofthetotalrewards (cid:80)T\nt=1\nrt. However,itis\nimportanttonotethatwhileintroducingabaselinedoesnotaltertheoverallvarianceoftherewards,ithelpsreduce\nthevarianceofthegradientestimates.Thisisbecausesubtractingthebaselinefromthetotalrewardseffectively\nreducesfluctuationsaroundtheirmean,whichmakesthegradientestimatesmorestable.Ingeneral,theoperation\n(cid:80)T\nt=1\nrt−bcenterstherewardsaroundzero(e.g.,bisdefinedastheexpectedvalueof\n(cid:80)T\nt=1\nrt),whichcanlead\ntoreducedvarianceintheproduct\n(cid:80)T\nt=1\nlogπ\nθ\n(at|st)(\n(cid:80)T\nt=1\nrt−b).",
    "10.3 Human Preference Alignment: RLHF 559\ntherewardsignal,makingtheupdatesmorestableandlesssensitivetoextremefluctuationsin\nindividualrewards.\nThispolicygradientmodelwithabaselinecanbegivenby\nT T\n∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:17)(cid:16)(cid:88) (cid:17)\n= logπ (a |s ) r −b\nθ t t t\n∂θ |D| ∂θ\nτ∈D t=1 t=1\nT T\n1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:17)(cid:105)\n= logπ (a |s ) r −b\nθ t t k\n|D| ∂θ\nτ∈D t=1 k=1\nT t−1 T\n1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:88) (cid:17)(cid:105)\n= logπ (a |s ) r + r −b (10.25)\nθ t t k k\n|D| ∂θ\nτ∈D t=1 k=1 k=t\nHerewewrite\n(cid:80)T\nr asthesumoftwoterms\n(cid:80)t−1\nr and\n(cid:80)T\nr todistinguishbetween\nk=1 k k=1 k k=t k\ntherewardsaccruedbeforeandaftertheactionattimestept. NotethatinMarkovdecision\nprocesses,thefutureisindependentofthepastgiventhepresent. Therefore,theactiontaken\nattimesteptcannotinfluencetherewardsreceivedbeforet,orinotherwords,therewards\npriortotarealready“fixed”bythetimetheactionattischosen. Theterm",
    "(cid:80)t−1\nr doesnot\nk=1 k\ncontributetothegradientandcanbeomitted,leadingtoasimplifiedversionofEq. (10.25)\nT T\n∂J(θ) 1 (cid:88) ∂ (cid:104)(cid:88) (cid:16)(cid:88) (cid:17)(cid:105)\n= logπ (a |s ) r −b (10.26)\nθ t t k\n∂θ |D| ∂θ\nτ∈D t=1 k=t\nAlsonotethatremoving\n(cid:80)T\nr canfurtherreducethevarianceofthegradient.\nk=t k\nThere are many ways to define the baseline b. Here we consider the value function of\nthestates ,thatis,theestimatedvalueofbeinginstates : V(s )=E(r +r +···+r ).\nt t t t t+1 T\nHencewehave\nT\n(cid:88)\nA(s ,a ) = r −b\nt t k\nk=t\nT\n(cid:88)\n= r −V(s ) (10.27)\nk t\nk=t\nwhere\n(cid:80)T\nr representstheactualreturnreceived,andV(s )representstheexpectedreturn.\nk=t k t\nA(s ,a )(orA forshort)iscalledtheadvantageattimestept,whichquantifiestherelative\nt t t\nbenefitoftheactiona comparedtotheexpectedvalueoffollowingthepolicyfromthestate\nt\ns onward.\nt\nByusingtheadvantagefunctionA(s ,a ),thegradientofJ(θ)canbewrittenintheform\nt t\nT\n∂J(θ) 1 (cid:88) ∂ (cid:16)(cid:88) (cid:17)",
    "= logπ (a |s )A(s ,a ) (10.28)\nθ t t t t\n∂θ |D| ∂θ\nτ∈D t=1",
    "560 Chapter 10. Alignment\nThisoptimizationobjectivecorrespondstotheadvantageactor-critic(A2C)methodin\nreinforcementlearning[Mnihetal.,2016]. Inthismethod,theactoraimsatlearningapolicy.\nIt updates the policy parameters using Eq. (10.28) to help focus more on actions that are\nlikely to improve performance. The critic, on the other hand, updates its estimation of the\nvaluefunction,whichisusedtocalculatetheadvantagefunctionA(s ,a ),thusservingasthe\nt t\nevaluatorofthepolicybeinglearnedbytheactor.\nIntheA2Cmethod,A(s ,a )istypicallyexpressedasthedifferenceoftheaction-value\nt t\nfunctionQ(s ,a )andthestate-valuefunctionV(s )\nt t t\nA(s ,a ) = Q(s ,a )−V(s ) (10.29)\nt t t t t\nAtfirstglance,thismodelmayseemchallengingtodevelopbecauseitrequirestwoseparatesub-\nmodelstocalculateQ(s ,a )andV(s )respectively. Fortunately,consideringthatQ(s ,a )\nt t t t t\ncanbedefinedasthereturnr +V(s ),wecanrewriteEq. (10.29)as\nt t+1\nA(s ,a ) = r +V(s )−V(s ) (10.30)\nt t t t+1 t",
    "oralternatively,introducethediscountfactorγ toobtainamoregeneralform\nA(s ,a ) = r +γV(s )−V(s ) (10.31)\nt t t t+1 t\nA(s ,a )=r +γV(s )−V(s )isalsocalledthetemporaldifference(TD)error. What\nt t t t+1 t\nweneedistotrainacriticnetworkforthevaluefunctionV(s ),andthenuseittocomputethe\nt\nadvantagefunction3.\nUptothispoint,wehavespentconsiderablespacediscussingthebasicsofreinforcement\nlearning,especiallyonhowtoderivetheoptimizationobjectivefortheA2Cmethod. However,\nreinforcement learning is a vast field, and many technical details cannot be covered here.\nTheinterestedreadercanrefertoreinforcementlearningbooksformoredetails[Suttonand\nBarto, 2018; Szepesvári, 2010]. Nevertheless, we now have the necessary knowledge to\nfurtherdiscussRLHF.Inthesubsequentsubsections,wewillreturntothediscussiononLLM\nalignment,demonstratinghowtousetheA2Cmethodforaligningwithhumanpreferences.\n10.3.2 Training Reward Models\nWehaveshownthatrewardmodelsplayaveryimportantroleinthegeneralreinforcement",
    "learningframeworkandformthebasisforcomputingvaluefunctions. Wenowconsiderthe\nproblemoftrainingtheserewardmodels.\nInRLHF,arewardmodelisaneuralnetworkthatmapsapairofinputandoutputtoken\n3Thetraininglossforthevaluenetwork(orcriticnetwork)inA2Cisgenerallyformulatedasthemeansquared\nerrorbetweenthecomputedreturnrt+γV(s t+1 )andthepredictedstatevalueV(st). Supposethatthevalue\nnetworkisparameterizedbyω.Thelossfunctionisgivenby\n1 (cid:88)(cid:0) (cid:1)2\nLv(ω) = rt+γVω(s t+1 )−Vω(st) (10.32)\nM\nwhereM isthenumberoftrainingsamples,forexample,forasequenceofT tokens,wecansetM =T.",
    "10.3 Human Preference Alignment: RLHF 561\nReward(Scalar)\nWr LinearMap\nRepresentation hx0 hx1 hx2 ··· hxm hy1 hy2 ··· h last\natEachPosition\nTransformerDecoder(LLM)\nx 0 x 1 x 2 ··· xm y 1 y 2 ··· yn\n(LastToken⟨EOS⟩)\nFigure10.8: ArchitectureoftherewardmodelbasedonTransformer. Themaincomponentof\nthismodelisstillanLLM.WeusetheTransformerdecoderasthesequencerepresentation\nmodel. Weextracttherepresentationofthelastpositionofthedecoderastherepresentation\nof the entire sequence [x,y]. We then map this representation to a scalar through a linear\ntransformation,whichservesastherewardscorefor[x,y].\nsequencestoascalar. Givenaninputxandanoutputy,therewardcanbeexpressedas\nr = Reward(x,y) (10.33)\nwhereReward(·)istherewardmodel. rcanbeinterpretedasameasureofhowwelltheoutput\nyalignswiththedesiredbehaviorgiventheinputx. Asdiscussedintheprevioussubsection,\nbothxandyareassumedtocompletetexts. Thismeansthattherewardmodelevaluatesthe",
    "relationshipbetweeninputsandoutputsthatprovidefullsemanticcontent. Forexample,when\napplyingtherewardmodel,itassignsavalueof0(oranotherpredeterminedvalue)ateach\npositiontintheoutputsequencey=y ...y . Onlyatthefinalposition,whent=n,doesthe\n1 n\nrewardmodelgeneratetheactualrewardscore. Tokeepthenotationuncluttered,wewilluse\nr(x,y)todenotetherewardmodelReward(x,y)fromhereon.\nTherearemanywaystoimplementtherewardmodel. Onesimpleapproachistobuildthe\nrewardmodelbasedonapre-trainedLLM.Morespecifically,wecanconcatenatexandyto\nformasingletokensequenceseq =[x,y]. Werunapre-trainedLLMonthissequence,as\nx,y\nusual,andateachposition,weobtainarepresentationfromthetop-mostTransformerlayer.\nThen,wetaketherepresentationatthelastposition(denotedbyh )andmapittoascalar\nlast\nvialineartransformation:\nr(x,y) = h W (10.34)\nlast r\nwhere h is a d-dimensional vector, and Wr is a d×1 linear mapping matrix. This\nlast\narchitectureoftherewardmodelisillustratedinFigure10.8.",
    "Totraintherewardmodel,thefirststepistocollecthumanfeedbackonasetofgenerated\noutputs. Givenaninputx,weusetheLLMtoproducemultiplecandidateoutputs{y ,...,y }.\n1 N",
    "562 Chapter 10. Alignment\nHumanfeedbackcanbeobtainedinseveralways:\n• PairwiseComparison(PairwiseRanking). Giventwodifferentoutputs,humanexperts\nselectwhichoneisbetter.\n• Rating. Human experts provide a score or rating to each output. This score is often\na continuous or discrete numerical value, such as a score on a scale (e.g., 1-5 stars,\nor 1-10 points). In some cases, the rating might be binary, indicating a “yes/no” or\n“positive/negative”preference.\n• ListwiseRanking. Humanexpertsareaskedtorankororderthegivensetofpossible\noutputs.\nHere we consider pairwise comparison feedback as it is one of the simplest and most\ncommon forms of human feedback used in RLHF. In this setting, each time, two outputs\n(y ,y )arerandomlydrawnfromthecandidatepool{y ,...,y }. Humanexpertsarethen\na b 1 N\npresented with these pairs and asked to decide which output they prefer based on specific\ncriteria, such as clarity, relevance, and accuracy. The human feedback can be encoded as a",
    "binarylabel,y ≻y forapreferencefory ,andy ≻y forapreferencefory .\na b a b a b\nOne simple and widely used model for describing such pairwise comparisons is the\nBradley-Terrymodel[BradleyandTerry,1952]. Itisaprobabilisticmodelthatestimatesthe\nprobabilitythatoneitemispreferredoveranother. Adaptingthismodeltothenotationused\nhere,wecanwritetheprobabilitythaty ispreferredovery intheform\na b\ner(x,ya)\nPr(y ≻y |x) =\na b\ner(x,ya)+er(x,y\nb\n)\ner(x,ya)−r(x,y\nb\n)\n=\ner(x,ya)−r(x,y\nb\n)+1\n= Sigmoid(r(x,y )−r(x,y )) (10.35)\na b\nWhentrainingtherewardmodel,wewanttomaximizethispreferenceprobability. Aloss\nfunctionbasedontheBradley-Terrymodelisgivenby\nL (ϕ) = −E (cid:2) logPr (y ≻y |x) (cid:3) (10.36)\nr (x,ya,y\nb\n)∼Dr ϕ a b\nwhere(x,y ,y )isdrawnfromahuman-annotateddatasetD consistingofpreferencepairs\na b r\nofoutputsandtheircorrespondinginputs. ϕrepresentstheparametersoftherewardmodel,\nwhichincludesboththeparametersoftheTransformerdecoderandthelinearmappingmatrix",
    "W . In practice, assuming (x,y ,y ) is uniformly sampled from D , we can replace the\nr a b r\nexpectationwithasummation\n1 (cid:88)\nL (ϕ) = − logPr (y ≻y |x) (10.37)\nr ϕ a b\n|D |\nr\n(x,ya,y\nb\n)∈Dr\nThegoaloftrainingtherewardmodelistofindtheoptimalparametersϕˆ\nthatminimize",
    "10.3 Human Preference Alignment: RLHF 563\nthislossfunction,givenby\nϕˆ = argminL (ϕ) (10.38)\nr\nϕ\nSincetherewardmodelitselfisalsoanLLM,wecandirectlyreusetheTransformertraining\nproceduretooptimizetherewardmodel. ThedifferencefromtrainingastandardLLMisthat\nweonlyneedtoreplacethecross-entropylosswiththepairwisecomparisonlossasdescribed\ninEq. (10.37). Afterthetrainingoftherewardmodel,wecanapplythetrainedrewardmodel\nr (·)tosupervisethetargetLLMforalignment.\nϕˆ\nItisworthnotingthatalthoughwetraintherewardmodeltoperformpairwiseranking,\nweapplyittoscoreeachinput-outputpairindependentlyduringthealignmentprocess. The\npairwise ranking objective ensures that the reward model is sensitive to subtle differences\nbetweenoutputs,butwerelyonthecontinuousscoresproducedbytherewardmodeltoguide\ntheoptimizationoftheLLM.Anadvantageofthisapproachisthatwecanchoosefromor\ncombinevariousrankinglossfunctions,andstillapplytheresultingrewardmodelsinthesame",
    "way as we have done in this subsection. This consistency ensures a unified framework for\naligningtheLLM,regardlessofthespecificrankinglossusedduringrewardmodeltraining.\n10.3.3 Training LLMs\nHavingobtainedtherewardmodel,wethentrainthepolicy(i.e.,theLLM)viatheA2Cmethod.\nRecallfromSection10.3.1thatastate-actionsequenceortrajectoryτ canbeevaluatedbythe\nutilityfunction\nT\n(cid:88)\nU(τ;θ) = logπ (a |s )A(s ,a ) (10.39)\nθ t t t t\nt=1\nwhere A(s ,a ) is the advantage of taking the action a given the state s . An estimate of\nt t t t\nA(s ,a )isdefinedastheTDerrorr +γV(s )−V(s ),wherethevaluefunctionV(s )is\nt t t t+1 t t\ntrainedwiththerewardmodel.\nGiventhisutilityfunction,theA2C-basedlossfunctioncanbewrittenintheform\nL(θ) = −E (cid:2) U(τ;θ) (cid:3)\nτ∼D\nT\n= −E (cid:2)(cid:88) logπ (a |s )A(s ,a ) (cid:3) (10.40)\nτ∼D θ t t t t\nt=1\nwhereD isaspaceofstate-actionsequences. Asusual, thegoaloftrainingthepolicyisto\nminimizethislossfunction\nθ˜ = argminL(θ) (10.41)\nθ",
    "If we map the problem back to the language modeling problem and adopt the notation",
    "564 Chapter 10. Alignment\nfromLLMs,thelossfunctioncanbewrittenas:\nL(θ) = −E (cid:2) U(x,y;θ) (cid:3) (10.42)\n(x,y)∼D\nwhere\nT\n(cid:88)\nU(x,y;θ) = logπ (y |x,y )A(x,y ,y ) (10.43)\nθ t <t <t t\nt=1\nHereπ (y |x,y )=Pr (y |x,y )istheLLMparameterizedbyθ.\nθ t <t θ t <t\nIn general, we do not have a human annotated input-output dataset D in RLHF, but a\ndatasetcontaininginputsonly. Theoutputs,inthiscase,aretypicallythepredictionsmadeby\ntheLLM.Thelossfunctionisthendefinedas\nL(θ) = −E E (cid:2) U(x,y;θ) (cid:3) (10.44)\nx∼D y∼π (·|x)\nθ\nwhereD denotestheinput-onlydataset,andy∼π (·|x)denotesthattheoutputyissampled\nθ\nbythepolicyπ (·|x).\nθ\nThe above formulation provides a basic form of the A2C method for LLMs. Improved\nversionsofthismodelaremorecommonlyusedinRLHF.Inthefollowingdiscussion,wewill\nstillusethereinforcementlearningnotationtosimplifythepresentationandwillgetbackthe\nlanguagemodelingnotationlater.\nOnecommonimprovementofpolicygradientmethodsistouseimportancesamplingto",
    "refinetheestimationofU(τ;θ). Thiscanbewrittenas\nT\n(cid:88) π θ (a t |s t )\nU(τ;θ) = A(s ,a ) (10.45)\nt t\nπ (a |s )\nθ t t\nt=1 ref\nHere we replace the log-probability logπ (a |s ) with the ratio\nπ\nθ\n(at|st)\n. θ denotes the\nθ t t π\nθref\n(at|st) ref\nparametersofthepreviouspolicy(suchasaninitialmodelfromwhichwestartthetraining).\nSo\nπ\nθ\n(at|st)\n, alsocalledtheratiofunction, canbeinterpretedasthelog-probabilityratio\nπ\nθref\n(at|st)\nbetweenthecurrentpolicyπ andthepreviouspolicyπ (callitthereferencepolicy). By\nθ θ\nref\nusing the ratio function we reweight the observed rewards based on the likelihood of the\nactionsunderthecurrentpolicyversusthereferencepolicy. When\nπ\nθ\n(at|st)\n>1,theaction\nπ\nθref\n(at|st)\na ismorefavoredbythecurrentpolicycomparedtothereferencepolicy. Bycontrast,when\nt\nπ θ (at|st) <1,theactiona islessfavoredbythecurrentpolicy4.\nπ\nθref\n(at|st) t\nA problem with the model presented in Eq. (10.47) (as well as in Eq. (10.39)) is that",
    "thevarianceinthegradientestimatesisoftenhigh,makingthelearningprocessunstable. To\n4Consideramoregeneralcasewherewewishtoevaluatethepolicyusingitsexpectedreward(alsoseeEq.\n(10.18))\n(cid:104) (cid:105)\nJ(θ) = E τ∼πθ R(τ) (10.46)\nHereτ ∼π meansthatthesequenceτ isgeneratedbythepolicyπ .Alternatively,wecanwriteJ(θ)inanother\nθ θ",
    "10.3 Human Preference Alignment: RLHF 565\nmitigatethisissue,techniquessuchasclippingareoftenemployedtoboundtheimportance\nweightsandpreventlargeupdates. Aclippedversionoftheutilityfunction(alsocalledthe\nclippedsurrogateobjectivefunction)isgivenby\nT\n(cid:88) (cid:16) π θ (a t |s t ) (cid:17)\nU (τ;θ) = Clip A(s ,a ) (10.49)\nclip t t\nπ (a |s )\nθ t t\nt=1 ref\n(cid:16) π θ (a t |s t ) (cid:17) (cid:16) π θ (a t |s t ) (cid:0) π θ (a t |s t ) (cid:1) (cid:17)\nClip = min ,bound ,1−ϵ,1+ϵ (10.50)\nπ (a |s ) π (a |s ) π (a |s )\nθ t t θ t t θ t t\nref ref ref\nHere the function bound(\nπ\nθ\n(at|st)\n,1−ϵ,1+ϵ) constrains the ratio function to the range\nπ\nθref\n(at|st)\n[1−ϵ,1+ϵ].\nAfurtherimprovementtotheabovemodelistoconsidertrustregionsinoptimization\n[Schulman et al., 2015]. In reinforcement learning, a large update to the policy can lead to\ninstability,wheretheagentmaystartperformingworseafteranupdate. Areasonableideaisto\noptimizethemodelinthetrustregion,whichreferstoaregionaroundthecurrentparameter",
    "estimatewherethemodeliswell-behaved. Oneapproachtoincorporatingtrustregionsisto\nimposeaconstraintonthesizeofthepolicyupdate,ensuringthatthecurrentpolicydoesnot\ndeviatetoosignificantlyfromthereferencepolicy. Thiscanbeachievedbyaddingapenalty\nbasedonsomeformofdivergencebetweenthecurrentandreferencepoliciestotheobjective\nfunction. Asimpleformofsuchapenaltyisgivenbythedifferenceinthelog-probabilityof\nthesequenceτ underthecurrentpolicyversusthereferencepolicy:\nPenalty = logπ (τ)−logπ (τ) (10.51)\nθ θ\nref\nform\n(cid:104) Pr (τ) (cid:105)\nJ(θ) = E τ∼πθref\nPr\nθr\nθ\nef\n(τ)\nR(τ) (10.47)\nIt is not difficult to find that the right-hand sides of these equations are essentially the same since\n(cid:104) (cid:105) (cid:104) (cid:105)\nE τ∼πθref P P rθ r\nr\nθ\ne\n(\nf\nτ (τ ) ) R(τ) = (cid:80) τ Pr θref (τ) P P rθ r\nr\nθ\ne\n(\nf\nτ (τ ) ) R(τ)= (cid:80) τ Pr θ (τ)R(τ)=E τ∼πθ R(τ)\nNotethatthisequivalenceholdsonlywhentheexpectationisperformedovertheentiresequencespace. In",
    "practice,however,weoftenonlysamplearelativelysmallnumberofsequencesusingapolicyinpolicylearning.\nAsaresult,thesamplingmethoditselfmatters.Eq.(10.47)offersaninterestingmannertoseparatethesampling\nandrewardcomputationprocesses: wefirstuseabaselinepolicy(withθ )tosampleanumberofsequences,\nref\nandthenusethetargetpolicy(withθ)tocomputetheexpectedreward.Inthisway,weseparatethepolicyused\nforcollectingthedata,andthepolicyusedforcomputingthegradient.Thisapproachavoidstheneedtodirectly\nsamplefromthepolicyweareevaluating,whichcanbebeneficialincaseswheregeneratingsequencesfrom\n(cid:104) (cid:105)\nthetargetpolicyisexpensiveordifficult.Inreinforcementlearning,E τ∼πθref P P rθ r\nr\nθ\ne\n(\nf\nτ (τ ) ) R(τ) isoftencalleda\nsurrogateobjective.\n(cid:104) (cid:105)\nEq. (10.47)canalsobeinterpretedfromapolicygradientperspective. ForE τ∼πθref P P rθ r\nr\nθ\ne\n(\nf\nτ (τ ) ) R(τ) ,the\ngradientatθ=θ isgivenby\nref",
    "∂ ∂ θ E τ∼πθref (cid:104) P P r θ r r θ e ( f τ (τ ) ) R(τ) (cid:105)(cid:12) (cid:12) (cid:12) θ=θref = E τ∼πθref (cid:104)∂Pr θ (τ ∂ ) θ | θ=θrefR(τ) (cid:105) (10.48)\nTheright-handsideisastandardformusedinpolicygradientmethods,meaningthatwecomputethedirection\noftheparameterupdateatthepointθ=θ ontheoptimizationsurface.\nref",
    "566 Chapter 10. Alignment\nInpractice,thispenaltycanbeapproximatedbyconsideringonlythepolicyprobabilitiesand\nignoringthedynamics. Thisgives\nT T\n(cid:88) (cid:88)\nPenalty = logπ (a |s )− logπ (a |s ) (10.52)\nθ t t θ t t\nref\nt=1 t=1\nBy including this penalty in the optimization objective, we encourage the current policy to\nremainclosetothereferencepolicy,limitingverylargeupdatesthatcoulddestabilizelearning.\nWecanincorporatethispenaltyintotheclippedsurrogateobjectivefunction,andobtain\nU (τ;θ) = U (τ;θ)−βPenalty (10.53)\nppo-clip clip\nwhereβ istheweightofthepenalty. Thistrainingmethodiscalledproximalpolicyoptimiza-\ntion(PPO),whichisoneofthemostpopularreinforcementlearningmethodsusedinLLMs\nandmanyotherfields[Schulmanetal.,2017].\nNowwecanwritetheobjectiveoftrainingLLMsintheformofPPO.\nU(x,y;θ) = U (x,y;θ)−βPenalty (10.54)\nppo-clip\nwhere\nT\n(cid:88) (cid:16) π θ (y t |x,y <t ) (cid:17)\nU (x,y;θ) = Clip A(x,y ,y ) (10.55)\nppo-clip <t t\nπ (y |x,y )\nθ t <t\nt=1 ref\nPenalty = logPr (y|x)−logPr (y|x)",
    "θ θ\nref\nT T\n(cid:88) (cid:88)\n= logPr (y |x,y )− logPr (y |x,y ) (10.56)\nθ t <t θ t <t\nref\nt=1 t=1\nAlthoughthenotationhereappearsabittedious,theideaofPPOissimple: wedevelopan\nobjectivebycombiningtheclippedlikelihoodratioofthetargetandreferencepolicieswithan\nadvantagefunction,andthenimposeapenaltythatensurespolicyupdatesarenottoolarge.\nThePPO-basedRLHFisillustratedinFigure10.9.\nTo summarize, implementing RLHF requires building four models, all based on the\nTransformerdecoderarchitecture.\n• RewardModel(r (·)whereϕdenotestheparameters). Therewardmodellearnsfrom\nϕ\nhuman preference data to predict the reward for each pair of input and output token\nsequences. ItisaTransformerdecoderfollowedbyalinearlayerthatmapsasequence\n(theconcatenationoftheinputandoutput)toareal-valuedrewardscore.\n• ValueModelorValueFunction(V (·)whereω denotestheparameters). Thevalue\nω\nfunction receives reward scores from the reward model and is trained to predict the",
    "expectedsumofrewardsthatcanbeobtainedstartingfromastate. Itisgenerallybased\nonthesamearchitectureastherewardmodel.",
    "10.3 Human Preference Alignment: RLHF 567\nHumanpreferencedataD ={(x,y ,y )}\nr a b\nToLearn\nMinimizingthelossbasedon\nRewardModel\nRewardModel theBradley-Terrymodel\nr (x,y)\nϕ\nTraining\nm\nϕ\nin−\n|D\n1\nr| (x,ya,\n(cid:80)\nyb)∈Dr\nlogσ(r\nϕ\n(x,ya)−r\nϕ\n(x,y\nb\n))\nFixed ToLearn ToLearn\nRefModel LLMPolicy ValueFunction\nPr (y |x,y ) Pr (y |x,y ) V (x,y )\nθold t <t θ t <t ω <t\nEvaluatethestate-actionpairusingtheadvantage\nfunctionortheTDerror(basedonthereward\nmodelandthevaluefunction)\nx1 x2 ··· y1 y2 ··· yt Actionyt\n(sampledwithPrθold )\nLLMPolicy\nPolicyTraining x0 x1 ··· xm y1 ··· yt−1\nState(x,y<t)\nLLMPolicy ValueFunction\nMinimizingtheclippedPPOloss MinimizingtheMSEbetweenthe\nwiththepenalty computedreturnandthepredicted\nstatevalue\nmin−(cid:80) (cid:80)T min 1 (cid:80) (cid:80)T\nθ x∈D,y∼Prθold (·|x) t=1 ω M x∈D t=1\n(cid:104) Clip (cid:0)\nP\nP\nrθ\nr\no\nθ\nld\n(y\n(\nt\ny\n|\nt\nx\n|\n,\nx\ny\n,\n<\ny<\nt)\nt)\n(cid:1) At− (cid:2) rt+γVω(x,y<t+1)−Vω(x,y<t) (cid:3)2\n(cid:0)\nβ· logPr\nθ\n(yt|x,y<t)−\n(cid:1)(cid:105)\nlogPr\nθold",
    "(yt|x,y<t)\n∗∗rt=r(x,y<t+1)denotestherewardreceivedasstept.\n∗∗Atdenotestheadvantageatstept,andcanbedefinedasrt+γVω(x,y<t+1)−Vω(x,y<t)\nInput-onlydataD={x}\nFigure10.9: IllustrationofRLHF.Thefirststepistocollecthumanpreferencedataandtrain\ntherewardmodelusingthisdata. Oncetherewardmodelisoptimized,alongwiththereference\nmodel,weproceedtotrainboththepolicyandthevaluefunction. Ateachpredictionstep,we\ncomputethesumofthePPO-basedlossandupdatetheparametersofthepolicy. Thisrequires\naccesstotherewardmodel,thereferencemodel,andthevaluefunctionathand. Atthesame\ntime,weupdatetheparametersofthevaluefunctionbyminimizingtheMSEloss.",
    "568 Chapter 10. Alignment\n• ReferenceModel(π (·)=Pr (·)whereθ denotestheparameters). Thereference\nθ θ ref\nref ref\nmodelisthebaselineLLMthatservesasastartingpointforpolicytraining. InRLHF,it\nrepresentsthepreviousversionofthemodeloramodeltrainedwithouthumanfeedback.\nIt is used to perform sampling over the space of outputs and contribute to the loss\ncomputationforpolicytraining.\n• TargetModelorPolicy(π (·)=Pr (·)whereθ denotestheparameters). Thispolicy\nθ θ\ngovernshowtheLLMdecidesthemostappropriatenexttokengivenitscontext. Itis\ntrainedunderthesupervisionofboththerewardmodelandthevaluemodel.\nInpractice,thesemodelsneedtobetrainedinacertainorder. First,weneedtoinitialize\nthem using some other models. For example, the reward model and the value model can\nbe initialized with a pre-trained LLM, while the reference model and the target model can\nbeinitializedwithamodelthathasbeeninstructionfine-tuned. Notethat,atthispoint,the",
    "referencemodelisreadyforuseandwillnotbefurtherupdated. Second,weneedtocollect\nhumanpreferencedataandtraintherewardmodelonthisdata. Third,boththevaluemodel\nandthepolicyaretrainedsimultaneouslyusingtherewardmodel. Ateachpositioninanoutput\ntokensequence,weupdatethevaluemodelbyminimizingtheMSEerrorofvalueprediction,\nandthepolicyisupdatedbyminimizingthePPOloss.\n10.4 Improved Human Preference Alignment\nIn the previous section, we reviewed the basic concepts of reinforcement learning and the\ngeneralframeworkofRLHF.Inthissection,wewilldiscusssomerefinementsofRLHFand\nalternativemethodstoachievehumanpreferencealignment.\n10.4.1 Better Reward Modeling\nInSection10.3.2,wehighlightedthetaskoflearningfromhumanpreferencesaswellasthe\nuse of pairwise ranking loss for training reward models. Here we consider more methods\nforrewardmodeling. Ourdiscussionwillberelativelygeneral,andsincetherewardmodel\niswidelyusedinmanyreinforcementlearningproblems, itwillbeeasyforustoapplythe",
    "methodsdiscussedheretoRLHFandrelatedapplications.\n1. Supervision Signals\nThe training of reward models can broadly be seen as a ranking problem, where the model\nlearns to assign scores to outputs so that their order reflects the preferences indicated by\nhumans. Thereareseveralmethodstotrainarewardmodelfromtheperspectiveofranking.\nOne approach is to extend pairwise ranking to listwise ranking. For each sample in a\ndataset, we can use the LLM to generate multiple outputs, and ask human experts to order\ntheseoutputs. Forexample,givenasetoffouroutputs{y ,y ,y ,y },onepossibleorderof\n1 2 3 4\nthemcanbey ≻y ≻y ≻y . Averysimplemethodtomodeltheorderingofthelistis\n2 3 1 4\ntoaccumulatethepairwisecomparisonloss. Forexample,wecandefinethelistwiselossby",
    "10.4 Improved Human Preference Alignment 569\naccumulatingthelossoverallpairsofoutputs:\n(cid:104) 1 (cid:88) (cid:105)\nL = −E logPr(y ≻y |x) (10.57)\nlist (x,Y)∼Dr N(N−1) a b\nya∈Y,y\nb\n∈Y\nya̸=y\nb\nwhereY isalistofoutputs,andN isthenumberofoutputsinthelist. Pr(y ≻y |x)canbe\na b\ndefinedusingtheBradley-Terrymodel,thatis,Pr(y ≻y |x)=Sigmoid(r(x,y )−r(x,y )).\na b a b\nHereweomittheϕsuperscriptonthePr(·)tokeepthenotationuncluttered.\nAnextensiontotheBradley-Terrymodelforlistwiserankingcouldinvolvearankingmech-\nanismthattakesintoaccounttheentirelistofoutputsratherthanjustpairwisecomparisons.\nOnesuchmodelisthePlackett-Lucemodel,whichgeneralizestheBradley-Terrymodelto\nhandlemultipleitemsinaranking[Plackett,1975]. InthePlackett-Lucemodel,foreachitem\ninalist,wedefinea“worth”forthisitemthatreflectsitsrelativestrengthofbeingchosenover\notheritems. Fortherewardmodelingproblemhere,theworthofyinthelistY canbedefined\nas\nα(y) = exp(r(x,y)) (10.58)\nThentheprobabilityofselectingyfromY isgivenby\nα(y)",
    "Pr(yisselected|x,Y) =\n(cid:80) α(y′)\ny′∈Y\nexp(r(x,y))\n= (10.59)\n(cid:80) exp(r(x,y′))\ny′∈Y\nSuppose Y˚ is an ordered list y ≻y ≻···≻y . The overall log-probability of this\nj1 j2 jN\nordered list can be defined as the sum of the conditional log-probabilities at each stage of\nselection,givenby\nlogPr(Y˚|x) = logPr(y ≻y ≻···≻y |x)\nj1 j2 jN\n= logPr(y |x,{y ,y ,...,y })+\nj1 j1 j2 jN\nlogPr(y |x,{y ,...,y })+\nj2 j2 jN\n···+\nlogPr(y |x,{y })\njN jN\nN\n(cid:88)\n= logPr(y |x,Y˚ ) (10.60)\nj k ≥k\nk=1\nwhereY˚ representsthesubsetofthelistofoutputsthatremainunselectedatthek-thstage,\n≥k\ni.e., Y˚ = {y ,...,y }. Given the log-probability logPr(Y˚|x), we can define the loss\n≥k j k jN\nfunctionbasedonthePlackett-Lucemodelby\nL = −E (cid:2) logPr(Y˚|x) (cid:3) (10.61)\npl (x,Y˚)∼Dr",
    "570 Chapter 10. Alignment\nThere are also many other pairwise and listwise methods for modeling rankings, such\nas RankNet [Burges et al., 2005] and ListNet [Cao et al., 2007]. All these methods can be\ncategorizedintoalargefamilyoflearning-to-rankapproaches,andmostofthemareapplicable\ntotheproblemofmodelinghumanpreferences. However,discussingthesemethodsisbeyond\nthescopeofthischapter. Interestedreaderscanrefertobooksonthistopicformoredetails\n[Liu,2009;Li,2011].\nInadditiontopairwiseandlistwiseranking,usingpointwisemethodstotrainrewardmodels\noffers an alternative way to capture human preferences. Unlike methods that focus on the\nrelativerankingsbetweendifferentoutputs,pointwisemethodstreateachoutputindependently.\nForexample,humanexpertsmightassignascoretoanindividualoutput,suchasaratingona\nfive-pointscale. Theobjectiveistoadjusttherewardmodelsothatitsoutputsalignwiththese\nscores. Asimplewaytoachievepointwisetrainingisthroughregressiontechniqueswhere",
    "therewardofeachoutputistreatedasatargetvariable. Letφ(x,y)bethescoreassignedtoy\ngivenxbyhumans. Pointwiserewardmodelscanbetrainedbyminimizingalossfunction,\noftenbasedonmeansquarederrororotherregressionlosses,betweenthepredictedreward\nr(x,y)andtheactualhumanfeedbackφ(x,y). Forexample,thelossfunctioncouldbe\nL =\n−E(cid:2)\nφ(x,y)−r(x,y)\n(cid:3)2\n(10.62)\npoint\nWhilepointwisemethodsareconceptuallysimplerandcandirectlyguidetherewardmodel\ntopredictscores,theymightnotalwaysbethebestchoiceinRLHF.Aproblemisthatthese\nmethodsmaystrugglewithhighvarianceinhumanfeedback,especiallywhendifferentexperts\nprovide inconsistent scores for similar outputs. Because they focus on fitting to absolute\nscores rather than relative differences, inconsistencies in scoring can lead to poor model\nperformance. Moreover, fitting to specific scored outputs might discourage generalization,\nparticularlygiventhattrainingdataisoftenverylimitedinRLHF.Incontrast,methodsthat",
    "considerrelativepreferencescanpromotethelearningofmoregeneralizedpatternsofsuccess\nandfailure. Nevertheless,therearescenarioswherepointwisemethodsmightstillbesuitable.\nFor example, in tasks where training data is abundant and the costs of obtaining accurate,\nconsistentannotationsarelow,pointwisemethodscanproveeffective.\nInfact,tomakethesupervisionsignalfortrainingtherewardmodelmorerobust,wecan\nalsointroduceadditionalregularizationtermsintotraining. Forexample,ifweconsiderthe\nfirsttermU (x,y;θ)inEq. (10.54)asatypeofgeneralizedreward,thenthesecondterm\nppo-clip\n(i.e.,thepenaltyterm)canbeviewedasaformofregularizationfortherewardmodel,except\nthatherethegoalistotrainthepolicyratherthantherewardmodel. Anotherexampleisthat\nEisensteinetal.[2023]developaregularizationtermbasedonthesquaredsumofrewards,\nandaddittothepairwisecomparisonlossinRLHF:\nL = L +(−E (cid:2) r(x,y )+r(x,y ) (cid:3)2 )\nreg pair (x,ya,y\nb\n)∼Dr a b\n= −E (cid:2) logPr (y ≻y |x) (cid:3)\n(x,ya,y\nb\n)∼Dr ϕ a b",
    "−E (cid:2) r(x,y )+r(x,y ) (cid:3)2 (10.63)\n(x,ya,y\nb\n)∼Dr a b",
    "10.4 Improved Human Preference Alignment 571\nOptimizingwiththisregularizationtermcanhelpmitigatetheunderdeterminationofreward\nmodels5.\n2. Sparse Rewards vs. Dense Rewards\nAsdiscussedinSection10.3,therewardsinRLHFareverysparse: theyareobservedonly\nattheendofsequences,ratherthancontinuouslythroughoutthegenerationprocess. Dealing\nwith sparse rewards has long been a concern in reinforcement learning, and has been one\nofthechallengesinmanypracticalapplications. Forexample,inrobotics,itoftenneedsto\nshapetherewardfunctiontoeaseoptimizationratherthanrelyingsolelyonend-of-sequence\nrewards. Variousmethodshavebeendevelopedtoaddressthisissue. Onecommonapproach\nis reward shaping, where the original function is modified to include intermediate rewards,\nthereby providing more immediate feedback. Also, one can adopt curriculum learning to\nsequentiallystructuretasksinawaythatthecomplexitygraduallyincreases. Thiscanhelp\nmodelstomastersimplertasksfirst,whichpreparesthemformorecomplexchallengesastheir",
    "skillsdevelop. Therearemanysuchmethodsthatcanmitigatetheimpactofsparserewards,\nsuchasMonteCarlomethodsandintrinsicmotivation. Mostofthesemethodsaregeneraland\nthediscussionofthemcanbefoundinthebroaderliteratureonreinforcementlearning,such\nasSuttonandBarto[2018]’sbook.\nAlthough we do not discuss methods for mitigating sparse rewards in detail here, an\ninteresting question arises: why are sparse rewards so successful in RLHF? Recall from\nSection 10.3.1 that the supervision signal received at each time step t is not the reward for\nthecurrentaction,butrathersomeformoftheaccumulatedrewardsfromtuntilthelasttime\nstep. Suchsupervisionsignalsaredenseoverthesequence,becausetherewardobtainedatthe\nendofthesequencecanbetransferredbacktothattimestep,regardlessofwhichtimestep\nit is. In other words, the sparse rewards are transformed into the dense supervision signals.\nFurthermore,fromtheperspectiveofrewardshaping,Ngetal.[1999]showthattherewardat\ntcanbedefinedas",
    "r′(s ,a ,s ) = r(s ,a ,s )+f(s ,a ,s ) (10.64)\nt t t+1 t t t+1 t t t+1\nwherer′(·)isthetransformedrewardfunction,r(·)istheoriginalrewardfunction,andf(·)\nistheshapingrewardfunction. Toensuretheoptimalityofthepolicyunderthetransformed\nrewardfunction,theshapingrewardfunctioncanbegivenintheform\nf(s ,a ,s ) = γΦ(s )−Φ(s ) (10.65)\nt t t+1 t+1 t\nwhereΦ(s)iscalledthepotentialvalueofthestates. IfwedefineΦ(s)asthecommonvalue\nfunctionasinEq. (10.15)andsubstituteEq. (10.65)intoEq. (10.64),weobtain\nr′(s ,a ,s ) = r(s ,a ,s )+γV(s )−V(s ) (10.66)\nt t t+1 t t t+1 t+1 t\n5Amodeliscalledunderdeterminediftherearemultiplealternativesetsofparametersthatcanachievethe\nsameobjective.",
    "572 Chapter 10. Alignment\nItisinterestingtoseethatthisfunctionisexactlythesameastheadvantagefunctionusedin\nPPO.Thisrelatesadvantage-basedmethodstorewardshaping: theadvantageisessentiallya\nshapedreward.\nOn the other hand, one of the reasons for adopting end-of-sequence rewards lies in the\nnatureoftheRLHFtasks. Unliketraditionalreinforcementlearningenvironmentswherethe\nagent interacts with a dynamic environment, RLHF tasks often involve complex decision-\nmakingbasedonlinguisticorotherhigh-levelcognitiveprocesses. Theseprocessesdonotlend\nthemselveseasilytofrequentandmeaningfulintermediaterewardsbecausethequalityand\nappropriatenessoftheactionscanonlybefullyevaluatedafterobservingtheirimpactinthe\nlargercontextoftheentiresequenceortask. Inthiscase,therewardsignalsbasedonhuman\nfeedback,thoughverysparse,aretypicallyveryinformativeandaccurate. Consequently,this\nsparsity,togetherwiththehighinformativenessandaccuracyofthehumanfeedback,canmake\nthelearningbothrobustandefficient.",
    "3. Fine-grained Rewards\nFormanyapplications,ourobjectivewillbemorecomplexthanmerelyevaluatinganentire\ntext. For example, in sentiment analysis, we often do not just determine the sentiment of a\ntext,butneedtoanalyzethesentimentinmoredetailbyassociatingitwithspecificaspects\nofatopicdiscussedinthetext. Considerthesentence\"Thecameraofthephoneisexcellent,\nbutthebatterylifeisdisappointing.\"Inthisexample, wewouldneedtoseparatelyanalyze\nthesentimentsexpressedaboutthecameraandthebattery. Suchanalysis,knownasaspect-\nbasedsentimentanalysis,helpsprovideafiner-grainedunderstandingofthecustomerreview\ncomparedtogeneralsentimentanalysis.\nFortheproblemofrewardmodeling,weoftenneedtomodeldifferentpartsofasequence\nas well. A simple and straightforward way to do this is to divide a sequence into different\nsegmentsandthencomputetherewardforeachsegment[Wuetal.,2023b]. Supposethatan\noutputtokensequenceycanbedividedinton segments{y¯ ,...,y¯ }bysomecriterion. We\ns 1 ns",
    "canusetherewardmodeltoevaluateeachofthesesegments. Bytakingx,yandy¯ asinput\nk\ntotherewardmodel,therewardscoreforthek-thsegmentisgivenby\nrk = r(x,y,y¯ ) (10.67)\nk\nThentherewardscorefortheentireoutputsequenceisgivenby\n(cid:88)\nns\nr(x,y) = r(x,y,y¯ ) (10.68)\nk\nk=1\nHerer(x,y)canbeusedtotrainthepolicyasusual.\nA problem with this model is that training reward models at the segment level is not\nas straightforward as learning from human preferences on entire texts, as it is difficult to\nobtainsegment-levelhumanpreferencedata. Forrating-likeproblems(e.g.,werateasegment\naccordingtoitslevelofmisinformation),onesimpleapproachistoassignaratingscoreto\neachsegmentandtraintherewardmodelusingpointwisemethods. Forexample,wecanusea",
    "10.4 Improved Human Preference Alignment 573\nstrongLLMtoratethesequencesy¯ ...y¯ andy¯ ...y¯ ,andobtainthescoress(y¯ ...y¯ )\n1 k−1 1 k 1 k−1\nand s(y¯ ...y¯ ). We can then define the score of the segment y¯ as the difference between\n1 k k\ns(y¯ ...y¯ )ands(y¯ ...y¯ )\n1 k 1 k−1\ns(y¯ ) = s(y¯ ...y¯ )−s(y¯ ...y¯ ) (10.69)\nk 1 k 1 k−1\nUsingthesesegment-levelscores,wecantraintherewardmodelwitharegressionloss\nfunction\nL = −E (cid:2) s(y¯ )−r(x,y,y¯ ) (cid:3)2 (10.70)\nrating y¯ k k k\nSometimes,alignmentcanbetreatedasaclassificationproblem,forexample,weassess\nwhether a segment has ethical issues. In this case, the segment can be labeled as ethical or\nunethical,eitherbyhumansorusingadditionalclassifiers. Giventhelabelofthesegment,we\ncantraintherewardmodelusingsomeclassificationlossfunction. Forexample,supposethat\nr(x,y,y¯ )=1ifthesegmentisclassifiedasunethical,andr(x,y,y¯ )=−1otherwise6. The\nk k\nhingelossoftrainingbinaryclassificationmodelsisgivenby\nL = max(0,1−r(x,y,y¯ )·rˆ) (10.71)\nhinge k",
    "whererˆ∈{1,−1}denotesthegroundtruthlabel.\nThe remaining issue here is how to split y into segments. One approach is to define a\nfixed-lengthsegmentation,whereyisdividedintoequal-lengthchunks. However,thismay\nnotalwaysbeideal,asthecontentofthesequencemaynotalignwellwithfixedboundaries.\nAnalternativeapproachistosegmentybasedonspecificlinguisticorsemanticcues,suchas\nsentenceboundaries,topicshifts,orothermeaningfulstructuresinthetext. Suchasegmenta-\ntioncanbeachievedbyusinglinguisticsegmentationsystemsorpromptingLLMstoidentify\nnaturalbreaksinthesequence. Anotherapproachistousedynamicsegmentationmethods\nbasedonthecomplexityofthesequence. Forexample,segmentscouldbedefinedwherethere\nisasignificantchangeintherewardscore,whichmightcorrespondtoshiftsinthetaskbeing\nmodeled.\n4. Combination of Reward Models\nArewardmodelcanbeviewedasaproxyfortheenvironment. Sincethetrueenvironmentis\noftentoocomplexorunknown,developingaperfectproxyfortheenvironmentisgenerallynot",
    "possible. Asaresult,over-aligningLLMswiththisimperfectproxymightleadtodecreased\nperformance, known as the overoptimization problem [Stiennon et al., 2020; Gao et al.,\n2023a]7. We can also explain this through Goodhart’s law, which states: when a measure\n6Toallowtherewardmodeltooutputcategories,wecanreplacethelinearlayerdescribedinSection10.3.2\nwithaSoftmaxlayer.\n7Thisproblemisalsocalledrewardhackingorrewardgaming[Krakovnaetal.,2020;Skalseetal.,2022;\nPanetal.,2022],whichreferstothephenomenonwheretheagentattemptstotricktherewardmodelbutfails\ntoalignitsactionswiththetrueintendedobjectivesofthetask. Imagineastudentwhoisassignedhomework\nandisrewardedwithpointsorpraiseforcompletingit.Thestudentmightthenfindwaystofinishthehomework",
    "574 Chapter 10. Alignment\nbecomesatarget,itceasestobeagoodmeasure[Goodhart,1984].\nAddressingtheoveroptimizationproblemisnoteasy,andthereisnomaturesolutionyet.\nThe ideal approach might be to develop an oracle reward model that can perfectly capture\nthetrueobjectivesofthetaskandpreventtheagentfrom“tricking”. However,creatingsuch\namodelisextremelydifficultduetothecomplexityofthereal-worldenvironment, aswell\nas the challenge of defining all the relevant factors that contribute to the desired outcome.\nInstead,amorepracticalapproachistocombinemultiplerewardmodels,therebyalleviating\nthemisalignmentbetweenthetrainingobjectiveandthetrueobjectivethatarisesfromusinga\nsingle,specificrewardmodel[Costeetal.,2024].\nGiven a set of reward models, combining them is straightforward, and in some cases,\nwe can simply treat this problem as an ensemble learning problem. A simple yet common\napproachistoaveragetheoutputsofthesemodelstoobtainamorepreciserewardestimation:\nK\n1 (cid:88)\nr = w ·r (x,y) (10.72)",
    "combine k k\nK\nk=1\nwhere r (·) is the k-th reward model in the ensemble, w is the weight of r (·), and K\nk k k\nis the number of reward models. This combined reward can then be used to supervise the\ntrainingofapolicy. Infact,therearemanywaystocombinedifferentmodels,forexample,\none can make predictions using Bayesian model averaging or develop a fusion network to\nlearn to combine the predictions from different models. Alternatively, one can frame this\ntaskasamulti-objectiveoptimizationproblem,andusemultiplerewardmodelstotrainthe\npolicy simultaneously. These methods have been intensively discussed in the literature on\noptimizationandmachinelearning[Miettinen,1999;Bishop,2006].\nInadditiontomodelcombinationmethods,anotherimportantissueishowtocollector\nconstruct multiple different reward models. One of the simplest approaches is to employ\nensemblelearningtechniques,suchasdevelopingdiverserewardmodelsfromdifferentsubsets",
    "of a given dataset or from various data sources. For RLHF, it is also possible to construct\nreward models based on considerations of different aspects of alignment. For example, we\ncandeveloparewardmodeltoevaluatethefactualaccuracyoftheoutputandanotherreward\nmodeltoevaluatethecompletenessoftheoutput. Thesetwomodelsarecomplementaryto\neach other, and can be combined to improve the overall evaluation of the output. Another\napproachistoemploydifferentoff-the-shelfLLMsasrewardmodels. Thisapproachissimple\nandpractical,astherehavebeenalotofwell-developedLLMsandwejustneedtousethem\nwithnoorlittlemodification. Aninterestingissue,thoughnotcloselyrelatedtothediscussion\nhere, arises: can an LLM that aligns with other LLMs outperform those LLMs? Probably\nnotatfirstglance. Inpart,thisisbecausethetargetLLMmerelyimitatesotherLLMsbased\non limited supervision and thus cannot capture well the nuances of the behaviors of these",
    "supervisors. However,giventhestronggeneralizationabilityofLLMs,thisapproachcan,in\nfact, be quite beneficial. For example, using open-sourced or commercial LLMs as reward\nwithminimalefforttomaximizethereward,suchascopyingandpastingsolutionsfromtheinternetorprevious\nassignments,ratherthansolvingtheproblemsthemselves.",
    "10.4 Improved Human Preference Alignment 575\nValueFunction\nPreference\nData\ny a ≻ y b trainingwithMLE RewardModel Training\nwithPPO\nPolicy\n(a)RLHF(PPO)\nPreference\nData\ny ≻ y trainingwithMLE\na b Policy\n(b)DPO\nFigure10.10: StandardRLHF(PPO)vs. DPO.InRLHF,thehumanpreferencedataisused\nto train a reward model, which is then employed in training the policy as well as the value\nfunction. InDPO,theuseofhumanpreferencedataismoredirect,andthepolicyistrainedon\nthisdatawithouttheneedforrewardmodeltraining.\nmodelshasdemonstratedstrongperformanceinaligningLLMs,evenachievingstate-of-the-art\nresultsonseveralpopulartasks[Lambertetal.,2024].\n10.4.2 Direct Preference Optimization\nAlthoughlearningrewardmodelsisastandardstepinreinforcementlearning,itmakesthe\nentiretrainingprocessmuchmorecomplexthansupervisedtraining. Trainingareliablereward\nmodel is itself not an easy task, and a poorly trained reward model can greatly affect the\noutcomeofpolicylearning. Wenowconsideranalternativealignmentmethod,calleddirect",
    "preferenceoptimization(DPO),whichsimplifiesthetrainingframeworkbyeliminatingthe\nneedtoexplicitlymodelrewards[Rafailovetal.,2024]. Thismethoddirectlyoptimizesthe\npolicybasedonuserpreferences,ratherthandevelopingaseparaterewardmodel. Asaresult,\nwe can achieve human preference alignment in a supervised learning-like fashion. Figure\n10.10showsacomparisonofthestandardRLHFmethodandtheDPOmethod.\nBeforederivingtheDPOobjective,letusfirstreviewtheobjectiveofpolicytrainingused\ninRLHF.AsdiscussedinSection10.3.3,thepolicyistypicallytrainedbyoptimizingaloss\nfunction with a penalty term. The DPO method assumes a simple loss function where the\nqualityoftheoutputygiventheinputxisevaluatedbytherewardmodelr(x,y). Thetraining",
    "576 Chapter 10. Alignment\nobjectiveisthusgivenby\nθ˜ = argminE E (cid:2) −r(x,y)+β(logπ (y|x)−logπ (y|x)) (cid:3) (10.73)\nx∼D y∼π θ (·|x) θ θ ref\nθ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nloss penalty\nNote that in this optimization problem, only the term π (y|x) depends on the target policy\nθ\nπ (·). Boththerewardmodelr(x,y)andthereferencemodelπ (y|x)areassumedtobe\nθ θ\nref\nfixedgivenxandy. ThisisastrongassumptioncomparedwithPPO,butaswillbeshown\nlater,itsimplifiestheproblemandcrucialforderivingtheDPOobjective.\nSinceθisthevariablewewanttooptimize,werearrangetheright-handsideofEq. (10.73)\ntoisolateπ (y|x)asanindependentterm:\nθ\nθ˜ = argminE E (cid:2) βlogπ (y|x)−βlogπ (y|x)−r(x,y) (cid:3)\nx∼D y∼π θ (·|x) θ θ ref\nθ\n= argminE E (cid:2) logπ (y|x)− (cid:0) logπ (y|x)+ 1 r(x,y) (cid:1)(cid:3)\nx∼D y∼π θ (·|x) θ θ ref β\nθ\n= argminE E (cid:2) logπ (y|x)−logπ (y|x)exp (cid:0)1 r(x,y) (cid:1)(cid:3) (10.74)\nx∼D y∼π θ (·|x) θ θ ref β",
    "θ (cid:124) (cid:123)(cid:122) (cid:125)\ndependentonθ (cid:124) (cid:123)(cid:122) (cid:125)\nnotdependentonθ\nThisequationdefinestheobjectivefunctionasthedifferencebetweenthelog-probability\ndistribution function of y and another function of y. This form of the objective function\nseems not “ideal”, as we usually prefer to see the difference between two distributions, so\nthat we can interpret this difference as some kind of divergence between the distributions.\nA simple idea is to convert the second term (i.e., logπ (y|x)exp(1r(x,y))) into a log-\nθ ref β\nprobability distribution over the domain of y. If we treat π (y|x)exp(1r(x,y)) as an\nθ ref β\nunnormalizedprobabilityofy,wecanconvertitintoanormalizedprobabilitybydividingit\nbyanormalizationfactor:\n(cid:88) (cid:0)1 (cid:1)\nZ(x) = π (y|x)exp r(x,y) (10.75)\nθ\nref β\ny\nHencewecandefineaprobabilitydistributionby\nπ (y|x)exp\n(cid:0)1r(x,y) (cid:1)\nπ∗(y|x) = θ ref β (10.76)\nZ(x)",
    "10.4 Improved Human Preference Alignment 577\nWethenrewriteEq. (10.74)as\nθ˜ = argminE E (cid:104) logπ (y|x)−log π θ ref (y|x)exp (cid:0) β 1r(x,y) (cid:1)\nx∼D y∼π (·|x) θ (cid:1)\nθ Z(x\nθ\n(cid:105)\n−logZ(x)\n(cid:104) (cid:105)\n= argminE E logπ (y|x)−logπ∗(y|x)−logZ(x)\nx∼D y∼π (·|x) θ\nθ\nθ\n(cid:20)\n(cid:104) (cid:105)\n= argminE E logπ (y|x)−logπ∗(y|x)\nx∼D y∼π (·|x) θ\nθ\nθ\n(cid:21)\n−E (cid:2) logZ(x) (cid:3)\ny∼π (·|x)\nθ\n(cid:104) (cid:105)\n= argminE KL (cid:0) π (·|x)||π∗(·|x) (cid:1) − logZ(x) (10.77)\nx∼D θ\nθ (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\nKLdivergence constantwrt.θ\nSincelogZ(x)isindependentofθ,itdoesnotaffecttheresultoftheargmin operation,\nθ\nandcanberemovedfromtheobjective. Nowweobtainanewtrainingobjectivewhichfinds\ntheoptimalpolicyπ byminimizingtheKLdivergencebetweenπ (·|x)andπ∗(·|x)\nθ θ\n(cid:104) (cid:105)\nθ˜ = argminE KL (cid:0) π (·|x)||π∗(·|x) (cid:1) (10.78)\nx∼D θ\nθ\nClearly,thesolutiontothisoptimizationproblemisgivenby\nπ (y|x) = π∗(y|x)",
    "θ\nπ (y|x)exp\n(cid:0)1r(x,y))\n= θ ref β (10.79)\n(cid:1)\nZ(x\nGiventhisequation,wecanexpresstherewardr(x,y)usingthetargetmodelπ (y|x),the\nθ\nreferencemodelπ (y|x),andthenormalizationfactorZ(x):\nθ\nref\n(cid:18) (cid:19)\nπ (y|x)\nθ\nr(x,y) = β log +logZ(x) (10.80)\nπ (y|x)\nθ\nref\nThis is interesting because we initially seek to learn the policy π (·) using the reward\nθ\nmodelr(x,y),buteventuallyobtainarepresentationoftherewardmodelbasedonthepolicy.\nGiventherewardmodeldefinedinEq. (10.80),wecanapplyittotheBradley-Terrymodelto",
    "578 Chapter 10. Alignment\ncalculatethepreferenceprobability(alsoseeSection10.3.2):\nPr (y ≻y |x) = Sigmoid(r(x,y )−r(x,y ))\nθ a b a b\n(cid:18)\n(cid:16) π (y |x) (cid:17)\nθ a\n= Sigmoid β log +logZ(x) −\nπ (y |x)\nθ a\nref\n(cid:19)\n(cid:16) π (y |x) (cid:17)\nθ b\nβ log +logZ(x)\nπ (y |x)\nθ b\nref\n(cid:18) (cid:19)\nπ (y |x) π (y |x)\nθ a θ b\n= Sigmoid βlog −βlog (10.81)\nπ (y |x) π (y |x)\nθ a θ b\nref ref\nThisformulaiselegantbecauseitconvertsthedifferenceinrewardsintothedifference\ninratiofunctions,andwedonotneedtocalculatethevalueofZ(x). Adirectresultisthat\nwe no longer need a reward model, but only need the target policy and reference model to\ncalculatetheprobabilityofpreferences. Finally,wecantrainthetargetpolicybyminimizing\nthefollowingDPOlossfunction\nL (θ) = −E (cid:2) logPr (y ≻y |x) (cid:3) (10.82)\ndpo (x,ya,y\nb\n)∼Dr θ a b\nTheformofthislossfunctionisverysimilartothatusedintrainingrewardmodelsinRLHF\n(seeEq. (10.36)). Butitshouldbenotedthatthelossfunctionheredependsontheparameters",
    "ofthepolicy(i.e.,θ)ratherthantheparametersoftherewardmodel(i.e.,ϕ).\nThemainadvantageofDPOliesinitssimplicityandefficiency. TheDPOobjectiveisvery\nstraightforward—itdirectlyoptimizesforpreference-basedfeedback,ratherthanrelyingon\nseparatelydevelopedrewardmodels. Moreover,DPOisgenerallymoresample-efficient,asit\nlearnsfromafixeddatasetwithouttheneedforthecomputationallyexpensivesamplingprocess\nusedinPPO.ThismakesDPOapopularmethodforhumanpreferencealignment,especially\nwhendevelopingandapplyingrewardmodelsviareinforcementlearningischallenging.\nDPO can broadly be viewed as an offline reinforcement learning method, where the\ntrainingdataispre-collectedandfixed,andthereisnoexploration. Incontrast,onlinerein-\nforcementlearningmethodslikePPO,whichrequireexploringnewstatesthroughinteraction\nwiththeenvironment(usingtherewardmodelasaproxy),alsohavetheiruniqueadvantages.\nOneofthebenefitsofonlinereinforcementlearningisthatitallowstheagenttocontinuously",
    "adapttochangesintheenvironmentbylearningfromreal-timefeedback. Thismeansthat,\nunlikeofflinemethods,onlinemethodsarenotconstrainedbythestaticnatureofpre-collected\ndataandcandiscovernewproblem-solvingstrategies. Inaddition,explorationcanhelpthe\nagentcoverawiderrangeofstate-actionpairs,thusimprovinggeneralization. Thiscouldbe\nanimportantadvantageforLLMs,asgeneralizationisconsideredacriticalaspectinapplying\nsuchlargemodels.\n10.4.3 Automatic Preference Data Generation\nAlthoughlearningfromhumanpreferencesisaneffectiveandpopularmethodforaligning\nLLMs,annotatingpreferencedataiscostly. Usinghumanfeedbackdoesnotonlyfacesthe\nproblem of limited scalability, but it may also introduce bias because human feedback is",
    "10.4 Improved Human Preference Alignment 579\ninherently subjective. As a result, one can turn to AI feedback methods to address these\nscalabilityandconsistencyissueswithoutthelimitationsassociatedwithhumanannotators.\nAswithdatagenerationforinstructionfine-tuning,generatingpreferencedatausingLLMs\nis straightforward. Given a set of inputs, we first use an LLM to generate pairs of outputs.\nThen,weprompttheLLMtolabelthepreferencebetweeneachpairofoutputs,alongwith\nitscorrespondinginput. BelowisanexampleofpromptingtheLLMtogenerateapreference\nlabelforapairofconsumerserviceresponses.\nConsideracustomerservicescenariowhereacustomerposesarequest. You\nwillreviewtworesponsestothisrequest. Pleaseindicatewhichresponseis\npreferred. Notethatagoodresponseshouldbecourteous,clear,andconcise.\nItshouldaddressthecustomer’sconcerndirectly,providehelpfulinformation\norasolution,andmaintainapositivetone.\nRequest:\nHello,Inoticedthatmyorderhasn’tarrivedyet,thoughitwasscheduledto",
    "arriveseveraldaysago. Couldyoupleaseupdatemeonitsstatus? Thank\nyou!\nResponseA:\nI’mverysorryforthedelayandunderstandhowdisappointingthiscanbe.\nWe’redoingourbesttosortthisoutquicklyforyou.\nResponseB:\nHey,stuffhappens! Yourpackagewillgettherewhenitgetsthere,noneedto\nstress.\nResponseAispreferred.\nOncewecollectsuchpreferencelabels,wecanusethem,alongwiththeoutputpairand\ninput,totraintherewardmodel. Ofcourse,wecanconsiderdemonstratingafewexamples\norusingadvancedpromptingtechniques,suchasCoT,toimprovelabelingperformance. For\nexample, we can include in the prompt an example showing how and why one of the two\nresponsesispreferredbasedonaCoTrationale.\nInadditiontopreferencelabels,wecanalsoobtaintheprobabilityassociatedwitheach\nlabel[Leeetal.,2023]. Asimplemethodistoextracttheprobabilitiesforthelabeltokens,\nsuchas“A”and“B”,fromtheprobabilitiesoutputbytheLLM.WecanthenusetheSoftmax\nfunctionorothernormalizationtechniquestore-normalizetheseprobabilitiesintoadistribution",
    "over the labels. These probabilities of preferred labels can serve as pointwise supervision\nsignalsfortrainingtherewardmodel,asdiscussedinSection10.4.1.\nFor data generation, although it is easy to scale up, it is often necessary to ensure the\ndataisaccurateanddiverse. Here,thedataqualityanddiversityissuesinvolvenotonlythe",
    "580 Chapter 10. Alignment\nlabelingofpreferencesbutalsotheinputsandoutputsofthemodel. Therefore,weoftenneed\ntouseavarietyoftechniquestoobtainlarge-scale,high-qualitydata. Forexample,onecan\ngeneratediversemodeloutputsandannotationsbyusingdifferentLLMs,prompts,in-context\ndemonstrations,andsoon[Cuietal.,2024]. Duboisetal.[2024]reportthatthevariabilityin\npairwisepreferencedataisimportantfortrainingLLMsfromeitherhumanorAIfeedback.\nWhilelearningfromAIfeedbackishighlyscalableandgenerallyobjective,thismethod\nismoresuitedtowell-definedtaskswhereobjectiveperformancemetricsareavailable. By\ncontrast,learningfromhumanfeedbackismoreadvantageouswhenaligningAIsystemswith\nhumanvalues,preferences,andcomplexreal-worldtasksthatrequireunderstandingofsubtle\norsubjectivecontext. ThesemethodscanbecombinedtotrainLLMsthatbenefitfromboth\nhumaninsightsandthescalabilityofAIfeedback.\n10.4.4 Step-by-step Alignment\nSo far, our discussion of alignment has primarily focused on the use of reward models for",
    "evaluatingentireinput-outputsequencepairs. Thesemethodscanbeeasilyadaptedtoscenarios\nwherethecorrectnessofanoutputcanbeexaminedbycheckingwhetherthedesiredresultis\nincluded. Forexample,inthetaskofcalculatingamathematicalexpression,arewardmodel\ncanprovidepositivefeedbackiftheansweriscorrect,andnegativefeedbackiftheansweris\nwrong. However, in many problems that requirecomplexreasoning, simply examining the\ncorrectnessofthefinalresultisinsufficientforlearning. Imagineastudentwhoisonlygiven\nthefinalanswertoachallengingmathproblem. Knowingwhetherthefinalanswerisrightor\nwrongdoesnothelpthestudentfigureoutwheretheywentwrongandhowtocalculatethe\ncorrectanswer. Abetterapproachwouldbetoguidethestudentwithastep-by-stepbreakdown\noftheproblem-solvingprocessandencourageunderstandingoftheunderlyingconceptsand\nlogicbehindthesesteps.\nInChapter9,westudiedCoTmethodstopromptLLMstoexplicitlywriteoutintermediate\nstepsorthereasoningprocessneededtoreachaconclusionorsolveaproblem. Wesawthat",
    "breakingdownaproblemintosmallerpartscouldmakeiteasiertounderstandthesolution\npath and increase the accuracy of the output. These methods can be naturally extended to\nthe alignment of LLMs, that is, we supervise the model during the intermediate steps of\nreasoning. ConsiderareasoningtaskwhereanLLMproducesasequenceofreasoningsteps\ny={y¯ ,...,y¯ }forthegiveninput. Theresultofthereasoningisassumedtobeincludedin\n1 ns\nthelaststepy¯ ,andcanbeeasilyverified. Forthisreasoningproblem,Uesatoetal.[2022]\nns\ncategorizeLLMfine-tuningapproachesintotwoclasses:\n• Outcome-basedApproaches. Supervisionoccursonlywhentheendresultisverified.\nThisisastandardmethodforlearningfromhumanfeedbackwehavediscussedinthis\nchapter. For example, the LLM is optimized to maximize some form of the reward\nr(x,y).\n• Process-basedApproaches. Supervisionisinvolvedinallintermediatestepsinaddition\ntothelaststep. Todothis,weneedtodevelopamodeltogiveasupervisionsignalat",
    "eachstep,anddeveloplossfunctionsthatcanmakeuseofsuchsupervisionsignals.",
    "10.4 Improved Human Preference Alignment 581\nQuestion(fromTheInternationalJuniorMathOlympiad):\nJessicahasalotofappsonherphone. Thenumberofappsshehasisdivisibleby5. Ifshe\ndownloaded10moreapps,shewouldstillhavelessthan50apps. Whatisthemaximum\noriginalnumberofappsJessicacouldhave?\nResponse1: Response2:\ny¯ 1: xmustbeamultipleof5,i.e., y¯ 1: Thenumberofappsisdivisibleby5,\nx x\nx≡0(mod5). soxmustbeamultipleof5.\ny¯ 2: Afterdownloading10apps,thetotal y¯ 2: Ifshedownloads10moreapps,her\ntotalnumberofappsshouldstillbeless newtotalwillbex+10,andthismust\nx x\nthan50: x+10<50. Solvingthis belessthan50. Therefore,x+10<50.\ninequality: x<40. Simplifyingthisgivesx<40.\ny¯ 3: Sincexmustbeamultipleof5and y¯ 3: xmustbelessthanorequalto problematic\nx x\nlessthan40,thelargestvalueofxis35. thelargestmultipleof5,whichis40.\ny¯ 4: Verifytheresult. Ifx=35,thenafter y¯ 4: Butx+10shouldnotbemore problematic\ndownloading10apps,Jessicawould thanorequalto50. Soweneedto x\nx\nhave: 35+10=45whichsatisfy subtract5from40.\n45<50.",
    "y¯ 5: Therefore,thefinalresultis35. x\ny¯ 5: Themaximumoriginalnumberof\nx\ncorrect\nappsis35.\ncorrect\nFigure 10.11: Two LLM responses to a math problem. In response 1, both the final result\nandallthereasoningstepsarecorrect. Inresponse2,thefinalresultiscorrect,butthereare\nmistakesinthereasoningprocess(highlightedinred). Foroutcome-basedapproaches,both\nresponsesareconsideredcorrect. Forprocess-basedapproaches,themistakesinresponse2\ncanbeconsideredinrewardmodeling.\nFigure10.11showstwoLLMoutputsforanexamplemathproblem. AlthoughtheLLM\ngives the correct final answer in both cases, it makes mistakes during the problem-solving\nprocessinthesecondoutput. Outcome-basedapproachesoverlookthesemistakesandgive\npositivefeedbackfortheentiresolution. Bycontrast,process-basedapproachescantakethese\nmistakesintoaccountandprovideadditionalguidanceonthedetailedreasoningsteps.\nAnimportantissueforprocess-basedapproachesisthatweneedtogetstep-levelfeedback",
    "duringa(potentially)longreasoningpath. Wecancollectorgeneratereasoningpathscorre-\nspondingtoproblemsfromexistingdatasets. Humanexpertsthenannotateeachstepinthese\npathsforcorrectness. TheseannotationscanbeusedtodirectlytrainLLMsorasrewardsin\nreward modeling. However, in practice, richer annotations are often introduced [Lightman\net al., 2024]. In addition to the correct and incorrect labels, a step can also be labeled as",
    "582 Chapter 10. Alignment\nneutraltoindicatethatwhilethestepmaybetechnicallycorrect,itmightstillbeproblematic\nwithintheoverallreasoningprocess. Furthermore,toimprovetheefficiencyofdataannotation,\ntechniquessuchasactivelearningcanbeemployed. Identifyingobviouserrorsusuallydoes\nnotsignificantlycontributetolearningfromreasoningmistakes. Instead,annotatingstepsthat\nthemodelconfidentlyconsiderscorrectbutareactuallyproblematicisoftenmoreeffective.\nGivenasetofstep-levelannotatedreasoningpathsandcorrespondinginputs,wecantrain\narewardmodeltoprovidefeedbackforsupervisingpolicylearning. Therewardmodelcanbe\ntreatedasaclassificationmodel,andsoitsarchitecturecanbeaTransformerdecoderwitha\nSoftmaxlayerstackedontop. Atstepk,therewardmodeltakesboththeproblemdescription\n(denotedbyx)andthereasoningstepsgeneratedsofar(denotedbyy¯ )asinputandoutputs\n≤k\naprobabilitydistributionoverthelabelset{correct,incorrect}or{correct,incorrect,neutral}.",
    "Thenthelearnedrewardmodelisusedtoevaluatereasoningpathsbyassessingthecorrectness\nofeachstep. Asimplemethodtomodelcorrectnessistocountthenumberofstepsthatare\nclassifiedascorrect,givenby\n(cid:88)\nns\nr(x,y) = δ(correct,C(x,y¯ )) (10.83)\n≤k\nk=1\nwhere C(x,y¯ ) denotes the label with the maximum probability. We can also use log-\n≤k\nprobabilitiesofclassificationtodefinetherewardoftheentirepath\n(cid:88)\nns\nr(x,y) = logPr(correct|x,y¯ ) (10.84)\n≤k\nk=1\nwherePr(correct|x,y¯ )denotestheprobabilityofthecorrectlabelgeneratedbythereward\n≤k\nmodel. Therewardscorer(x,y)canthenbeusedtotrainthepolicyinRLHFasusual.\nWhile we restrict our discussion to math problems, the approaches described here are\ngeneral and can be applied to a wide variety of tasks that involve multi-step reasoning and\ndecision-making. Moreover,wecanconsidervariousaspectswhenassessingthequalityofa\nstep, rather thanjust itscorrectness. For example, indialogue systems, responsesmust not",
    "onlybeaccuratebutalsocontextuallyappropriateacrossmultipleturnsofconversation. Ifa\nmodelprovidesacorrectresponsebutfailstomaintaincoherenceinthecontextoftheongoing\ndialogue, step-level feedback could help the model identify and correct such discrepancies.\nAlsonotethattheprocess-basedapproachesarerelatedtothefine-grainedrewardmodeling\napproachesdiscussedinSection10.4.1. Alltheseapproachesessentiallyaimtoprovidemore\ndetailedsupervisiontoLLMsbybreakingtheiroutputsintosmaller,moremanageablesteps.\nHowever,process-basedfeedbackfocusesmoreonevaluatingthecorrectnessofastepbased\nonitsprecedingsteps,whiletheapproachesinSection10.4.1emphasizeevaluatingeachstep\nindependently.\nTheideaofaligningLLMsstepbystephasgreatapplicationpotential,especiallyconsider-\ningtherecentshifttowardsmorecomplexreasoningtasksintheuseofLLMs. Forexample,\nboththeGPT-o1andGPT-o3modelsaredesignedwithmoreadvancedreasoningtechniques\n(such as long internal CoT) to solve challenging problems like scientific and mathematical",
    "10.4 Improved Human Preference Alignment 583\nreasoning[OpenAI,2024]. Thesetasksoftenrelyonlongandcomplexreasoningpaths,and\ntherefore,itseemsessentialtointroducedetailedsupervisionsignalsinthereasoningprocess.\nMoreover, from a practical perspective, effective supervision on long reasoning paths not\nonly improves reasoning performance, but it also helps the model eliminate redundant or\nunnecessaryreasoningsteps,therebyreducingreasoningcomplexityandimprovingefficiency.\n10.4.5 Inference-time Alignment\nIn this section we explored a variety of methods to align models with human preferences\nand annotations. However, one of the significant limitations of many such methods is that\nLLMsmustbefine-tuned. ForRLHFanditsvariants,trainingLLMswithrewardmodelscan\nbecomputationallyexpensiveandunstable,leadingtoincreasedcomplexityandcostswhen\napplyingtheseapproaches. Inthiscase,wecanconsideraligningmodelsatinferencetime,\nthusavoidingtheadditionalcomplexityandeffortinvolved.",
    "Onesimplewaytoachieveinference-timealignmentistousetherewardmodeltoselect\nthebestonefromN alternativeoutputsgeneratedbytheLLM,amethodknownasBest-of-N\nsampling(BoNsampling). WecanconsiderBoNsamplingasaformofreranking. Infact,\nrerankingmethodshavebeenwidelyusedinNLPtasks,suchasmachinetranslation,foralong\ntime. Theyaretypicallyappliedinsituationswheretrainingcomplexmodelsiscostly. Insuch\ncases,directlyrerankingtheoutputsallowsfortheincorporationofthesecomplexmodelsata\nverylowcost8.\nIn the BoN sampling process, the LLM takes the input sequence x and generates N\ndifferentoutputsequences{yˆ ,...,yˆ }:\n1 N\n{yˆ ,...,yˆ } = argTopN[Pr(y|x)] (10.85)\n1 N\ny\nwheretheargTopNoperationreturnsthetop-N outputsthatmaximizethefunctionPr(y|x).\nTheseoutputscanbegeneratedinavarietyofways,dependingonthesearchalgorithmusedby\nthemodel(e.g.,samplingorbeamsearch). OncetheN-bestoutputcandidatesaregenerated,\ntherewardmodelisusedtoevaluateandselectthebestone:\nyˆ = max{r(x,yˆ ),...,r(x,yˆ )} (10.86)\nbest 1 N",
    "It is worth noting that the result of BoN sampling is also influenced by the diversity of\ntheN-bestlist. Thisisacommonissuewithmostrerankingmethods. Typically,wewishthe\nN-bestoutputcandidatestohaverelativelyhighqualitybutbesufficientlydifferentfromeach\nother. Inmanytextgenerationsystems,theN-bestoutputsareverysimilar,oftendifferingby\n8Rerankingmethodscanalsohelpusexplorewhatareknownasmodelerrorsandsearcherrors,althoughthese\nissuesarenotoftendiscussedinthecontextofLLMs. Forexample,supposewehaveanoldmodelandanew,\nmorepowerfulmodel.WecanusethenewmodeltoselectthebestoutputfromtheN-bestlistoftheoldmodel\nastheoracleoutput.Theperformancedifferencebetweentheoracleoutputandthetop-1outputoftheoriginal\nN-bestlistreflectstheperformancegainbroughtbythenewmodel.Iftheperformancegainissignificant,wecan\nsaythattheoldmodelhasmoremodelerrors.Ifthegainissmall,itmayindicatethattheissueliesinsearcherrors,\nasthebestcandidateswerenotfound.",
    "584 Chapter 10. Alignment\njustoneortwowords. ThediversityissueisevenmorechallenginginLLMs,astheN-best\noutputsgeneratedbyanLLMcanbedifferentintheirwordings,yettheirsemanticmeanings\nareoftenquitesimilar. Inpractice, onecanadjustthemodelhyperparametersand/oradopt\ndifferentLLMstogeneratemorediverseoutputcandidatesforreranking. Nevertheless,aswith\nmanypracticalsystems,weneedtomakeatrade-offbetweenselectinghigh-qualitycandidates\nandensuringsufficientvariationinthegeneratedoutputs.\nBoNsamplingcanbeusedfortrainingLLMsaswell. Acloselyrelatedmethodisrejection\nsampling. Inthismethod,wefirstselectthe“best”outputsfromtheN-bestlistsviathereward\nmodel,andthentaketheseselectedoutputstofine-tunetheLLM.Inthisway,wecanintroduce\nhumanpreferencesintothetrainingofLLMsviaamuchsimplerapproachcomparedtoRLHF.\nManyLLMshaveadoptedrejectionsamplingforfine-tuning[Nakanoetal.,2021;Touvron\netal.,2023b].\n10.5 Summary\nInthischapter,wehaveexploredarangeoftechniquesforaligningLLMs. Inparticular,we",
    "havediscussedfine-tuningmethodsthatenableLLMstofollowinstructionsandalignthem\nwithhumanpreferences. Oneofthebenefitsoffine-tuningLLMsiscomputationefficiency.\nUnlike pre-training based on large-scale neural network optimization, fine-tuning is a post-\ntrainingstepandsoislesscomputationallyexpensive. Moreover,itisbettersuitedtoaddress\nproblems that are not easily solved in pre-training, such as human value alignment. The\nwidespreadattentiontothealignmentissuehasalsoledtoasurgeofresearchpapersonthis\ntopic, which has posed challenges in writing this chapter, as it is difficult to cover all the\nlatesttechniques. However,wehavetriedtoprovidearelativelydetailedintroductiontothe\nfundamentalapproachestoalignment,suchasinstructionfine-tuningandRLHF.\nWhilewehavefocusedonLLMalignmenttechniquesinthischapter,thetermAIalignment\nisawide-rangingconcept. Itgenerallyreferstotheprocessofensuringthatthebehaviorofan\nAIsystemalignswithhumanvalues,goals,andexpectations. TheideaofAIalignmentcanbe",
    "tracedbacktotheearlydaysofAI.AwidelyciteddescriptionofAIalignmentcomesfrom\nanarticlebythemathematicianandcomputerscientistNorbertWiener[Wiener,1960]. The\nquoteisasfollows\nIfweuse,toachieveourpurposes,amechanicalagencywithwhoseopera-\ntionwecannotefficientlyinterfere... wehadbetterbequitesurethatthe\npurposeputintothemachineisthepurposewhichwereallydesire.\nAt that time, AI alignment was a distant concern for researchers. But today, it greatly\ninfluences thedesign ofvariousAI systems. Forexample, in robotics, alignmentis critical\ntoensuringthatautonomousrobotssafelyinteractwithhumansandtheirenvironments. In\nautonomousdriving,carsmustnotonlyfollowtrafficlawsbutalsomakecomplex,real-time\ndecisionsthatprioritizehumansafety,avoidaccidents,andnavigateethicaldilemmas.\nIncurrentAIresearch,alignmentisusuallyachievedbydevelopingasurrogateobjective\nthatisanalogoustotherealgoalandsteeringtheAIsystemtowardsthisobjective. However,",
    "10.5 Summary 585\ndesigningtheobjectiveofAIalignmentisverydifficult. Onereasonisthathumanvaluesare\ndiverseandoftencontext-dependent,makingitdifficulttodistillthemintoasingle,universally\napplicableobjectivefunction. Also,thecomplexityofreal-worldenvironments,wherevalues\nand goalsoften conflictor evolve over time, furthercomplicates alignment efforts. Even if\nwecoulddefineanappropriateobjective,AIsystemsmayfindunintendedwaystoachieveit,\nleadingto“misaligned”outcomesthatstilltechnicallysatisfytheobjectivebutinaharmfulor\ncounterproductiveway.\nThese challenges have motivated and are motivating AI research towards more aligned\nsystems,eitherthroughdevelopingnewmechanismsforperceivingtheworldormoreefficient\nand generalizable methods to adapt these systems to given tasks. More importantly, as AI\nsystemsbecomemorepowerfulandintelligent,especiallygiventhatrecentadvancesinLLMs\nhaveshownremarkablecapabilitiesindealingwithmanychallengingproblems,theneedfor",
    "AI alignment has become more urgent. Researchers have started to be concerned with AI\nsafetyandwarnthecommunitythattheyneedtodevelopandreleaseAIsystemswithgreat\ncautiontopreventthesesystemsfrombeingmisaligned[Russell,2019;Bengioetal.,2024].",
    "https://github.com/NiuTrans/NLPBook\nhttps://niutrans.github.io/NLPBook\nChapter 11\nInference\nOnce we have pre-trained and fine-tuned an LLM, we can apply it to make predictions on\nnewdata. Thisprocessiscalledinference,inwhichtheLLMcomputestheprobabilitiesof\ndifferentpossibleoutputsgivenaninput,andselectstheoutputthatmaximizestheprobability.\nTheinferenceproblemisgenerallyexpressedinthefollowingform:\nyˆ = argmaxPr(y|x) (11.1)\ny\nwhere x is the input sequence, y is a possible output sequence, and yˆ is the best output\nsequence.\nThisisperhapsoneofthemostwidelyadoptedformulasinNLP,anddatesbacktotheearly\ndays of speech recognition and machine translation systems based on probabilistic models.\nAlthough for some applications, such as predicting a token using a very small language\nmodel,solvingthisoptimizationproblemseemstrivial,formostsituationsthecomputational\nchallengesarisefrombothcalculatingPr(y|x)andperformingtheargmaxoperation. The",
    "problemswethereforewishtoaddressinthischapterinvolve: 1)computingtheprediction\nprobabilityefficientlygivenatrainedLLM,and2)devisinganefficient(suboptimal)search\nforyˆ.\nAt a high level, these are fundamental issues in artificial intelligence, which have been\nextensivelystudied. Somanywell-establishedtechniquescanbedirectlyapplied,forexample,\nonecanusegreedysearchalgorithmstoimplementanefficientinferencesystem. Ontheother\nhand,model-specificoptimizations,suchasefficientattentionmodelsforTransformers,canbe\nconsideredtofurtherimproveefficiency. But,inmanypracticalapplications,westillneedto\nmakeatrade-offbetweenaccuracyandefficiency,bycarefullycombiningvarioustechniques.\nTheimportanceoftheinferenceprobleminLLMsalsoliesinthefactthatmanyapplication\nscenariosrequireprocessingextremelylongsequences. Recentstudieshavefoundthatinjecting\nadditionalpromptsandcontextualinformation,suchaslongchain-of-thoughtprompts,during\ninferencecansignificantlyimprovetheperformanceofLLMs. Thisprovidesanewapproach",
    "toscalingLLMs: betterresultscanbeachievedbyincreasingthecomputeatinferencetime.\nForinstance,throughinference-timescaling,OpenAI[2024]’so1andDeepseek[2025]’sR1",
    "588 Chapter 11. Inference\nsystemshavedemonstratedimpressiveperformanceoncomplexreasoningandprogramming\ntasks. This, in turn, has encouraged the NLP field to focus more on the issue of efficient\ninference.\nInthischapter,wewillintroducebasicconceptsandalgorithmsofLLMinference,includ-\ningprefilling-decodingframeworks,search(decoding)algorithms,andevaluationmetricsof\ninferenceperformance. WewillthenpresentmethodsforimprovingtheefficiencyofLLM\ninference, covering a range of techniques for speeding up the system and compressing the\nmodel. Finally, we will discuss inference-time scaling, which is considered an important\napplicationofinferenceoptimization.\n11.1 Prefilling and Decoding\nInthissection,wepresenttheprefilling-decodingframework,themostcommonapproachfor\ninterpretingandimplementingLLMinferenceprocesses. Wefirstintroducethenotationand\nbackgroundknowledge,andthendescribethedetailsoftheframework,suchasthedecoding\nalgorithmsforLLMinference.\n11.1.1 Preliminaries",
    "AlthoughwehavedescribedLLMsmanytimesinthisbook,webeginbybrieflydefiningthe\nnotationtofacilitatethesubsequentdiscussion,andtomakethischapterself-contained.\nx: Theinputtokensequence. Itisconceptuallyequivalenttoa“prompt”,\nwhich includes instructions, user inputs, and any additional context\nintendedasinputtotheLLM.xcomprisesm+1tokens,denotedby\nx ...x ,wherex isthestartsymbol⟨SOS⟩.\n0 m 0\ny: The output token sequence, also called the response to the input. y\ncomprisesntokens,denotedbyy ...y .\n1 n\ny : Theoutputtokensthatprecedepositioni,thatis,y =y ...y .\n<i <i 1 i−1\nPr(y|x): TheprobabilityofgeneratingygivenxusingtheLLM.IftheLLMis\nparameterizedbyθ,wecanwriteitasPr (y|x).\nθ\n[x,y]: The concatenated token sequence of x and y. That is, [x,y] =\nx ...x y ...y . Occasionally,weusethenotationseq torepresent\n0 m 1 n x,y\n[x,y].\nPr([x,y]): Theprobabilityofgeneratingthetokensequence[x,y]usingtheLLM.\nAsdescribedinEq. (11.1),thegoalofLLMinferenceistomaximizePr(y|x). Modeling",
    "this conditional probability is common in NLP. At first glance, it seems to be a sequence-\nto-sequenceproblem,wherewetransformasequenceintoanotherusingencoding-decoding\nmodels. However,wearenotdiscussingsequence-to-sequenceproblemsorencoding-decoding\narchitectures. Instead,asdiscussedinearlierchapters,thismodelingproblemcanbeaddressed",
    "11.1 Prefilling and Decoding 589\nbyusingdecoder-onlymodels. Todothis,wecaninterpretthelog-scaleprobabilitylogPr(y|x)\nasthedifferencebetweenlogPr([x,y])andlogPr(x)\nlogPr(y|x) = logPr([x,y])−logPr(x) (11.2)\nwhere logPr([x,y]) and logPr(x) can be obtained by running the LLM on the sequences\n[x,y]andx,respectively. Forexample,wecancalculatetheprobabilityofgeneratingxusing\nthechainrule\nlogPr(x) = logPr(x ...x )\n0 m\n(cid:2) (cid:3)\n= log Pr(x )Pr(x |x )···Pr(x |x ...x )\n0 1 0 m 0 m−1\nm\n(cid:88)\n= logPr(x )+ logPr(x |x )\n0 j <j\n(cid:124) (cid:123)(cid:122) (cid:125)\nj=1\n=0\nm\n(cid:88)\n= logPr(x |x ) (11.3)\nj <j\nj=1\nInotherwords, wecalculatethetokenpredictionlog-probabilityateachpositionofx, and\nsumalltheselog-probabilities.\nIn common implementations of LLMs, however, we do not need to compute the log-\nprobabilityoftheinputsequence,butusetheLLMtodirectlycomputethelog-probabilityof\ntheoutputsequenceinthefollowingform\nn\n(cid:88)\nlogPr(y|x) = logPr(y |x,y ) (11.4)\ni <i\ni=1",
    "where [x,y ] represents the context for predicting y . We use Pr(y |x,y ) to denote\n<i i i <i\nPr(y |[x,y ]),followingthecommonlyusednotationintheliterature.\ni <i\nNow,wehavetwosub-problemsinaddressingtheinferenceissuedescribedinEq. (11.1):\n• ModelComputation: wemodelPr(y |x,y )andcomputeitinanefficientmanner.\ni <i\n• Search: wefindtheoptimal(orsub-optimal)outputsequenceintermsoflogPr(y|x).\nThe second sub-problem is a classic issue in NLP. We will show in Section 11.1.3 that\nthere are several well-studied algorithms that can be applied to efficiently search the space\nofpossibleoutputsequences. Thefirstsub-problemrequiresalanguagemodeltoproducea\ndistributionoveravocabularyV givenasequenceofcontexttokens. Wecandothisbytraining\naTransformerdecoder,whichoutputsthedistribution\nPr(·|x,y ) = Softmax(HWo) (11.5)\n<i m+i\nH = Dec([x,y ]) (11.6)\n<i\nHereDec(·)producesasequenceofrepresentations,eachcorrespondingtoapositionofthe",
    "input sequence. So, if we input [x,y ] to the LLM, H is an i′×d matrix, where d is the\n<i",
    "590 Chapter 11. Inference\nPr(·|x) Pr(·|x,y1) ··· Pr(·|x,y<i)\nSoftmaxLayer\ndecoderoutput ··· ···\nFFN\n··· ···\nSelf-attention Llayers\n··· ···\nLinearMapping\n··· ···\nEmbeddingLayer\nx 0 x 1 ··· xm y 1 ··· y i−1\nFigure11.1: Thedecoder-onlyarchitectureforLLMs. Thedecoderconsistsofanembedding\nlayerandastackofTransformerlayers. IneachTransformerlayer,theinputpassesthrougha\nlinearmapping,aself-attentionnetwork,andanFFN.Theoutputofthedecoderisasequence\nofrepresentationsthataretakenasinputtoaSoftmaxlayer,whichgeneratesadistributionof\ntokensforeachposition.\ndimensionality of each representation, and i′ =m+i is the number of context tokens. We\ncanthenuseaSoftmaxlayertotransformtheserepresentationsintodistributionsoftokens.\nWo ∈Rd×|V| is the linear mapping matrix of the Softmax layer, and HWo transforms the\nd-dimensionalrepresentationsinHintothe|V|-dimensionalrepresentations. Theuseofthe\nsubscriptm+iindicatesthattheSoftmaxfunctionisperformedonlyontherepresentationat",
    "positionm+i. SeeFigure11.1foranillustrationofthisarchitecture.\nDec(·) is a Transformer decoding network that consists of an embedding network and\nanumberofstackedself-attentionandFFNnetworks. WewillnotdiscussTransformersin\ndetailhere,asreaderscaneasilylearnaboutthesemodelsfromtheliterature. However,itis\nworthpointingoutthatthedifficultyofinferenceisinpartfromtheuseoftheself-attention\nmechanisminTransformers. Recallthatageneralformofsingle-headself-attentionisgiven\nby\nq KT\nAtt (q ,K,V) = Softmax(\ni√ ′\n)V (11.7)\nqkv i′\nd\nwhereq ∈Rd isthequeryatthepositioni′ (i.e.,positionofy ),andKandV∈Ri′×d are\ni′ i\nthekeysandvaluesuptoi′,respectively.\nAteachstepduringinference,wecalltheself-attentionfunctionAtt (·),followedby\nqkv",
    "11.1 Prefilling and Decoding 591\nanFFN,togeneratead-dimensionalrepresentationthatintegratesinformationfromboththe\ncurrenttokenanditsleftcontext. ThisprocessisrepeatedthroughLlayersofself-attention\nandFFN,formingastackofTransformerlayers. TheoutputoftheL-thlayerinthisstackis\nthefinalrepresentation.\nEach time, the model attends position i′ to all previous positions, which results in 2i′\nvectorproducts(i′ timesforq\ni′\nKT andi′ timesfortheproductofSoftmax( q i√′KT )andV).\nd\nHence, generating a sequence of length len has a time complexity of O(L×len2) for the\nself-attentionnetwork. Clearly,theinferenceofthismodelisslowforlongsequencesdueto\nitsquadratictimecomplexitywithrespecttosequencelength. Therefore,manyimprovements\ntoTransformersandalternativemodelshavefocusedonefficientmethodsthatarefasterthan\nthisquadratictimecomplexity,suchassparseattentionmechanismsandlinear-timemodels. A\ndetaileddiscussionofefficientTransformerscanbefoundinthepreviouschapters,andthis",
    "sectionwillfocusonthestandardTransformerarchitecture.\nNote that in self-attention, the queries, keys, and values of a layer are linear mappings\nfrom the same input (i.e., the output of the previous layer). Once a new key-value pair is\ngenerated,itisrepeatedlyusedinsubsequentinferencesteps. Ratherthanregeneratingthese\nkey-valuepairsduringinference,amoredesirablewayistostoretheminastructure,called\nthekey-valuecache,ortheKVcache. Thus,(K,V)canstraightforwardlybeconsidereda\nKVcache. Thiscacheisupdatedasfollows\nK = Append(K,k ) (11.8)\ni′\nV = Append(V,v ) (11.9)\ni′\nwhere(k ,v )isthenewlygeneratedkey-valuepairatpositioni′,andAppend(a,b)denotes\ni′ i′\nafunctionthatappendsarowvectorbtoamatrixa. Figure11.2showshowaTransformer\ndecoderworkswithaKVcache.\nFinally,theprocessofcomputinglogPr(y|x)issummarizedasfollows:\n1. Weconcatenatexandyintoasequence[x,y]. Foreachpositioni′ ofthissequence,we\nperformthefollowingsteps.\n(a) We compute the embedding of the token at position i′, and feed the resulting",
    "embeddingasaninitialrepresentationintothestackofTransformerlayers.\n(b) In each Transformer layer, we pass the input representation through the self-\nattentionnetworkfirstandthenthroughanFFN.Intheself-attentionnetwork,the\ninputrepresentationistransformedintoq ,k ,andv . Then,weupdatetheKV\ni′ i′ i′\ncache (K,V) using k and v (see Eqs. (11.8-11.9)). Then, we compute the\ni′ i′\noutputoftheattentionmodelbyattendingq to(K,V)(seeEq. (11.7)).\ni′\n(c) Ifi′ >m(i.e., i=i′−m≥0), wetaketheoutputoftheTransformerstackand\ncomputethetokenpredictionprobabilityPr(y |x,y )viatheSoftmaxlayer(see\ni <i\nEq. (11.5)).\n2. Whenreachingtheendofthesequence,weobtainlogPr(y|x)bysumminglogPr(y |x,y )\ni <i\noveri∈[1,n](seeEq. (11.4)).",
    "592 Chapter 11. Inference\nattention\nk 1 k 2 ··· k i′−1 k i′ q i′ query\nv 1 v 2 ··· v i′−1 v i′\nKVCache(positions1toi′−1)\nvalue key\nv i′ k i′\nLinearmaps\nInput\nInputatpositioni′\n(a)UpdatingtheKVCacheatPositioni′\nattention\nk 1 k 2 ··· k i′−1 k i′ k i′+1 q i′+1 query\nv 1 v 2 ··· v i′−1 v i′ v i′+1\nKVCache(positions1toi′)\nvalue key\nv i′+1 k i′+1\nLinearmaps\nInput\nInputatpositioni′+1\n(b)UpdatingtheKVCacheatPositioni′+1\nFigure11.2: IllustrationoftheKVcache. WeupdatetheKVcacheataposition,performthe\nattentionoperation,andthenmovetothenextpositiontorepeattheprocess.",
    "11.1 Prefilling and Decoding 593\n11.1.2 A Two-phase Framework\nAswehaveseen,languagemodelingisastandardautoregressiveprocess,whereeachtokenis\ngeneratedoneatatime,conditionedontheprevioustokens. ForTransformers,thisrequiresthe\nmodeltomaintainaKVcachethatstorespastrepresentations,andattendthenewlygenerated\nrepresentation to this KV cache. If we think of the model Pr(y|x) from the perspective of\ncomputingtheKVcache,itisnaturaltodivideinferenceintotwophases:\n• Prefilling. The prefilling phase computes the KV cache for the input sequence x. It\niscalledprefillingbecausethemodelpreparesandstoresthekey-valuepairsforeach\ntoken in the input before the actual inference begins. The process of prefilling in an\nLLMcanbeexpressedas\ncache = Dec (x) (11.10)\nkv\nwhereDec (·)isthedecodingnetwork(i.e.,thesameasDec(·)),butitreturnstheKV\nkv\ncacheinself-attentioninsteadoftheoutputrepresentations. cacheisalist,givenby\ncache = {cache1,...,cacheL} (11.11)\nwherecachel representsthekey-valuepairsforthel-thlayer.",
    "• Decoding. ThedecodingphasecontinuesgeneratingtokensbasedontheKVcache,as\nillustratedinFigure11.2. Whenanewtokenisinputintothedecoder,weupdatetheKV\ncacheineachlayerbyaddingthenewkey-valuepair. Theupdatedcacheisthenused\nforself-attentioncomputation. Thetokengenerationstopswhensomestoppingcriterion\nismet,suchaswhenthegeneratedtokenistheendsymbol. Thegoalofdecodingisto\nfindthebestpredictedsequence,whichisgivenby\nyˆ = argmaxPr(y|cache) (11.12)\ny\nHereweusePr(y|cache)insteadofPr(y|x)toemphasizethatthedecodingprocess\nactuallyreliesontheKVcacheratherthanx.\nTheprefillinganddecodingprocessesareillustratedinFigure11.3. Notethatboththese\nprocessesareautoregressive. However,asshowninTable11.1,theydifferinseveralaspects,\nwhichleadtoverydifferentimplementationsinpractice.\nInessence,whiletheunderlyingmodelofprefillingisbasedontokenprediction,itcan\nbeconsideredanencodingprocess. Thisisbecauseourgoalisnottogeneratetokens,butto",
    "build acontext representation(i.e., the KVcache)forthe subsequentstepsin thedecoding\nphase. Inthissense,itissimilartoBERT,whereweencodetheinputsequenceintoasequence\nof contextualized token representations. On the other hand, unlike BERT which generates\nbidirectionalsequencerepresentations,prefillingisbasedonstandardlanguagemodelingtasks,\nandisthusunidirectional. Notethat, sincetheentiresequencexisinputintothemodelall\natonce,allqueriescanbepackedtogetherandtheself-attentionoperationisperformedonx",
    "594 Chapter 11. Inference\nPrefilling Decoding\nGoal Setupinitialcontextx. Continuegeneratingtokens\nyaftertheinitialinput.\nAll-at-onceVisibility Tokensinxarepresented Tokensinyarepresented\nallatonce. sequentially,thatis,\npredictingatokenrequires\nwaitingfortheprevious\ntokenstobepredictedfirst.\nContextUse Buildthecontextor Usethecachedkey-value\nencodedrepresentationof pairs(fromprefilling)to\ntheinput. generatefurthertokens.\nResourceLimitation Compute-bound Memory-bound\nComputationalCost High VeryHigh\nTable11.1: PrefillingvsDecoding.\ninparallel. LetQbethequeriesthatarepackedintoonematrix. Theself-attentionmodelin\nprefillingcanbedefinedas\nQKT\nAtt (Q,K,V) = Softmax( √ +Mask)V (11.13)\nqkv\nd\nwhereQ,K,V∈Rd×(m+1). Mask∈R(m+1)×(m+1) isamaskthatensuresthateachtoken\nonlyattendstoitselfandthetokensthatprecedeitinthesequence. Itisrepresentedbysetting\nthevaluesinthemaskcorrespondingtofuturetokenstoalargenegativenumber,forexample,",
    "for the query q and the key k , we set the value of the entry (i,j) to −∞ if i<j. One\ni j\nadvantageofprocessingthesequencewithasingleself-attentioncomputationisthatwecan\nmake better use of the parallel computing capabilities of modern GPUs, and so speed up\nprefilling. In general, the prefilling process is considered compute-bound. This is because\nmerging multiple computational operations into one operation reduces the number of data\ntransfersandtheperformancebottleneckusuallycomesfromthecomputationalcapacityrather\nthanmemorybandwidth.\nDecodingisastandardleft-to-righttextgenerationprocess. Thetokensequenceisgener-\natedautoregressivelybypredictingonetokenatatimebasedontheKVcache. Eachtimeanew\ntokenisgenerated,weneedtoattendittoprevioustokens,followingEq. (11.7). Therefore,\nthedecodingprocessismemory-boundduetoitsfrequentaccesstotheKVcache. Thecost\nof decoding grows significantly as more tokens are generated. In most cases, decoding is",
    "computationallymoreexpensivethanprefilling. Notethatthisisnotjustbecause,indecoding,\ntheLLMgeneratestokensonebyoneandrepeatedlyupdatestheKVcache. Aswewillseein\nthefollowingsubsection,wemayneedtoexploremultipledifferenttokensequencesduring\ndecoding,whichmakestheproblemmorecomplexandincreasesitscostfurther.",
    "11.1 Prefilling and Decoding 595\n···\nqueries q0 q1 ··· qm−1\nkeys k0 k1 ··· km−1\nvalues v0 v1 ··· vm−1\n···\nreyaLaninoitnetta-fleS\nTransformerDecoder\n···\n···\n···\nEmbeddingLayer\nx 0 x 1 ··· x m−1\nProcessedallatonce\n(a)Prefilling\n···\nqueries qm qm+1 ··· qm+n\nkeys k0 k1 ··· km−1 km km+1 ··· km+n\nvalues v0 v1 ··· vm−1 vm vm+1 ··· vm+n\n···\nreyaLaninoitnetta-fleS\nPr(y1|x)Pr(y2|x,y1) ··· Pr(yn|x,y<n)\nSoftmaxLayer\nTransformerDecoder\n···\n···\n···\nEmbeddingLayer\nxm y 1 ··· y n−1\nProcessedstepbystep(nsteps)\n(b)Decoding(atthen-thstep)\nFigure11.3: Illustrationoftheprefillinganddecodingprocesses. Inprefilling,theentireinput\nsequenceisprocessedtogetherandtheKVcacheisfilled. Indecoding,theLLMgeneratesthe\noutputsequencestepbystepbasedontheprefilledKVcache.",
    "596 Chapter 11. Inference\n11.1.3 Decoding Algorithms\nSo far our discussion of LLM inference has primarily focused on the model computation\nproblem, that is, how to compute Pr(y|x). Now we turn to the discussion of the search\nproblem. Theproblemcanbestatedas: givenanLLMPr(y|x),howdoweefficientlysearch\nfor the best output sequence yˆ given the input sequence x (or the generated KV cache)?\nNaively,wecanconsideralloftheoutputsequences,computethepredictionprobabilityfor\neach, and then select the output sequence having the highest probability. This method can\nguaranteethegloballyoptimalsolution,butdirectexhaustivesearchisimpracticalforLLMs\nasthenumberofpossibleoutputsequencesgrowsexponentiallywiththelengthofy.\nInpractice,variousheuristicsearchalgorithms,suchasgreedysearchandsampling-based\nsearch,arecommonlyemployedtoapproximatethesolution. Eachofthesemethodsoffers\ntrade-offsbetweensearchqualityandcomputationalefficiency. Thesearchproblem,therefore,",
    "becomesabalancingactbetweenexplorationandexploitation, wherethegoalistofindan\nefficientstrategythatproduceshigh-qualityoutputswithoutexploringtheentirespace.\nBeforegivingamoredetaileddiscussionofthesemethods,letusfirstinformallydefine\nwhatasearchspaceisandhowitisrepresented. InLLMinference,wedefineahypothesisasa\ntupleofinputandoutputsequences. Sincexisfixedduringinference,wecansimplyconsider\neachhypothesisasanoutputsequence. Thesearchspace,denotedbyY,isthenthesetofall\npossiblehypotheses(i.e.,outputsequences)thatthemodelcangenerate. Thesearchproblem\nforLLMinferencecanbere-expressedas\nyˆ = argmaxPr(y|x) (11.14)\ny∈Y\nIn NLP, Y is commonly represented in a tree data structure to facilitate search. Figure\n11.4showsanexampleofthesearchtreeresultingfromasmallvocabulary. Inthisexample,\na node represents a prefix subsequence that can be shared by many sequences. The search\nstartswiththerootofthetree,whichcanberegardedasthebeginningofallsequencesthat",
    "canbegenerated1. Eachchildnodeextendstheprefixofitsparentnodebyaddingonetoken\nfromthevocabularytothesequence,alongwiththeprobabilityofpredictingthetokengiven\nthe prefix. This process continues as each node further branches out into additional child\nnodes,eachrepresentinganewpossibleextensionofthesequencewithanothertoken. The\nsearchtreethusgrowsdeeperandwider,representinganever-increasingnumberofpotential\nsequencesasmoretokensareappended. Thisstructureallowsustoefficientlytraversethrough\npossiblesequences,evaluatingeachintermsofthelog-probabilityaccumulatedoverthepath\nfrom the root to that node. For example, in Figure 11.4, the path from the root to the node\n17 corresponds to the output sequence “Cats are playful.”. The prediction log-probability\nlogPr(y|x)isthesumofthelog-probabilitiesofallthenodesonthispath.\nIngeneral,thesearchtreeisorganizedaslevels,whereeachlevelconsistsofallnodesthat\narethesamedistancefromtherootnode. Thus,abreadth-firstsearchoverthetreeessentially",
    "performsleft-to-rightgenerationoftokens. Nodesinthesamelevelcorrespondtosequences\n1Here,sincethepredictionsinLLMsarebasedonx,wecanthinkoftherootasarepresentationofx.",
    "11.1 Prefilling and Decoding 597\nPath: node0→node3→node9→node11→node17\nOutput: catsareplayful.\nProbability:\nnode0→0\nnode3→logPr(\"cats\"|x)\nnode9→logPr(\"are\"|x,\"cats\")\nnode11→logPr(\"playful\"|x,\"catsare\")\nnode17→logPr(\".\"|x,\"catsareplayful\")\n16\nplayful\n1 6 17\nplayful playful .\n2 7 11 18\n. . playful cats\n0 3 8 12 19\nroot cats cats . are\n4 9 13 20\nare are cats is\n5 10 14\nis is are\n15\nis\nFigure11.4: Asearchtreefordecoding. Ateachnode,weexpandthetreebyconsideringall\npossibletokens,eachleadingtoanewnoderepresentingapotentialcontinuationofthetext.\nHere we highlight a path through nodes 0, 3, 9, 11, and 17. The path represents the output\nsequence “cats are playful.”, whose log-probability can be computed by accumulating the\nlog-probabilitiesofthesenodes.\nof the same length. As the search progresses, new tokens are appended to these sequences,\nexpandingthemincrementally.\nLetY bethesetofthesequencesthattheLLMgeneratesatstepi. Y canbeobtainedby\ni i",
    "expandingeachsequenceinY withallpossiblenexttokensinthevocabularyV,givenin\ni−1\nthefollowingrecursiveform\nY = Y ×V (11.15)\ni i−1\nwhereY ×V denotestheCartesianproductofY andV (i.e.,eachsequenceinY is\ni−1 i−1 i−1\nconcatenatedwitheachtokeninV). NotethatifasequenceinY iscomplete(e.g.,ending\ni−1\nwiththe⟨EOS⟩token),itwillnotbeexpandedanyfurther. LetΨ(Y )bethesetofallcomplete\ni\nsequencesinY . Then,thesearchspacecanbeexpressedas\ni\n(cid:91) (cid:91) (cid:91)\nY = Ψ(Y ) Ψ(Y ) ··· Ψ(Y ) (11.16)\n1 2 nmax\nwheren isthemaximumlengthofasequence.\nmax\nMostdecodingalgorithmsfollowthislevel-by-levelsearchprocess. However,Y consists",
    "598 Chapter 11. Inference\nof an exponentially large number of sequences, and a direct search in such a vast space is\ncomputationallyinfeasible. Therefore,practicaldecodingalgorithmsoftenrelyonstrategies\ntoprunethesearchspaceandavoidexploringlow-qualitysequences. Forexample,ateach\ndecodingstep,Y canbeobtainedinthefollowingway\ni\nY = Prune(Y ×V) (11.17)\ni i−1\nwherePrune(·)isafunctionthatselectivelyremovessequenceslesslikelytoresultinhigh-\nquality outcomes. In general, we expect that |Y |<<|Y |·|V|. Thus we can drastically\ni i−1\nreducethenumberofsequencesunderconsiderationateachstep,ensuringthatthecomputa-\ntionalloaddoesnotgrowexponentiallywiththesequencelength.\nNext, we will introduce these decoding algorithms. Some of them have already been\ndiscussedinsequence-to-sequencemodels(seeChapter5),whileothersaremorecommonly\nusedinLLMs.\n1. Greedy Decoding\nGreedysearch(orgreedydecoding)isoneofthemostwidelyuseddecodingmethodsinNLP,",
    "particularlyintextgenerationtaskslikemachinetranslation. Theideabehindgreedysearch\nis straightforward: at each step in generation, it selects the next token that has the highest\nprediction probability. For each sequence y=y ...y ∈Y ×V, we can evaluate it using\n1 i i−1\nlogPr(y|x). Thislog-probabilitycanbeeasilycomputedbynotingthat\nlogPr(y|x) = logPr(y ...y |x)\n1 i\n= logPr(y |x) + logPr(y |x,y ) (11.18)\n<i i <i\n(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)\naccumulateduptotheparentnode newlycomputedforthecurrentnode\nHerethefirsttermisthesumofthelog-probabilitiesofthepathfromtheroottotheparent\nnode, which has been computed in the previous decoding steps. At step i, we only need to\ncomputethesecondtermwhichisthestandardtokenpredictionlog-probabilityproducedby\ntheLLM.\nThe“best”tokenatstepiisthenchosenas\nytop1 = argmaxlogPr(y ...y |x)\ni 1 i\nyi∈V\n(cid:2) (cid:3)\n= argmax logPr(y |x)+logPr(y |x,y )\n<i i <i\nyi∈V (cid:124) (cid:123)(cid:122) (cid:125)\nfixedwrt.yi",
    "= argmaxlogPr(y |x,y ) (11.19)\ni <i\nyi∈V\nThus,the“best”sequencegenerateduptostepiisgivenby\nytop1 = y ...y ytop1 (11.20)\n1 i−1 i",
    "11.1 Prefilling and Decoding 599\nFinally,Y containsonlythissequence\ni\nY = {ytop1} (11.21)\ni\nThegreedychoiceinonedecodingstepisillustratedinFigure11.5(a). Greedysearch\noffers computational efficiency and simplicity in implementation for LLM inference. Its\nprimary disadvantage, however, lies in its suboptimal nature — high-quality sequences are\nlikelyprunedatearlystagesofdecoding. Therefore,greedysearchisappealingfortasksthat\ndemandspeedandsimplicity. Fortasksthatrequirebettersearchresults,alternativestrategies\nsuchasbeamsearch,whichexploresmultiplepotentialpathssimultaneously,arepreferable.\n2. Beam Decoding\nBeamsearch(orbeamdecoding)isanaturalextensionofgreedysearch. Insteadofselecting\nthesinglemostprobabletokenateachstep,beamsearchmaintainsafixednumberofthebest\ncandidatesateachstep,knownasthe“beamwidth”. SeeFigure11.5(b)foranillustrationof\nbeamsearch.\nLetK bethebeamwidth. Givenaparentnode,whichcorrespondstotheprefixy ...y ,\n1 i−1\nwecanselectthetop-K nexttokensby",
    "{ytop1,...,ytopK} = argTopK Pr(y |x,y ) (11.22)\ni i i <i\nyi∈V\nwhereargTopKisafunctionthatranksthepredictionprobabilitiesofallpossiblenexttokens\nandselectsthetopK candidates. Giventhesetokens,thetop-K sequencesforstepiaregiven\nby\nytop1 = y ...y ytop1 (11.23)\n1 i−1 i\n.\n.\n.\nytopK = y ...y ytopK (11.24)\n1 i−1 i\nThen,wecandefineY as\ni\nY = {ytop1,...,ytopK} (11.25)\ni\nWecanadjustthebeamwidthK tobalancesearchefficiencyandaccuracy. Butaverylarge\nbeamwidthmightnotbehelpful. Inmanypracticalapplications,selectingarelativelysmall\nnumberforK,suchasK =2orK =4,isoftensufficienttoachievesatisfactoryperformance\ninLLMinference.\n3. Sampling-based Decoding\nBothgreedyandbeamsearchgeneratedeterministicoutputs,thatis,givenanLLM,theoutput\nofthemodelwillalwaysbethesameeverytimeitprocessesthesameinput. Thedeterministic\nnatureofgreedyandbeamsearchensurespredictabilityandreliabilityinapplicationswhere",
    "600 Chapter 11. Inference\n①Expansion ②Ranking ③Output(1-best)\n5 . 6 cute Pr=.34 ok =⇒ 6 cute\n6 7\ncute on Pr=.32 pruned\n4 7 9\nis on sick Pr=.21 pruned\n8 8\nare are Pr=.12 pruned\n9 sick 5 . Pr=.01 pruned\n(a)Greedysearch\n①Expansion ②Ranking ③Output(K-best)\n5 . 6 cute Pr=.34 ok =⇒ 6 cute\n6 cute 7 on Pr=.32 ok =⇒ 7 on beamwidth(K)=3\n4 is 7 on 9 sick Pr=.21 ok =⇒ 9 sick\n8 8\nare are Pr=.12 pruned\n9 sick 5 . Pr=.01 pruned\n(b)Beamsearch\n①Expansion ②Ranking ③Selection&Sampling ④Output\n5 . 6 cute Pr=.34 ok =⇒ 6 cute Pr=.39 pruned\n6 cute 7 on Pr=.32 ok =⇒ 7 on Pr=.36 ok =⇒ 7 on\n4 is 7 on 9 sick Pr=.21 ok =⇒ 9 sick Pr=.25 pruned\n8 8\nare are Pr=.12 pruned selecttop-khypotheses(k=3),\nrenormalizetheirproababilities,\n9 sick 5 . Pr=.01 pruned andselectoneviasampling.\n(c)Top-kSampling\n①Expansion ②Ranking ③Selection&Sampling ④Output\n5 . 6 cute Pr=.34 ok =⇒ 6 cute Pr=.51 pruned\n6 cute 7 on Pr=.32 ok =⇒ 7 on Pr=.49 ok =⇒ 7 on\n4 7 9\nis on sick Pr=.21 pruned selecttop-rankedhypotheses",
    "whoseprobabilitysum≥p=0.6,\n8 8\nare are Pr=.12 pruned renormalizetheirproababilities,\nandselectoneviasampling.\n9 sick 5 . Pr=.01 pruned\n(d)Top-pSampling\nFigure 11.5: Illustrations of greedy decoding, beam decoding, top-k decoding and top-p\ndecodingmethods(inonedecodingstep).",
    "11.1 Prefilling and Decoding 601\nconsistentoutcomesarecritical,suchasinformaldocumentgeneration,wherevaryingoutputs\ncould cause confusion or errors. On the other hand, one disadvantage of these methods is\nthe lack of diversity and flexibility. For example, in creative tasks like story generation or\nconversational agents, generic or repetitive outputs generated by deterministic systems are\noftenlessengaging.\nToaddvariationintoLLMoutputs,wecanusesampling-baseddecodingmethods. There\naretwocommonlyusedmethods.\n• Top-k Sampling. Thismethodselectsthenexttokenfromthetop-k mostlikelycandi-\ndatesateachstepofthegenerationprocess[Fanetal.,2018]. LetV betheselection\ni\npoolfortop-k sampling. Wecandefineitas\nV = {ytop1,...,ytopk} (11.26)\ni i i\nwhere {ytop1,...,ytopk} are the top-k tokens selected based on their prediction prob-\ni i\nabilities (see Eq. (11.22)). Once the selection pool is determined, we recompute the",
    "prediction probability distribution over V . One of the simplest ways to do this is to\ni\nrenormalizetheirprobabilities:\nPr(y |x,y )\ni <i\nPr(y |x,y ) = (11.27)\ni <i (cid:80)\nPr(y |x,y )\nyj∈Vi j <i\nAlternatively,wecancalculatethedistributionbyusingtheSoftmaxfunction:\nexp(u )\nPr(y |x,y ) =\nyi\n(11.28)\ni <i (cid:80)\nexp(u )\nyj∈Vi yj\nwhereu isthelogitfortokeny . Then,wesampleatokeny¯ fromthisdistribution:\nyi i i\ny¯ ∼ Pr(y |x,y ) (11.29)\ni i <i\nThecorrespondingsequenceisy¯ =y ...y y¯,andY isgivenby\n1 i−1 i i\nY = {y¯} (11.30)\ni\n• Top-p Sampling. This sampling method, also known as nucleus sampling, follows\na procedure similar to that of top-k sampling. Instead of drawing from a fixed size\ncandidatepool,itselectsthenexttokenfromthesmallestsetoftokensthattogetherhave\nacumulativeprobabilityhigherthanapredefinedthresholdp[Holtzmanetal.,2020b].\nInthiswaywepreventthepredictionfromchoosingfromlow-probabilitytokensinthe\nlongtailthatcouldleadtoincoherentornonsensicaloutputs. Toobtainthecandidate",
    "poolinthetop-psamplingmethod,wecansortalltokensbytheirpredictedprobabilities.\nThen,startingwiththetokenwiththehighestprobability,wecontinuetoaddtokensto\nthecandidatepooluntilthecumulativeprobabilityofthetokensinthepoolreachesor\nexceedsp(wedenotethesizeofthecandidatepoolatthispointask ). Thecandidate\np",
    "602 Chapter 11. Inference\n(a)β=0.1 (b)β=0.8 (c)β=2.0\nFigure11.6: HistogramestimatesofthedistributionsgeneratedbytheSoftmaxfunctionwith\ndifferentvaluesofthetemperatureparameterβ.\npoolcanthenbeexpressedas\nV = {ytop1,...,y topkp} (11.31)\ni i i\nThesubsequentsteps,suchastherenormalizationofthedistributionandsampling,are\nthesameasinthetop-k samplingmethod(seeEqs.(11.27-11.30)).\nSeeFigure11.5(c-d)forillustrationsofthetop-kandtop-psamplingmethods. Bylimiting\nthechoicestoasmallersetofhigh-probabilitytokens,thesemethodsstrikeabalancebetween\nrandomness and coherence. They allow for more diverse outputs while still maintaining a\nreasonable level of relevance and fluency. However, the value of k or p must be carefully\nchosen: ifk orpistoosmall,theoutputmaystillbeoverlydeterministic(morelikegreedy\ndecoding),andifk orpistoolarge,theLLMmightproducedegenerateoutputs.\nInordertofurthercontroltherandomnessofthetokenselectionprocess,therenormalized",
    "distribution Pr(·) is typically obtained by using the Softmax function with the temperature\nparameter,givenby\nexp(u /β)\nPr(y |x,y ) =\nyi\n(11.32)\ni <i (cid:80)\nexp(u /β)\nyj∈Vi yj\nHereβ isatemperatureparameterβ thatcontrolsthesharpnessoftheprobabilitydistribution\nderivedfromlogits. InFigure11.6,weshowsimpleexamplesinvolvingdistributionsgenerated\nby the above function with different temperatures. When the temperature is set to a higher\nvalue,theresultingprobabilitydistributionbecomesmoreuniform,asthedifferencesbetween\nthelogitsarediminished. Thismeansthateachtokeninthecandidatepoolhasamoreequal\nchance of being selected, leading to greater diversity in the generated output. By contrast,\nwhenthetemperatureissettoalowervalue,thedistributionbecomessharper,makingthehigh-\nprobabilitytokensevenmorelikelytobechosen,whichoftenresultsinmoredeterministic\noutputs. Forexample,ifwesetpto1andβ toaverysmallnumber(approachingzero),the\ntop-psamplingmethodwillbecomeequivalenttothegreedysearchmethod.",
    "4. Decoding with Penalty Terms\nOne common improvement to decoding methods in text generation is to modify the search\nobjective. Forexample,onecanreplacemaximumaposteriori(MAP)decodingwithminimum",
    "11.1 Prefilling and Decoding 603\nBayesrisk(MBR)decoding[KumarandByrne,2004b],wherethefocusshiftsfromselecting\nthesinglemostprobableoutputtochoosinganoutputthatminimizestheexpectedriskovera\ndistributionofpossibleoutputs. MoredetailsonMBRdecodingcanbefoundinChapter5.\nHereweexploremethodsthatincorporatepenaltytermsintodecoding. Thesemethodsoffera\nsimplebuteffectivewaytomakedecodingmorecontrollable.\nRecall from Eq. (11.14) that the goal of decoding is to maximize the likelihood of the\noutputsequence. Withpenaltyterms,theobjectiveisextendedtoincludeadditionalfactors\nthat penalize or reward certain behaviors in the generated text. A general form of the new\nobjectiveisgivenby\n(cid:2) (cid:3)\nyˆ = argmax Pr(y|x)−λ·Penalty(x,y) (11.33)\ny∈Y\nwherePenalty(x,y)isafunctionthatquantifiesthedegreetowhichthegeneratedsequence\nyviolatescertainconstraintsorexhibitsundesirablebehaviorsgiventheinputx. Thedesign\nofPenalty(·)isveryflexible,thusallowingustoincorporateawiderangeofconstraintsor",
    "priorknowledgeintoit. Below,wepresentsomecommontypesofpenaltyfunctions.\n• RepetitionPenalty. Arepetitionpenaltydiscouragesthemodelfromgeneratingrepeti-\ntiveorredundanttext. Thepenaltyfunctionmightmeasurethefrequencyofrepeated\ntokensorphrasesinthegeneratedsequenceandimposeapenaltyproportionaltotheir\noccurrence.\n• Length Penalty. A length penalty ensures that the generated sequence adheres to a\ndesired length. For example, in text summarization tasks, the penalty function could\npenalizeoutputsthataretooshortortoolong.\n• Diversity Penalty. A diversity penalty promotes variation in the generated text. For\nexample,inbeamsearch,wecanmeasurethesimilaritybetweengeneratedhypotheses,\nandencouragethemodeltoexploredifferenthypotheses.\n• Constraint-based Penalty. A constraint-based penalty enforces specific constraints\nrelated to the content or style of the generated text. For example, in machine transla-\ntion, the penalty function could penalize outputs that deviate from a desired tone or",
    "terminology.\nIngeneral,wecanconsiderPenalty(x,y)asafunctionthatdefinesthecostofgenerating\nthe surface form of the output sequence y given the input sequence x. Alternatively, this\nfunctioncanbedefinedtoassessthehiddenstatesofanLLMwhengeneratingy. Forexample,\nSu et al. [2022] develop a penalty term that calculates the maximum distance between the\nrepresentationofthepredictedtokenandtherepresentationsofthepreviouslygeneratedtokens.\nTherefore, the search objective will penalize degenerated outputs, such as texts with many\nrepetitions.\nThemethoddescribedinEq. (11.33)isgeneralandcanbeeasilyadaptedtodifferentsearch\nalgorithms. Forexample,ingreedysearch,wecankeepthesinglesequencethatmaximizes\nPr(y|x)−λ·Penalty(x,y)ateachdecodingstep;insampling-basedsearch,wecanrankand",
    "604 Chapter 11. Inference\nselectthetop-rankedsequencesbasedonPr(y|x)−λ·Penalty(x,y)toformthecandidate\npool.\n5. Speculative Decoding\nSpeculative decoding stems from the concept of speculative execution, where a system\nmakeseducatedguessesaboutfutureactionsandperformstheminadvance. Iftheguessis\ncorrect,theresultsareimmediatelyavailable,whichspeedsupprocessing. InthecaseofLLM\ninference,supposewehavetwomodels. Oneisasmaller,fastermodel(calleddraftmodel),\nandtheotheristhefull,moreaccuratemodel(calledverificationmodel). Thesetwomodels\nrepresenttwobaselinesinLLMinference: thedraftmodelisefficientbutnotveryaccurate;\ntheverificationmodelisusuallytheonewewanttorun, butitisveryslow. Givenaprefix,\nwefirstusethedraftmodeltospeculativelypredictasequenceoflikelyfuturetokens. This\nis a standard autoregressive decoding process, but it is still fast in practice due to the high\nefficiencyofthedraftmodel. Then,theverificationmodelevaluatesthespeculatedtokensin",
    "parallel. Itcheckswhetherthepredictedtokensarecorrectorneedtobeadjusted. Notethat,\nsincewecandealwiththesetokensallatonce,theverificationcanbedoneinasinglestepfor\nallthetokenssimultaneously,ratherthaninatoken-by-tokenmanner. Ifthespeculatedtokens\narecorrect,theyareaccepted,andtheprocesscontinueswiththenextsetoftokens. Ifthey\nareincorrect,theincorrectspeculationsarediscarded,andtheverificationmodelisusedto\ngeneratethecorrecttokens.\nTobemorespecific, letusseethespeculativedecodingmethodpresentedinLeviathan\netal.[2023]’swork. Inthismethod,thedraftmodelisasmalllanguagemodel,denotedby\nPr (y |x,y ),whiletheverificationmodelisanormalLLM,denotedbyPr (y |x,y ). The\nq i <i p i <i\ngoalisthat,givenaprefix,weusethedraftmodeltoautoregressivelypredictuptoτ tokens.\nTheverificationmodelisthenemployedtogeneratethelasttokenatthepointwhereerrors\nbegintooccurinthespeculativepredictions. Figure11.7illustratesonestepinthisdecoding\nprocess.\nThespeculativedecodingalgorithmcanbesummarizedasfollows.",
    "• Giventheprefix[x,y ],weusethedraftmodeltopredictthenextτ consecutivetokens,\n≤i\ndenotedby{yˆ ,...,yˆ }. Thisisatoken-by-tokengenerationprocess,givenby\ni+1 i+τ\nyˆ = argmaxPr (y |x,y ,yˆ ...yˆ ) (11.34)\ni+t q i+t ≤i i+1 i+t−1\nyi+t\n• Weevaluate{yˆ ,...,yˆ }usingtheverificationmodel,thatis,wecompute{Pr (yˆ |x,y )\ni+1 i+τ p i+1 ≤i\n,...,Pr (yˆ |x,y ,yˆ ...yˆ )}. Notethatwecancomputetheseprobabilitiesin\np i+τ ≤i i+1 i+τ−1\nparallel,andsothisverificationstepisefficient.\n• Wedeterminethemaximumnumberofacceptedspeculatedtokens. Inordertokeepthe\nnotationuncluttered,wedenotePr (yˆ |x,y ,yˆ ...yˆ )andPr (yˆ |x,y ,yˆ ...yˆ )\nq i+t ≤i i+1 i+t−1 p i+t ≤i i+1 i+t−1\nsimplybyq(yˆ )andp(yˆ ),respectively. Wethendefinethat,ifq(yˆ )≤p(yˆ ),\ni+t i+t i+t i+t\nthenweacceptthisspeculation. Bycontrast,ifq(yˆ )>p(yˆ ),werejectthisspecula-\ni+t i+t\ntionwithprobability1−\np(yˆi+t)\n. Startingfromyˆ ,themaximumnumberofaccepted\nq(yˆi+t) i+1",
    "11.1 Prefilling and Decoding 605\nContext(x,y ) yˆ yˆ yˆ yˆ yˆ\n<i i+1 i+2 i+3 i+4 i+5\npredict\nDraftModelPr (·)\nq\n(a)Predictthenextτ tokensgiventhecontextusingthedraftmodel(τ =5)\nEvaluationModelPr (·)\np\nevaluate\nContext(x,y ) yˆ yˆ yˆ yˆ yˆ\n<i i+1 i+2 i+3 i+4 i+5\nDraftModelPr (·)\nq\n(b)Evaluatethepredictedtokensusingtheevaluationmodel\nEvaluationModelPr (·)\np\naccepted rejected\nContext(x,y ) yˆ yˆ yˆ yˆ yˆ\n<i i+1 i+2 i+3 i+4 i+5\nDraftModelPr (·)\nq\n(c)Determinethenumberofacceptedtokens\nEvaluationModelPr (·)\np\nContext(x,y <i ) yˆ i+1 yˆ i+2 yˆ i+3 y¯ i+4\nDraftModelPr (·)\nq\n(d)Predictanewtokenfollowingtheacceptedtokensusingtheevaluationmodel\nFigure11.7: Illustrationofonestepofspeculativedecoding. Thegoalistopredictasmany\nnexttokensaspossibleusingthedraftmodel. Therearefoursub-steps. Giventhecontext,we\nfirstusethedraftmodeltopredictthenextτ tokens. Then,weevaluatethesepredictionsin\nparallelusingtheevaluationmodel. Next,wedeterminethemaximumnumberofpredicted",
    "tokens that can be accepted. Finally, we use the evaluation model to predict a new token\nfollowingtheseacceptedtokens.",
    "606 Chapter 11. Inference\nconsecutivespeculatedtokensisdefinedas\n(cid:26) (cid:27)\np(yˆ )\ni+t\nn = min t−1|1≤t≤τ,r > (11.35)\na t\nq(yˆ )\ni+t\nwherer isavariabledrawnfromtheuniformdistributionU(0,1).\nt\n• Givenn ,wekeepthespeculatedtokens{yˆ ,...,yˆ }. Wethenusetheverification\na i+1 i+na\nmodeltomakeanewpredictionati+n +1\na\ny¯ = argmaxPr (y |x,y ,yˆ ...yˆ ) (11.36)\ni+na+1 p i+ns+1 ≤i i+1 i+ns\nyi+ns+1\n• Above, we have described one step of speculative decoding. The result sequence\n(includingboththecontextandpredictedtokens)isillustratedasfollows\n[x, y ] yˆ ...yˆ y¯\n<i i+1 i+na i+na+1\nContext natokens Onetoken\npredictedusing predictedusing\nthedraftmodel theverificationmodel\nOncewehavefinishedthisstep,weaddthepredictedtokens{yˆ ,...,yˆ ,y¯ }\ni+1 i+na i+na+1\ntothecontext,andrepeattheaboveprocess.\nInpractice,weusuallywishtouseasmallerdraftmodelsothatpredicting{yˆ ,...,yˆ }\ni+1 i+na\nwouldbecomputationallycheaper. Butaverysmalldraftmodelislessaccurateandcanresult",
    "in smaller n . We therefore need to carefully select the draft model to make the trade-off\na\nbetweenthecomputationalefficiencyandaccuracy.\n6. Stopping Criteria\nStoppingcriteriaareacriticalcomponentofLLMinference. Theytypicallyinvolverulesor\nconditions that specify when the model should stop generating text during decoding. Most\nLLMsaretrainedtogenerateanend-of-sequencetoken(e.g.,⟨EOS⟩or⟨/s⟩)tosignaltheend\nofthegeneratedtext. Sooneofthesimpleststrategiesisthatthegenerationprocessstopswhen\nthistokenisproduced. Forbeamsearch,whichexploresmultiplehypothesessimultaneously,\ntheprocesscancontinueuntilagivennumberofcompletesequenceshavebeengenerated.\nInpracticalapplications,itwillgenerallybeundesirabletogenerateverylongsequences,\nandsoweneedtoreducethedecodingcostandunnecessaryverbosity. Onecommonly-used\nstoppingcriterionisthemaximumlengthoftheoutput. Themodelstopsgeneratingtextonce\nithasproducedapredeterminednumberoftokens. Alternatively,wecanstopthedecoding",
    "basedontherealcost,suchasthecomputationalresourcesortimeconstraints. Forexample,\ninreal-timeapplicationslikechatbots,decodingmayneedtostopafteracertaintimelimitto\nensureresponsiveness.\nAnother approach is to design stopping criteria based on the behavior of LLMs. For\nexample,decodingcanbestoppediftheprobabilityofpredictingthenexttokenfallsbelowa\ncertainthreshold. Inadditiontoprobability-basedstopping,arepetitiondetectionmodulecan",
    "11.1 Prefilling and Decoding 607\nbeimplementedtotriggerthemodeltostopifitbeginsrepeatingtokensorphrasesbeyonda\npredefinedlimit. Thishelpspreventredundantorincoherentoutputs.\n11.1.4 Evaluation Metrics for LLM Inference\nEvaluatingtheperformanceofLLMsduringinferenceinvolvesavarietyofmetricstoassess\nhow well these models meet desired standards, such as accuracy, robustness, usability, and\nefficiency. AswithmostNLPsystems,wecanevaluateLLMsusingaccuracy-basedmetrics,\nsuchasperplexityandF1score. Wecanalsoexaminetheirrobustnessbytestinghowwellthey\nhandleambiguousorchallenginginputs,includingadversarial,perturbed,orout-of-distribution\ndata. Additionally, usability can be assessed by measuring how well the generated outputs\nalignwithuserexpectationsintermsoffluency,coherence,relevance,anddiversity. Human\nevaluatorscanratethenaturalnessofthetextorassesswhethertheresponsesarecontextually\nappropriateandlogicallyconsistent. Ethicalandfairnessmetricscanalsobeincludedtoensure",
    "LLMsavoidperpetuatingbiasesorgeneratingharmfulcontent.\nAlloftheevaluationmetricsmentionedaboveessentiallyfocusonassessingthequalityof\ntheoutputs. GiventhehighcostofdeployingandapplyingLLMs,efficiencymetricsarealso\nveryimportantforpractitioners. Belowaresomecommonlyusedefficiencymetrics[Nvidia,\n2025]:\n• RequestLatency. Thismetricmeasuresthetotaltimetakenfromwhenarequestissent\ntotheLLMuntilthecompleteresponseisreceived. Thisincludesthetimetakenfordata\ntransmission,processingbythemodel,andthereturnoftheoutputtotheuser.\n• Throughput. Itreferstothenumberoftokensorrequeststhemodelcanprocessper\nsecond.\n• TimetoFirstToken(TTFT).Thismetricmeasuresthetimeittakesfromthebeginning\nof a request being sent to the generation of the first token of the response. If data\ntransmission does not consume too much time, then TTFT is mainly the time for\nprefillingandpredictingthefirsttoken.\n• Inter-token Latency (ITL). This metric refers to the time taken to generate each",
    "subsequenttokenafterthefirstone. Itreflectstheefficiencyofthedecodingprocess.\n• TokensPerSecond(TPS).Thismetricquantifiesthenumberoftokensthatthemodel\ncangeneratepersecond.\n• ResourceUtilization. Thisinvolvesmeasuringthecomputationalresourceusage(e.g.,\nCPUandGPUutilization)andmemoryconsumptionofthemodelduringinference.\nInadditiontothesemetrics,energyefficiencyandcostefficiencyarepracticalconsidera-\ntionsfordeployingLLMsatscale. Energyefficiencymeasurestheamountofelectricalpower\nconsumedbythemodelduringinference. Costefficiency,ontheotherhand,evaluatesthetotal\nexpensesrelatedtodeployingandmaintainingthemodel.\nIngeneral,choosingtherightevaluationmetricsdependsonthespecifictaskandapplica-\ntion. Whilequality-focusedmetricsareessentialforassessingLLMs,efficiencymetricsare\nequally crucial for their effective deployment in real-world applications. A comprehensive",
    "608 Chapter 11. Inference\nevaluation framework should include both sets of metrics to accurately estimate an LLM’s\nperformanceandpracticality.\n11.2 Efficient Inference Techniques\nIn practical applications, we often wish a system to be as efficient as possible. For LLM\ninference,thistypicallyinvolvestwotypesofimprovements: reducingmemoryrequirements\nand accelerating the system. For example, we can modify the Transformer architecture to\navoidmemoryexplosionwhenprocessingverylonginputsequences. Anotherexampleisthat\nwecan compressinput sequencestoreduce computationaloverheadwhile preservingtheir\nsemanticinformation. Inaddition,techniqueslikequantizationandpruningcanbeemployed\ntofurtheroptimizememoryusageandinferencespeed.\nEfficientinferenceisawide-rangingtopicthatoverlapswithseveralsub-fieldsofLLMs,\nsuchasarchitecturedesignandmodelcompression. Mostofthesetopicshavebeencoveredin\npreviouschapters. Forexample,inChapter6,wediscussedefficientTransformerarchitectures;",
    "in Chapter 8, we discussed long-context LLMs; and in Chapter 9, we discussed prompt\ncompressionmethodsforreducingpromptlength. Inthissection,wefocusontechniquesthat\narecommonlyusedinLLMdeploymentandserving.\n11.2.1 More Caching\nIn real-world applications, it is common practice to store frequent requests and their corre-\nsponding responses in a cache. When a new request hits the cache, the system can retrieve\ntheresponsedirectlyfromthecacheinsteadofrecomputingtheresult. Onestraightforward\nimplementation is a key-value datastore (e.g., a hash table) that maps input sequences to\ntheirLLM-generatedoutputsequences. Inthesimplestcase,wecancollectfrequentqueries,\ngeneratetheirresponsesusingtheLLM,andstorethesequery-responsepairsinthedatastore.\nThiscreatesabasicsequence-levelcachingmechanismthatallowsthesystemtobypassLLM\ncomputationwhentheinputsequenceexactlymatchesacachedquery.\nA straightforward extension of the caching mechanism is to cache prefixes and their",
    "correspondinghiddenstates. GivenaninputsequencexinadatasetD,wecanprocessitasin\nthestandardprefillingphase. Thus,weobtainasequenceofprefixesandtheircorresponding\nKVcachestates:\nx (x ) ⇒ cache\n0 <1 <1\nx x (x ) ⇒ cache\n0 1 <2 <2\n...\nx x ...x (x ) ⇒ cache\n0 1 m−1 <m <m\nwhere cache denotes the KV cache for the prefix x (see also Eq. (11.10)). All these\n<i <i\nmappingscanbestoredintheprefixcacheforefficientreuse.\nWhen processing a new sequence that shares a common prefix with a previously seen\nsequenceinD,wecanloadthecorrespondingcachedhiddenstatesinsteadofrecomputing",
    "11.2 Efficient Inference Techniques 609\nthem. Specifically, if a new input x′ has x (i.e., x′ =x for some k ≤m), we can\n<k <k <k\ninitializetheKVcachewithcache andonlycomputethehiddenstatesfortheremaining\n<k\ntokensx′ .\n≥k\nAsusual,wecanmaintainakey-valuedatastorethatmapsfrequentlyencounteredprefixes\nto their precomputed KV caches. The lookup can be performed using a hash of the prefix\ntokens, allowing constant-time access to the cached states. Care must be taken to manage\nmemoryusage,asstoringallpossibleprefixesmaybeinfeasibleforlargedatasets. Practical\nsystemsoftenemployleastrecentlyused(LRU)cachingmethodsorotherstrategiestobalance\nbetweencomputationalsavingsandmemoryconstraints.\n11.2.2 Batching\nBatchinginLLMinferencereferstotheprocessofprocessingmultipleinputsequencessimul-\ntaneouslyasagroup(calledabatch)ratherthanoneatatime. BecausemodernGPUsexcelat\nparallelprocessing,batchingallowsthemtocomputemultiplesequencesinasingleforward",
    "pass,keepingthehardwarefullyoccupied. Therefore,whenservingLLMsatscale,batching\nisimportantforimprovingcomputationalefficiencyandmaximizinghardwareutilization2.\nTo illustrate the idea of batching, Figure 11.8 (a-b) show simple examples with batch\nsizesof1and4,respectively. Whenusingabatchsizeof1(i.e.,withoutbatching),theGPU\nprocessesoneinputsequenceatatime. Thus,theprocessingissequential: thenextsequence\nmust wait for the current computation to finish. By contrast, when using a batch size of 4,\nthe GPU can process four sequences simultaneously in a single forward pass. As the input\nsequencesvaryinlength,weneedtostandardizetheirlengthusingpaddingtechniques. Here\nweuseleftpadding,whichaddsdummytokenstothebeginningsofshortsequences,soallthe\nsequences in the batchwouldhave the same length forprefilling. For decoding, tokens are\ngeneratedsimultaneouslyforallthesesequences,andthegenerationprocesscontinuesuntil\nthelongestsequencereachescompletion.",
    "Theaboveexamplesimplyatrade-offbetweenthroughputandlatency, whichisavery\nimportantconsiderationindesigningandimplementingLLMinferencesystems. Ifwechoose\na smaller batch size, the latency would be lower, as fewer tokens need to be processed in a\nsinglerunofinference. Imaginethatwehaveonlyonesequence. Theresultbecomesavailable\nimmediatelyaftergenerationcompletes,withnoadditionalcomputationaloverhead. However,\nthislow-latencyadvantagecomesatthecostofunderutilizingparallelcomputingresources,\nasthe parallelism ofGPUsremainslargelyidleduring sequential processing. Onthe other\nhand, if we use a larger batch, we can make better use of the parallelism, as GPUs can be\noccupiedbylarge-scalematrixcomputations. Asaresult,wecanprocessmoretokensinthe\nsameperiodoftimeandthethroughputisimproved. However,sincetheresultisobtainedonly\nwhenthelasttokeninthebatchispredicted,thelatencywouldbehigher.\nInpractice, weusuallyprefertouseaslightlylargerbatch, buttrytofillthebatchwith",
    "sequences of similar lengths to reduce the number of padding tokens and improve device\nutilization. Forexample,wecangrouptheincominguserrequestsinashortperiodoftimeinto\n2See https://docs.nvidia.com/deeplearning/performance/\ndl-performance-gpu-background/index.html#understand-perfforasimpleevaluation.",
    "610 Chapter 11. Inference\nprefilling decoding prefilling decoding\npad 1 1 1 1 1 1 1 1 1\npad pad 2 2 2 2 2 2 2\npad pad pad 3 3 3 3 3\n4 4 4 4 4 4 4 4 4 4 4 4\n(a)batchsize=1 (b)batchsize=4\ntransfertheKVcache\nengine1 engine2\nprefilling decoding prefilling decoding\npad 1 1 1 1 1 1 1 1 1 1 1 pad 1 1 1 1 1 1 1 1 1\npad 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2\n3 3 3 3 3 3 3 3 3 3 3 4 4 5 5 5 5 3 3 3 3\n4 4 4 4 4 4 4 4 4 4 4 4 6 6 6 6 6 6 4 4 4\n5 5 5 5 5\n6 6 6 6 6 6\n(c)batchsize=4(similarsequencelengths) (d)disaggregationofprefillinganddecoding\nFigure 11.8: Illustrations of basic batching methods. We use a 2D layout to illustrate the\nbatch,whereeachsquarerepresentsatoken. Redsquaresindicatetokensintheprefillingstage,\nblue squares represent tokens in the decoding stage, green squares denote padding tokens,\nandgraysquarescorrespondtomeaninglesstokens. Subfigures(a)and(b)comparethecases\nwhere the batch size is 1 and 4, respectively. Subfigure (c) shows the strategy of grouping",
    "sequenceswithsimilarlengthsintothesamebatch. Subfigure(d)illustratesthedisaggregation\nofprefillinganddecoding. Inthisapproach,wecanmakebetteruseoftheparallelismofGPUs\nby concatenating multiple short sequences into a single long sequence for joint processing.\nThisallowsustomaximizethenumberoftokensprocessedinabatchwhileminimizingthe\nnumber of padding tokens. However, as a trade-off, we need to copy the KV cache to the\ndecodingengineandreorganizeitaftertheprefillingphase,whichintroducesadditionaldata\ntransferoverhead.\nbuckets,eachofwhichcontainssequenceswithsimilarlengths. Then,wecanfillthebatch\nwithsequencesinthesamebucket,sothatwecanminimizewastedcomputationalresources,\nasillustratedinFigure11.8(c).\nAnotherapproachtoimplementingbatchinginLLMsistodisaggregatetheprefillingand\ndecodingprocesses[Wuetal.,2023a;Pateletal.,2024;Zhongetal.,2024]. Forexample,we\ncanperformprefillingononeGPU,andperformdecodingonanotherGPU.Oneadvantageof",
    "disaggregationisthatwecanrearrangetheinputsequencesinthebatchtobetterfillit,because\nthere is no interference between prefilling and decoding. For example, we can concatenate\nmultiple short sequences into a longer one, thus ensuring that the lengths of sequences in\nthebatchareasconsistentaspossible,asillustratedinFigure11.8(d). Inthisway,wecan",
    "11.2 Efficient Inference Techniques 611\nmaximizethethroughputoftheprefillingphase. However,asatrade-off,weneedtotransfer\nthe KV cache to the devices performing decoding, which also incurs extra communication\noverhead. Typically,thismethodrequiresahigh-bandwidth,low-latencynetworktoachieve\noptimalperformance.\nInthissection,wewilldiscussseveralimprovementstotheabovebasicbatchingstrategies.\nMostofthemarebasedonanaggregatedarchitecture,thatis,decodingandprefillingcanbe\nconsideredasdifferentstagesofamodelexecutedonthesamedevice.\n1. Scheduling\nApracticalLLMinferencesystemtypicallyconsistsoftwocomponents:\n• Scheduler. Its primary role is to efficiently queue and dispatch tasks (i.e., input se-\nquences)totheinferenceenginebasedonthecurrentsystemloadandtaskpriorities.\nThisofteninvolvesavarietyofbatchingstrategiesthatgroupcertainrequeststogether\ntomaximizeprocessingefficiencyinsomeway.\n• InferenceEngine. ItisresponsiblefortheactualexecutionoftheLLMs,processing",
    "thequeuedrequestsastheycomein. Asdiscussedpreviously,thisengineinvolvesboth\nprefillinganddecodingprocesses.\nThis architecture is illustrated in Figure 11.9. Incorporating scheduling into batch pro-\ncessingprovidesaflexiblewaytooptimizeboththesystem’sthroughputandlatency,thereby\nachievingabetterbalancebetweenthem. Forexample,thebatchingmethodsshowninFigure\n11.8(a)and(b)canbeconsideredoneofthesimplestschedulingstrategies,calledrequest-\nlevelscheduling. Inthisstrategy,onceabatchisfilledandsenttotheengine,theprocessing\noftheentirebatchcannotbeinterrupted. Theschedulerwaitsforthisbatchtobeprocessed\nbeforehandlingthenextbatch[Timoninetal.,2022].\nA more sophisticated scheduling strategy, called iteration-based scheduling, interacts\nwiththeinferenceengineateachtokenpredictionstepratherthanatthesequencelevel. This\napproach allows dynamic batch adjustment during inference, as illustrated in Figure 11.10.\nSuchfine-grainedcontrolletsthesystemprioritizecriticaltokensorsequencesinreal-time. For",
    "instance,ifanurgentrequestarrivesatsomedecodingstep,theschedulercanaddthisrequest\nintothebatchsothatitcanbeprocessedasearlyaspossible. Inthefollowingsubsections,we\nwilldiscussbatchingmethodsbasedoniteration-basedscheduling.\n2. Continuous Batching\nContinuousbatchingisaniteration-basedschedulingmethodusedintheOrcasystem[Yu\net al., 2022]. In this method, an iteration refers to either the entire prefilling procedure or\na single decoding step. For example, given an input sequence x=x ...x and an output\n0 m\nsequencey=y ...y ,therearen+1iterationsintotal: oneforprefilling,andnforgenerating\n1 n\nthe output tokens (one per token). During scheduling, the batch can be adjusted between\niterations. For example, we can either add a new input sequence to the batch, or remove a\ncomplete sequence from the batch at some iteration, even if the batch processing is not yet",
    "612 Chapter 11. Inference\nRequestPool\nx 1,x 2,x 3,...\nbatch\nInference\nScheduler\nEngine\nbatch(afterprocessing)\nPredictions\ny 2,y 1,y 3,...\nFigure 11.9: Illustration of the LLM inference architecture involving a scheduler and an\ninferenceengine. Eachtime,theschedulerselectsanumberofuserrequeststoformabatch\nandsendsittotheinferenceengine. Theschedulercaninteractwiththeinferenceengineand\nadjustthebatchatcertainpointsduringinference,suchasatthebeginningofbatchprocessing\nandatthestartofeachtokenprediction.\nfinished.\nThegeneralprocessofcontinuousbatchingincludesthefollowingsteps:\n• Initially,abatchiscreatedwithoneormoreinputsequences,basedonboththeinference\nengine’sprocessingcapacityandthecurrentuserrequests. Thebatchisthenfedintothe\ninferenceengine.\n• Theinferenceengineprocessesthebatchiterationbyiteration. Aftereachiteration,the\nschedulermayadjustthebatchinoneofthefollowingways:\n– Ifasequenceinthebatchcompletesgeneration(i.e.,generatestheend-of-sequence",
    "symbol),thatsequenceisremovedfromthebatch.\n– Ifanewuserrequestarrivesandtheinferenceenginehasadditionalprocessing\ncapacity,itisaddedtothebatch.\n– If no sequences are added to or removed from the batch, the batch remains un-\nchanged.\n• Theprocessingterminatesonlywhenallsequenceshavebeencompletedandnonew\nuserrequestsarrive.\nSeeFigure11.11foranexampleofcontinuousbatching. Inthisexample,westartwithtwo\nuserrequests,x andx . Thesetwosequencesarepackedintoabatchandsenttotheinference\n1 2\nengine for processing. After the engine completes two iterations, a new user request, x ,\n3\narrives. Atthispoint,thescheduleradjuststhebatchbyaddingx toit. Theinferenceengine\n3",
    "11.2 Efficient Inference Techniques 613\nRequests Requests\nx1,x2,x3arrived x1,x2,x3arrived\nBegin Begin\nIteration1 1 1 Iteration1 1 1\n(prefilling) 2 (prefilling) 2\n3 3 3 3 3 3\n1 1 1 1 1 1\nIteration2 2 2 Iteration2 2 2\n3 3 3 3 3 3 3 3\nRequest Request\nx4arrived x4arrived\n1 1 1 1 1 1 onedecodingstep\nIteration3 2 2 2 2 2 2\nforx1,x2,x3\nIteration3\n3 3 3 3 3 3 3 3 3 3\n4 4 4\nprefillingforx4\n1 1 1 1 1 1\nIteration4 2 2 2 2 2 2\nIteration4\n3 3 3 3 3 3 3 3 3 3 3 3\n4 4 4 4\nEnd\nx4isaddedto Moreiterations\nthenextbatch\nEnd\n(a)Request-levelScheduling (b)Iteration-levelScheduling\nFigure 11.10: Illustrations of request-level scheduling and iteration-based scheduling. In\nrequest-levelscheduling,onceabatchiscreatedandsenttotheinferenceengine,wecannot\nadjustthebatch. Inotherwords,schedulingonlyoccursaftertheprocessingofabatchfinishes.\nIniteration-levelscheduling,wecanperformschedulingduringbatchprocessing. Forexample,\nifanewrequestarrivesatsomepointduringinference,wecanaddittothebatchandcontinue\nprocessing.",
    "thencontinuesprocessingtheupdatedbatch. Notethattheinferenceenginenowprocesses\ndifferent sequences in different ways: x and x proceed with the decoding process (i.e.,\n1 2\npredictingthenexttokens),whilex undergoestheprefillingprocess. Aftersometime,the\n3\ngenerationforx completes. Asithappens,twomoreuserrequests,x andx ,arrive. The\n2 4 5\nschedulerremovesthecompletedsequencex fromthebatchand,consideringthecurrentload\n2\noftheinferenceengine,addsx tothebatch. However,x mustwaituntilanothersequencein\n4 5\nthebatchfinishesbeforeitcanbeadded.\nThe idea behind continuous batching is to keep the inference engine fully utilized by",
    "614 Chapter 11. Inference\ninput input\nx1,x2\narrived\nbatch batch\n1 1 1 1 1 1 1\nScheduler 2 2 Scheduler 2 2 2\noutput\nprefillingforx1andx2.\noutput\no\nx\nn\n1\ne\na\nd\nn\ne\nd\nco\nx\nd\n2\ni\n.\nngstepfor\n(a)Iteration1 (b)Iteration2\ninput input\nx3arrived\nbatch batch\n1 1 1 1 1 1 1 1 1 1 1\nScheduler 2 2 2 2 Scheduler 2 2 2 2 2\n3 3\nonedecodingstepfor onedecodingstepfor\noutput x1andx2. output x1andx2,andprefilling\nforx3.\n(c)Iteration3 (d)Iteration4\ninput input\nx4,x5\narrived continuethesecondsequence\ncomplete inthebatchwithx4\nbatch batch\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\nScheduler 2 2 2 2 2 2 Scheduler 4 4 4\n3 3 3 3 3 3 3\nonedecodingstepfor onedecodingstepfor\noutput x1,x2andx3. output x1andx3,andprefilling\ny2 x2’spredictioncompletes. forx4.\n(e)Iteration5 (f)Iteration6\nFigure 11.11: Illustration of batch adjustment in continuous batching. Instead of fixing a\nbatch of input sequences and processing them to completion (as in request-level batching),",
    "continuousbatchingdynamicallyupdatesthebatchduringinference. Thesystemcontinuously\naccepts and adds new requests (e.g., x and x ) into the current batch as long as there is\n3 4\navailablecomputecapacity.\nprocessingasmanysequencesaspossible,therebymaximizingcomputationalresourceusage.\nAkeydifferencebetweencontinuousbatchingandstandardbatching(seeFigure11.8)lies\nin the fact that, in continuous batching, prefilling and decoding can occur simultaneously\nacross different sequences, whereas in standard batching, these two phases are performed\nsequentially for the entire batch. As discussed in Section 11.1.2, prefilling is considered a",
    "11.2 Efficient Inference Techniques 615\ncompute-boundprocess,whiledecodingisconsideredamemory-boundprocess. Theintuition\nbehindoverlappingprefillinganddecodingistoreduceidletimesforbothcomputationand\ndatatransfer. Considertwomini-batches: oneforprefillingandonefordecoding. Whilethe\nprefillingmini-batchkeepstheGPUsoccupied,thedecodingmini-batchcanperformmemory\ntransfersconcurrently.\nAnotherdifferencebetweencontinuousbatchingandstandardbatchingisthatcontinuous\nbatching is prefilling-prioritized, while standard batching is decoding-prioritized [Agrawal\net al., 2024]. In continuous batching, once the inference engine has spare computational\nresources, the scheduler will add new requests to the batch. In other words, these newly\naddedrequestswillbeprocessedforprefillingasearlyaspossible. Thisapproachimproves\nsystemthroughput,butatthecostofincreasedlatency,asthenewlyaddedrequestsextendthe\nprocessingtimeofearlierones. Incontrast,instandardbatching,oncethebatchiscreated,we",
    "mustwaitforthelastsequenceinthebatchtocompletebeforeprocessingnewrequests. This\nensuresrelativelylowlatency,butresultsinlowerdeviceutilizationandsystemthroughput.\nItisimportanttonotethatthecostofcontinuousbatchingisthatweneedtocontinuously\nreorganize the batches, which involves rearranging the data in memory. Each time a new\nrequestisadded,theschedulerneedstoreassessandoptimizethecurrentbatchstructure. This\ndynamicadjustmentcanincuradditionalmemoryandcomputationaloverhead,especiallywhen\nthebatchesarefrequentlyadjusted. Therefore,whilethismethodcanimprovethroughput,it\nmayalsoleadtoincreasedmemoryfragmentationand, insomecases, introduceadditional\nlatency.\n3. PagedAttention\nPagedAttention(orpagedKVcaching)isatechniqueusedinthevLLMsystem[Kwonetal.,\n2023]. Inspiredbyoperatingsystempaging,itoptimizesmemoryusageduringLLMinference\n—particularlyfortheKVcache—byaddressingfragmentedmemoryallocationindynamic\nbatching scenarios with variable-length sequences. The idea behind PagedAttention is to",
    "break down large memory requirements for KV caching into more manageable \"pages\" or\nchunksofmemory. Inthisway,wedonotneedtostoretheKVcacheofthefullsequencein\nacontinuousmemory. Instead,theKVcacheisdividedintofixed-sizeblocks(analogousto\nmemorypagesinanoperatingsystem),whichcanbenon-contiguouslyallocatedinphysical\nmemory. OneadvantageofPagedAttentionisthatitenablesflexiblememorymanagement,\nsupportingdynamicsequencegrowthwithoutrequiringexpensivereallocationorcopyingof\nlargecontiguousmemoryregions. NotethatPagedAttentionisnotspecificallydesignedfor\nbatching. Butitindeedhelpsimprovememoryefficiencyinbatchedinferencescenarios,where\nmemorymanagementismoredemandingandcomplicated.\nConsiderasimpleexampleofmemoryallocationinFigure11.12inwhichself-attention\nisperformedforabatchconsistingoftwosequences. Foreachsequence,weneedtoattend\nthe current token to the key-value pairs in the KV cache of this sequence, as required by",
    "self-attention. In the standard implementation of self-attention, the KV cache is stored in\na contiguous block of memory, allowing us to efficiently access this continuous memory.\nHowever, in a paged KV caching system, the KV cache is divided into smaller, fixed-size",
    "616 Chapter 11. Inference\nattend\nKVCache\nSequence1 ⟨SOS⟩ I think this moive is better than\nSequence2 ⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩ I really like reading\n(a)Twosequencesinabatch\nPhysicalMemoryBlocks used\nthan\nreading\n⟨SOS⟩ I think this\nmovie is better\n⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩\nI really like\nyromemdetnemgarf\n(b)MemoryallocationforKVcachinginstandardself-attention\nPhysicalMemoryBlocks used\nthan\nreading ⟨SOS⟩ I think this\n⟨pad⟩ ⟨pad⟩ ⟨pad⟩ ⟨SOS⟩\nmovie is better\nI really like\n(c)MemoryallocationforKVcachinginPagedAttention\nFigure11.12: IllustrationofmemoryallocationinPagedAttention. Therearetwosequencesin\nthebatch,asillustratedinsub-figure(a). Sincethememoryisfragmented,theKVcacheis\nstoredinalargeunusedblockofmemoryinstandardself-attention(seesub-figure(b)),butthe\nfragmentedmemoryisnotused. Bycontrast,inPagedAttention(seesub-figure(c)),theKV\ncacheisdividedintosmallerblocksandthusfitsintofragmentedmemory.",
    "11.2 Efficient Inference Techniques 617\nmemory blocks which are not necessarily contiguous. These smaller KV cache blocks can\nbe more effectively allocated to fragmented memory regions, thereby improving memory\nutilization. AnotherbenefitofdistributingchunksoftheKVcacheacrossdifferentmemory\nblocks is that it enables parallelization of the caching process. For example, if the input\nsequenceislongandthememorybandwidthissufficient,itwouldbebeneficialtowriteand\nreadthekeyandvaluevectorsofdifferentsegmentsofthesequenceinparallelacrossmultiple\nmemoryblocks.\nIngeneral,storingcontiguousdatainnon-contiguousregionscancauseissues,forexam-\nple, accessing fragmented data requires additional seek time, which reduces I/O efficiency.\nHowever,whenhandlinglarge-scaledata(e.g.,performingmultiplicationonextremelylarge\nmatrices), we typically do not process all the data at once but instead divide it into smaller\nblocksforblock-levelcomputation. Fromthisperspective, itisalsoreasonabletopartition",
    "the attention computation. If the paging strategy is well designed, the additional overhead\nin memory access can be minimal, while the improvement in memory utilization can be\nsignificant.\n4. Chunked Prefilling\nWehaveseenthat,initeration-levelscheduling,prefillinganddecodingfordifferentsequences\ncan occur simultaneously. This can be seen as a prefilling-prioritized strategy which can\nmaximize the throughput. However, one such iteration can take a long time if the input\nsequence is very long and the prefilling process dominates the computation. In this case,\ndecodingforothersequenceshastowaituntiltheprefillingcompletes,leadingtoincreased\nlatency for generating output tokens. Therefore, while prefilling-prioritized strategies are\neffective for maximizing hardware utilization, they may introduce significant variability in\ntoken generation latency, particularly when the system is handling a mix of long and short\ninputsequences.",
    "Asimplewaytoreducedecodinglatencyistomakecomputationsfordifferentsequences\ninthebatchcomparable. Onesuchmethodistodividesequencesintochunksandperform\nprefillingchunkbychunk. Thisapproach,oftenreferredtoaschunkedprefilling,processes\nsmallerportionsofeachsequenceatatime,allowingthesystemtobetterbalancethecomputa-\ntionalloadacrosssequences[Agrawaletal.,2023]. Bychoosinganappropriatechunksize,\nwecanensurethatwhenprefillinganddecodingoverlapfortwosequences,theirprocessing\nwithinthesameiterationtendstotakeasimilaramountoftime. Asaresult,decodingidletime\nisreducedandoverallthroughputisimproved.\nFigure11.13showsanillustrationofchunkedprefillinginafewiterations. Inthisexample,\nthebatchcontainstwosequences. Thewholeprefillingprocessofthefirstsequenceisdivided\ninto three prefilling steps, giving rise to the chunks denoted P , P and P . Each chunk\n11 12 13\ncorrespondstooneiterationandcanthusoverlapwithonedecodingstep. Inthisway,during",
    "theprefillingofthefirstsequence, wecan perform threedecodingsteps, ratherthanonlya\nsingledecodingstep,asisthecaseinstandarditeration-levelscheduling. Asaresult,theidle\ntimeofthedecodingprocessisreduced,andtheoutputtokenscanbegeneratedearlier.\nChunkedPrefillingimprovesdecodingefficiencybyoverlappingprefillinganddecoding,",
    "618 Chapter 11. Inference\nPrefillinginOneGo\nIter.1 Iter.2 Iter.3 Iter.4 Iter.5\nSequence1 P D D D ···\n11 11 12 13\nSequence2 P D D D D ···\n21 21 22 23 24\nIdleTime\nThepredictionofthe\nsecondoutputtoken\nisdelayed.\n(a)SimpleIteration-levelScheduling\nChunk1 Chunk2 Chunk3\nIter.1 Iter.2 Iter.3 Iter.4 Iter.5 Iter.6\nSequence1 P P P D D ···\n11 12 13 11 12\nSequence2 P D D D D D ···\n21 21 22 23 24 25\nThesecondoutputtoken\ncanbepredictedduring\nprefillingforsequence1.\n(b)ChunkedPrefilling\nFigure11.13: Comparisonofsimpleiteration-basedschedulingandchunkedprefilling. P\nxy\ndenotes the y-th prefilling step for sequence x, and D denotes the y-th decoding step for\nxy\nsequencex. Insimpleiteration-basedscheduling(orprefilling-prioritizedscheduling),since\nprefillingistreatedasasingleiteration,D hastowaitforthecompletionoftheprefilling\n22\nofsequence1. Inchunkedprefilling,theprefillingprocesscanbedividedintomultiplesteps.\nThus,D canexecuteduringprefillingforsequence1(i.e.,duringP ).\n22 12",
    "butatthecostofadditionalmemoryoverheadandschedulingcomplexity. Instandardprefilling,\nweprocessthewholeinputsequenceonce,buildingtheKVcacheinonego. Bycontrast,in\nchunkedprefilling,eachchunkneedsaseparateforwardpasstocomputeitsattentionoutputs\nandupdatetheKVcache. Asaresult,weneedtomaintaintheKVcacheofearlychunkswhile\nprocessinglaterchunks. Thisalsocompromisestheparallelismofcompletingtheprefillingfor\ntheentiresequenceinasinglepass. Inpractice,itisusuallypossibletobalancethroughput\nandlatencybychoosinganappropriatechunksize.\nItisworthnotingthatthemethodsdiscussedinthissubsectioncanbroadlybecategorized\naspriority-basedschedulingmethods. Inthesemethods,wecangiveprioritytocertainrequests,\nortocertainprefillingordecodingsteps,sothatsystemresourcesareallocatedinawaythat\nbetter aligns with specific performance goals. As presented above, for example, we may\nprioritizedecodingoverprefillingtominimizetokengenerationlatency,orprioritizeprefilling",
    "over decoding to maximize overall throughput in batch-processing scenarios. Practitioners\ncandesigncustomprioritypoliciesforspecificneedsandoperationalconstraintsinreal-world\napplications,suchasrequestdeadlinesandimportancelevelsdefinedbyusers.",
    "11.2 Efficient Inference Techniques 619\n11.2.3 Parallelization\nParallelizationisawidelyusedapproachtoscaleupLLMinference,especiallyforlarge-scale\ndeployments. In Chapter 7, we have discussed several common parallelization strategies\ntoparallelizeLLMpre-training,suchasmodelparallelism,tensorparallelism,andpipeline\nparallelism. Wehavealsodiscussedefficientarchitecturesthatareeasytodeployindistributed\ncomputingsystems. Forexample,inMoEmodels,weassignsdifferentexpertstodifferent\ndevices3. Onlytheactiveexpertsforagiveninputareexecuted,whichsignificantlyimproves\ncomputational efficiency while maintaining model quality. Many of these methods can be\ndirectlyappliedtoLLMinferencewithminimalmodifications.\nHowever, applying these parallelization techniques to inference poses new challenges\ncompared to pre-training. These issues become especially pronounced in real-time or low-\nlatencyinferencescenarios,whereloadimbalanceacrossdevicesandcommunicationoverhead",
    "cansignificantlyimpactperformance. Forexample, unlikepre-training, wherebatchescan\nbepreparedinadvance,inferencemusthandlevariable-lengthsequencesinrealtime. This\nmakes it harder to maintain optimal device utilization and complicates scheduling across\nheterogeneous computational resources. A related issue is load balancing. When a large\nnumber of requests arrive in a short period of time, the system must efficiently distribute\nworkloadsacrossavailabledevices. Forexample,real-worldrequeststypicallyexhibithighly\nvariable computational demands due to differences in task types and prompt lengths. Such\nvariabilityrenderssimplestaticloadbalancingapproachesineffective,andsoweneedtouse\nfiner-grainedstrategiesthatcanadapttoruntimeconditions. Theproblembecomesevenmore\ncomplicatedwhenwedeploythesystemonheterogeneoushardwareandtherearestrictlatency\nconstraints.\nInthedevelopmentofLLMs,parallelizationiscloselyrelatedtoLLMserving. Generally,",
    "building a high-quality LLM serving system is not a simple task — it typically requires\nthecombinationofmultipletechniques,suchasarchitecturaldesign,workloaddistribution,\nand LLM-specific hardware/software optimizations. As such, LLM serving constitutes an\nexceptionallybroadsubjectthatoftendemandssubstantialengineeringexpertise. Here,we\nwillnotgointothedetailsofLLMserving. Forrelatedconceptsandtechniques,readersmay\nrefertorelevantopen-sourcesystems(suchasvLLM4,TensorRT-LLM5 andTGI6)andpapers\n[Popeetal.,2023;Lietal.,2024a].\n11.2.4 Remarks\nWehaveconsideredmanymethodsforimprovingtheefficiencyofLLMsinthisandprevious\nchapters. Althoughtheseapproachesaddressdifferentissues,mostofthemessentiallyexplore\ntrade-offsbetweenvariousperformancefactors. Oneimportanttrade-offisbetweeninference\nspeed and accuracy. For example, techniques like quantization, pruning, and knowledge\n3InLLMs,theexpertsaretypicallymodularFFNs. SoeachexpertisapartoftheFFNcomponentinthe\nTransformerarchitecture.",
    "4https://github.com/vllm-project/vllm\n5https://github.com/NVIDIA/TensorRT-LLM\n6https://github.com/huggingface/text-generation-inference",
    "620 Chapter 11. Inference\ndistillation can significantly reduce computational overhead and latency but may introduce\nminordegradationsinmodelperformance. Conversely,preservingfullprecisionorusinglarger\nmodelsenhancesaccuracybutatthecostofslowerinferenceandhigherresourcedemands.\nAnotherimportantconsiderationinLLMinferenceisthememory-computetrade-off. As\nin computer system design, we need to consider the balance between memory usage and\ncomputationrequiredtogeneratetheoutput. Inparticular,storingintermediateresultssuchas\nKVcachesduringinferencecansignificantlyreduceredundantcomputation,butatthecostof\nincreasedmemoryusage. InKVcaching,storingpastattentionstatesavoidsrecomputation\nof self-attention over previous tokens, thereby reducing compute time per token. However,\nas the number of tokens grows, so does the memory footprint of the KV cache, especially\nwhenprocessingverylongsequencesormultiplesequencesinparallel. Inresponse,various",
    "techniques have been developed to reduce memory consumption by partially recomputing\nintermediatestates. Forinstance,chunkedorwindowedattentionlimitstheattentionspantoa\nrecentsubsetoftokens,reducingKVcachesizeatthecostofreducedcontextoradditional\ncomputeifpastinformationmustbereprocessed.\nNote that considering the memory-compute trade-off is a very general principle. It can\nbeextendedbeyondattentionmechanismsandTransformerstoothercomponentsinsystem\ndesign. Anexample isthe choiceofdata precision. Usinglower-precisionformats suchas\nFP16orINT8canreducebothmemoryusageandmemorybandwidthrequirements,effectively\nalleviatingpressureonthememorysubsystem. However,lowerprecisionmayleadtonumerical\ninstabilityorslightaccuracydegradation,requiringcarefulcalibrationorretraining. Thus,this\ntrade-offcanalsobeseenasamemory-compute-accuracytriangle,whereimprovementsin\nonedimensionmaycomeattheexpenseofanother.\nBeyond speed, accuracy, and memory, several other dimensions also influence LLM",
    "inference efficiency. Some of these dimensions have been discussed in this chapter, while\nothershavenot. Hereweoutlinethemasfollows.\n• Throughputvs. Latency: Inlarge-scalemulti-userLLMservingscenarios,weoften\naim to maximize system throughput. For example, as discussed in this section, we\ncan batch multiple requests together to increase the number of tokens processed at\nthesametime. However,batchingincreaseswaitingtimeandmayleadtohigherper-\nrequestlatency,especiallyforshortorinteractiverequests. Bycontrast,optimizingfor\nlow latency often requires serving requests individually or in smaller batches, which\nunderutilizes hardware resources and reduces throughput. Achieving a good balance\ndependsonthequality-of-servicerequirementsanduserinteractionpatterns.\n• Generalization vs. Specialization: General-purpose LLMs are trained to perform a\nwide range of tasks with a single set of parameters. While flexible, they may be less",
    "efficientoraccurateforspecifictasks. Specializedmodelscanyieldbetterperformance\nand lower inference costs for targeted applications. However, maintaining multiple\nspecializedmodelsincreasessystemcomplexityandstoragerequirements. Thetrade-off\nbetweenmaintainingasinglegeneralmodelversusmultiplespecializedmodelsisan\nimportantsystem-leveldesignchoice.",
    "11.3 Inference-time Scaling 621\necnamrofreP\nInference-time Scaling\nFine-tuning Scaling\nPre-training Scaling\nCompute\nFigure11.14: Scalingforpre-training,fine-tuningandinferencestages[Briski,2025].\n• EnergyEfficiencyvs. Performance: High-performanceinferenceoftenrequiresrun-\nninglargemodelsathighthroughputonpowerfulaccelerators,whichconsumescon-\nsiderableenergy. Thismaybeproblematicforedgedeploymentsorenergy-sensitive\nenvironments. Techniqueslikemodelcompressioncanimproveenergyefficiency,but\nusuallywithsomedegradationinoutputqualityorincreaseinlatency. Energyconstraints\nthusintroduceanotherimportantdimensioninoptimizingLLMinference.\n11.3 Inference-time Scaling\nScalinglawscanbeconsideredoneofthefundamentalprinciplesguidingthedevelopmentof\nLLMs. Inpreviouschapters,wediscussedseveraltimesthatscalinguptrainingdata,model\nsize,andcomputecaneffectivelyimprovetheperformanceofpretraining. Infact,scalinglaws\nalsoapplytodownstreamstagessuchasfine-tuningandinference(seeFigure11.14). Herewe",
    "considerinference-timescaling,whichhasbeenwidelyemployedbyrecentLLMstosolve\ncomplexproblems,suchascomplexmathproblems[Snelletal.,2025]. Unlikepre-trainingand\nfine-tuningscaling,whichfocusesonimprovingLLMsviaparameterupdates,inference-time\nscalingimprovesthesemodelsduringinferencewithoutfurthertraining. Thisincludesalarge\nvarietyofmethodswhichscaleLLMsindifferentdimensions,suchasensemblingmultiple\nmodeloutputs,increasingcontextlength,adoptingmoreaggressivedecodingalgorithms,and\nusingexternaltoolstoextendmodelcapabilities.\nWhileinference-timescalingiswide-ranging,inthissectionweconsiderthosemethods\nthatincorporatemorecomputeintoinference(calledinference-timecomputescaling). Hereis",
    "622 Chapter 11. Inference\nalistofinference-time(test-time)computescalingmethods,organizedbycategory:\n• Context Scaling. It involves scaling the input or context to improve generation (or\npotentiallyscaletheoutput).\n• SearchScaling. Itinvolvesincreasingcomputationaleffortduringdecoding.\n• OutputEnsembling. Itinvolvescombiningmultiplemodeloutputs.\n• GeneratingandVerifyingThinkingPaths. ItinvolvesguidingLLMstogenerateand\nverifythinkingpathsforsolvingcomplexreasoningproblems.\nWewilldescribethesemethodsinthefollowingsubsections.\n11.3.1 Context Scaling\nContextscalingimprovesLLMperformancebyextendingtheinputtothemodel. Astraight-\nforwardapproachistoincorporatemorehelpfulcontextduringinference,allowingthemodel\ntoconditionitspredictionsonmorepriorinformation. Oneexampleisfew-shotprompting.\nIt augments the context with multiple input-output examples, and so the model can learn\ntaskbehaviorimplicitlyfromtheseexampleswithoutparameterupdates. Ontopoffew-shot",
    "prompting,wecanusechain-of-thoughtpromptingtoencouragethemodeltoproduceinter-\nmediate reasoning steps before final answers. Note that chain-of-thought prompting is one\nof the most important methods in addressing reasoning problems. By explicitly providing\nintermediate steps in problem-solving, we can prompt the model to break down complex\ntasksintosimplersub-tasks,whichisfoundtobeverybeneficialforgeneratingaccurateand\ninterpretableoutputs.\nBeyond extending the prompt with examples or reasoning steps, another approach to\ncontextscalinginvolvesdynamicallyincorporatingexternalknowledge. Thisisoftenachieved\nthroughRAG.RAGsystemsfirstretrieverelevantdocumentsnippetsfromalargecollectionof\ndocumentsoradatabasebasedonthecurrentinput. Theseretrievedpiecesofinformationare\nthenaddedtothecontextprovidedtotheLLM.Thisessentiallyexpandsthecontexttoinclude\ntimely or specialized external knowledge. By doing so, the model grounds its responses in",
    "specificknowledgefoundintheexternalsource. TheLLMthuscangenerateresponsesthat\narenotonlyrelevanttotheinputbutalsofactuallyaccurateandup-to-date.\nHowever,asthecontextgrows,thesemethodsoftensufferfromtheconstraintsoffinite\ncontext window length. While model architectures and techniques (like efficient attention\nmodels)arecontinuallyevolvingtosupportlongercontexts,processingextremelylonginputs\nstillposeschallenges. Increasedcomputationalcostisonefactor. Morecritically,whenthe\ncontext window becomes very large, the model might struggle to attend effectively to the\nmostrelevantinformation(e.g.,the“lostinthemiddle”phenomenon). Therefore,effective\ncontextscalingisnotjustaboutaddingmoreinformation,butalsoaboutstrategicallyselecting,\nstructuring, and presenting the most pertinent information within the model’s processing\ncapabilities.\nHereweomitthedetaileddiscussionofthesemethods,astheyhavealreadybeencovered\ninpreviouschapters. SeeChapters8and9formoredetails,includingprompting,RAG,and",
    "11.3 Inference-time Scaling 623\nlong-sequencemodelingmethods.\n11.3.2 Search Scaling\nInLLMs,decodingisasearchprocessthataimstoefficientlyfindthebestoutputsequence\ngiventheinputsequence. Searchscaling(ordecodingscaling)typicallyinvolvestwoaspects:\nscalingtheoutputlengthandscalingthesearchspace.\nScaling the output length refers to increasing the number of tokens generated during\ninference. This is especially important in tasks that require long-form generation, such as\nstorywriting. Morerecently,generatingoutputswithlongthinkingpathshasshownstrong\nperformance in math problem solving and code generation. For example, encouraging the\nmodeltogeneratelongthinkingpathsbeforeproducingthefinalanswershasbeenfoundtobe\nverybeneficialinperformingcomplexreasoning. Thisideahasbeenwidelyusedindeveloping\nrecentLLMsforreasoning,suchasOpenAI[2024]’so1andDeepseek[2025]’sR1. Wewill\ndiscussmoreaboutoutputlengthscalinginSection11.3.4.",
    "Scalingthesearchspace,ontheotherhand,referstoexpandingthesetofcandidateoutput\nsequencesconsideredduringsearch,sothathigher-qualityoutputscanbefound. Asdiscussed\ninSection11.1.3,asimpleexampleisthatinbeamsearchweincreasethebeamwidthtoallow\nmore candidate sequences to be explored in parallel at each decoding step. This increases\nthechanceofdiscoveringbetteroutputs,especiallyintaskswheretheoptimalsolutionisnot\nimmediatelyapparentfromlocaldecisions.\nIn addition to decoding algorithm adjustments, it is also possible to explore compact\nstructurestoencodealargenumberofoutputs. Forexample,wecanconstructandnavigatea\ntreeorgraphofreasoningsteps[Yaoetal.,2024]. Inthisparadigm,eachnoderepresentsa\npartialsolutionorintermediatestep,andedgesrepresenttransitionsbetweenreasoningstates.\nSuchstructuredsearchenablesthemodeltoconsidermultiplepathssimultaneously. Another\nrelateddirectionisMonteCarlotreesearch-inspireddecoding,wherethemodelstochastically",
    "exploresandscoresdifferentpathsbasedonlearnedheuristicsorexternalrewardmodels.\nSearchscalingisaverygeneralidea,anditisoftenimplicitlyinvolvedinthedesignof\nsearchproceduresthatexploitsearchstructure,heuristics,andmodeluncertainty. Manyofthe\nabovemethodshavebeendiscussedpreviously,thoughtheywerenotoriginallydevelopedwith\nscalingastheirprimarygoal. However,searchscalinginherentlycomeswithcomputational\ncosts. Increasing beam width, for instance, directly translates to higher memory usage and\nlongerinferencetimes. Inpractice,thereisoftenapointofdiminishingreturns,wherefurther\nexpansionofthesearchspaceyieldsmarginalimprovementsinoutputqualityatasignificant\ncomputational expense. Therefore, an effective strategy often involves finding an optimal\nbalancebetweenscalingandcomputationalfeasibility.\n11.3.3 Output Ensembling\nIfwehavemultiplemodeloutputs,itisoftenbeneficialtocombinethemtomitigatetheimpact\nofindividualmodelerrorsandsynthesizeasuperiorfinaloutput. Eachmodelmightcapture",
    "differentaspectsoftheunderlyingdatadistributionorpossessuniquestrengthsandweaknesses.\nByensembling,wecanaverageoutthenoiseorrandomerrorspresentinindividualpredictions,",
    "624 Chapter 11. Inference\nleading to a more stable and reliable outcome. In LLM ensembling, one of the simplest\napproachesistoaveragetheprobabilitydistributionsoverthenexttokenfromeachmodel,and\nselectthebesttokenusingthisaverageddistribution. Or,ifweregardtheproblemasadiscrete\ndecision-makingtask,majorityvotingcanbeemployed. Moresophisticatedmethodsmight\ninvolvere-rankingcandidateoutputsgeneratedbydifferentmodelsbasedonaseparatescoring\nfunctionorevenusingameta-learnertointelligentlycombinethepredictions.\nThe“scaling”fromoutputensemblingcomesatthecostofrunningmultiplemodelsor\nsampling multiple outputs. This not only increases the latency of inference but also leads\nto the additional complexity of managing multiple models. But the quality of outputs does\nnotcontinuetoimproveindefinitelyasmoremodelsareadded. Insomecases,thebenefits\nof output ensembling may diminish as the number of component models in the ensemble",
    "exceedsacertainthreshold. Instead,thebenefitsofensemblingaregenerallygreaterwhenthe\nindividualmodelsarediverse(i.e.,theymakedifferenterrors),eveniftherearearelatively\nsmallnumberofcomponentmodels. Therefore,itiscommonpracticetouseasetofdiverse\nLLMswhichdifferintheirtrainingdata,modelarchitectures,orfine-tuningobjectives.\nInLLMs,“scaling”oftenimpliesmakingthings“bigger”forqualitywithmoreresources.\nHowever, in addition to scaling up the quality, scaling can mean more. It can also signify\nscaling up the robustness (making the system less prone to errors and more reliable) and\nexploration (covering a wider range of potential solutions). In output ensembling, these\ndimensionsarenaturallyintegrated. Forinstance,theveryactofaveragingorvotingacross\ndifferent model outputs is a direct strategy to scale up robustness against individual model\nfailures. Furthermore, by intentionally including varied models, ensembling increases the",
    "chances of discovering novel or superior solutions. In this sense, scaling is not limited to\nmakingmodelslargerorrunningthemlonger—italsomeansstrategiesformakinginference\nmorerobust,exploratory,andadaptive.\n11.3.4 Generating and Verifying Thinking Paths\nSofar,wehaveviewedinference-timescalingasageneralclassofmethodsforscalingvarious\naspectsofinference,suchassequencelength,modelsize,and/orsearchstrategies. Infact,one\nsuccessfulapplicationistheuseofinference-timescalingtoenhancethereasoningcapabilities\nofLLMs. Aswehaveseen,thereasoningperformanceofLLMscanbeimprovedbyusing\nchain-of-thought methods. We can therefore make use of the chain-of-thought prompts to\ngenerateintermediatereasoningstepsandreachacorrectanswer. However,reasoningproblems\nareoftensocomplicatedthatwecannotobtainhigh-qualitysolutionsbyprovidingsimplechain-\nof-thoughtprompts. Forexample,whensolvingamathproblem,wetypicallyneedtoreason\noverasequenceofsteps. Ateachstep,weneedtoworkoutsomeintermediateresult,verifyit,",
    "andthendeterminewhattodonext. Thereasoningpathisnotafixedpatternbutadynamically\ngeneratedthinkingprocessthatofteninvolvestrial-and-error,backtracking,andself-correction.\nThisrequiresmoresophisticatedpromptingstrategiesorsearchalgorithmstonavigatesuch\ncomplexreasoning. Inthissubsection,wefocusoninference-scalingmethodsthatgobeyond\nsimplechain-of-thoughttoaddresscomplexreasoningproblemsmoreeffectively.\nAtahighlevel,methodsforscalingthereasoningofLLMscanbecategorizedintotwo",
    "11.3 Inference-time Scaling 625\nclasses:\n• Training-freeMethods. Thesemethodsaimtoimprovereasoningcapabilitieswithout\nrequiring any modification or retraining of the pre-trained parameters. Instead, they\nfocusontechniquesappliedduringinference,suchassophisticatedpromptingstrategies\n(e.g.,chain-of-thought)andalgorithmiccontroloverthereasoningprocess(e.g.,search).\n• Training-based Methods. These methods involve further training or fine-tuning the\nmodel parameters to explicitly improve reasoning abilities, such as supervised fine-\ntuning on datasets with reasoning examples (e.g., math problems with step-by-step\nsolutions).\nInthefollowing,wefirstdiscusstraining-freemethods,andthentraining-basedmethods.\n1. Solution-level Search with Verifiers\nGiven an input sequence (e.g., a math problem), there are many possible output sequences\n(e.g.,solutionstotheproblem). Ifwehaveamodeltoevaluateorverifyeachsolution,wecan",
    "selectthebestone. Thisisthefundamentalprinciplebehindmethodslikebest-of-N sampling,\nwheremultipleoutputsaregenerated,andtheoptimalresultispickedbasedonsomeselection\nmechanism. Suchaselectionprocesscanbeviewedasasearchproblem,whichinvolvestwo\ncomponents:\n• Search Algorithm. This defines the strategy used to explore the space of possible\noutputsequences(solutions)andgenerateasetofcandidates. Itcanrangefromsimple\nindependentsamplingtomoresophisticatedsearchtechniquesasdiscussedinSection\n11.1.3.\n• Verifier. Thisisamodelorfunctionresponsibleforevaluatingthequality,correctness,\nor utility of each candidate solution generated by the search algorithm. It provides\na score, a probability, or a judgment that allows the system to select the best among\nthecandidates. TheverifiercanbeanotherLLM,orevenasetofpredefinedrulesor\nheuristics.\nGiven an input problem x, we define that an output solution y can be represented as a\nsequenceofreasoningsteps:\ny = (a ,a ,...,a ) (11.37)\n1 2 nr",
    "wherea isthei-threasoningstep,anda isthelaststepwhichshouldcontaintheanswerto\ni nr\ntheproblem. SeeFigure11.15foranexampleofamulti-stepreasoningpath.\nThesearchalgorithmcanefficientlygenerateasetofcandidatesolutions\nD = {y ,...,y } (11.38)\nc 1 K\nThen,wecanuseaverifier,whichevaluateseachsolutionbythefunctionV(y),toscore",
    "626 Chapter 11. Inference\nExpress(5−4i)−2(3+6i)asa x\nProblem(x)\ncomplexnumber.Thinkstepbystep.\nNowwedistributethe2totheterms\nStep1(a 1)\nintheparenthesis.2(3+6i)=6+12i.\nSo(5−4i)−2(3+6i)isequivalentto\nStep2(a 2)\n(5−4i)−(6+12i).\nNowwesubtracttheterms. x Solutionwith\nStep3(a 3)\n5−4i−6−12i=−1−16i. AReasoningPath\nThat’stheanswer. x\nStep4(a 4)\nYoucanalsowriteitas−1−16i. x\nStep5(a 5)\nAnswer −1−16i.\nFigure11.15: Illustrationofmulti-stepreasoning. ThisexampleisfromthePRM800Kdataset\n[Lightmanetal.,2024]. Givenamathproblem,theLLMispromptedtogenerateathinking\npath (or reasoning path) consisting of several reasoning steps. Each step addresses a sub-\nproblem based on the results of the previous steps. The answer to the original problem is\ncontainedinthelaststep.\nthecandidatesinD . Thefinaloutputisthebestcandidateselectedbytheverifier\nc\nyˆ = argmaxV(y) (11.39)\ny∈Dc\nAlthoughverifyingtheentirereasoningpathispossible,asimpleralternativeistoverify",
    "onlythefinalreasoningstep. InthiswaytheverifierfunctionV(y)issimplifiedtodepend\nsolely on the final answer contained within a . This can be achieved in various ways,\nnr\ndependingonthenatureoftheproblemandtheexpectedanswerformat.\n• Forsomemathandcodingproblems,wecanuseoff-the-shelftoolsasverifiers. Examples\nincludeproofcheckersformathematicaltheorems,interpretersorcompilersforcode\nexecution,andunittestsystemsforverifyingprogramcorrectnessagainstpredefined\ntestcases.\n• Ifthereislabeleddataforevaluatingtheanswer,suchashumanpreferencedata,wecan\ntrainarewardmodelonsuchdata. Thelearnedrewardmodelisthenusedastheverifier",
    "11.3 Inference-time Scaling 627\nwhichassignsascalarscoretoeachcandidateanswer.\n• Iftherearenoexistingsystemsorsuitablerewardmodels,wecanuseanotherLLMto\nactastheverifier. ThisLLMispromptedtoassessthequalityofthecandidateanswer.\nIt could potentially be a more capable model, or the same LLM used with a specific\n“evaluator”prompt.\n• Alternatively, simpler heuristic-based verifiers can be designed. A commonly used\napproach is to employ majority voting, where the most frequently occurring answer\namongasetofcandidatesisselected.\nBasedontheseverifiers,wecansearchtoobtainasetofcandidatesolutionsforselection.\nOnesimplestrategy,whichisoftenreferredtoasparallelscaling[Brownetal.,2024;Snell\netal.,2024], involvesgeneratingK candidatesolutionsbyrunningthebaseLLMK times\nindependently. In this process, we can adjust the temperature in sampling to control the\ndiversityintheoutputs. TheverifierthenassesseseachoftheseK completesolutions,and",
    "theonewiththehighestscoreisselectedasthefinaloutput. Thisisconceptuallyverysimilar\nto best-of-N sampling, which in previous chapters we primarily described as a method of\nselectingthebestonefromasetofsampledoutputsusingarewardmodel.\nAnotherapproachissequentialscaling,whichbuildsasequenceofsolutionsincrementally\n[Gouetal.,2024;Zhangetal.,2024]. ItstartswithaninitialsolutiongeneratedbytheLLM\nwithprompting. Then,weuseaverifier(oftenthesameLLM)toevaluatethesolution. This\ncan be seen as a critique stage. The output of this stage is some form of feedback, such as\ntextualcritiquespinpointingerrorsorsuggestingimprovements,numericalscoresreflecting\nsolutionquality,orevenarevisedplanorintermediatesteptoguidethenextgeneration. This\nfeedback, along with the original problem and the current solution, is then used to prompt\ntheLLMtogenerateapotentiallyimprovedsolution. Thiscanbeseenasarefinestage. This\ncritique-refinecyclecanberepeated,forminganiterativeloop:\ny = Refine(x,y ,Feedback(y )) (11.40)",
    "k+1 k k\nwhereFeedback(y )representsthefeedbackfromtheverifier. TheRefine(·)functiongen-\nk\nerates the improved solution y by prompting the LLM with the original problem x, the\nk+1\nprevious solution y , and this feedback. The process can be iterated for K times, or until\nk\nthe solution quality, as assessed by the verifier, converges to a satisfactory level. This iter-\native framework, where a solution is progressively improved through cycles of generation,\nevaluation(critique),andrevision,ispreciselywhatconstitutesself-refinement[Shinnetal.,\n2023;Madaanetal.,2024]. Insuchscenarios,theroleoftheverifierisnotjusttopickthebest\ncompletesolutionfromastaticset,buttoactivelyguidethegenerationprocessitself.\nSee Figure 11.16 for illustrations of parallel scaling and sequential scaling. Note that\nthereareotherwaystoperformsearchandobtaindifferentsetsofcandidatesolutions. One\nalternativemethodistoorganizesearchasatreestructure. Thisapproach,oftenreferredtoas",
    "treesearch,providesamorestructuredwaytoexplorethespaceofpossiblereasoningpaths. In\nsolution-levelsearch,eachnodeofthetreerepresentsacompletesolution. Duringsearch,we\nneedtoexpandanodetoasetofchildnodes,representingnewsolutionsthatcanbeconsidered",
    "628 Chapter 11. Inference\nSolution1\nSampling\ny\n1\nProblem Solution2\nx y\n2\nSolution3\ny\n3\n(a)ParallelScaling\nSelf-refinement\nProblem Solution1 Solution2 Solution3\nx y y y\n1 2 3\n(b)SequentialScaling\nFigure11.16: Illustrationsofparallelscalingandsequentialscaling. Inparallelscaling,we\nobtain multiple solutions by running the LLM several times independently. In sequential\nscaling,theLLMgeneratesaninitialsolution. Then,weusetheLLMtorefineititeratively,\nwitheachrefinementyieldinganew,possiblybettersolution.\ninverification. Theexpansionprocesstypicallyinvolvestakinganexistingsolution(theparent\nnode)andusingtheLLMtogeneratevariationsoralternativesolutions.\n2. Step-level Search with Verifiers\nWhilethemethodsdiscussedaboveprimarilyfocusongeneratingcompletesolutionsbefore\nfinal selection, the search process can also be integrated more deeply into the step-by-step\ngenerationofthereasoningpathitself. Thisleadstoapproachesthatperformstep-levelsearch",
    "withverifiers,whereguidanceorpruningoccursatintermediatereasoningsteps{a ,...,a }\n1 n\nk\nratherthanonlyafterafullsolutionyisformed.\nSuchfine-grainedcontrolisparticularlybeneficialforcomplexreasoningproblemswhere\nasingleincorrectintermediatestepcanrendertheentiresubsequentreasoningchaininvalid.\nByevaluatingorguidingthegenerationateachintermediatestep,theLLMcanexplorethe\nreasoningspacemoreeffectively,potentiallypruningunpromisingpathsearlyorallocating\nmoreresourcestoexploremoreplausibleones.\nStep-level search with verifiers can also be modeled as a tree search problem. In this\nparadigm, each node (or state) corresponds to a partial reasoning path, a = (a ,...,a ),\n≤i 1 i\nrepresentingthesequenceofireasoningstepstakensofar(i.e.,apathfromtherootnodeto\nthecurrentnode). Theobjectiveofthesearchprocessistoexploretheunderlyingstatespace,\nstartingfromaninitialemptypath,tofindacompletepaththatconstitutesacorrectsolution.",
    "Note that we use a here to represent a partial reasoning path instead of y . While this\n≤i ≤i\nmakesnotationabitinconsistentwiththatusedforrepresentingcompletesolutions(y)orfull",
    "11.3 Inference-time Scaling 629\npathsinsolution-levelsearch,itservestohighlightthefocusonindividualactionsorsteps.\nThecorecomponentsofstep-levelsearchwithverifiersare:\n• NodeRepresentation. Anodeisapartialreasoningpatha =(a ,...,a ). Theroot\n≤i 1 i\nnodeisanemptypath,andterminalnodesarecompletereasoningpaths.\n• Node Expansion. Given a current partial path a , the LLM is used to generate one\n≤i\n(1) (M)\nor more candidate next reasoning steps {a ,...,a }. Each candidate step, when\ni+1 i+1\n(j)\nappendedtoa ,formsanewpotentialpartialpatha =(a ,...,a ,a ).\n≤i ≤i+1 1 i i+1\n• Verification. The verifier V(·) evaluates the quality of a newly generated step in the\ncontext of the current partial path a =(a ,...,a ) and the original problem x. As\n≤i 1 i\nwith solution-level verification, step-level verifiers might output a numerical score, a\ncategoricallabel,andtextualfeedback.\n• Search. Thisgovernshowthesearchspaceisexplored. Basedontheevaluationsfrom",
    "theverifier,thesearchstrategydecideswhichpartialpathstoextendfurther,whichto\nprune,andtheorderofexploration.\nThisstep-by-stepverificationallowsfordynamicadjustmentstothereasoningprocess. Ifa\nstepa isdeemedincorrectorunpromisingbyV(·),thesearchalgorithmcanbacktrackand\ni+1\nexplorealternativestepsfroma ,orevenfromanearliernodea (wherei′<i). Conversely,\n≤i ≤i′\nifastepishighlyrated,resourcescanbefocusedonextendingthatpath. SeeFigure11.17for\nanillustrationofstep-levelsearchwithverifiers.\nClearly,thissearchframeworkisverysimilartothatusedindecodingmethodsforLLMs,\nasdiscussedinSection11.1.3. Forexample,beamsearchmaintainsasetofK mostpromising\npartialsequencesateachgenerationstep. Thisisaformofstep-levelsearchwherethe“verifier”\nisimplicitlytheLLM’sownprobabilitymodel,andthe“search”isthepruningmechanismto\nmaintainthebeamsize.\nHowever,step-levelsearchwithexplicitverifiers,asdescribedhere,presentsdifferences\nfromstandarddecoding. Oneofthemisthatthe verifiercanbea muchmoresophisticated",
    "componentthanjusttherawoutputprobabilitiesofthegenerativeLLM.Thedesignofstep-\nlevelverifiersbasicallyfollowsthatofsolution-levelverification. Astep-levelverifiermight\nbealanguagemodelthatassessesthequalityofanindividualreasoningstepwithinthecontext\noftheprecedingpath. ThisLLMcanevenbefine-tunedtoenhanceitsverificationcapability.\nAlternatively,fordomainswithwell-definedrules,itcouldbeasymbolicengineorasetof\nprogrammaticchecks. Furthermore,verifierscanbedesignedtopredictthefutureutilityor\nlikelihoodofsuccessgiventhecurrentpartialpath,drawinginspirationfromvaluefunctions\ninreinforcementlearning. Humanexpertisecanalsobeincorporatedtoprovidejudgmentson\ncriticalsteps,especiallyinhigh-stakesscenarios.\nOneexampleofsuchastep-levelverifier,particularlywhenusinghumanfeedbacktoassess\nintermediateprogress,istheprocessrewardmodel(PRM).APRMistypicallyaseparate\nlanguagemodeltrainedtooutputascalarrewardforeachreasoningstepa withinapartial\ni′",
    "patha . Itprovidesamoredirectandfine-grainedsupervisorysignalcomparedtooutcome\n≤i\nrewardmodels(ORMs)whichonlyevaluatethefinalsolution. However,thedevelopment",
    "630 Chapter 11. Inference\nInputProblem(Root)\nx\nPrunedbyLikelihood\n(1) (2) (3) (4) (5)\na a a a a\n1 ⊠ 1 ⊠ 1 ⊠ 1 1 ⊠\nPrunedbyVerification\n(1) (2) (3) (4) (5)\na a a a a\n2 ⊠ 2 ⊠ 2 2 ⊠ 2 ⊠\nSelectedReasoningStep\n(1) (2) (3) (4) (5)\na a a a a\n3 3 ⊠ 3 ⊠ 3 ⊠ 3 ⊠\n(1) (2) (3) (4) (5)\na a a a a\n4 ⊠ 4 ⊠ 4 4 ⊠ 4 ⊠\n(j)\nFigure11.17: Illustrationofstep-levelsearchwithverifiers. a =thej-thcandidateforthe\ni\ni-threasoningstep,⊠=candidateprunedbytheLLM’soutputprobability,and⊠=candidate\npruned by the verifier. Given the input problem as the root node, we expand the tree by\ngeneratingmultiplereasoningstepsateachexpansion. Eachcandidatecanbeprunedbyeither\nlikelihood(asinstandarddecoding)orstep-levelverification. Theunprunedcandidatesare\nthen expanded to generate further reasoning steps. The process is iterated until a complete\nreasoning chain leading to a final answer is generated, or until a predefined search limit is\nreached.\nofPRMsreliesonstep-levelhumanannotations,suchaspreferencesondifferentnextsteps.",
    "Collecting supervision for each intermediate step is considerably more labor-intensive and\nrequiresgreatercognitiveeffortfromhumanannotatorsthansimplylabelingfinaloutcomes.\nOnealternativeapproachtodevelopingtrainingdataforstep-levelverificationistouse\nLLMstogeneratesuchannotationsautomatically. Forexample,wecantakeastrongLLM,\nreferredtoasateachermodel,andpromptittofirstgenerateacompletereasoningpathfor\nagivenproblem. Then,ateachintermediatestepwithinthispath,wecanpromptthesame\nteacherLLM(oranothercapableLLM)togenerateseveralalternativecandidatenextsteps\nin addition to the one it originally chose. The teacher LLM can then be prompted again to\nevaluatethesealternatives. Theseevaluationresults(e.g.,correctvs. incorrect)canthenserve\nasdataannotations. Alternatively,thegeneralizationcapabilitiesofPRMscanbeleveraged.\nWecantrainaPRMontaskswherestep-levelverificationiseasierandthengeneralizethis\nPRMtoothertaskswithlittleornoadditionaltraining.",
    "11.3 Inference-time Scaling 631\nNotethatstep-levelverificationalsocomeswithitsownproblems. Frequentverification,\nespecially if using an LLM as the verifier, can substantially increase computational costs\nand latency. The design of effective step-level verifiers is non-trivial itself. An inaccurate\nverifiermightprematurelydiscardgoodreasoningpathsorfailtoidentifyflawedones,thereby\nmisleading the search. This makes the development of such systems more complex and\ndifficult.\n3. Encouraging Long Thinking\nSofarinthissubsection,mostofthemethodsareimplicitlybasedonasimpleidea: generating\nlonger reasoning paths can help. In addition to CoT and search with verifications, we can\nconsideralternativemethodstoachievethis. Forexample,wecanprompttheLLMbyexplicitly\naskingforextendeddeliberation. Beyonddirectprompting,wecanalsomakemodifications\ntothedecodingprocessitself,suchasadjustingtokenlimitsorapplyingpenaltiesforshort",
    "outputs. Another approach is to employ multi-stage generation schemes where the model\nincrementallybuildsuponitsreasoning.\n4. Training-based Scaling\nAs well as considering inference-time scaling methods without training, we also wish to\nconsider methods that can improve intrinsic reasoning capabilities of LLMs by modifying\ntheirparametersthroughfurthertraining. Whilesuchtraining-basedscalingmethodstypically\nrequireadditionaltrainingcostandcomputationalresources,theyinstillstrongerreasoning\nskillsdirectlyintothemodelparameters,whichinturncanleadtomoreeffectiveandefficient\nreasoning performance. We can even combine them with training-free methods for better\ninference-timescalingresults.\nAlthough our discussion here is restricted to reasoning problems, methods for training-\nbasedscalingarecommon. MostofthemhavebeendiscussedinChapter10. Here,wewill\nbriefly describe how these methods can be applied to improving inference-time scaling for\nreasoningproblems.",
    "• Fine-tuningonReasoningData. Oneofthemostdirectwaystoenhancereasoning\nisbyfine-tuningpre-trainedLLMsondatasetsspecificallycuratedforreasoningtasks.\nThesedatasetscanrangefromsimpleinput-outputpairstomorestructuredformatsthat\nincludestep-by-stepreasoningprocesses. Typicalexamplesincludedatasetsofmath\nwordproblems,logicaldeductionexercises,orcodegenerationwithexplanations. By\ntrainingonsuchdata,themodellearnsfromcommonreasoningpatterns,andthuscan\ngeneratedetailedandcoherentreasoningpathsattesttime.\n• ReinforcementLearningforReasoning. Ifweregardaverifierasarewardmodel,we\ncanseethatthemethodsdiscussedintheprevioussubsectionareadirectapplication\noftherewardmodeltoreasoningproblems, thoughtheyaretraining-free. Ofcourse,\nwecanapplythisrewardmodeltoLLMfine-tuning. Thisfollowsastandardparadigm\nofreinforcementlearning. Givenarewardmodel,theLLM,actingasapolicy,isfine-\ntuned using reinforcement learning algorithms. The LLM generates reasoning steps",
    "632 Chapter 11. Inference\norfullsolutions,receivesfeedback(rewards)fromtherewardmodel,andupdatesits\nparameters to produce outputs that maximize these rewards. This process aligns the\nLLMoutputwithnotionsofhigh-qualityreasoning,therebyencouragingtheLLMto\ngeneratemorereliablereasoningpaths. Anotherkeyissueisthetrainingofthereward\nmodel. Generally,thisrewardmodelcouldbeanoutcomerewardmodelthatevaluates\nthecorrectnessorqualityofthefinalanswer,oraprocessrewardmodelthatassesses\nthequalityofeachintermediatereasoningstep,asdiscussedinthecontextofstep-level\nverifiers. In some cases, we can even develop a reward model based on simple rules,\nsuchasgivingbonusestolongeroutputs.\n• Knowledge Distillation for Reasoning. In this approach, a smaller, more efficient\nstudentLLMistrainedtomimicthereasoningoutputsorinternalrepresentationsofa\nlarger,morecapableteacherLLM.Theteachermodelmightgeneratedetailedreasoning\nstepsforavarietyofproblems. Thestudentmodelthenlearnstoreproducethesehigh-",
    "qualityreasoningdemonstrations. Thisstrategymakesstrongerreasoningcapabilities\nmore accessible by deploying them in smaller models that are less computationally\nexpensiveatinferencetime.\n• IterativeRefinement. Training-basedscalingcanalsoinvolveiterativerefinement. For\nexample,anLLMcangeneratesolutionstoasetofproblems. Thesesolutionsandtheir\nreasoningpathsarethenverified,eitherbyhumansorautomaticverifiers. Thecorrect\nreasoning paths are subsequently added to the training data, and the LLM is further\nfine-tunedonthisaugmenteddataset. ThiscreatesacyclewheretheLLMprogressively\nimprovesitsreasoningcapabilitiesthroughrepeatedgeneration,critique,andlearning.\nTheprimaryadvantageofthesetraining-basedscalingmethodsisthattheyendowtheLLM\nwithstrongerinherentreasoningskills. Thisdirectlycontributestoimprovedinference-time\nscalinginseveralways: itcanleadtomoreefficientinference,astheLLMmightrequireless\nextensive search or fewer generation samples to arrive at a correct solution. Moreover, the",
    "basequalityofgeneratedstepsorsolutionsishigher. Therefore,awell-trainedLLMmight\ngeneralize its learned reasoning abilities to novel problems more effectively than an LLM\nrelyingsolelyonin-contextlearningortraining-freeinferenceschemes.\nOn the other hand, training-based approaches also present challenges, compared to the\ntraining-free counterparts. The creation of high-quality, large-scale training datasets for\nreasoningcanbeexpensiveandlabor-intensive. Thefine-tuningprocessitself,particularlyfor\nthelargestLLMsorwhenusingRL,canbecomputationallyintensiveandrequiresubstantial\nengineering effort. There is also the risk of the model overfitting to the specific types of\nproblemsorreasoningstylespresentinthetrainingdata,potentiallylimitingitsperformance\nonout-of-distributiontasks.\n11.4 Summary\nIn this chapter, we have discussed the inference issue for LLMs. We have presented the\nprefilling-decodingframeworkandrelateddecodingalgorithmsforLLMinference. Then,we",
    "11.4 Summary 633\nhavedescribedseveraltechniquesforefficientinference. Wehavealsodiscussedinference-\ntime scaling, which has been considered one of the most important methods for improving\nLLMreasoning.\nInference over sequential data has long been a concern in AI [Wozengraft and Reiffen,\n1961;Viterbi,1967;Forney,1972]. InthecontextofNLP,thislineofworkdatesbacktothe\nveryearlydaysofspeechrecognitionandstatisticalmachinetranslation[Koehn,2010],where\nresearchersfacedthechallengeofefficientlysearchingvasthypothesisspacestofindthemost\nprobableoutputsequence. Techniqueslikebeamsearchandvariouspruningstrategieswere\ndevelopedthentomakethiscomputationallytractable. Atthattime,modelswererelatively\nweak,andmuchoftheresearchfocusedondevelopingpowerfulsearchalgorithmstoreduce\nsearcherrors. Thesefoundationalideascontinuetoinfluencemodernapproaches.\nAsweentertheeradominatedbydeeplearningmethods,modelsbasedondeepneural\nnetworkshavebecomeextremelypowerful. Evenwithverysimplesearchalgorithms,these",
    "modelscanachieveexcellentresults. Inthiscontext,inferencenolongerseemsas“important”\nasitoncewas,andresearchattentionhasgraduallyshiftedtowardmodelarchitectures,training\nmethods,andscalingupmodels.\nHowever,historytendstorepeatitself. WiththeriseofLLMs,inferencehasonceagain\nattractedsignificantattention. Thisrenewedfocusisprimarilymanifestedintwoaspects:\n• The inference cost for LLMs is very high. For example, efficiently deploying LLMs\nin high-concurrency, low-latency scenarios remains a challenging problem, making\ninferenceefficiencycriticallyimportant. Inthiscontext,efficientarchitecturedesigns,\noptimizedsearchalgorithms,andvariousinferenceoptimizationstrategiesholdsubstan-\ntialpracticalsignificance.\n• Input and output sequence lengths have significantly increased in complex tasks. Es-\npeciallyintaskslikemathematicalreasoning,thegrowthofsequencelengthsfurther\nhighlightstheimportanceofinferenceefficiency. Moreover,scalingtheinferencepro-",
    "cesshasrecentlyproventobeaneffectivewaytoimprovethereasoningcapabilitiesof\nmodels. Therefore,achievingefficientinferencescalingisemergingasaparticularly\npromisingresearchdirection.\nInferenceisnowawide-rangingtopicthatencompassesmanytechniques. Itinvolvesnot\nonly the development of model architectures and decoding algorithms, but is increasingly\nshapedbytheintricateengineeringandsophisticatedsystems-leveloptimizationsrequiredto\ndeployLLMseffectivelyandefficiently. ManyofthesetechniquesarebeyondthescopeofNLP\noraspecificAIarea. Instead,thefrontierofLLMinferenceoptimizationnowextendsdeeply\nintodomainstraditionallyconsideredcorecomputerscienceandengineering. Thissystemic\nperspectivehasbroughtmanynewideastothestudyofinferenceproblems. Unfortunately,\nthischaptercannotcoverallrelevanttechniques—indeed,thatwouldbeanalmostimpossible\ntaskinitself. Ultimately,thebestwaytobetterunderstandandmasterthesetechniquesmay\nstilllieinhands-onpractice.",
    "Bibliography\n[Ackley et al., 1985] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning\nalgorithmforboltzmannmachines. Cognitivescience,9(1):147–169,1985.\n[AcsandKornai,2016] JuditAcsandAndrásKornai. Evaluatingembeddingsondictionary-based\nsimilarity. InProceedingsofthe1stWorkshoponEvaluatingVector-SpaceRepresentationsforNLP,\npages78–82,2016.\n[Adi et al., 2016] Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg.\nFine-grainedanalysisofsentenceembeddingsusingauxiliarypredictiontasks. InProceedingsof\nInternationalConferenceonLearningRepresentations,2016.\n[Agirre et al., 2009] Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalová, Marius Pasca,\nand Aitor Soroa. A study on similarity and relatedness using distributional and wordnet-based\napproaches. InProceedingsofHumanLanguageTechnologies: The2009AnnualConferenceofthe\nNorthAmericanChapteroftheAssociationforComputationalLinguistics,pages19–27,2009.",
    "[Agrawaletal.,2023] AmeyAgrawal,AshishPanwar,JayashreeMohan,NipunKwatra,BhargavS\nGulavani,andRamachandranRamjee. Sarathi: Efficientllminferencebypiggybackingdecodes\nwithchunkedprefills. arXivpreprintarXiv:2308.16369,2023.\n[Agrawaletal.,2024] AmeyAgrawal,NitinKedia,AshishPanwar,JayashreeMohan,NipunKwatra,\nBhargavGulavani,AlexeyTumanov,andRamachandranRamjee. Taming{Throughput-Latency}\ntradeoff in {LLM} inference with {Sarathi-Serve}. In 18th USENIX Symposium on Operating\nSystemsDesignandImplementation(OSDI24),pages117–134,2024.\n[Ainslieetal.,2020] JoshuaAinslie,SantiagoOntanon,ChrisAlberti,VaclavCvicek,ZacharyFisher,\nPhilipPham,AnirudhRavula,SumitSanghai,QifanWang,andLiYang. Etc: Encodinglongand\nstructuredinputsintransformers. InProceedingsofthe2020ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing(EMNLP),pages268–284,2020.\n[Ainslieetal.,2023] JoshuaAinslie,JamesLee-Thorp,MichieldeJong,YuryZemlyanskiy,Federico",
    "Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from\nmulti-headcheckpoints. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages4895–4901,2023.\n[Akhbardehetal.,2021] FarhadAkhbardeh,ArkadyArkhangorodsky,MagdalenaBiesialska,Ondˇrej\nBojar,RajenChatterjee,VishravChaudhary,MartaR.Costa-jussa,CristinaEspaña-Bonet,Angela\nFan,ChristianFedermann,MarkusFreitag,YvetteGraham,RomanGrundkiewicz,BarryHaddow,\nLeonieHarter,KennethHeafield,ChristopherHoman,MatthiasHuck,KwabenaAmponsah-Kaakyire,\nJungoKasai,DanielKhashabi,KevinKnight,TomKocmi,PhilippKoehn,NicholasLourie,Christof\nMonz,MakotoMorishita,MasaakiNagata,AjayNagesh,ToshiakiNakazawa,MatteoNegri,Santanu\nPal,AllahseraAugusteTapo,MarcoTurchi,ValentinVydrin,andMarcosZampieri. Findingsof",
    "636 BIBLIOGRAPHY\nthe2021conferenceonmachinetranslation(WMT21). InProceedingsoftheSixthConferenceon\nMachineTranslation,pages1–88,2021.\n[Akyüreketal.,2023] EkinAkyürek,DaleSchuurmans,JacobAndreas,TengyuMa,andDennyZhou.\nWhatlearningalgorithmisin-contextlearning? investigationswithlinearmodels. InProceedingsof\nTheEleventhInternationalConferenceonLearningRepresentations,2023.\n[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai.\nRevisitingneuralscalinglawsinlanguageandvision. AdvancesinNeuralInformationProcessing\nSystems,35:22300–22312,2022.\n[Alayracetal.,2022] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,\nYanaHasson,KarelLenc,ArthurMensch,KatieMillican,MalcolmReynolds,RomanRing,Eliza\nRutherford,SerkanCabi,TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,Jacob\nMenick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj\nBinkowski,RicardoBarreira,OriolVinyals,AndrewZisserman,andKarenSimonyan. Flamingo: a",
    "visuallanguagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessingSystems,\n35:23716–23736,2022.\n[Allaletal.,2024] LoubnaBenAllal,AntonLozhkov,andDanielvanStrien.cosmopedia:howtocreate\nlarge-scalesyntheticdataforpre-training. https://huggingface.co/blog/cosmopedia,\n2024.\n[Allauzenetal.,2014] CyrilAllauzen,BillByrne,AdriàdeGispert,GonzaloIglesias,andMichael\nRiley. Pushdown automata in statistical machine translation. Computational Linguistics, 40(3):\n687–723,2014.\n[AllenandHospedales,2019] CarlAllenandTimothyHospedales. Analogiesexplained: Towards\nunderstandingwordembeddings. InInternationalConferenceonMachineLearning,pages223–231.\nPMLR,2019.\n[Almazroueietal.,2023] EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,Alessandro\nCappelli,RuxandraCojocaru,MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,JulienLaunay,\nQuentinMalartic,DanieleMazzotta,BadreddineNoune,BaptistePannier,andGuilhermePenedo.\nThefalconseriesofopenlanguagemodels. arXivpreprintarXiv:2311.16867,2023.",
    "[Alzantot et al., 2018] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani\nSrivastava,andKai-WeiChang. Generatingnaturallanguageadversarialexamples. InProceedings\nofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2890–2896,\n2018.\n[Ammaretal.,2020] WaleedAmmar,GeorgeMulcaire,YuliaTsvetkov,GuillaumeLample,Chris\nDyer, and Noah A Smith. Massively multilingual word embeddings. In Proceedings of the 8th\nInternationalConferenceonLearningRepresentations(ICLR),2020.\n[Andersonetal.,2017] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould. Guided\nopen vocabulary image captioning with constrained beam search. In Proceedings of the 2017\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages936–945,2017.\n[Andreasetal.,2016] JacobAndreas,MarcusRohrbach,TrevorDarrell,andDanKlein. Neuralmodule\nnetworks. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages\n39–48,2016.",
    "[Antoletal.,2015] StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,\nCLawrenceZitnick,andDeviParikh. Vqa: Visualquestionanswering. InProceedingsoftheIEEE",
    "BIBLIOGRAPHY 637\ninternationalconferenceoncomputervision,pages2425–2433,2015.\n[Arjovskyetal.,2016] MartinArjovsky,AmarShah,andYoshuaBengio. Unitaryevolutionrecurrent\nneuralnetworks. InInternationalconferenceonmachinelearning,pages1120–1128,2016.\n[AronoffandFudeman,2011] MarkAronoffandKirstenFudeman. Whatismorphology?,volume8.\nJohnWiley&Sons,2011.\n[Aroraetal.,2017] SanjeevArora,YingyuLiang,andTengyuMa. Asimplebuttough-to-beatbaseline\nforsentenceembeddings. InInternationalconferenceonlearningrepresentations,2017.\n[Artetxe et al., 2017] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Learning bilingual word\nembeddings with (almost) no bilingual data. In Proceedings of the 55th Annual Meeting of the\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages451–462,2017.\n[Aschenbrenner,2024] LeopoldAschenbrenner. Situationalawareness: Thedecadeahead,2024. URL\nhttps://situational-awareness.ai/.\n[AshandDoléans-Dade,1999] RobertB.AshandCatherineA.Doléans-Dade. Probability&Measure",
    "Theory. AcademicPress,1999.\n[Askell et al., 2021] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom\nHenighan,AndyJones,NicholasJoseph,BenjaminMann,NovaDasSarma,NelsonElhage,Zac\nHatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario\nAmodei,TomB.Brown,JackClark,SamMcCandlish,ChrisOlah,andJaredKaplan. Ageneral\nlanguageassistantasalaboratoryforalignment. arXivpreprintarXiv:2112.00861,2021.\n[ÅströmandWittenmark,2013] KarlJÅströmandBjörnWittenmark. Computer-controlledsystems:\ntheoryanddesign. CourierCorporation,2013.\n[Atkinson and Shiffrin, 1968] Richard C Atkinson and Richard M Shiffrin. Human memory: A\nproposedsystemanditscontrolprocesses. InPsychologyoflearningandmotivation,volume2,\npages89–195.Elsevier,1968.\n[Augusteetal.,2017] JeremyAuguste,ArnaudRey,andBenoitFavre. Evaluationofwordembed-\ndingsagainstcognitiveprocesses: primedreactiontimesinlexicaldecisionandnamingtasks. In",
    "Proceedingsofthe2ndworkshoponevaluatingvectorspacerepresentationsforNLP,pages21–26,\n2017.\n[Baetal.,2016] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXiv\npreprintarXiv:1607.06450,2016.\n[Bach et al., 2022] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel,\nNihalV.Nayak,AbheeshtSharma,TaewoonKim,M.SaifulBari,ThibaultFévry,ZaidAlyafeai,\nManan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani,\nHanWang,JasonAlanFries,MagedSaeedAlShaibani,ShanyaSharma,UrmishThakker,Khalid\nAlmubarak, Xiangru Tang, Dragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Rush.\nPromptsource: Anintegrateddevelopmentenvironmentandrepositoryfornaturallanguageprompts.\nInProceedingsofthe60thAnnualMeetingoftheAssociationforComputationalLinguistics: System\nDemonstrations,pages93–104,2022.\n[Bachlechneretal.,2021] ThomasBachlechner,BodhisattwaPrasadMajumder,HenryMao,Gary",
    "Cottrell,andJulianMcAuley.Rezeroisallyouneed:Fastconvergenceatlargedepth.InProceedings\nofUncertaintyinArtificialIntelligence,pages1352–1361.PMLR,2021.\n[Baevski et al., 2020] Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-",
    "638 BIBLIOGRAPHY\nsupervisedlearningofdiscretespeechrepresentations. InProceedingsofICLR2020,2020.\n[Bahdanauetal.,2014] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachine\ntranslationbyjointlylearningtoalignandtranslate. arXivpreprintarXiv:1409.0473,2014.\n[Bahdanauetal.,2016] DzmitryBahdanau,JanChorowski,DmitriySerdyuk,PhilemonBrakel,and\nYoshuaBengio. End-to-endattention-basedlargevocabularyspeechrecognition. In2016IEEE\ninternationalconferenceonacoustics,speechandsignalprocessing(ICASSP),pages4945–4949.\nIEEE,2016.\n[Bahl and Mercer, 1976] L. R. Bahl and R. L. Mercer. Part of speech assignment by a statistical\ndecisionalgorithm. InProceedingsofIEEEInternationalSymposiumonInformationTheory,pages\n88–89,1976.\n[Baietal.,2021] JiangangBai,YujingWang,YirenChen,YamingYang,JingBai,JingYu,andYunhai\nTong. Syntax-bert: Improvingpre-trainedtransformerswithsyntaxtrees. InProceedingsofthe16th\nConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics: MainVolume,",
    "pages3011–3020,2021.\n[Bakarov,2018] AmirBakarov. Asurveyofwordembeddingsevaluationmethods. arXivpreprint\narXiv:1801.09536,2018.\n[Banarescuetal.,2013] LauraBanarescu,ClaireBonial,ShuCai,MadalinaGeorgescu,KiraGriffitt,\nUlf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract\nMeaningRepresentationforsembanking. InProceedingsofthe7thLinguisticAnnotationWorkshop\nandInteroperabilitywithDiscourse,pages178–186,2013.\n[Baoetal.,2021] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: Bertpre-trainingofimage\ntransformers. InProceedingsofInternationalConferenceonLearningRepresentations,2021.\n[Bapnaetal.,2018] AnkurBapna,MiaXuChen,OrhanFirat,YuanCao,andYonghuiWu. Training\ndeeperneuralmachinetranslationmodelswithtransparentattention. InProceedingsofthe2018\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3028–3033,2018.\n[Barber, 2012] DavidBarber. BayesianReasoningandMachineLearning. CambridgeUniversity\nPress,2012.",
    "[Barham et al., 2022] Paul Barham, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven\nHand,DanHurt,MichaelIsard,HyeontaekLim,RuomingPang,SudipRoy,BrennanSaeta,Parker\nSchuh,RyanSepassi,LaurentElShafey,ChandramohanA.Thekkath,andYonghuiWu. Pathways:\nAsynchronous distributed dataflow for ml. In Proceedings of Machine Learning and Systems,\nvolume4,pages430–449,2022.\n[BaroniandLenci,2010] MarcoBaroniandAlessandroLenci. Distributionalmemory: Ageneral\nframeworkforcorpus-basedsemantics. ComputationalLinguistics,36(4):673–721,2010.\n[Baronietal.,2014] MarcoBaroni,GeorgianaDinu,andGermánKruszewski. Don’tcount,predict! a\nsystematiccomparisonofcontext-countingvs.context-predictingsemanticvectors. InProceedings\nof the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers),pages238–247,2014.\n[Barrault et al., 2020] Loïc Barrault, Magdalena Biesialska, Ondˇrej Bojar, Marta R. Costa-jussà,",
    "ChristianFedermann,YvetteGraham,RomanGrundkiewicz,BarryHaddow,MatthiasHuck,Eric\nJoanis,TomKocmi,PhilippKoehn,Chi-kiuLo,NikolaLjubešic´,ChristofMonz,MakotoMorishita,\nMasaakiNagata,ToshiakiNakazawa,SantanuPal,MattPost,andMarcosZampieri. Findingsof",
    "BIBLIOGRAPHY 639\nthe2020conferenceonmachinetranslation(WMT20). InProceedingsoftheFifthConferenceon\nMachineTranslation,pages1–55,2020.\n[Baum and Petrie, 1966] Leonard E Baum and Ted Petrie. Statistical inference for probabilistic\nfunctionsoffinitestatemarkovchains. Theannalsofmathematicalstatistics,37(6):1554–1563,\n1966.\n[Baumetal.,1970] LeonardEBaum,TedPetrie,GeorgeSoules,andNormanWeiss. Amaximization\ntechniqueoccurringinthestatisticalanalysisofprobabilisticfunctionsofmarkovchains. Theannals\nofmathematicalstatistics,41(1):164–171,1970.\n[Belinkov, 2022] Yonatan Belinkov. Probing classifiers: Promises, shortcomings, and advances.\nComputationalLinguistics,48(1):207–219,2022.\n[BelinkovandBisk,2018] YonatanBelinkovandYonatanBisk. Syntheticandnaturalnoisebothbreak\nneuralmachinetranslation. InInternationalConferenceonLearningRepresentations,2018.\n[Bello,2020] IrwanBello. Lambdanetworks: Modelinglong-rangeinteractionswithoutattention. In",
    "ProceedingsofInternationalConferenceonLearningRepresentations,2020.\n[Beltagy et al., 2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-\ndocumenttransformer. arXiv:2004.05150,2020.\n[Bengioetal.,2015] EmmanuelBengio,Pierre-LucBacon,JoellePineau,andDoinaPrecup. Condi-\ntionalcomputationinneuralnetworksforfastermodels. arXivpreprintarXiv:1511.06297,2015.\n[Bengio,1991] YoshuaBengio.Artificialneuralnetworksandtheirapplicationtosequencerecognition.\nPhDthesis,McGillUniversity,1991.\n[Bengio et al., 1994] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term\ndependencieswithgradientdescentisdifficult. IEEEtransactionsonneuralnetworks,5(2):157–166,\n1994.\n[Bengioetal.,2000] YoshuaBengio,RéjeanDucharme,andPascalVincent. Aneuralprobabilistic\nlanguagemodel. AdvancesinNeuralInformationProcessingSystems,13,2000.\n[Bengioetal.,2003] YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJauvin. Aneural",
    "probabilisticlanguagemodel. JournalofMachineLearningResearch,3:1137–1155,2003a.\n[Bengioetal.,2003] YoshuaBengio,RéjeanDucharme,PascalVincent,andChristianJauvin. Aneural\nprobabilisticlanguagemodel. JournalofMachineLearningResearch,3(Feb):1137–1155,2003b.\n[Bengioetal.,2006] YoshuaBengio,PascalLamblin,DanPopovici,andHugoLarochelle. Greedy\nlayer-wisetrainingofdeepnetworks. Advancesinneuralinformationprocessingsystems,19,2006.\n[Bengioetal.,2013] YoshuaBengio,NicholasLéonard,andAaronCourville.Estimatingorpropagating\ngradientsthroughstochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432,\n2013.\n[Bengioetal.,2024] YoshuaBengio,GeoffreyHinton,AndrewYao,DawnSong,PieterAbbeel,Trevor\nDarrell,YuvalNoahHarari,Ya-QinZhang,LanXue,ShaiShalev-Shwartz,GillianK.Hadfield,Jeff\nClune,TeganMaharaj,FrankHutter,AtilimGunesBaydin,SheilaA.McIlraith,QiqiGao,Ashwin\nAcharya,DavidKrueger,AncaDragan,PhilipTorr,StuartRussell,DanielKahneman,JanMarkus",
    "Brauner,andSörenMindermann. Managingextremeairisksamidrapidprogress. Science,384\n(6698):842–845,2024.\n[BentivogliandGiampiccolo,2011] LuisaBentivogliandDaniloGiampiccolo. Pascalrecognizing",
    "640 BIBLIOGRAPHY\ntextualentailmentchallenge(rte-7)attac2011. https://tac.nist.gov/2011/RTE/,2011.\n[Berg-Kirkpatricketal.,2012] TaylorBerg-Kirkpatrick,DavidBurkett,andDanKlein. Anempirical\ninvestigation of statistical significance inNLP. In Proceedings of the2012 Joint Conference on\nEmpiricalMethodsinNaturalLanguageProcessingandComputationalNaturalLanguageLearning,\npages995–1005,2012.\n[Bestaetal.,2024] MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,MichalPodstawski,\nLukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and\nTorstenHoefler. Graphofthoughts: Solvingelaborateproblemswithlargelanguagemodels. In\nProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages17682–17690,2024.\n[Bhattamishraetal.,2020] SatwikBhattamishra,KabirAhuja,andNavinGoyal. Ontheabilityand\nlimitationsoftransformerstorecognizeformallanguages. InProceedingsofthe2020Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages7096–7116,2020.",
    "[Bhattasalietal.,2020] ShohiniBhattasali,JonathanBrennan,Wen-MingLuh,BertaFranzluebbers,\nandJohnHale. Thealicedatasets: fMRI&EEGobservationsofnaturallanguagecomprehension.\nInProceedingsofthe12thLanguageResourcesandEvaluationConference,pages120–125,2020.\n[BickelandDoksum,2015] PeterJBickelandKjellADoksum. Mathematicalstatistics: basicideas\nandselectedtopics,volumesI-IIpackage. ChapmanandHall/CRC,2015.\n[Bidermanetal.,2021] StellaBiderman,SidBlack,CharlesFoster,LeoGao,EricHallahan,Horace\nHe, Ben Wang, and Phil Wang. Rotary embeddings: A relative revolution. https://blog.\neleuther.ai/rotary-embeddings/,2021.\n[Birchetal.,2018] AlexandraBirch,AndrewFinch,Minh-ThangLuong,GrahamNeubig,andYusuke\nOda. Findingsofthesecondworkshoponneuralmachinetranslationandgeneration. InProceedings\nofthe2ndWorkshoponNeuralMachineTranslationandGeneration,pages1–10,2018.\n[Bishop,1995] ChristopherBishop. Regularizationandcomplexitycontrolinfeed-forwardnetworks.",
    "InProceedingsInternationalConferenceonArtificialNeuralNetworksICANN’95,pages141–148,\n1995a.\n[Bishop,1995] ChristopherM.Bishop. Trainingwithnoiseisequivalenttotikhonovregularization.\nNeuralcomputation,7(1):108–116,1995b.\n[Bishop,2006] ChristopherM.Bishop. PatternRecognitionandMachineLearning. Springer,2006.\n[Blacoe and Lapata, 2012] William Blacoe and Mirella Lapata. A comparison of vector-based\nrepresentationsforsemanticcomposition. InProceedingsofthe2012jointconferenceonempirical\nmethodsinnaturallanguageprocessingandcomputationalnaturallanguagelearning,pages546–\n556,2012.\n[Blei,2012] DavidMBlei. Probabilistictopicmodels. CommunicationsoftheACM,55(4):77–84,\n2012.\n[Bleietal., 2003] DavidMBlei, AndrewYNg, andMichaelIJordan. Latentdirichletallocation.\nJournalofmachineLearningresearch,3(Jan):993–1022,2003.\n[BlumandMitchell,1998] AvrimBlumandTomMitchell. Combininglabeledandunlabeleddatawith\nco-training. InProceedingsoftheeleventhannualconferenceonComputationallearningtheory,",
    "pages92–100,1998.\n[Bojanowski et al., 2017] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov.\nEnrichingwordvectorswithsubwordinformation. Transactionsoftheassociationforcomputational",
    "BIBLIOGRAPHY 641\nlinguistics,5:135–146,2017.\n[Bommasanietal.,2021] RishiBommasani,DrewA.Hudson,EhsanAdeli,RussAltman,Simran\nArora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun-\nskill, Erik Brynjolfsson, S. Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S.\nChen,KathleenA.Creel,JaredDavis,DoraDemszky,ChrisDonahue,MoussaDoumbouya,Esin\nDurmus,StefanoErmon,JohnEtchemendy,KawinEthayarajh,LiFei-Fei,ChelseaFinn,Trevor\nGale,LaurenE.Gillespie,KaranGoel,NoahD.Goodman,ShelbyGrossman,NeelGuha,Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,\nThomasF.Icard,SaahilJain,DanJurafsky,PratyushaKalluri,SiddharthKaramcheti,GeoffKeeling,\nFereshte Khani, O. Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,\nAnanyaKumar,FaisalLadhak,MinaLee,TonyLee,JureLeskovec,IsabelleLevent,XiangLisaLi,\nXuechenLi,TengyuMa,AliMalik,ChristopherD.Manning,SuvirP.Mirchandani,EricMitchell,",
    "ZaneleMunyikwa,SurajNair,AvanikaNarayan,DeepakNarayanan,BenjaminNewman,AllenNie,\nJuanCarlosNiebles,HamedNilforoshan,J.F.Nyarko,GirayOgut,LaurelOrr,IsabelPapadimitriou,\nJoonSungPark,ChrisPiech,EvaPortelance,ChristopherPotts,AditiRaghunathan,RobertReich,\nHongyuRen, FriedaRong, YusufH.Roohani, CamiloRuiz, JackRyan, ChristopherR’e, Dorsa\nSadigh,ShioriSagawa,KeshavSanthanam,AndyShih,KrishnaParasuramSrinivasan,AlexTamkin,\nRohanTaori,ArminW.Thomas,FlorianTramèr,RoseE.Wang,WilliamWang,BohanWu,Jiajun\nWu,YuhuaiWu,SangMichaelXie,MichihiroYasunaga,JiaxuanYou,MateiA.Zaharia,Michael\nZhang,TianyiZhang,XikunZhang,YuhuiZhang,LuciaZheng,KaitlynZhou,andPercyLiang. On\ntheopportunitiesandrisksoffoundationmodels. ArXiv,2021.\n[Bondarenkoetal.,2021] YelyseiBondarenko,MarkusNagel,andTijmenBlankevoort. Understanding\nandovercomingthechallengesofefficienttransformerquantization. InProceedingsofthe2021\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages7947–7969,2021.",
    "[BorjiandItti,2012] AliBorjiandLaurentItti. State-of-the-artinvisualattentionmodeling. IEEE\ntransactionsonpatternanalysisandmachineintelligence,35(1):185–207,2012.\n[Boulanger-Lewandowskietal., 2013] NicolasBoulanger-Lewandowski, YoshuaBengio, andPas-\ncal Vincent. Audio chord recognition with recurrent neural networks. In Proceedings of 14th\nInternationalSocietyforMusicInformationRetrievalConference,2013.\n[BourlardandWellekens,1990] HerveBourlardandChristianJWellekens. Linksbetweenmarkov\nmodelsandmultilayerperceptrons. IEEETransactionsonpatternanalysisandmachineintelligence,\n12(12):1167–1178,1990.\n[BourlardandMorgan,1993] HerveA.BourlardandNelsonMorgan. ConnectionistSpeechRecogni-\ntion: AHybridApproach. KluwerAcademicPublishers,USA,1993.\n[Boxetal.,2015] GeorgeEPBox,GwilymMJenkins,GregoryCReinsel,andGretaMLjung. Time\nseriesanalysis: forecastingandcontrol(4thed.). JohnWiley&Sons,2015.\n[BradleyandTerry,1952] RalphAllanBradleyandMiltonE.Terry. Rankanalysisofincompleteblock",
    "designs: I.themethodofpairedcomparisons. Biometrika,39(3/4):324–345,1952.\n[Brandonetal.,2024] WilliamBrandon,MayankMishra,AniruddhaNrusimha,RameswarPanda,and\nJonathanRaganKelly. Reducingtransformerkey-valuecachesizewithcross-layerattention. arXiv\npreprintarXiv:2405.12981,2024.\n[Breiman,1996] LeoBreiman. Baggingpredictors. MachineLearning,24(2):123–140,1996.\n[Brill,1992] EricBrill. Asimplerule-basedpartofspeechtagger. InSpeechandNaturalLanguage:",
    "642 BIBLIOGRAPHY\nProceedingsofaWorkshopHeldatHarriman,NewYork,February23-26,1992,1992.\n[Briski,2025] KariBriski. Howscalinglawsdrivesmarter,morepowerfulai,2025.\n[Brown et al., 2024] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le,\nChristopherRé,andAzaliaMirhoseini. Largelanguagemonkeys: Scalinginferencecomputewith\nrepeatedsampling. arXivpreprintarXiv:2407.21787,2024.\n[Brownetal.,1993] PeterF.Brown,StephenA.DellaPietra,VincentJ.DellaPietra,andRobertL.\nMercer. Themathematicsofstatisticalmachinetranslation: Parameterestimation. Computational\nLinguistics,19(2):263–311,1993.\n[Brownetal.,2020] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,\nPrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDanielZiegler,JeffreyWu,ClemensWinter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,",
    "Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners. Advancesinneural\ninformationprocessingsystems,33:1877–1901,2020.\n[Bubeck et al., 2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,\nEric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori,\nHamidPalangi,MarcoTúlioRibeiro,andYiZhang. Sparksofartificialgeneralintelligence: Early\nexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.\n[Buckman et al., 2016] Jacob Buckman, Miguel Ballesteros, and Chris Dyer. Transition-based\ndependencyparsingwithheuristicbacktracking. InProceedingsofthe2016Conferenceonempirical\nmethodsinnaturallanguageprocessing,pages2313–2318,2016.\n[Bulatovetal.,2022] AydarBulatov,YuryKuratov,andMikhailBurtsev. Recurrentmemorytrans-\nformer. AdvancesinNeuralInformationProcessingSystems,35:11079–11091,2022.",
    "[BurchiandVielzeuf,2021] MaximeBurchiandValentinVielzeuf. Efficientconformer: Progressive\ndownsamplingandgroupedattentionforautomaticspeechrecognition. InProceedingsof2021IEEE\nAutomaticSpeechRecognitionandUnderstandingWorkshop(ASRU),pages8–15.IEEE,2021.\n[Burgesetal.,2005] ChrisBurges,TalShaked,ErinRenshaw,AriLazier,MattDeeds,NicoleHamilton,\nandGregHullender.Learningtorankusinggradientdescent.InProceedingsofthe22ndinternational\nconferenceonMachinelearning,pages89–96,2005.\n[BurnhamandAnderson,2002] KennethP.BurnhamandDavidR.Anderson. Modelselectionand\nmultimodelinference: apracticalinformation-theoreticapproach. Spinger,2002.\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao,\nLeopoldAschenbrenner,YiningChen,AdrienEcoffet,ManasJoglekar,JanLeike,IlyaSutskever,\nandJeffWu. Weak-to-stronggeneralization: Elicitingstrongcapabilitieswithweaksupervision.\narXivpreprintarXiv:2312.09390,2023a.",
    "[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschenbrenner, Jeffrey Wu, Pavel Izmailov,\nLeoGao,BowenBaker,andJanHendrikKirchner. Weak-to-stronggeneralization,2023b. URL\nhttps://https://openai.com/index/weak-to-strong-generalization.\n[Buttcher et al., 2016] Stefan Buttcher, Charles LA Clarke, and Gordon V Cormack. Information\nretrieval: Implementingandevaluatingsearchengines. MITPress,2016.\n[Caballeroetal.,2023] EthanCaballero,KshitijGupta,IrinaRish,andDavidKrueger. Brokenneural",
    "BIBLIOGRAPHY 643\nscalinglaws. InICLR2023WorkshoponMathematicalandEmpiricalUnderstandingofFoundation\nModels,2023.\n[Campbell,1997] JosephPCampbell. Speakerrecognition: Atutorial. ProceedingsoftheIEEE,85(9):\n1437–1462,1997.\n[Caoetal.,2007] ZheCao,TaoQin,Tie-YanLiu,Ming-FengTsai,andHangLi. Learningtorank:\nfrompairwiseapproachtolistwiseapproach. InProceedingsofthe24thinternationalconferenceon\nMachinelearning,pages129–136,2007.\n[Caronetal.,2021] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,Piotr\nBojanowski,andArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. In\nProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages9650–9660,2021.\n[Casacubertaetal.,2009] FranciscoCasacuberta,JorgeCivera,ElsaCubel,AntonioLLagarda,Guy\nLapalme, Elliott Macklovitch, and Enrique Vidal. Human interaction for high-quality machine\ntranslation. CommunicationsoftheACM,52(10):135–138,2009.\n[Ceretal.,2018] DanielCer,YinfeiYang,ShengyiKong,NanHua,NicoleLimtiaco,RhomniSt.",
    "John,NoahConstant,MarioGuajardo-Cespedes,SteveYuan,ChrisTar,Yun-HsuanSung,Brian\nStrope,andRayKurzweil. Universalsentenceencoder. arXivpreprintarXiv:1803.11175,2018.\n[Chanetal.,2016] WilliamChan,NavdeepJaitly,QuocLe,andOriolVinyals. Listen,attendandspell:\nAneuralnetworkforlargevocabularyconversationalspeechrecognition. In2016IEEEinternational\nconferenceonacoustics,speechandsignalprocessing(ICASSP),pages4960–4964.IEEE,2016.\n[Changetal.,2024] KaiyanChang,SongchengXu,ChenglongWang,YingfengLuo,TongXiao,and\nJingbo Zhu. Efficient prompting methods for large language models: A survey. arXiv preprint\narXiv:2404.01077,2024.\n[Chang,1967] Wing-TsitChang. Reflectionsonthingsathand. ColumbiaUniversityPress,1967.\n[ChangandCollins,2011] Yin-WenChangandMichaelCollins. Exactdecodingofphrase-basedtrans-\nlationmodelsthroughlagrangianrelaxation. InProceedingsofthe2011ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing,pages26–37,2011.",
    "[Charniak,1997] EugeneCharniak. Statisticalparsingwithacontext-freegrammarandwordstatistics.\nAAAI/IAAI,2005(598-603):18,1997.\n[Chatfield,2003] ChrisChatfield. Theanalysisoftimeseries: anintroduction. Chapmanandhall/CRC,\n2003.\n[Chaudhari et al., 2021] Sneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath.\nAnattentivesurveyofattentionmodels. ACMTransactionsonIntelligentSystemsandTechnology\n(TIST),12(5):1–32,2021.\n[Chenetal.,2023] BanghaoChen,ZhaofengZhang,NicolasLangrené,andShengxinZhu. Unleashing\nthe potential of prompt engineering in large language models: a comprehensive review. arXiv\npreprintarXiv:2310.14735,2023a.\n[Chenetal.,2018] KehaiChen,RuiWang,MasaoUtiyama,EiichiroSumita,andTiejunZhao. Syntax-\ndirectedattentionforneuralmachinetranslation. InProceedingsoftheAAAIconferenceonartificial\nintelligence,2018a.\n[Chenetal.,2023] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,VikasYadav,\nZhengTang,VijaySrinivasan,TianyiZhou,HengHuang,andHongxiaJin. Alpagasus: Traininga",
    "betteralpacawithfewerdata. arXivpreprintarXiv:2307.08701,2023b.",
    "644 BIBLIOGRAPHY\n[Chenetal.,2024] LichangChen,ShiyangLi,JunYan,HaiWang,KalpaGunaratna,VikasYadav,\nZhengTang,VijaySrinivasan,TianyiZhou,HengHuang,andHongxiaJin. Alpagasus: Traininga\nbetteralpacawithfewerdata. InTheTwelfthInternationalConferenceonLearningRepresentations,\n2024a.\n[Chenetal.,2020] MarkChen,AlecRadford,RewonChild,JeffreyWu,HeewooJun,DavidLuan,and\nIlyaSutskever. Generativepretrainingfrompixels. InInternationalconferenceonmachinelearning,\npages1691–1703.PMLR,2020a.\n[Chenetal.,2018] MiaXuChen,OrhanFirat,AnkurBapna,MelvinJohnson,WolfgangMacherey,\nGeorgeFoster,LlionJones,MikeSchuster,NoamShazeer,NikiParmar,AshishVaswani,Jakob\nUszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. The best of both\nworlds: Combiningrecentadvancesinneuralmachinetranslation. InProceedingsofthe56thAnnual\nMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages76–86,\n2018b.\n[Chenetal.,2018] RickyTQChen,YuliaRubanova,JesseBettencourt,andDavidKDuvenaud. Neural",
    "ordinarydifferentialequations. Advancesinneuralinformationprocessingsystems,31,2018c.\n[Chenetal.,2022] SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,ZhuoChen,\nJinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,LongZhou,ShuoRen,YanminQian,Yao\nQian,JianWu,MichaelZeng,andFuruWei.Wavlm:Large-scaleself-supervisedpre-trainingforfull\nstackspeechprocessing. IEEEJournalofSelectedTopicsinSignalProcessing,16(6):1505–1518,\n2022.\n[Chen et al., 2023] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Ex-\ntending context window of large language models via positional interpolation. arXiv preprint\narXiv:2306.15595,2023c.\n[ChenandGoodman,1999] StanleyF.ChenandJoshuaGoodman. Anempiricalstudyofsmoothing\ntechniquesforlanguagemodeling. ComputerSpeechandLanguage,13:359–394,1999.\n[Chenetal.,2020] TianlongChen,JonathanFrankle,ShiyuChang,SijiaLiu,YangZhang,Zhangyang\nWang,andMichaelCarbin. Thelotterytickethypothesisforpre-trainedbertnetworks. Advancesin",
    "neuralinformationprocessingsystems,33:15834–15846,2020b.\n[Chenetal.,2015] TianqiChen,IanGoodfellow,andJonathonShlens. Net2net: Acceleratinglearning\nviaknowledgetransfer. arXivpreprintarXiv:1511.05641,2015.\n[ChenandHe,2021] XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning.\nInProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages\n15750–15758,2021.\n[Chenetal.,2020] Yen-ChunChen,LinjieLi,LichengYu,AhmedElKholy,FaisalAhmed,ZheGan,\nYuCheng,andJingjingLiu. Uniter: Universalimage-textrepresentationlearning. InProceedingsof\nEuropeanconferenceoncomputervision,pages104–120,2020c.\n[Chen et al., 2024] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu.\nSelf-playfine-tuningconvertsweaklanguagemodelstostronglanguagemodels. arXivpreprint\narXiv:2401.01335,2024b.\n[Chevalieretal.,2023] AlexisChevalier,AlexanderWettig,AnirudhAjith,andDanqiChen. Adapting\nlanguagemodelstocompresscontexts.InProceedingsofthe2023ConferenceonEmpiricalMethods",
    "inNaturalLanguageProcessing,pages3829–3846,2023.",
    "BIBLIOGRAPHY 645\n[Chietal.,2022] Ta-ChungChi,Ting-HanFan,PeterJRamadge,andAlexanderRudnicky. Kerple:\nKernelizedrelativepositionalembeddingforlengthextrapolation. AdvancesinNeuralInformation\nProcessingSystems,35:8386–8399,2022.\n[Chietal.,2023] Ta-ChungChi,Ting-HanFan,AlexanderRudnicky,andPeterRamadge. Dissecting\ntransformerlengthextrapolationviathelensofreceptivefieldanalysis. InProceedingsofthe61st\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages\n13522–13537,2023.\n[Chiang,2005] DavidChiang. Ahierarchicalphrase-basedmodelforstatisticalmachinetranslation. In\nProceedingsofthe43rdannualmeetingoftheassociationforcomputationallinguistics(acl’05),\npages263–270,2005.\n[Chiang,2007] DavidChiang. Hierarchicalphrase-basedtranslation. computationallinguistics,33(2):\n201–228,2007.\n[ChiangandCholak,2022] DavidChiangandPeterCholak. Overcomingatheoreticallimitationof\nself-attention. arXivpreprintarXiv:2202.12172,2022.",
    "[Chiangetal.,2023] DavidChiang,PeterCholak,andAnandPillay. Tighterboundsontheexpressivity\noftransformerencoders. arXivpreprintarXiv:2301.10743,2023a.\n[Chiangetal.,2023] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,\nLianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing.\nVicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023b. URL\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\n[Child et al., 2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long\nsequenceswithsparsetransformers. arXivpreprintarXiv:1904.10509,2019.\n[ChiuandRaffel,2018] Chung-ChengChiuandColinRaffel. Monotonicchunkwiseattention. In\nProceedingsofthe8thInternationalConferenceonLearningRepresentationsICLR,2018.\n[Choetal.,2021] JaeminCho,JieLei,HaoTan,andMohitBansal. Unifyingvision-and-languagetasks\nviatextgeneration. InInternationalConferenceonMachineLearning,pages1931–1942.PMLR,\n2021.",
    "[ChoandEsipova,2016] KyunghyunChoandMashaEsipova. Canneuralmachinetranslationdo\nsimultaneoustranslation? arXivpreprintarXiv:1606.02012,2016.\n[Cho et al., 2014] Kyunghyun Cho, Bart van Merriënboer, Çag˘lar Guu˙lçehre, Dzmitry Bahdanau,\nFethiBougares,HolgerSchwenk,andYoshuaBengio. Learningphraserepresentationsusingrnn\nencoder–decoder for statistical machine translation. In Proceedings of the 2014 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1724–1734,2014.\n[ChoeandCharniak,2016] DoKookChoeandEugeneCharniak. Parsingaslanguagemodeling. In\nProceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages\n2331–2336,2016.\n[Chollet,2021] FrançoisChollet. DeepLearningwithPython(2nded.). ManningPublications,2021.\n[Choromanski et al., 2020] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan,\nXingyouSong,AndreeaGane,TamásSarlós,PeterHawkins,JaredQuincyDavis,AfrozMohiuddin,",
    "LukaszKaiser,DavidBenjaminBelanger,LucyJ.Colwell,andAdrianWeller. Rethinkingattention\nwithperformers. InProceedingsofInternationalConferenceonLearningRepresentations,2020.\n[ChorowskiandJaitly,2017] JanChorowskiandNavdeepJaitly. Towardsbetterdecodingandlanguage",
    "646 BIBLIOGRAPHY\nmodelintegrationinsequencetosequencemodels. Proc.Interspeech2017,pages523–527,2017.\n[Chorowski et al., 2019] Jan Chorowski, Ron J Weiss, Samy Bengio, and Aäron Van Den Oord.\nUnsupervisedspeechrepresentationlearningusingwavenetautoencoders. IEEE/ACMtransactions\nonaudio,speech,andlanguageprocessing,27(12):2041–2053,2019.\n[Chorowskietal.,2015] JanKChorowski,DzmitryBahdanau,DmitriySerdyuk,KyunghyunCho,and\nYoshuaBengio. Attention-basedmodelsforspeechrecognition. Advancesinneuralinformation\nprocessingsystems,28,2015.\n[Chowdheryetal.,2022] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,Gau-\nravMishra,AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,\nParkerSchuh,KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,ParkerBarnes,\nYiTay,NoamShazeer,VinodkumarPrabhakaran,EmilyReif,NanDu,BenHutchinson,ReinerPope,\nJamesBradbury,JacobAustin,MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,Anselm",
    "Levskaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,Kevin\nRobinson,LiamFedus,DennyZhou,DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph,\nAlexanderSpiridonov,RyanSepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM.\nDai,ThanumalayanSankaranarayanaPillai,MariePellat,AitorLewkowycz,EricaMoreira,Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, OrhanFirat, MicheleCatasta, JasonWei, KathyMeier-Hellstern, DouglasEck, JeffDean,\nSlavPetrov, andNoahFiedel. Palm: Scalinglanguagemodelingwithpathways. arXivpreprint\narXiv:2204.02311,2022.\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg,\nand Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural\ninformationprocessingsystems,30,2017.\n[Chuetal.,2023] ZhengChu,JingchangChen,QianglongChen,WeijiangYu,TaoHe,HaotianWang,\nWeihuaPeng,MingLiu,BingQin,andTingLiu. Asurveyofchainofthoughtreasoning: Advances,",
    "frontiersandfuture. arXivpreprintarXiv:2309.15402,2023.\n[Chuangetal.,2020] Yung-SungChuang,Chi-LiangLiu,Hung-yiLee,andLin-shanLee. Speechbert:\nAnaudio-and-textjointlylearnedlanguagemodelforend-to-endspokenquestionanswering. In\nProceedingsofInterspeech2020,pages4168–4172,2020.\n[Chungetal.,2022] HyungWonChung,LeHou,S.Longpre,BarretZoph,YiTay,WilliamFedus,\nEricLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,AlbertWebson,ShixiangShaneGu,\nZhuyunDai,MiracSuzgun,XinyunChen,AakankshaChowdhery,DashaValter,SharanNarang,\nGauravMishra,AdamsWeiYu,VincentZhao,YanpingHuang,AndrewM.Dai,HongkunYu,Slav\nPetrov,EdHuaihsinChi,JeffDean,JacobDevlin,AdamRoberts,DennyZhou,QuocV.Le,and\nJasonWei. Scalinginstruction-finetunedlanguagemodels. arXivpreprintarXiv:2210.11416,2022.\n[Chungetal.,2014] JunyoungChung,CaglarGulcehre,KyunghyunCho,andYoshuaBengio. Empiri-\ncalevaluationofgatedrecurrentneuralnetworksonsequencemodeling. InProceedingsofNIPS\n2014WorkshoponDeepLearning,December2014,2014.",
    "[Church,2011] KennethChurch.Apendulumswungtoofar.LinguisticIssuesinLanguageTechnology,\n6,2011.\n[ChurchandHanks,1990] KennethWardChurchandPatrickHanks. Wordassociationnorms,mutual\ninformation, and lexicography. Computational Linguistics, 16(1):22–29, 1990. URL https:\n//aclanthology.org/J90-1003.",
    "BIBLIOGRAPHY 647\n[Clark et al., 2019] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning.\nWhatdoesbertlookat? ananalysisofbert’sattention. InProceedingsofthe2019ACLWorkshop\nBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages276–286,2019a.\n[Clarketal.,2019] KevinClark,Minh-ThangLuong,QuocVLe,andChristopherDManning. Electra:\nPre-trainingtextencodersasdiscriminatorsratherthangenerators. InProceedingsofInternational\nConferenceonLearningRepresentations,2019b.\n[Clarketal.,2008] StephenClark,BobCoecke,andMehrnooshSadrzadeh. Acompositionaldistribu-\ntionalmodelofmeaning. InProceedingsoftheSecondQuantumInteractionSymposium(QI-2008),\npages133–140.Oxford,2008.\n[ClineandDhillon,2014] AlanKaylorClineandInderjitS.Dhillon. Computationofthesingularvalue\ndecomposition. InLeslieHogben,editor,HandbookofLinearAlgebra(2dned.).CRCPress,2014.\n[Cobbeetal.,2021] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,",
    "Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168,2021.\n[Cohnetal.,2016] TrevorCohn,CongDuyVuHoang,EkaterinaVymolova,KaishengYao,Chris\nDyer,andGholamrezaHaffari. Incorporatingstructuralalignmentbiasesintoanattentionalneural\ntranslationmodel. InProceedingsofthe2016ConferenceoftheNorthAmericanChapterofthe\nAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages876–885,2016.\n[CollobertandWeston,2008] RonanCollobertandJasonWeston. Aunifiedarchitecturefornatural\nlanguage processing: deep neural networks with multitask learning. In Proceedings of the 25th\ninternationalconferenceonMachinelearning(ICML08),pages160–167,2008.\n[Conneauetal.,2017] AlexisConneau,DouweKiela,HolgerSchwenk,LoïcBarrault,andAntoine\nBordes. Supervisedlearningofuniversalsentencerepresentationsfromnaturallanguageinference",
    "data. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,\npages670–680,2017a.\n[Conneauetal.,2017] AlexisConneau,DouweKiela,HolgerSchwenk,LoïcBarrault,andAntoine\nBordes. Supervisedlearningofuniversalsentencerepresentationsfromnaturallanguageinference\ndata. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,\npages670–680,2017b.\n[Conneauetal.,2017] AlexisConneau,HolgerSchwenk,LoïcBarrault,andYannLecun. Verydeep\nconvolutionalnetworksfortextclassification. InProceedingsofthe15thConferenceoftheEuropean\nChapteroftheAssociationforComputationalLinguistics: Volume1,LongPapers,pages1107–1116,\n2017c.\n[Conneauetal.,2018] AlexisConneau,GermánKruszewski,GuillaumeLample,LoïcBarrault,and\nMarcoBaroni. Whatyoucancramintoasinglevector: Probingsentenceembeddingsforlinguistic\nproperties. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics(Volume1: LongPapers),pages2126–2136,2018.",
    "[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary,\nGuillaumeWenzek,FranciscoGuzmán,ÉdouardGrave,MyleOtt,LukeZettlemoyer,andVeselin\nStoyanov. Unsupervisedcross-lingualrepresentationlearningatscale. InProceedingsofthe58th\nAnnualMeetingoftheAssociationforComputationalLinguistics,pages8440–8451,2020.\n[Cortes and Vapnik, 1995] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Mind,",
    "648 BIBLIOGRAPHY\nMachineLearning:273–297,1995.\n[Costeetal.,2024] ThomasCoste,UsmanAnwar,RobertKirk,andDavidKrueger. Rewardmodel\nensembleshelpmitigateoveroptimization. InTheTwelfthInternationalConferenceonLearning\nRepresentations,2024.\n[CotterellandSchütze,2015] RyanCotterellandHinrichSchütze. Morphologicalword-embeddings.\nIn Proceedings of the 2015 Conference of the North American Chapter of the Association for\nComputationalLinguistics: HumanLanguageTechnologies,pages1287–1292,2015.\n[Cuietal.,2024] GanquCui,LifanYuan,NingDing,GuanmingYao,BingxiangHe,WeiZhu,Yuan\nNi,GuotongXie,RuobingXie,YankaiLin,ZhiyuanLiu,andMaosongSun. ULTRAFEEDBACK:\nBoosting language models with scaled AI feedback. In Proceedings of the 41st International\nConferenceonMachineLearning,volume235,pages9722–9744,2024.\n[CurreyandHeafield,2018] AnnaCurreyandKennethHeafield. Multi-sourcesyntacticneuralmachine\ntranslation. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing,pages2961–2966,2018.",
    "[Cybenko,1989] GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathe-\nmaticsofcontrol,signalsandsystems,2(4):303–314,1989.\n[Daietal.,2023] DamaiDai,YutaoSun,LiDong,YaruHao,ShumingMa,ZhifangSui,andFuruWei.\nWhycangptlearnin-context? languagemodelssecretlyperformgradientdescentasmeta-optimizers.\nInFindingsoftheAssociationforComputationalLinguistics: ACL2023,pages4005–4019,2023.\n[Daietal.,2019] ZihangDai,ZhilinYang,YimingYang,JaimeGCarbonell,QuocLe,andRuslan\nSalakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In\nProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages\n2978–2988,2019.\n[Daoetal.,2022] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention:\nFast and memory-efficient exact attention with io-awareness. Advances in Neural Information\nProcessingSystems,35:16344–16359,2022.\n[Daoetal.,2023] TriDao,DanielHaziza,FranciscoMassa,andGrigorySizov.Flash-decodingforlong-",
    "context inference. https://pytorch.org/blog/flash-decoding/, 2023. Retrieved\n2023-10-23.\n[DavisandMermelstein,1980] StevenDavisandPaulMermelstein. Comparisonofparametricrepre-\nsentationsformonosyllabicwordrecognitionincontinuouslyspokensentences. IEEEtransactions\nonacoustics,speech,andsignalprocessing,28(4):357–366,1980.\n[Dayanetal.,1995] PeterDayan,GeoffreyEHinton,RadfordMNeal,andRichardSZemel. The\nhelmholtzmachine. Neuralcomputation,7(5):889–904,1995.\n[deGispertetal.,2010] AdriàdeGispert,GonzaloIglesias,GraemeBlackwood,EduardoR.Banga,\nandWilliamByrne. Hierarchicalphrase-basedtranslationwithweightedfinite-statetransducersand\nshallow-ngrammars. Computationallinguistics,36(3):505–533,2010.\n[Deepseek,2025] Deepseek. Deepseek-r1: Incentivizingreasoningcapabilityinllmsviareinforcement\nlearning. arXivpreprintarXiv:2501.12948,2025.\n[Deerwesteretal.,1990] ScottDeerwester,SusanTDumais,GeorgeWFurnas,ThomasKLandauer,",
    "andRichardHarshman. Indexingbylatentsemanticanalysis. JournaloftheAmericansocietyfor\ninformationscience,41(6):391–407,1990.",
    "BIBLIOGRAPHY 649\n[Dehghanietal.,2018] MostafaDehghani,StephanGouws,OriolVinyals,JakobUszkoreit,andŁukasz\nKaiser. Universaltransformers. arXivpreprintarXiv:1807.03819,2018.\n[Del Corro et al., 2023] Luciano Del Corro, Allie Del Giorno, Sahaj Agarwal, Bin Yu, Ahmed\nAwadallah,andSubhabrataMukherjee. Skipdecode: Autoregressiveskipdecodingwithbatching\nandcachingforefficientllminference. arXivpreprintarXiv:2307.02628,2023.\n[Deletangetal.,2024] GregoireDeletang,AnianRuoss,Paul-AmbroiseDuquenne,ElliotCatt,Tim\nGenewein,ChristopherMattern,JordiGrau-Moya,LiKevinWenliang,MatthewAitchison,Laurent\nOrseau, Marcus Hutter, and Joel Veness. Language modeling is compression. In The Twelfth\nInternationalConferenceonLearningRepresentations,2024.\n[Dempsteretal.,1977] ArthurPDempster,NanMLaird,andDonaldBRubin. Maximumlikelihood\nfrom incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B\n(Methodological),39(1):1–22,1977.",
    "[Dengetal.,2022] MingkaiDeng,JianyuWang,Cheng-PingHsieh,YihanWang,HanGuo,Tianmin\nShu, Meng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with\nreinforcementlearning. InProceedingsofthe2022ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages3369–3391,2022.\n[Deorasetal.,2011] AnoopDeoras,TomášMikolov,andKennethChurch. Afastre-scoringstrategyto\ncapturelong-distancedependencies. InProceedingsofthe2011ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages1116–1127,2011.\n[Devereuxetal.,2010] BarryDevereux,ColinKelly,andAnnaKorhonen. Usingfmriactivationto\nconceptualstimulitoevaluatemethodsforextractingconceptualrepresentationsfromcorpora. In\nProceedingsoftheNAACLHLT2010FirstWorkshoponComputationalNeurolinguistics,pages\n70–78,2010.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\nPre-trainingofdeepbidirectionaltransformersforlanguageunderstanding. InProceedingsofthe",
    "2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,Volume1(LongandShortPapers),pages4171–4186,2019.\n[DiGangietal.,2019] MattiaAntoninoDiGangi,MatteoNegri,RoldanoCattoni,RobertoDessi,and\nMarcoTurchi. Enhancingtransformerforend-to-endspeech-to-texttranslation. InProceedingsof\nMachineTranslationSummitXVII:ResearchTrack,pages21–31,2019.\n[Dingetal.,2021] MingDing,ZhuoyiYang,WenyiHong,WendiZheng,ChangZhou,DaYin,Junyang\nLin,XuZou,ZhouShao,HongxiaYang,andJieTang. Cogview: Masteringtext-to-imagegeneration\nviatransformers. AdvancesinNeuralInformationProcessingSystems,34:19822–19835,2021.\n[Ding et al., 2024] Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang,\nJiahangXu,FanYang,andMaoYang. Longrope: Extendingllmcontextwindowbeyond2million\ntokens. arXivpreprintarXiv:2402.13753,2024.\n[Doersch,2016] CarlDoersch. Tutorialonvariationalautoencoders. stat,1050:13,2016.",
    "[DolanandBrockett,2005] BillDolanandChrisBrockett. Automaticallyconstructingacorpusof\nsententialparaphrases. InProceedingsofThirdInternationalWorkshoponParaphrasing(IWP2005),\n2005.\n[Dongetal.,2019] LiDong,NanYang,WenhuiWang,FuruWei,XiaodongLiu,YuWang,Jianfeng\nGao,MingZhou,andHsiao-WuenHon. Unifiedlanguagemodelpre-trainingfornaturallanguage",
    "650 BIBLIOGRAPHY\nunderstandingandgeneration. Advancesinneuralinformationprocessingsystems,32,2019.\n[Dongetal.,2022] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,\nJingjingXu,andZhifangSui. Asurveyonin-contextlearning. arXivpreprintarXiv:2301.00234,\n2022.\n[Dongetal.,2021] YiheDong,Jean-BaptisteCordonnier,andAndreasLoukas. Attentionisnotall\nyouneed: Pureattentionlosesrankdoublyexponentiallywithdepth. InInternationalConferenceon\nMachineLearning,pages2793–2803.PMLR,2021.\n[Dosovitskiyetal.,2021] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,\nXiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,Sylvain\nGelly,JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage\nrecognitionatscale. InProceedingsofICLR2021,2021.\n[Downey,2021] AllenB.Downey. ThinkBayes: BayesianStatisticsinPython(2nded.). O’Reilly\nMedia,2021.\n[Droretal.,2020] RotemDror,LotemPeled-Cohen,andSegevShlomov. NeuralNetworkMethodsfor",
    "NaturalLanguageProcessing. Morgan&ClaypoolPublishers,2020.\n[Drozdovetal.,2022] AndrewDrozdov,NathanaelSchärli,EkinAkyürek,NathanScales,Xinying\nSong, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with\nlarge language models. In Proceedings of The Eleventh International Conference on Learning\nRepresentations,2022.\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive\npromptingfordecomposingcomplexquestions. InProceedingsofthe2022ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing,pages1251–1265,2022.\n[Dubeyetal.,2024] AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,Ahmad\nAl-Dahle,AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3\nherdofmodels. arXivpreprintarXiv:2407.21783,2024.\n[Duboisetal.,2024] YannDubois,ChenXuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,\nJimmyBa,CarlosGuestrin,PercySLiang,andTatsunoriBHashimoto. Alpacafarm: Asimulation",
    "frameworkformethodsthatlearnfromhumanfeedback. AdvancesinNeuralInformationProcessing\nSystems,36,2024.\n[Duchietal.,2011] JohnDuchi,EladHazan,andYoramSinger. Adaptivesubgradientmethodsfor\nonlinelearningandstochasticoptimization. Journalofmachinelearningresearch,12(7),2011.\n[Dufteretal., 2022] PhilippDufter, MartinSchmitt, andHinrichSchütze. Positioninformationin\ntransformers: Anoverview. ComputationalLinguistics,48(3):733–763,2022.\n[Dyeretal.,2013] ChrisDyer,VictorChahuneau,andNoahASmith. Asimple,fast,andeffective\nreparameterizationofibmmodel2. InProceedingsofthe2013ConferenceoftheNorthAmerican\nChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages\n644–648,2013.\n[Ebrahimietal.,2018] JavidEbrahimi,AnyiRao,DanielLowd,andDejingDou. HotFlip: White-box\nadversarial examples for text classification. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pages 31–36, Melbourne,\nAustralia,2018.",
    "[Edunovetal.,2018] SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understanding",
    "BIBLIOGRAPHY 651\nback-translationatscale. InProceedingsofthe2018ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages489–500,2018.\n[Ee,2017] WeinanEe. Aproposalonmachinelearningviadynamicalsystems. Communicationsin\nMathematicsandStatistics,5:1–11,022017.\n[EikemaandAziz,2020] BryanEikemaandWilkerAziz.Ismapdecodingallyouneed?theinadequacy\nofthemodeinneuralmachinetranslation. InProceedingsofthe28thInternationalConferenceon\nComputationalLinguistics,pages4506–4520,2020.\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex\nD’Amour,DJDvijotham,AdamFisch,KatherineHeller,StephenPfohl,DeepakRamachandran,\nandPeterShaw. Helpingorherding? rewardmodelensemblesmitigatebutdonoteliminatereward\nhacking. arXivpreprintarXiv:2312.09244,2023.\n[Elbayadetal.,2020] MahaElbayad,JiataoGu,EdouardGrave,andMichaelAuli. Depth-adaptive\ntransformer. InProceedingsofInternationalConferenceonLearningRepresentations,2020.",
    "[Elman,1990] JeffreyLElman. Findingstructureintime. Cognitivescience,14(2):179–211,1990.\n[Elskenetal.,2019] ThomasElsken,JanHendrikMetzen,andFrankHutter. Neuralarchitecturesearch:\nAsurvey. JournalofMachineLearningResearch,20(55):1–21,2019a.\n[Elskenetal.,2019] ThomasElsken,JanHendrikMetzen,andFrankHutter. Neuralarchitecturesearch:\nAsurvey. TheJournalofMachineLearningResearch,20(1):1997–2017,2019b.\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why\ndoesunsupervisedpre-traininghelpdeeplearning? InProceedingsofthethirteenthinternational\nconferenceonartificialintelligenceandstatistics,pages201–208,2010.\n[Fanetal.,2018] AngelaFan,MikeLewis,andYannDauphin. Hierarchicalneuralstorygeneration. In\nProceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:\nLongPapers),pages889–898,2018.\n[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth",
    "on demand with structured dropout. In Proceedings of International Conference on Learning\nRepresentations,2019.\n[Fanetal.,2021] HaoqiFan,BoXiong,KarttikeyaMangalam,YanghaoLi,ZhichengYan,Jitendra\nMalik,andChristophFeichtenhofer.Multiscalevisiontransformers.InProceedingsoftheIEEE/CVF\nInternationalConferenceonComputerVision,pages6824–6835,2021.\n[Fanetal.,2020] YangFan,ShufangXie,YingceXia,LijunWu,TaoQin,Xiang-YangLi,andTie-Yan\nLiu. Multi-branchattentivetransformer. arXivpreprintarXiv:2006.10270,2020.\n[Faruquietal.,2016] ManaalFaruqui,YuliaTsvetkov,PushpendreRastogi,andChrisDyer. Problems\nwithevaluationofwordembeddingsusingwordsimilaritytasks. InProceedingsofthe1stWorkshop\nonEvaluatingVector-SpaceRepresentationsforNLP,pages30–35,2016.\n[Fedusetal.,2022] WilliamFedus,JeffDean,andBarretZoph. Areviewofsparseexpertmodelsin\ndeeplearning. arXivpreprintarXiv:2209.01667,2022a.\n[Fedusetal.,2022] WilliamFedus,BarretZoph,andNoamShazeer. Switchtransformers: Scaling",
    "totrillionparametermodelswithsimpleandefficientsparsity. TheJournalofMachineLearning\nResearch,23(1):5232–5270,2022b.\n[Fellbaum,2005] ChristianeFellbaum. Wordnetandwordnets. InKeithBrown,editor,Encyclopedia",
    "652 BIBLIOGRAPHY\nofLanguageandLinguistics(2nded.).Elsevier,2005.\n[Fengetal.,2016] ShiFeng,ShujieLiu,NanYang,MuLi,MingZhou,andKennyZhu. Improving\nattentionmodelingwithimplicitdistortionandfertilityformachinetranslation. InProceedingsof\nCOLING2016,the26thInternationalConferenceonComputationalLinguistics: TechnicalPapers,\npages3082–3092,2016.\n[Feng et al., 2021] Steven Y. Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,\nTerukoMitamura,andEduardHovy.AsurveyofdataaugmentationapproachesforNLP.InFindings\noftheAssociationforComputationalLinguistics: ACL-IJCNLP2021,pages968–988,2021.\n[Fernandesetal.,2023] PatrickFernandes,AmanMadaan,EmmyLiu,AntónioFarinhas,PedroHen-\nrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham\nNeubig,andAndréF.T.Martins. Bridgingthegap: Asurveyonintegrating(human)feedbackfor\nnatural languagegeneration. Transactions ofthe Association forComputational Linguistics, 11:\n1643–1668,2023.",
    "[Firth,1957] JohnRFirth. Asynopsisoflinguistictheory,1930-1955. Studiesinlinguisticanalysis,\n1957.\n[Forney,1972] GDJRForney. Maximum-likelihoodsequenceestimationofdigitalsequencesinthe\npresenceofintersymbolinterference. IEEETransactionsonInformationtheory,18(3):363–378,\n1972.\n[FranklinandGraesser,1996] StanFranklinandArtGraesser. Isitanagent,orjustaprogram?: A\ntaxonomyforautonomousagents. InInternationalworkshoponagenttheories,architectures,and\nlanguages,pages21–35.Springer,1996.\n[Freedmanetal.,2007] DavidFreedman,RobertPisani,andRogerPurves. Statistics(4thed.). W.W.\nNorton&Company,2007.\n[Freedman,2009] DavidA.Freedman. StatisticalModels: TheoryandPractice(2nded.). Cambridge\nUniversityPress,2009.\n[FreitagandAl-Onaizan,2017] MarkusFreitagandYaserAl-Onaizan. Beamsearchstrategiesfor\nneuralmachinetranslation. InProceedingsoftheFirstWorkshoponNeuralMachineTranslation,\npages56–60,2017.\n[Freitagetal., 2022] MarkusFreitag, DavidGrangier, QijunTan, andBowenLiang. Highquality",
    "ratherthanhighmodelprobability: Minimumbayesriskdecodingwithneuralmetrics. Transactions\noftheAssociationforComputationalLinguistics,10:811–825,2022.\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The\nEuropeanperspective. PsychologyPress,2014.\n[Friedl,2006] JeffreyFriedl. MasteringRegularExpressions(3rded.). O’ReillyMedia,2006.\n[Fu et al., 2022] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and\nChristopherRe. Hungryhungryhippos: Towardslanguagemodelingwithstatespacemodels. In\nProceedingsofTheEleventhInternationalConferenceonLearningRepresentations,2022.\n[Fuller,2009] WayneAFuller. Introductiontostatisticaltimeseries. JohnWiley&Sons,2009.\n[Gage, 1994] PhilipGage. Anewalgorithmfordatacompression. CUsersJournal, 12(2):23–38,\n1994.\n[Galeetal.,2019] TrevorGale,ErichElsen,andSaraHooker. Thestateofsparsityindeepneural",
    "BIBLIOGRAPHY 653\nnetworks. arXivpreprintarXiv:1902.09574,2019.\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamile\nLukosiute,AnnaChen,AnnaGoldie,AzaliaMirhoseini,CatherineOlsson,DannyHernandez,Dawn\nDrain,DustinLi,EliTran-Johnson,EthanPerez,JacksonKernion,JamieKerr,JaredMueller,Joshua\nLandau,KamalNdousse,KarinaNguyen,LianeLovitt,MichaelSellitto,NelsonElhage,Noemí\nMercado,NovaDasSarma,OliverRausch,RobertLasenby,RobinLarson,SamRinger,Sandipan\nKundu,SauravKadavath,ScottJohnston,ShaunaKravec,SheerElShowk,TameraLanham,Timothy\nTelleen-Lawton,TomHenighan,TristanHume,YuntaoBai,ZacHatfield-Dodds,BenMann,Dario\nAmodei,NicholasJoseph,SamMcCandlish,TomBrown,ChristopherOlah,JackClark,SamuelR.\nBowman,andJaredKaplan. Thecapacityformoralself-correctioninlargelanguagemodels. arXiv\npreprintarXiv:2302.07459,2023.\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model",
    "overoptimization. InInternationalConferenceonMachineLearning,pages10835–10866.PMLR,\n2023a.\n[Gaoetal.,2023] LuyuGao,AmanMadaan,ShuyanZhou,UriAlon,PengfeiLiu,YimingYang,Jamie\nCallan,andGrahamNeubig. Pal: Program-aidedlanguagemodels. InInternationalConferenceon\nMachineLearning,pages10764–10799.PMLR,2023b.\n[Gaoetal.,2023] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,\nJiaweiSun,andHaofenWang. Retrieval-augmentedgenerationforlargelanguagemodels: Asurvey.\narXivpreprintarXiv:2312.10997,2023c.\n[Gargetal.,2019] SarthakGarg,StephanPeitz,UdhyakumarNallasamy,andMatthiasPaulik. Jointly\nlearningtoalignandtranslatewithtransformermodels. InProceedingsofthe2019Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon\nNaturalLanguageProcessing(EMNLP-IJCNLP),pages4453–4462,2019.\n[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What",
    "can transformers learn in-context? a case study of simple function classes. Advances in Neural\nInformationProcessingSystems,35:30583–30598,2022.\n[Geetal.,2024] YuanGe,YilunLiu,ChiHu,WeibinMeng,ShiminTao,XiaofengZhao,Hongxia\nMa,LiZhang,BoxingChen,HaoYang,BeiLi,TongXiao,andJingboZhu. Clusteringandranking:\nDiversity-preservedinstructionselectionthroughexpert-alignedqualityestimation. arXivpreprint\narXiv:2402.18191,2024.\n[Gehringetal.,2017] JonasGehring,MichaelAuli,DavidGrangier,andYannDauphin.Aconvolutional\nencodermodelforneuralmachinetranslation. InProceedingsofthe55thAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages123–135,2017a.\n[Gehringetal.,2017] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannNDauphin.\nConvolutionalsequencetosequencelearning. InInternationalconferenceonmachinelearning,\npages1243–1252.PMLR,2017b.\n[Gelmanetal.,2020] AndrewGelman,JohnB.Carlin,HalS.Stern,DavidB.Dunson,AkiVehtari,",
    "andDonaldB.Rubin. BayesianDataAnalysis(2nded.). ChapmanandHall/CRC,2020.\n[GemmaTeam,2024] GoogleDeepMindGemmaTeam. Gemma: OpenModelsBasedonGemini\nResearchandTechnology,2024.\n[Germannetal.,2004] UlrichGermann,MichaelJahr,KevinKnight,DanielMarcu,andKenjiYamada.\nFastandoptimaldecodingformachinetranslation. ArtificialIntelligence,154(1-2):127–143,2004.",
    "654 BIBLIOGRAPHY\n[Géron,2019] AurélienGéron.Hands-OnMachineLearningwithScikit-Learn,Keras,andTensorFlow:\nConcepts,Tools,andTechniquestoBuildIntelligentSystems(2nded.). O’ReillyMedia,2019.\n[Gersetal.,2000] FelixAGers,JürgenSchmidhuber,andFredCummins.Learningtoforget:Continual\npredictionwithlstm. Neuralcomputation,12(10):2451–2471,2000.\n[Ghazvininejadetal.,2019] MarjanGhazvininejad,OmerLevy,YinhanLiu,andLukeZettlemoyer.\nMask-predict: Paralleldecodingofconditionalmaskedlanguagemodels. InProceedingsofthe2019\nConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint\nConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages6112–6121,2019.\n[Gholamietal.,2022] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney,\nand Kurt Keutzer. A survey of quantization methods for efficient neural network inference. In\nLow-PowerComputerVision,pages291–326.ChapmanandHall/CRC,2022.\n[GildeaandJurafsky,2002] DanielGildeaandDanielJurafsky. Automaticlabelingofsemanticroles.",
    "ComputationalLinguistics,28(3):245–288,2002.\n[Gladkovaetal., 2016] AnnaGladkova, AleksandrDrozd, andSatoshiMatsuoka. Analogy-based\ndetectionofmorphologicalandsemanticrelationswithwordembeddings: whatworksandwhat\ndoesn’t. InProceedingsoftheNAACLStudentResearchWorkshop,pages8–15,2016.\n[GlorotandBengio,2010] XavierGlorotandYoshuaBengio. Understandingthedifficultyoftraining\ndeepfeedforwardneuralnetworks. InProceedingsofthethirteenthinternationalconferenceon\nartificialintelligenceandstatistics,pages249–256.JMLRWorkshopandConferenceProceedings,\n2010.\n[GoelandByrne,2000] VaibhavaGoelandWilliamJByrne. Minimumbayes-riskautomaticspeech\nrecognition. ComputerSpeech&Language,14(2):115–135,2000.\n[Gomez et al., 2017] Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The\nreversible residual network: Backpropagation without storing activations. Advances in neural\ninformationprocessingsystems,30,2017.\n[Goodfellowetal.,2015] IanGoodfellow,JonathonShlens,andChristianSzegedy. Explainingand",
    "harnessingadversarialexamples. InInternationalConferenceonLearningRepresentations,2015.\n[Goodfellowetal.,2016] IanGoodfellow,YoshuaBengio,andAaronCourville. DeepLearning. MIT\nPress,2016.\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience.\nSpringer,1984.\n[Goodman,1996] JoshuaGoodman. Parsingalgorithmsandmetrics. In34thAnnualMeetingofthe\nAssociationforComputationalLinguistics,pages177–183,1996a.\n[Goodman,1996] JoshuaGoodman. Parsingalgorithmsandmetrics. In34thAnnualMeetingofthe\nAssociationforComputationalLinguistics,pages177–183,1996b.\n[Gordonetal.,2021] MitchellAGordon,KevinDuh,andJaredKaplan. Dataandparameterscaling\nlawsforneuralmachinetranslation. InProceedingsofthe2021ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages5915–5922,2021.\n[Gouetal.,2021] JianpingGou,BaoshengYu,StephenJMaybank,andDachengTao. Knowledge\ndistillation: Asurvey. InternationalJournalofComputerVision,129:1789–1819,2021.",
    "[Gouetal.,2024] ZhibinGou,ZhihongShao,YeyunGong,YujiuYang,NanDuan,WeizhuChen,",
    "BIBLIOGRAPHY 655\netal. Critic: Largelanguagemodelscanself-correctwithtool-interactivecritiquing. InTheTwelfth\nInternationalConferenceonLearningRepresentations,2024.\n[GravesandJaitly,2014] AlexGravesandNavdeepJaitly. Towardsend-to-endspeechrecognition\nwithrecurrentneuralnetworks. InProceedingsofInternationalconferenceonmachinelearning,\npages1764–1772,2014.\n[Gravesetal.,2006] AlexGraves,SantiagoFernández,FaustinoGomez,andJürgenSchmidhuber.\nConnectionisttemporalclassification: labellingunsegmentedsequencedatawithrecurrentneural\nnetworks. InProceedingsofthe23rdinternationalconferenceonMachinelearning,pages369–376,\n2006.\n[Graves et al., 2013] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech\nrecognitionwithdeepbidirectionallstm. InIEEEworkshoponautomaticspeechrecognitionand\nunderstanding,pages273–278.IEEE,2013a.\n[Gravesetal.,2013] AlexGraves,Abdel-rahmanMohamed,andGeoffreyHinton. Speechrecognition",
    "withdeeprecurrentneuralnetworks. In2013IEEEinternationalconferenceonacoustics,speech\nandsignalprocessing,pages6645–6649.IEEE,2013b.\n[Gravesetal.,2014] AlexGraves,GregWayne,andIvoDanihelka. Neuralturingmachines. arXiv\npreprintarXiv:1410.5401,2014.\n[Graves et al., 2016] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka,\nAgnieszkaGrabska-Barwinska,SergioGomezColmenarejo,EdwardGrefenstette,TiagoRamalho,\nJohnAgapiou,AdriáPuigdoménechBadia,KarlMoritzHermann,YoriZwols,GeorgOstrovski,\nAdamCain,HelenKing,ChristopherSummerfield,PhilBlunsom,KorayKavukcuoglu,andDemis\nHassabis. Hybridcomputingusinganeuralnetworkwithdynamicexternalmemory. Nature,538\n(7626):471–476,2016.\n[Gray,1998] RobertM.Gray.Quantization.IEEEtransactionsoninformationtheory,44(6):2325–2383,\n1998.\n[Grissom II et al., 2014] Alvin Grissom II, He He, Jordan Boyd-Graber, John Morgan, and Hal\nDaumé III. Don’t until the final verb wait: Reinforcement learning for simultaneous machine",
    "translation. InProceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing(EMNLP),pages1342–1352,2014.\n[GroverandLeskovec,2016] AdityaGroverandJureLeskovec. node2vec: Scalablefeaturelearning\nfornetworks. InProceedingsofthe22ndACMSIGKDDinternationalconferenceonKnowledge\ndiscoveryanddatamining,pages855–864,2016.\n[GuandDao,2023] AlbertGuandTriDao. Mamba: Linear-timesequencemodelingwithselective\nstatespaces. arXivpreprintarXiv:2312.00752,2023.\n[Guetal.,2021] AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswith\nstructuredstatespaces. InProceedingsofInternationalConferenceonLearningRepresentations,\n2021.\n[Guetal.,2022] AlbertGu,KaranGoel,AnkitGupta,andChristopherRé.Ontheparameterizationand\ninitializationofdiagonalstatespacemodels. AdvancesinNeuralInformationProcessingSystems,\n35:35971–35983,2022a.\n[Guetal.,2022] AlbertGu,KaranGoel,KhaledSaab,andChrisRé.Structuredstatespaces:Combining",
    "continuous-time,recurrent,andconvolutionalmodels. https://hazyresearch.stanford.",
    "656 BIBLIOGRAPHY\nedu/blog/2022-01-14-s4-3,2022b. Retrieved2022-01-14.\n[Gu et al., 2017] Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. Learning to\ntranslateinreal-timewithneuralmachinetranslation. InProceedingsoftheEuropeanChapterof\ntheAssociationforComputationalLinguistics(EACL)Conference,2017,2017.\n[Guetal., 2018] JiataoGu, JamesBradbury, CaimingXiong, VictorO.K.Li, andRichardSocher.\nNon-autoregressive neural machine translation. In Proceedings of International Conference on\nLearningRepresentations,2018.\n[Gulatietal.,2020] AnmolGulati,JamesQin,Chung-ChengChiu,NikiParmar,YuZhang,Jiahui\nYu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer:\nConvolution-augmentedtransformerforspeechrecognition. ProceedingsofInterspeech2020,pages\n5036–5040,2020.\n[Gulcehreetal.,2016] CaglarGulcehre,MarcinMoczulski,MishaDenil,andYoshuaBengio. Noisy\nactivationfunctions. InMariaFlorinaBalcanandKilianQ.Weinberger,editors,Proceedingsof",
    "The33rdInternationalConferenceonMachineLearning,volume48ofProceedingsofMachine\nLearningResearch,pages3059–3068.PMLR,2016.\n[Gulcehreetal.,2017] CaglarGulcehre,OrhanFirat,KelvinXu,KyunghyunCho,andYoshuaBengio.\nOnintegratingalanguagemodelintoneuralmachinetranslation. ComputerSpeech&Language,45:\n137–148,2017.\n[Gunasekar et al., 2023] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes,\nAllie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli\nSaarikivi,AdilSalim,ShitalShah,HarkiratSinghBehl,XinWang,SébastienBubeck,RonenEldan,\nAdam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv preprint\narXiv:2306.11644,2023.\n[Guo et al., 2019] Maosheng Guo, Yu Zhang, and Ting Liu. Gaussian transformer: a lightweight\napproach for natural language inference. In Proceedings of the AAAI Conference on Artificial\nIntelligence,pages6489–6496,2019.\n[Guoetal.,2024] QingyanGuo,RuiWang,JunliangGuo,BeiLi,KaitaoSong,XuTan,GuoqingLiu,",
    "JiangBian,andYujiuYang. Connectinglargelanguagemodelswithevolutionaryalgorithmsyields\npowerfulpromptoptimizers. InTheTwelfthInternationalConferenceonLearningRepresentations,\n2024.\n[Guoetal.,2020] QipengGuo,XipengQiu,PengfeiLiu,XiangyangXue,andZhengZhang. Multi-\nscale self-attention for text classification. In Proceedings of the AAAI Conference on Artificial\nIntelligence,volume34,pages7847–7854,2020.\n[GuptaandBerant,2020] AnkitGuptaandJonathanBerant. Gmat: Globalmemoryaugmentationfor\ntransformers. arXivpreprintarXiv:2006.03274,2020.\n[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant.\nMemory-efficienttransformersviatop-kattention. InProceedingsoftheSecondWorkshoponSimple\nandEfficientNaturalLanguageProcessing,pages39–52,2021.\n[Gupta et al., 2004] Madan Gupta, Liang Jin, and Noriyasu Homma. Static and dynamic neural\nnetworks: fromfundamentalstoadvancedtheory. JohnWiley&Sons,2004.",
    "[Guu et al., 2020] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.\nRetrievalaugmentedlanguagemodelpre-training. InProceedingsofInternationalconferenceon",
    "BIBLIOGRAPHY 657\nmachinelearning,pages3929–3938.PMLR,2020.\n[Guyon and Elisseeff, 2003] Isabelle Guyon and André Elisseeff. An introduction to variable and\nfeatureselection. Journalofmachinelearningresearch,3(Mar):1157–1182,2003.\n[Haber and Ruthotto, 2017] Eldad Haber and Lars Ruthotto. Stable architectures for deep neural\nnetworks. Inverseproblems,34(1):014004,2017.\n[Hahn, 2020] Michael Hahn. Theoretical limitations of self-attention in neural sequence models.\nTransactionsoftheAssociationforComputationalLinguistics,8:156–171,2020.\n[Hamilton,1994] JamesDouglasHamilton. TimeSeriesAnalysis. PrincetonUniversityPress,1994.\n[Hanetal.,2022] KaiHan,YunheWang,HantingChen,XinghaoChen,JianyuanGuo,ZhenhuaLiu,\nYehuiTang,AnXiao,ChunjingXu,YixingXu,ZhaohuiYang,YimanZhang,andDachengTao. A\nsurveyonvisiontransformer. IEEEtransactionsonpatternanalysisandmachineintelligence,45(1):\n87–110,2022.\n[Han et al., 2020] Wei Han, Zhengdong Zhang, Yu Zhang, Jiahui Yu, Chung-Cheng Chiu, James",
    "Qin,AnmolGulati,RuomingPang,andYonghuiWu. Contextnet: Improvingconvolutionalneural\nnetworksforautomaticspeechrecognitionwithglobalcontext. InProceedingsofInterspeech2020,\npages3610–3614,2020.\n[Hanetal.,2021] XuHan,ZhengyanZhang,NingDing,YuxianGu,XiaoLiu,YuqiHuo,Jiezhong\nQiu,LiangZhang,WentaoHan,MinlieHuang,QinJin,YanyanLan,YangLiu,ZhiyuanLiu,Zhiwu\nLu,XipengQiu,RuihuaSong,JieTang,Ji-RongWen,JinhuiYuan,WayneXinZhao,andJunZhu.\nPre-trainedmodels: Past,presentandfuture. AIOpen,2:225–250,2021a.\n[Hanetal.,2021] YizengHan,GaoHuang,ShijiSong,LeYang,HonghuiWang,andYulinWang. Dy-\nnamicneuralnetworks: Asurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,\n44(11):7436–7456,2021b.\n[Hanetal.,2024] ZeyuHan,ChaoGao,JinyangLiu,JeffZhang,andSaiQianZhang. Parameter-\nefficientfine-tuningforlargemodels: Acomprehensivesurvey. arXivpreprintarXiv:2403.14608,\n2024.\n[Hannunetal.,2014] AwniHannun,CarlCase,JaredCasper,BryanCatanzaro,GregDiamos,Erich",
    "Elsen,RyanPrenger,SanjeevSatheesh,ShubhoSengupta,andAdamCoates. Deepspeech: Scaling\nupend-to-endspeechrecognition. arXivpreprintarXiv:1412.5567,2014.\n[Hao et al., 2019] Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and Zhaopeng Tu. Multi-\ngranularityself-attentionforneuralmachinetranslation. InProceedingsofthe2019Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon\nNaturalLanguageProcessing(EMNLP-IJCNLP),pages887–897,2019.\n[Haoetal.,2022] YidingHao,DanaAngluin,andRobertFrank. Formallanguagerecognitionbyhard\nattentiontransformers: Perspectivesfromcircuitcomplexity. TransactionsoftheAssociationfor\nComputationalLinguistics,10:800–810,2022.\n[Harlapetal.,2018] AaronHarlap,DeepakNarayanan,AmarPhanishayee,VivekSeshadri,Nikhil\nDevanur,GregGanger,andPhilGibbons. Pipedream:Fastandefficientpipelineparalleldnntraining.\narXivpreprintarXiv:1806.03377,2018.\n[Harris,1954] ZelligSHarris. Distributionalstructure. Word,10(2-3):146–162,1954.",
    "[Hasleretal.,2018] EvaHasler,AdriàdeGispert,GonzaloIglesias,andBillByrne. Neuralmachine\ntranslationdecodingwithterminologyconstraints. InProceedingsofthe2018Conferenceofthe",
    "658 BIBLIOGRAPHY\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies,Volume2(ShortPapers),pages506–512,2018.\n[Hastieetal.,2009] TrevorHastie,RobertTibshirani,andJeromeFriedman. TheElementsofStatistical\nLearning. Spinger,2009.\n[Heetal.,2017] DiHe,HanqingLu,YingceXia,TaoQin,LiweiWang,andTie-YanLiu. Decoding\nwithvaluenetworksforneuralmachinetranslation. AdvancesinNeuralInformationProcessing\nSystems,30,2017.\n[He et al., 2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into\nrectifiers: Surpassinghuman-levelperformanceonimagenetclassification. InProceedingsofthe\nIEEEinternationalconferenceoncomputervision,pages1026–1034,2015.\n[Heetal.,2016] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearning\nfor image recognition. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition,pages770–778,2016a.\n[Heetal.,2016] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Identitymappingsindeep",
    "residualnetworks. InProceedingsofECCV2016,pages630–645,2016b.\n[Heetal.,2019] KaimingHe,RossGirshick,andPiotrDollár. Rethinkingimagenetpre-training. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 4918–4927,\n2019.\n[Heetal.,2022] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.\nMaskedautoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFConferenceon\nComputerVisionandPatternRecognition,pages16000–16009,2022.\n[Heetal.,2021] PengchengHe,XiaodongLiu,JianfengGao,andWeizhuChen. Deberta: Decoding-\nenhancedbertwithdisentangledattention. InProceedingsofInternationalConferenceonLearning\nRepresentations,2021.\n[Heetal.,2016] WeiHe,ZhongjunHe,HuaWu,andHaifengWang. Improvedneuralmachinetransla-\ntionwithsmtfeatures. InProceedingsoftheThirtiethAAAIconferenceonartificialintelligence,\n2016c.\n[Heetal.,2018] XuanliHe,GholamrezaHaffari,andMohammadNorouzi. Sequencetosequencemix-",
    "turemodelfordiversemachinetranslation. InProceedingsofthe22ndConferenceonComputational\nNaturalLanguageLearning,pages583–592,2018.\n[Heafieldetal.,2021] KennethHeafield,QianqianZhu,andRomanGrundkiewicz. Findingsofthe\nWMT2021sharedtaskonefficienttranslation. InProceedingsoftheSixthConferenceonMachine\nTranslation,pages639–651,2021.\n[HendrycksandGimpel,2016] DanHendrycksandKevinGimpel. Gaussianerrorlinearunits(gelus).\narXivpreprintarXiv:1606.08415,2016.\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh\nKrishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In\nProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages\n2744–2751,2020.\n[Hendrycksetal.,2021] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,\nDawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In\nProceedingsofInternationalConferenceonLearningRepresentations,2021.",
    "BIBLIOGRAPHY 659\n[Hestnessetal.,2017] JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,\nHassanKianinejad,MdMostofaAliPatwary,YangYang,andYanqiZhou. Deeplearningscalingis\npredictable,empirically. arXivpreprintarXiv:1712.00409,2017.\n[Hewitt,2024] JohnHewitt. Instructionfollowingwithoutinstructiontuning,2024. URLhttps:\n//nlp.stanford.edu/~johnhew/instruction-following.html.\n[HewittandLiang,2019] JohnHewittandPercyLiang. Designingandinterpretingprobeswithcontrol\ntasks. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing\nandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),\npages2733–2743,2019.\n[Hewittetal.,2024] JohnHewitt,NelsonFLiu,PercyLiang,andChristopherDManning. Instruction\nfollowingwithoutinstructiontuning. arXivpreprintarXiv:2409.14254,2024.\n[HildebrandandVogel,2008] AlmutSiljaHildebrandandStephanVogel. Combinationofmachine\ntranslationsystemsviahypothesisselectionfromcombinedn-bestlists. InProceedingsofthe8th",
    "ConferenceoftheAssociationforMachineTranslationintheAmericas: StudentResearchWorkshop,\npages254–261,2008.\n[Hilletal.,2016] FelixHill,KyunghyunCho,andAnnaKorhonen.Learningdistributedrepresentations\nofsentencesfromunlabelleddata. InProceedingsofthe2016ConferenceoftheNorthAmerican\nChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages\n1367–1377,2016.\n[Hinton, 2018] Geoff Hinton. Coursera neural networks for machine learning lecture 6,\n2018. URL http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_\nslides_lec6.pdf.\n[Hintonetal.,2015] GeoffreyHinton,OriolVinyals,andJeffDean. Distillingtheknowledgeina\nneuralnetwork. arXivpreprintarXiv:1503.02531,2015.\n[Hinton,2007] GeoffreyEHinton. Learningmultiplelayersofrepresentation. Trendsincognitive\nsciences,11(10):428–434,2007.\n[Hinton and Roweis, 2002] Geoffrey E Hinton and Sam Roweis. Stochastic neighbor embedding.\nAdvancesinneuralinformationprocessingsystems,15,2002.",
    "[Hinton and Salakhutdinov, 2006] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the\ndimensionalityofdatawithneuralnetworks. science,313(5786):504–507,2006.\n[Hintonetal.,2006] GeoffreyEHinton,SimonOsindero,andYee-WhyeTeh.Afastlearningalgorithm\nfordeepbeliefnets. Neuralcomputation,18(7):1527–1554,2006.\n[Hinton et al., 2012] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and\nRuslanRSalakhutdinov. Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectors.\narXivpreprintarXiv:1207.0580,2012.\n[Hoangetal.,2017] CongDuyVuHoang,GholamrezaHaffari,andTrevorCohn. Towardsdecoding\nascontinuousoptimisationinneuralmachinetranslation. InProceedingsofthe2017Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,pages146–156,2017.\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term\nmemory. Neuralcomputation,9(8):1735–1780,1997.\n[Hoffmannetal.,2022] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,",
    "TrevorCai,ElizaRutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,Aidan",
    "660 BIBLIOGRAPHY\nClark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and\nLaurentSifre. Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,\n2022.\n[HokampandLiu,2017] ChrisHokampandQunLiu. Lexicallyconstraineddecodingforsequence\ngenerationusinggridbeamsearch. InProceedingsofthe55thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages1535–1546,2017.\n[Holmström and Koistinen, 1992] Lasse Holmström and Petri Koistinen. Using additive noise in\nback-propagationtraining. IEEETransactionsonNeuralNetworks,3(1):24–38,1992.\n[Holtzmanetal.,2020] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecurious\ncaseofneuraltextdegeneration. InProceedingsofthe6thInternationalConferenceonLearning\nRepresentationsICLR,2020a.\n[Holtzmanetal.,2020] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecurious",
    "caseofneuraltextdegeneration. InInternationalConferenceonLearningRepresentations,2020b.\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural\ninstructions: Tuninglanguagemodelswith(almost)nohumanlabor. InProceedingsofthe61st\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages\n14409–14428,2023.\n[Hopfield,1982] JohnJHopfield. Neuralnetworksandphysicalsystemswithemergentcollective\ncomputationalabilities. Proceedingsofthenationalacademyofsciences,79(8):2554–2558,1982.\n[Hopfield, 1984] John J Hopfield. Neurons with graded response have collective computational\npropertieslikethoseoftwo-stateneurons. Proceedingsofthenationalacademyofsciences,81(10):\n3088–3092,1984.\n[Houetal.,2020] LuHou,ZhiqiHuang,LifengShang,XinJiang,XiaoChen,andQunLiu. Dynabert:\nDynamicbertwithadaptivewidthanddepth. AdvancesinNeuralInformationProcessingSystems,\n33:9782–9793,2020.",
    "[Houlsbyetal.,2019] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,Quentin\nDeLaroussilhe,AndreaGesmundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransfer\nlearningforNLP. InProceedingsofthe36thInternationalConferenceonMachineLearning,pages\n2790–2799.PMLR,2019.\n[Howardetal.,2019] AndrewHoward,RuomingPang,HartwigAdam,QuocV.Le,MarkSandler,\nBoChen,WeijunWang,Liang-ChiehChen,MingxingTan,GraceChu,VijayVasudevan,andYukun\nZhu. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on\ncomputervision,pages1314–1324,2019.\n[Hsuetal.,2021] Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai,KushalLakhotia,Ruslan\nSalakhutdinov,andAbdelrahmanMohamed. Hubert: Self-supervisedspeechrepresentationlearning\nbymaskedpredictionofhiddenunits. IEEE/ACMTransactionsonAudio,Speech,andLanguage\nProcessing,29:3451–3460,2021.\n[Huetal.,2021] ChiHu,ChenglongWang,XiangnanMa,XiaMeng,YinqiaoLi,TongXiao,Jingbo",
    "Zhu, and Changliang Li. Ranknas: Efficient neural architecture search by pairwise ranking. In\nProceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages\n2469–2480,2021.",
    "BIBLIOGRAPHY 661\n[Huetal., 2022] EdwardJHu, yelongshen, PhillipWallis, ZeyuanAllen-Zhu, YuanzhiLi, Shean\nWang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternationalConferenceonLearningRepresentations,2022.\n[Huangetal.,2018] Cheng-ZhiAnnaHuang,AshishVaswani,JakobUszkoreit,IanSimon,Curtis\nHawthorne,NoamShazeer,AndrewMDai,MatthewDHoffman,MonicaDinculescu,andDouglas\nEck. Musictransformer: Generatingmusicwithlong-termstructure. InProceedingsofInternational\nConferenceonLearningRepresentations,2018.\n[Huang et al., 2012] Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng.\nImprovingwordrepresentationsviaglobalcontextandmultiplewordprototypes. InProceedingsof\nthe50thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),\npages873–882,2012.\n[Huangetal.,2016] GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQWeinberger. Deep\nnetworkswithstochasticdepth. InProceedingsofthe14thEuropeanConference,pages646–661.",
    "Springer,2016.\n[Huangetal.,2017] GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQWeinberger.\nDenselyconnectedconvolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition,pages4700–4708,2017a.\n[Huang,2009] LiangHuang. Dynamicprogramming-basedsearchalgorithmsinNLP. InProceedings\nofHumanLanguageTechnologies: The2009AnnualConferenceoftheNorthAmericanChapterof\ntheAssociationforComputationalLinguistics,CompanionVolume: TutorialAbstracts,2009.\n[Huangetal.,2017] LiangHuang,KaiZhao,andMingboMa. Whentofinish? optimalbeamsearch\nforneuraltextgeneration(modulobeamsize). InProceedingsofthe2017ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing,pages2134–2139,2017b.\n[Huangetal., 2020] XiaoShiHuang, FelipePerez, JimmyBa, andMaksimsVolkovs. Improving\ntransformeroptimizationthroughbetterinitialization. InProceedingsofInternationalConferenceon\nMachineLearning,pages4475–4483.PMLR,2020a.",
    "[Huang et al., 2019] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen,\nDehaoChen,HyoukJoongLee,JiquanNgiam,QuocVLe,YonghuiWu,andZhifengChen. Gpipe:\nEfficienttrainingofgiantneuralnetworksusingpipelineparallelism. Advancesinneuralinformation\nprocessingsystems,32,2019.\n[Huangetal.,2015] ZhihengHuang,WeiXu,andKaiYu. Bidirectionallstm-crfmodelsforsequence\ntagging. arXivpreprintarXiv:1508.01991,2015.\n[Huangetal.,2020] ZhihengHuang,DavisLiang,PengXu,andBingXiang. Improvetransformer\nmodelswithbetterrelativepositionembeddings. InFindingsoftheAssociationforComputational\nLinguistics: EMNLP2020,pages3327–3335,2020b.\n[HubelandWiesel,1959] DavidHHubelandTorstenNWiesel. Receptivefieldsofsingleneuronesin\nthecat’sstriatecortex. TheJournalofphysiology,148(3):574,1959.\n[Hurley,2011] PatrickHurley. AConciseIntroductiontoLogic(11thed.). WadsworthPublishing,\n2011.\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam",
    "Neyshabur. Block-recurrenttransformers. Advancesinneuralinformationprocessingsystems,35:\n33248–33261,2022.",
    "662 BIBLIOGRAPHY\n[Hutchison et al., 2013] Keith A Hutchison, David A Balota, James H Neely, Michael J Cortese,\nEmilyRCohen-Shikora,Chi-ShingTse,MelvinJYap,JesseJBengson,DaleNiemeyer,andErin\nBuchanan. Thesemanticprimingproject. Behaviorresearchmethods,45(4):1099–1114,2013.\n[IoffeandSzegedy, 2015] SergeyIoffeandChristianSzegedy. Batchnormalization: Accelerating\ndeepnetworktrainingbyreducinginternalcovariateshift. InInternationalconferenceonmachine\nlearning,pages448–456.PMLR,2015.\n[Ivanovetal.,2021] AndreiIvanov,NikoliDryden,TalBen-Nun,ShigangLi,andTorstenHoefler.\nDatamovementisallyouneed:Acasestudyonoptimizingtransformers. InProceedingsofMachine\nLearningandSystems,volume3,pages711–732,2021.\n[Jackendoff,1992] RaySJackendoff. Semanticstructures,volume18. MITpress,1992.\n[Jacobetal.,2018] BenoitJacob,SkirmantasKligys,BoChen,MenglongZhu,MatthewTang,Andrew\nHoward,HartwigAdam,andDmitryKalenichenko. Quantizationandtrainingofneuralnetworks",
    "forefficientinteger-arithmetic-onlyinference. InProceedingsoftheIEEEconferenceoncomputer\nvisionandpatternrecognition,pages2704–2713,2018.\n[Jaderbergetal.,2015] MaxJaderberg,KarenSimonyan,AndrewZisserman,andkoraykavukcuoglu.\nSpatialtransformernetworks. Advancesinneuralinformationprocessingsystems,28,2015.\n[JaegerandHaas,2004] HerbertJaegerandHaraldHaas. Harnessingnonlinearity: Predictingchaotic\nsystemsandsavingenergyinwirelesscommunication. science,304(5667):78–80,2004.\n[Jaegleetal.,2021] AndrewJaegle,SebastianBorgeaud,Jean-BaptisteAlayrac,CarlDoersch,Catalin\nIonescu,DavidDing,SkandaKoppula,DanielZoran,AndrewBrock,EvanShelhamer,OlivierJ.\nHénaff,MatthewM.Botvinick,AndrewZisserman,OriolVinyals,andJoãoCarreira. Perceiverio:\nAgeneralarchitectureforstructuredinputs&outputs. InProceedingsofInternationalConference\nonLearningRepresentations,2021.\n[Janssen,2012] TheoM.V.Janssen. Compositionality: itshistoriccontext. InM.Werning,W.Hinzen,",
    "andE.Machery,editors,TheOxfordhandbookofcompositionality.OxfordUniversityPress,2012.\n[Jean et al., 2015] Sébastien Jean, Orhan Firat, Kyunghyun Cho, Roland Memisevic, and Yoshua\nBengio. Montreal neural machine translation systems for wmt’15. In Proceedings of the tenth\nworkshoponstatisticalmachinetranslation,pages134–140,2015.\n[Jelinek,1998] FrederickJelinek. Statisticalmethodsforspeechrecognition. MITPress,1998.\n[Jia and Liang, 2017] Robin Jia and Percy Liang. Adversarial examples for evaluating reading\ncomprehensionsystems. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing,pages2021–2031,2017.\n[Jiangetal.,2023] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample,\nLucileSaulnier,LélioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,Thibaut\nLavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint",
    "arXiv:2310.06825,2023a.\n[Jiangetal.,2023] HuiqiangJiang,QianhuiWu,Chin-YewLin,YuqingYang,andLiliQiu. Llmlingua:\nCompressingpromptsforacceleratedinferenceoflargelanguagemodels. InProceedingsofthe2023\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages13358–13376,2023b.\n[Jiangetal.,2020] ZhengbaoJiang,FrankFXu,JunAraki,andGrahamNeubig. Howcanweknow\nwhatlanguagemodelsknow? TransactionsoftheAssociationforComputationalLinguistics, 8:",
    "BIBLIOGRAPHY 663\n423–438,2020.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang\nWang,andQunLiu. Tinybert: Distillingbertfornaturallanguageunderstanding. InFindingsofthe\nAssociationforComputationalLinguistics: EMNLP2020,pages4163–4174,2020.\n[Jolliffe,2002] IanTJolliffe. Principalcomponentanalysisforspecialtypesofdata. Springer,2002.\n[Joshietal.,2017] MandarJoshi,EunsolChoi,DanielSWeld,andLukeZettlemoyer. Triviaqa: A\nlargescaledistantlysupervisedchallengedatasetforreadingcomprehension. InProceedingsofthe\n55thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),\npages1601–1611,2017.\n[Joshietal.,2020] MandarJoshi,DanqiChen,YinhanLiu,DanielSWeld,LukeZettlemoyer,and\nOmerLevy. Spanbert: Improvingpre-trainingbyrepresentingandpredictingspans. Transactionsof\ntheassociationforcomputationallinguistics,8:64–77,2020.\n[Joulinetal.,2017] ArmandJoulin,ÉdouardGrave,PiotrBojanowski,andTomášMikolov. Bagof",
    "tricksforefficienttextclassification. InProceedingsofthe15thConferenceoftheEuropeanChapter\noftheAssociationforComputationalLinguistics: Volume2,ShortPapers,pages427–431,2017.\n[JurafskyandMartin,2008] DanJurafskyandJamesH.Martin. SpeechandLanguageProcessing\n(2nded.). PrenticeHall,2008.\n[Kahneman,2011] DanielKahneman. Thinking,fastandslow. macmillan,2011.\n[Kalchbrenneretal.,2014] NalKalchbrenner,EdwardGrefenstette,andPhilBlunsom. Aconvolu-\ntionalneuralnetworkformodellingsentences. InProceedingsofthe52ndAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages655–665,2014.\n[Kaplanetal.,2020] JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,\nRewonChild,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglawsforneural\nlanguagemodels. arXivpreprintarXiv:2001.08361,2020.\n[Katharopoulosetal.,2020] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançois\nFleuret.Transformersarernns:Fastautoregressivetransformerswithlinearattention.InInternational",
    "conferenceonmachinelearning,pages5156–5165.PMLR,2020.\n[KellyandStone,1975] EdwardF.KellyandPhilipJ.Stone. ComputerrecognitionofEnglishword\nsenses. AmericanElsevierPub,1975.\n[Kernes, 2021] Jonathan Kernes. Master positional encoding: Part\ni, 05 2021. URL https://towardsdatascience.com/\nmaster-positional-encoding-part-i-63c05d90a0c3.\n[Khanetal.,2020] AsifullahKhan,AnabiaSohail,UmmeZahoora,andAqsaSaeedQureshi. Asurvey\noftherecentarchitecturesofdeepconvolutionalneuralnetworks. Artificialintelligencereview,53\n(8):5455–5516,2020.\n[Khandelwaletal.,2019] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMike\nLewis. Generalizationthroughmemorization: Nearestneighborlanguagemodels. InProceedingsof\nInternationalConferenceonLearningRepresentations(ICLR),2019.\n[Khandelwaletal.,2020] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMike\nLewis. Generalizationthroughmemorization: Nearestneighborlanguagemodels. InInternational\nConferenceonLearningRepresentations,2020.",
    "664 BIBLIOGRAPHY\n[Khayrallahetal.,2017] HudaKhayrallah,GauravKumar,KevinDuh,MattPost,andPhilippKoehn.\nNeurallatticesearchfordomainadaptationinmachinetranslation. InProceedingsoftheEighth\nInternationalJointConferenceonNaturalLanguageProcessing(Volume2: ShortPapers),pages\n20–25,2017.\n[Khotetal.,2023] TusharKhot,HarshTrivedi,MatthewFinlayson,YaoFu,KyleRichardson,Peter\nClark,andAshishSabharwal. Decomposedprompting: Amodularapproachforsolvingcomplex\ntasks. InProceedingsofTheEleventhInternationalConferenceonLearningRepresentations,2023.\n[Kidger,2022] PatrickKidger. Onneuraldifferentialequations. arXivpreprintarXiv:2202.02435,\n2022.\n[Kikuchietal.,2016] YutaKikuchi,GrahamNeubig,RyoheiSasano,HiroyaTakamura,andManabu\nOkumura. Controlling output length in neural encoder-decoders. In Proceedings of the 2016\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages1328–1338,2016.\n[KimandCho,2021] GyuwanKimandKyunghyunCho.Length-adaptivetransformer:Trainoncewith",
    "lengthdrop,useanytimewithsearch. InProceedingsofthe59thAnnualMeetingoftheAssociation\nforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguage\nProcessing(Volume1: LongPapers),pages6501–6511,2021.\n[Kim et al., 2019] Najoung Kim, Roma Patel, Adam Poliak, Alex Wang, Patrick Xia, R. Thomas\nMcCoy,IanTenney,AlexisRoss,TalLinzen,BenjaminVanDurme,SamuelR.Bowman,andEllie\nPavlick. Probingwhatdifferentnlptasksteachmachinesaboutfunctionwordcomprehension. In\nProceedingsoftheEighthJointConferenceonLexicalandComputationalSemantics(*SEM2019),\npages235–249,2019.\n[Kimetal.,2023] SehoonKim,ColemanHooper,ThanakulWattanawong,MinwooKang,Ruohan\nYan,HasanGenc,GraceDinh,QijingHuang,KurtKeutzer,MichaelW.Mahoney,YakunSophia\nShao,andAmirGholami. Fullstackoptimizationoftransformerinference: asurvey. arXivpreprint\narXiv:2302.14017,2023.\n[Kimetal.,2021] WonjaeKim,BokyungSon,andIldooKim. Vilt: Vision-and-languagetransformer",
    "withoutconvolutionorregionsupervision. InProceedingsofInternationalConferenceonMachine\nLearning,pages5583–5594.PMLR,2021.\n[Kim,2014] YoonKim. Convolutionalneuralnetworksforsentenceclassification. InProceedings\nofthe2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages\n1746–1751,October2014.\n[KimandRush,2016] YoonKimandAlexanderMRush. Sequence-levelknowledgedistillation. In\nProceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages\n1317–1327,2016.\n[Kimetal.,2016] YoonKim,YacineJernite,DavidSontag,andAlexanderMRush. Character-aware\nneurallanguagemodels. InProceedingsoftheThirtiethAAAIconferenceonartificialintelligence,\n2016.\n[KimandAwadalla,2020] YoungJinKimandHanyHassanAwadalla. Fastformers: Highlyefficient\ntransformermodelsfornaturallanguageunderstanding. InProceedingsofSustaiNLP:Workshopon\nSimpleandEfficientNaturalLanguageProcessing,pages149–158,2020.\n[KingmaandBa,2014] DiederikPKingmaandJimmyBa.Adam:Amethodforstochasticoptimization.",
    "arXivpreprintarXiv:1412.6980,2014.",
    "BIBLIOGRAPHY 665\n[KingmaandWelling,2014] DiederikP.KingmaandMaxWelling. Auto-encodingvariationalbayes.\nInProceedingsof2ndInternationalConferenceonLearningRepresentations,ICLR2014,2014.\n[KingmaandWelling,2019] DiederikP.KingmaandMaxWelling. Anintroductiontovariational\nautoencoders. FoundationsandTrends®inMachineLearning,2019.\n[Kirkpatricketal., 2017] JamesKirkpatrick, RazvanPascanu, NeilRabinowitz, JoelVeness, Guil-\nlaumeDesjardins,AndreiA.Rusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-\nBarwinska,DemisHassabis,ClaudiaClopath,DharshanKumaran,andRaiaHadsell. Overcoming\ncatastrophicforgettinginneuralnetworks. Proceedingsofthenationalacademyofsciences,114\n(13):3521–3526,2017.\n[Kirosetal.,2015] RyanKiros,YukunZhu,RussRSalakhutdinov,RichardZemel,RaquelUrtasun,\nAntonioTorralba,andSanjaFidler. Skip-thoughtvectors. Advancesinneuralinformationprocessing\nsystems,28,2015.\n[Kitaevetal.,2020] NikitaKitaev,LukaszKaiser,andAnselmLevskaya. Reformer: Theefficient",
    "transformer. InProceedingsofInternationalConferenceonLearningRepresentations,2020.\n[Kleinetal.,2017] GuillaumeKlein,YoonKim,YuntianDeng,JeanSenellart,andAlexanderMRush.\nOpennmt: Open-sourcetoolkitforneuralmachinetranslation. InProceedingsofACL2017,System\nDemonstrations,pages67–72,2017.\n[Klementievetal.,2012] AlexandreKlementiev,IvanTitov,andBinodBhattarai. Inducingcrosslingual\ndistributedrepresentationsofwords. InProceedingsofCOLING2012,pages1459–1474,2012.\n[Klerkeetal.,2015] SigridKlerke,HéctorMartínezAlonso,andAndersSøgaard. Lookinghard: Eye\ntrackingfordetectinggrammaticalityofautomaticallycompressedsentences. InProceedingsofthe\n20thNordicConferenceofComputationalLinguistics(NODALIDA2015),pages97–105,2015.\n[Knight,1999] KevinKnight. Decodingcomplexityinword-replacementtranslationmodels. Compu-\ntationallinguistics,25(4):607–615,1999.\n[Knight,2009] KevinKnight. Bayesianinferencewithtears,2009.\n[Knight,2018] LindaKnight. Thesparrowtweets,2018.",
    "[Kochenderfer and Wheeler, 2019] Mykel J. Kochenderfer and Tim A. Wheeler. Algorithms for\nOptimization. TheMITPress,2019.\n[Koehn,2004] PhilippKoehn. Pharaoh: abeamsearchdecoderforphrase-basedstatisticalmachine\ntranslationmodels. InConferenceoftheAssociationforMachineTranslationintheAmericas,pages\n115–124.Springer,2004.\n[Koehn,2010] PhilippKoehn. StatisticalMachineTranslation. CambridgeUniversityPress,2010.\n[KoehnandKnowles,2017] PhilippKoehnandRebeccaKnowles. Sixchallengesforneuralmachine\ntranslation. InProceedingsoftheFirstWorkshoponNeuralMachineTranslation, pages28–39,\n2017.\n[Koehn et al., 2003] Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based\ntranslation. In Proceedings of the 2003 Human Language Technology Conference of the North\nAmericanChapteroftheAssociationforComputationalLinguistics,pages127–133,2003.\n[Koehnetal.,2007] PhilippKoehn,HieuHoang,AlexandraBirch,ChrisCallison-Burch,Marcello",
    "Federico,NicolaBertoldi,BrookeCowan,WadeShen,ChristineMoran,RichardZens,ChrisDyer,\nOndˇrejBojar,AlexandraConstantin,andEvanHerbst. Moses: Opensourcetoolkitforstatistical",
    "666 BIBLIOGRAPHY\nmachinetranslation.InProceedingsofthe45thAnnualMeetingoftheAssociationforComputational\nLinguisticsCompanionVolumeProceedingsoftheDemoandPosterSessions,pages177–180,2007.\n[Kojimaetal.,2022] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusuke\nIwasawa. Largelanguagemodelsarezero-shotreasoners. Advancesinneuralinformationprocessing\nsystems,35:22199–22213,2022.\n[KonishiandKitagawa,2007] SadanoriKonishiandGenshiroKitagawa. InformationCriteriaand\nStatisticalModeling. Spinger,2007.\n[Korthikantietal.,2023] VijayAnandKorthikanti,JaredCasper,SangkugLym,LawrenceMcAfee,\nMichaelAndersch,MohammadShoeybi,andBryanCatanzaro. Reducingactivationrecomputation\ninlargetransformermodels. ProceedingsofMachineLearningandSystems,5,2023.\n[Krakovna et al., 2020] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz,\nTom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specification gam-",
    "ing: the flip side of ai ingenuity. https://deepmind.google/discover/blog/\nspecification-gaming-the-flip-side-of-ai-ingenuity,2020.\n[Krebs et al., 2018] Alicia Krebs, Alessandro Lenci, and Denis Paperno. SemEval-2018 task 10:\nCapturingdiscriminativeattributes. InProceedingsofThe12thInternationalWorkshoponSemantic\nEvaluation,pages732–740,2018.\n[Krizhevskyetal., 2017] AlexKrizhevsky, IlyaSutskever, andGeoffreyEHinton. Imagenetclas-\nsification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90,\n2017.\n[Kudo,2018] TakuKudo. Subwordregularization: Improvingneuralnetworktranslationmodelswith\nmultiplesubwordcandidates. InProceedingsofthe56thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages66–75,2018.\n[KudoandRichardson,2018] TakuKudoandJohnRichardson. Sentencepiece: Asimpleandlanguage\nindependentsubwordtokenizeranddetokenizerforneuraltextprocessing. InProceedingsofthe",
    "2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing: SystemDemonstrations,\npages66–71,2018.\n[Kulikovetal.,2019] IliaKulikov,AlexanderMiller,KyunghyunCho,andJasonWeston.Importanceof\nsearchandevaluationstrategiesinneuraldialoguemodeling.InProceedingsofthe12thInternational\nConferenceonNaturalLanguageGeneration,pages76–87,2019.\n[Kulis,2013] BrianKulis. Metriclearning: Asurvey. FoundationsandTrends®inMachineLearning,\n5(4):287–364,2013.\n[Kumaretal.,2016] AnkitKumar,OzanIrsoy,PeterOndruska,MohitIyyer,JamesBradbury,Ishaan\nGulrajani,VictorZhong,RomainPaulus,andRichardSocher. Askmeanything: Dynamicmemory\nnetworksfornaturallanguageprocessing. InInternationalconferenceonmachinelearning,pages\n1378–1387,2016.\n[Kumaretal.,2021] SachinKumar,EricMalmi,AliakseiSeveryn,andYuliaTsvetkov. Controlledtext\ngenerationascontinuousoptimizationwithmultipleconstraints. AdvancesinNeuralInformation\nProcessingSystems,34:14542–14554,2021.",
    "[KumarandByrne,2004] ShankarKumarandWilliamByrne. MinimumBayes-riskdecodingfor\nstatisticalmachinetranslation. InProceedingsoftheHumanLanguageTechnologyConferenceof\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HLT-NAACL2004,",
    "BIBLIOGRAPHY 667\npages169–176,2004a.\n[Kumar and Byrne, 2004] Shankar Kumar and William Byrne. Minimum bayes-risk decoding for\nstatisticalmachinetranslation. InProceedingsoftheHumanLanguageTechnologyConferenceof\ntheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HLT-NAACL2004,\npages169–176,2004b.\n[KungandPeng,2023] Po-NienKungandNanyunPeng. Domodelsreallylearntofollowinstructions?\nanempiricalstudyofinstructiontuning. arXivpreprintarXiv:2305.11383,2023.\n[Kupiec,1992] JulianKupiec. Robustpart-of-speechtaggingusingahiddenmarkovmodel. Computer\nSpeech&Language,6:225–242,1992.\n[Kwon et al., 2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,\nCodyHaoYu,JosephEGonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementfor\nlargelanguagemodelservingwithpagedattention. arXivpreprintarXiv:2309.06180,2023.\n[Laffertyetal.,2001] JohnLafferty,AndrewMcCallum,andFernandoPereira. Conditionalrandom",
    "fields: Probabilisticmodelsforsegmentingandlabelingsequencedata. InProceedingsofthe18th\nInternationalConferenceonMachineLearning2001,pages282–289,2001.\n[Lagunasetal.,2021] FrançoisLagunas,EllaCharlaix,VictorSanh,andAlexanderMRush. Block\npruningforfastertransformers. InProceedingsofthe2021ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing,pages10619–10629,2021.\n[Lai et al., 2015] Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent convolutional neural\nnetworksfortextclassification. InTwenty-ninthAAAIconferenceonartificialintelligence,2015.\n[LakeandBaroni,2018] BrendenLakeandMarcoBaroni. Generalizationwithoutsystematicity: On\nthecompositionalskillsofsequence-to-sequencerecurrentnetworks. InInternationalconferenceon\nmachinelearning,pages2873–2882.PMLR,2018.\n[Lambertetal.,2024] NathanLambert,ValentinaPyatkin,JacobMorrison,LJMiranda,BillYuchen\nLin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and",
    "Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. arXiv\npreprintarXiv:2403.13787,2024.\n[LampleandConneau,2019] GuillaumeLampleandAlexisConneau. Cross-linguallanguagemodel\npretraining. arXivpreprintarXiv:1901.07291,2019.\n[Lample et al., 2016] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya\nKawakami,andChrisDyer. Neuralarchitecturesfornamedentityrecognition. InProceedingsofthe\n2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,pages260–270,2016.\n[Lampleetal.,2019] GuillaumeLample,AlexandreSablayrolles,Marc’AurelioRanzato,Ludovic\nDenoyer,andHervéJégou. Largememorylayerswithproductkeys. AdvancesinNeuralInformation\nProcessingSystems,32,2019.\n[Lanetal.,2020] ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,\nandRaduSoricut. Albert: Alitebertforself-supervisedlearningoflanguagerepresentations. In\nProceedingsofInternationalConferenceonLearningRepresentations,2020.",
    "[Landaueretal.,1998] ThomasKLandauer,PeterWFoltz,andDarrellLaham. Anintroductionto\nlatentsemanticanalysis. Discourseprocesses,25(2-3):259–284,1998.\n[Langetal.,1990] KevinJLang,AlexHWaibel,andGeoffreyEHinton. Atime-delayneuralnetwork",
    "668 BIBLIOGRAPHY\narchitectureforisolatedwordrecognition. Neuralnetworks,3(1):23–43,1990.\n[LapesaandEvert,2014] GabriellaLapesaandStefanEvert. Alargescaleevaluationofdistributional\nsemanticmodels: Parameters,interactionsandmodelselection. TransactionsoftheAssociationfor\nComputationalLinguistics,2:531–546,2014.\n[Lawson,2003] MarkV.Lawson. FiniteAutomata(1sted.). ChapmanandHall/CRC,2003.\n[LeandMikolov,2014] QuocLeandTomasMikolov. Distributedrepresentationsofsentencesand\ndocuments. InInternationalconferenceonmachinelearning,pages1188–1196.PMLR,2014.\n[Leblondetal.,2021] RémiLeblond,Jean-BaptisteAlayrac,LaurentSifre,MirunaPislar,LespiauJean-\nBaptiste,IoannisAntonoglou,KarenSimonyan,andOriolVinyals. Machinetranslationdecoding\nbeyond beam search. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguageProcessing,pages8410–8434,2021.\n[LeCun and Bengio, 1995] Yann LeCun and Yoshua Bengio. Convolutional networks for images,",
    "speech,andtimeseries. Thehandbookofbraintheoryandneuralnetworks,3361(10):1995,1995.\n[LeCunetal., 1989] YannLeCun, BernhardBoser, JohnSDenker, DonnieHenderson, RichardE\nHoward,WayneHubbard,andLawrenceDJackel. Backpropagationappliedtohandwrittenzipcode\nrecognition. Neuralcomputation,1(4):541–551,1989.\n[LeCunetal.,2012] YannALeCun,LéonBottou,GenevieveBOrr,andKlaus-RobertMüller. Efficient\nbackprop. InNeuralnetworks: Tricksofthetrade,pages9–48.Springer,2012.\n[Leeetal.,2023] HarrisonLee,SamratPhatale,HassanMansoor,KellieRenLu,ThomasMesnard,\nJohan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling\nreinforcementlearningfromhumanfeedbackwithaifeedback. arXivpreprintarXiv:2309.00267,\n2023.\n[Leeetal.,2017] JasonLee,KyunghyunCho,andThomasHofmann. Fullycharacter-levelneural\nmachinetranslationwithoutexplicitsegmentation.TransactionsoftheAssociationforComputational\nLinguistics,5:365–378,2017.",
    "[Leeetal.,2020] JasonLee,ElmanMansimov,andKyunghyunCho. Deterministicnon-autoregressive\nneuralsequencemodelingbyiterativerefinement. In2018ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing,EMNLP2018,pages1173–1182,2020.\n[Lee et al., 2019] John Boaz Lee, Ryan A Rossi, Sungchul Kim, Nesreen K Ahmed, and Eunyee\nKoh. Attentionmodelsingraphs: Asurvey. ACMTransactionsonKnowledgeDiscoveryfromData\n(TKDD),13(6):1–25,2019.\n[Lenci,2018] AlessandroLenci. Distributionalmodelsofwordmeaning. AnnualreviewofLinguistics,\n4:151–171,2018.\n[Lepikhinetal.,2021] DmitryLepikhin,HyoukJoongLee,YuanzhongXu,DehaoChen,OrhanFirat,\nYanpingHuang,MaximKrikun,NoamShazeer,andZhifengChen. Gshard: Scalinggiantmodels\nwithconditionalcomputationandautomaticsharding. InProceedingsofInternationalConference\nonLearningRepresentations,2021.\n[Lesteretal.,2021] BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-\nefficientprompttuning. InProceedingsofthe2021ConferenceonEmpiricalMethodsinNatural",
    "LanguageProcessing,pages3045–3059,2021.\n[Leviathan et al., 2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from\ntransformers via speculative decoding. In Proceedings of International Conference on Machine",
    "BIBLIOGRAPHY 669\nLearning,pages19274–19286.PMLR,2023.\n[LevyandGoldberg,2014] OmerLevyandYoavGoldberg. Dependency-basedwordembeddings. In\nProceedingsofthe52ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume\n2: ShortPapers),pages302–308,2014a.\n[Levy and Goldberg, 2014] Omer Levy and Yoav Goldberg. Linguistic regularities in sparse and\nexplicitwordrepresentations. InProceedingsoftheeighteenthconferenceoncomputationalnatural\nlanguagelearning,pages171–180,2014b.\n[LevyandGoldberg,2014] OmerLevyandYoavGoldberg. Neuralwordembeddingasimplicitmatrix\nfactorization. Advancesinneuralinformationprocessingsystems,27,2014c.\n[Levyetal.,2015] OmerLevy,YoavGoldberg,andIdoDagan. Improvingdistributionalsimilarity\nwith lessons learned from word embeddings. Transactions of the association for computational\nlinguistics,3:211–225,2015.\n[Lewisetal.,2020] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMo-",
    "hamed,OmerLevy,VeselinStoyanov,andLukeZettlemoyer. Bart: Denoisingsequence-to-sequence\npre-trainingfornaturallanguagegeneration,translation,andcomprehension. InProceedingsofthe\n58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages7871–7880,2020a.\n[Lewisetal.,2020] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel,\nandDouweKiela. Retrieval-augmentedgenerationforknowledge-intensivenlptasks. Advancesin\nNeuralInformationProcessingSystems,33:9459–9474,2020b.\n[Lietal.,2024] BaolinLi,YankaiJiang,VijayGadepally,andDeveshTiwari. Llminferenceserving:\nSurveyofrecentadvancesandopportunities. arXivpreprintarXiv:2407.12391,2024a.\n[Lietal.,2020] BeiLi,HuiLiu,ZiyangWang,YufanJiang,TongXiao,JingboZhu,TongranLiu,and\nChangliangLi. Doesmulti-encoderhelp? acasestudyoncontext-awareneuralmachinetranslation.",
    "InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages\n3512–3518,2020a.\n[Lietal.,2020] BeiLi,ZiyangWang,HuiLiu,YufanJiang,QuanDu,TongXiao,HuizhenWang,and\nJingboZhu. Shallow-to-deeptrainingforneuralmachinetranslation. InProceedingsofthe2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 995–1005,\n2020b.\n[Lietal.,2021] BeiLi,ZiyangWang,HuiLiu,QuanDu,TongXiao,ChunliangZhang,andJingbo\nZhu. Learninglight-weighttranslationmodelsfromdeeptransformer. InProceedingsoftheAAAI\nConferenceonArtificialIntelligence,volume35,pages13217–13225,2021a.\n[Lietal., 2022] BeiLi, QuanDu, TaoZhou, YiJing, ShuhanZhou, XinZeng, TongXiao, Jingbo\nZhu, Xuebo Liu, and Min Zhang. Ode transformer: An ordinary differential equation-inspired\nmodelforsequencegeneration. InProceedingsofthe60thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages8335–8351,2022a.",
    "[Lietal.,2022] BeiLi,TongZheng,YiJing,ChengboJiao,TongXiao,andJingboZhu. Learning\nmultiscaletransformermodelsforsequencegeneration. InInternationalConferenceonMachine\nLearning,pages13225–13241.PMLR,2022b.\n[Lietal.,2023] BeiLi,RuiWang,JunliangGuo,KaitaoSong,XuTan,HanyHassan,ArulMenezes,\nTongXiao,JiangBian,andJingBoZhu. Deliberatethengenerate: Enhancedpromptingframework",
    "670 BIBLIOGRAPHY\nfortextgeneration. arXivpreprintarXiv:2305.19835,2023a.\n[Li,2011] HangLi. LearningtoRankforInformationRetrievalandNaturalLanguageProcessing.\nOnlineaccess: Morgan&ClaypoolSynthesisCollectionFive.Morgan&ClaypoolPublishers,2011.\nISBN9781608457076.\n[Lietal.,2022] HongkangLi,MengWang,SijiaLiu,andPin-YuChen. Atheoreticalunderstanding\nofshallowvisiontransformers: Learning,generalization,andsamplecomplexity. InTheEleventh\nInternationalConferenceonLearningRepresentations,2022c.\n[Lietal.,2022] HuayangLi,YixuanSu,DengCai,YanWang,andLemaoLiu. Asurveyonretrieval-\naugmentedtextgeneration. arXivpreprintarXiv:2202.01110,2022d.\n[Li et al., 2021] Jicheng Li, Pengzhi Gao, Xuanfu Wu, Yang Feng, Zhongjun He, Hua Wu, and\nHaifengWang. Mixupdecodingfordiversemachinetranslation. InFindingsoftheAssociationfor\nComputationalLinguistics: EMNLP2021,pages312–320,2021b.\n[Lietal.,2020] JingLi,AixinSun,JiangleiHan,andChenliangLi. Asurveyondeeplearningfor",
    "namedentityrecognition. IEEETransactionsonKnowledgeandDataEngineering,34(1):50–70,\n2020c.\n[LiandJurafsky,2016] JiweiLiandDanJurafsky. Mutualinformationanddiversedecodingimprove\nneuralmachinetranslation. arXivpreprintarXiv:1601.00372,2016.\n[Li et al., 2016] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. A\ndiversity-promotingobjectivefunctionforneuralconversationmodels. InProceedingsofthe2016\nConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\nLanguageTechnologies,pages110–119,2016.\n[Lietal.,2017] JiweiLi,WillMonroe,andDanJurafsky. Learningtodecodeforfuturesuccess. arXiv\npreprintarXiv:1701.06549,2017a.\n[Lietal.,2017] JunhuiLi,DeyiXiong,ZhaopengTu,MuhuaZhu,MinZhang,andGuodongZhou.\nModelingsourcesyntaxforneuralmachinetranslation. InProceedingsofthe55thAnnualMeeting\noftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages688–697,2017b.",
    "[Lietal.,2021] JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,ShafiqJoty,CaimingXiong,\nandStevenChuHongHoi. Alignbeforefuse: Visionandlanguagerepresentationlearningwith\nmomentumdistillation. Advancesinneuralinformationprocessingsystems,34:9694–9705,2021c.\n[Lietal.,2022] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-\nimage pre-training for unified vision-language understanding and generation. In International\nConferenceonMachineLearning,pages12888–12900.PMLR,2022e.\n[Lietal.,2024] ShandaLi,ChongYou,GuruGuruganesh,JoshuaAinslie,SantiagoOntanon,Manzil\nZaheer,SumitSanghai,YimingYang,SanjivKumar,andSrinadhBhojanapalli. Functionalinter-\npolation for relative positions improves long context transformers. In The Twelfth International\nConferenceonLearningRepresentations,2024b.\n[Lietal.,2023] ShengguiLi,FuzhaoXue,ChaitanyaBaranwal,YongbinLi,andYangYou. Sequence\nparallelism: Longsequencetrainingfromsystemperspective. InProceedingsofthe61stAnnual",
    "MeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages2391–2404,\n2023b.\n[LiandLiang,2021] XiangLisaLiandPercyLiang. Prefix-tuning: Optimizingcontinuousprompts\nforgeneration. InProceedingsofthe59thAnnualMeetingoftheAssociationforComputational",
    "BIBLIOGRAPHY 671\nLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:\nLongPapers),pages4582–4597,2021.\n[Li et al., 2019] Xintong Li, Guanlin Li, Lemao Liu, Max Meng, and Shuming Shi. On the word\nalignment from neural machine translation. In Proceedings of the 57th Annual Meeting of the\nAssociationforComputationalLinguistics,pages1293–1303,2019.\n[Lietal.,2022] YanghaoLi,Chao-YuanWu,HaoqiFan,KarttikeyaMangalam,BoXiong,Jitendra\nMalik,andChristophFeichtenhofer. Mvitv2: Improvedmultiscalevisiontransformersforclassifica-\ntionanddetection. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern\nRecognition,pages4804–4814,2022f.\n[Lietal.,2018] YanyangLi,TongXiao,YinqiaoLi,QiangWang,ChangmingXu,andJingboZhu. A\nsimpleandeffectiveapproachtocoverage-awareneuralmachinetranslation. InProceedingsofthe\n56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2: ShortPapers),\npages292–297,2018.",
    "[Li,2023] YinhengLi. Apracticalsurveyonzero-shotpromptdesignforin-contextlearning. InPro-\nceedingsofthe14thInternationalConferenceonRecentAdvancesinNaturalLanguageProcessing,\npages641–647,2023.\n[Lietal.,2023] YuchengLi,BoDong,FrankGuerin,andChenghuaLin. Compressingcontextto\nenhanceinferenceefficiencyoflargelanguagemodels. InProceedingsofthe2023Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,pages6342–6353,2023c.\n[Li et al., 2021] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of\nconvolutionalneuralnetworks: analysis,applications,andprospects. IEEEtransactionsonneural\nnetworksandlearningsystems,2021d.\n[Lialinetal.,2023] VladislavLialin,VijetaDeshpande,andAnnaRumshisky. Scalingdowntoscale\nup: Aguidetoparameter-efficientfine-tuning. arXivpreprintarXiv:2303.15647,2023.\n[Liaoetal.,2021] KaiyuanLiao,YiZhang,XuanchengRen,QiSu,XuSun,andBinHe. Aglobalpast-\nfutureearlyexitmethodforacceleratinginferenceofpre-trainedlanguagemodels. InProceedingsof",
    "the2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,pages2013–2023,2021.\n[Lightmanetal.,2024] HunterLightman,VineetKosaraju,YuriBurda,HarrisonEdwards,Bowen\nBaker,TeddyLee,JanLeike,JohnSchulman,IlyaSutskever,andKarlCobbe. Let’sverifystepby\nstep. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.\n[Likhomanenkoetal.,2021] TatianaLikhomanenko,QiantongXu,GabrielSynnaeve,RonanCollobert,\nandAlexRogozhnikov. Cape: Encodingrelativepositionswithcontinuousaugmentedpositional\nembeddings. AdvancesinNeuralInformationProcessingSystems,34:16079–16092,2021.\n[Linetal.,2022] TianyangLin,YuxinWang,XiangyangLiu,andXipengQiu.Asurveyoftransformers.\nAIOpen,2022a.\n[Linetal.,2022] YeLin,ShuhanZhou,YanyangLi,AnxiangMa,TongXiao,andJingboZhu. Multi-\npathtransformerisbetter: Acasestudyonneuralmachinetranslation. InFindingsoftheAssociation\nforComputationalLinguistics: EMNLP2022,pages5646–5656,2022b.",
    "[Linetal.,2017] ZhouhanLin,MinweiFeng,CiceroNogueiradosSantos,MoYu,BingXiang,Bowen\nZhou,andYoshuaBengio. Astructuredself-attentivesentenceembedding. InProceedingsofthe5th\nInternationalConferenceonLearningRepresentations(ICLR),2017.",
    "672 BIBLIOGRAPHY\n[Lingetal.,2015] WangLing,ChrisDyer,AlanWBlack,IsabelTrancoso,RamónFermandez,Silvio\nAmir,LuisMarujo,andTiagoLuís. Findingfunctioninform: Compositionalcharactermodelsfor\nopenvocabularywordrepresentation. InProceedingsofthe2015ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages1520–1530,2015.\n[Linzen,2016] TalLinzen. Issuesinevaluatingsemanticspacesusingwordanalogies. InProceedings\nofthe1stWorkshoponEvaluatingVector-SpaceRepresentationsforNLP,pages13–18,2016.\n[Lippmann,1989] RichardPLippmann. Reviewofneuralnetworksforspeechrecognition. Neural\ncomputation,1(1):1–38,1989.\n[Lipton et al., 2015] Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of\nrecurrentneuralnetworksforsequencelearning. arXivpreprintarXiv:1506.00019,2015.\n[Liuetal.,2024] AixinLiu,BeiFeng,BingXue,BingxuanWang,BochaoWu,ChengdaLu,Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv\npreprintarXiv:2412.19437,2024a.",
    "[Liuetal.,2020] FenglinLiu,XuanchengRen,ZhiyuanZhang,XuSun,andYuexianZou. Rethinking\nskipconnectionwithlayernormalization. InProceedingsofthe28thinternationalconferenceon\ncomputationallinguistics,pages3586–3598,2020a.\n[Liuetal., 2023] HaotianLiu, ChunyuanLi, QingyangWu, andYongJaeLee. Visualinstruction\ntuning. arXivpreprintarXiv:2304.08485,2023a.\n[LiuandMotoda,2012] HuanLiuandHiroshiMotoda. Featureselectionforknowledgediscoveryand\ndatamining,volume454. SpringerScience&BusinessMedia,2012.\n[Liuetal.,2022] JiachangLiu,DinghanShen,YizheZhang,WilliamBDolan,LawrenceCarin,and\nWeizhuChen. Whatmakesgoodin-contextexamplesforgpt-3? InProceedingsofDeepLearning\nInsideOut(DeeLIO2022): The3rdWorkshoponKnowledgeExtractionandIntegrationforDeep\nLearningArchitectures,pages100–114,2022.\n[Liuetal.,2016] LemaoLiu,MasaoUtiyama,AndrewFinch,andEiichiroSumita. Agreementon\ntarget-bidirectionalneuralmachinetranslation. InProceedingsofthe2016ConferenceoftheNorth",
    "AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,\npages411–416,2016a.\n[Liuetal.,2016] LemaoLiu,MasaoUtiyama,AndrewFinch,andEiichiroSumita. Neuralmachine\ntranslation with supervised attention. In Proceedings of COLING 2016, the 26th International\nConferenceonComputationalLinguistics: TechnicalPapers,pages3093–3102,2016b.\n[Liuetal.,2020] LiyuanLiu,XiaodongLiu,JianfengGao,WeizhuChen,andJiaweiHan. Under-\nstandingthedifficultyoftrainingtransformers. InProceedingsofthe2020ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing(EMNLP),pages5747–5763,2020b.\n[Liuetal.,2020] LiyuanLiu,XiaodongLiu,JianfengGao,WeizhuChen,andJiaweiHan. Under-\nstandingthedifficultyoftrainingtransformers. InProceedingsofthe2020ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing(EMNLP),pages5747–5763,November2020c.\n[Liuetal.,2023] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGraham",
    "Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural\nlanguageprocessing. ACMComputingSurveys,55(9):1–35,2023b.\n[Liuetal.,2018] PeterJLiu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,Lukasz\nKaiser,andNoamShazeer. Generatingwikipediabysummarizinglongsequences. InProceedings",
    "BIBLIOGRAPHY 673\nofInternationalConferenceonLearningRepresentations,2018.\n[Liuetal.,2017] ShusenLiu,Peer-TimoBremer,JayaramanJThiagarajan,VivekSrikumar,BeiWang,\nYardenLivnat,andValerioPascucci. Visualexplorationofsemanticrelationshipsinneuralword\nembeddings. IEEEtransactionsonvisualizationandcomputergraphics,24(1):553–562,2017.\n[Liuetal.,2024] TianqiLiu,YaoZhao,RishabhJoshi,MishaKhalman,MohammadSaleh,PeterJ\nLiu,andJialuLiu. Statisticalrejectionsamplingimprovespreferenceoptimization. InTheTwelfth\nInternationalConferenceonLearningRepresentations,2024b.\n[Liu, 2009] Tie-YanLiu. Learningtorankforinformationretrieval. FoundationsandTrends®in\nInformationRetrieval,3(3):225–331,2009.\n[Liuetal.,2023] XiaoLiu,YananZheng,ZhengxiaoDu,MingDing,YujieQian,ZhilinYang,andJie\nTang. Gptunderstands,too. AIOpen,2023c.\n[Liuetal.,2023] XiaoxiaLiu,JingyiWang,JunSun,XiaohanYuan,GuoliangDong,PengDi,Wenhai\nWang, and Dongxia Wang. Prompting frameworks for large language models: A survey. arXiv",
    "preprintarXiv:2311.12785,2023d.\n[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang\nWang,TongXiao,andJingboZhu. Forgettingcurve: Areliablemethodforevaluatingmemorization\ncapabilityforlong-contextmodels. InProceedingsofthe2024ConferenceonEmpiricalMethodsin\nNaturalLanguageProcessing,pages4667–4682,2024c.\n[Liuetal.,2023] YangLiu,YaoZhang,YixinWang,FengHou,JinYuan,JiangTian,YangZhang,\nZhongchaoShi,JianpingFan,andZhiqiangHe. Asurveyofvisualtransformers. IEEETransactions\nonNeuralNetworksandLearningSystems,2023e.\n[Liuetal.,2019] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer\nLevy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov. Roberta: Arobustlyoptimizedbert\npretrainingapproach. arXivpreprintarXiv:1907.11692,2019.\n[Liuetal.,2020] YuchenLiu,JunnanZhu,JiajunZhang,andChengqingZong. Bridgingthemodality\ngapforspeech-to-texttranslation. arXivpreprintarXiv:2010.14920,2020d.",
    "[Longpreetal.,2023] ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,\nDennyZhou,QuocV.Le,BarretZoph,JasonWei,andAdamRoberts.Theflancollection:Designing\ndataandmethodsforeffectiveinstructiontuning. InInternationalConferenceonMachineLearning,\npages22631–22648.PMLR,2023.\n[Lopez,2008] AdamLopez. Statisticalmachinetranslation. ACMComputingSurveys(CSUR),40(3):\n1–49,2008.\n[Luetal.,2016] JiasenLu,JianweiYang,DhruvBatra,andDeviParikh. Hierarchicalquestion-image\nco-attentionforvisualquestionanswering. Advancesinneuralinformationprocessingsystems,29,\n2016.\n[Lund,1995] KevinLund. Semanticandassociativepriminginhigh-dimensionalsemanticspace. In\nProc.ofthe17thAnnualconferencesoftheCognitiveScienceSociety,1995,1995.\n[LundandBurgess,1996] KevinLundandCurtBurgess. Producinghigh-dimensionalsemanticspaces\nfromlexicalco-occurrence. Behaviorresearchmethods,instruments,&computers,28(2):203–208,\n1996.\n[Luongetal.,2015] Minh-ThangLuong,HieuPham,andChristopherDManning.Effectiveapproaches",
    "toattention-basedneuralmachinetranslation. InProceedingsofthe2015ConferenceonEmpirical",
    "674 BIBLIOGRAPHY\nMethodsinNaturalLanguageProcessing,pages1412–1421,2015.\n[Maetal.,2019] MingboMa,LiangHuang,HaoXiong,RenjieZheng,KaiboLiu,BaigongZheng,\nChuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. Stacl:\nSimultaneoustranslationwithimplicitanticipationandcontrollablelatencyusingprefix-to-prefix\nframework. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics,pages3025–3036,2019.\n[MaandHovy,2016] XuezheMaandEduardHovy. End-to-endsequencelabelingviabi-directional\nlstm-cnns-crf. InProceedingsofthe54thAnnualMeetingoftheAssociationforComputational\nLinguistics(Volume1: LongPapers),pages1064–1074,2016.\n[Maetal.,2023] XuezheMa,ChuntingZhou,XiangKong,JunxianHe,LiangkeGui,GrahamNeubig,\nJonathanMay, andLukeZettlemoyer. Mega: Movingaverageequippedgatedattention. InThe\nEleventhInternationalConferenceonLearningRepresentations,2023.\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang,",
    "Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm\npretrainingandinferencewithunlimitedcontextlength. arXivpreprintarXiv:2404.08801,2024.\n[Madaan et al., 2024] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao,\nSarahWiegreffe,UriAlon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,ShashankGupta,Bod-\nhisattwaPrasadMajumder,KatherineHermann,SeanWelleck,AmirYazdanbakhsh,andPeterClark.\nSelf-refine: Iterativerefinementwithself-feedback. AdvancesinNeuralInformationProcessing\nSystems,36,2024.\n[Malaviya et al., 2018] Chaitanya Malaviya, Pedro Ferreira, and André FT Martins. Sparse and\nconstrainedattentionforneuralmachinetranslation. InProceedingsofthe56thAnnualMeetingof\ntheAssociationforComputationalLinguistics(Volume2: ShortPapers),pages370–376,2018.\n[ManningandSchütze,1999] ChrisManningandHinrichSchütze. FoundationsofStatisticalNatural\nLanguageProcessing. TheMITPress,1999.",
    "[Manning,2022] ChristopherDManning. Humanlanguageunderstanding&reasoning. Daedalus,\n151(2):127–138,2022.\n[Manningetal.,2008] ChristopherD.Manning,PrabhakarRaghavan,andHinrichSchütze.Introduction\ntoInformationRetrieval. CambridgeUniversityPress,2008.\n[Manningetal.,2020] ChristopherDManning,KevinClark,JohnHewitt,UrvashiKhandelwal,and\nOmerLevy. Emergentlinguisticstructureinartificialneuralnetworkstrainedbyself-supervision.\nProceedingsoftheNationalAcademyofSciences,117(48):30046–30054,2020.\n[Marcus,1993] GaryFMarcus. Negativeevidenceinlanguageacquisition. Cognition,46(1):53–85,\n1993.\n[Markman,2013] ArthurBMarkman. Knowledgerepresentation. PsychologyPress,2013.\n[Markov,1913] AAMarkov. Essaid’unerecherchestatistiquesurletexteduroman. EugeneOnegin”\nillustrantlaliaisondesepreuveenchain(‘Exampleofastatisticalinvestigationofthetextof“Eugene\nOnegin”illustratingthedependencebetweensamplesinchain”).In:IzvistiaImperatorskoiAkademii",
    "Nauk(Bulletindel’AcadémieImpérialedesSciencesdeSt.-Pétersbourg).6thser,7:153–162,1913.\n[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho, and André FT Martins. ∞-former:\nInfinite memory transformer-former: Infinite memory transformer. In Proceedings of the 60th\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages",
    "BIBLIOGRAPHY 675\n5468–5485,2022.\n[Marufetal.,2019] SameenMaruf,AndréFTMartins,andGholamrezaHaffari. Selectiveattention\nforcontext-awareneuralmachinetranslation. InProceedingsofthe2019ConferenceoftheNorth\nAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,\nVolume1(LongandShortPapers),pages3092–3102,2019.\n[MasoudniaandEbrahimpour,2014] SaeedMasoudniaandRezaEbrahimpour. Mixtureofexperts: a\nliteraturesurvey. TheArtificialIntelligenceReview,42(2):275,2014.\n[Matusovetal.,2006] EvgenyMatusov,NicolaUeffing,andHermannNey. Computingconsensus\ntranslationformultiplemachinetranslationsystemsusingenhancedhypothesisalignment. In11th\nConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages33–40,\n2006.\n[Mavietal.,2024] VaibhavMavi,AnubhavJangra,andAdamJatowt. Multi-hopquestionanswering.\nFoundationsandTrends®inInformationRetrieval,17(5):457–586,2024.\n[McCallumetal.,2000] AndrewMcCallum,DayneFreitag,andFernandoC.N.Pereira. Maximumen-",
    "tropymarkovmodelsforinformationextractionandsegmentation. InProceedingsoftheSeventeenth\nInternationalConferenceonMachineLearning,pages591–598,2000.\n[McCannetal.,2017] BryanMcCann,JamesBradbury,CaimingXiong,andRichardSocher. Learned\nintranslation: Contextualizedwordvectors. Advancesinneuralinformationprocessingsystems,30,\n2017.\n[McCarleyetal.,2019] JSMcCarley,RishavChakravarti,andAvirupSil. Structuredpruningofa\nbert-basedquestionansweringmodel. arXivpreprintarXiv:1910.06360,2019.\n[McClaveandSincich,2006] JamesT.McClaveandTerrySincich. Statistics(10thed.). PrenticeHall,\n2006.\n[McCullochandPitts,1943] WarrenS.McCullochandWalterPitts. Alogicalcalculusoftheideas\nimmanentinnervousactivity. Thebulletinofmathematicalbiophysics,5(4):115–133,1943.\n[McElreath,2020] RichardMcElreath. StatisticalRethinking: ABayesianCoursewithExamplesinR\nandSTAN(2nded.). ChapmanandHall/CRC,2020.\n[McNamara,2005] TimothyPMcNamara. Semanticpriming: Perspectivesfrommemoryandword\nrecognition. PsychologyPress,2005.",
    "[Meisteretal.,2020] ClaraMeister,TimVieira,andRyanCotterell. Best-firstbeamsearch. Transac-\ntionsoftheAssociationforComputationalLinguistics,8:795–809,2020.\n[Merrilletal.,2022] WilliamMerrill,AshishSabharwal,andNoahASmith. Saturatedtransformers\nareconstant-depththresholdcircuits. TransactionsoftheAssociationforComputationalLinguistics,\n10:843–856,2022.\n[MeyerandSchvaneveldt,1971] DavidEMeyerandRogerWSchvaneveldt.Facilitationinrecognizing\npairsofwords: evidenceofadependencebetweenretrievaloperations. Journalofexperimental\npsychology,90(2):227,1971.\n[Mi et al., 2016] Haitao Mi, Baskaran Sankaran, Zhiguo Wang, and Abe Ittycheriah. Coverage\nembedding models for neural machine translation. In Proceedings of the 2016 Conference on\nEmpiricalMethodsinNaturalLanguageProcessing,pages955–960,2016a.\n[Mi et al., 2016] Haitao Mi, Zhiguo Wang, and Abe Ittycheriah. Supervised attentions for neural",
    "676 BIBLIOGRAPHY\nmachine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguageProcessing,pages2283–2288,2016b.\n[Micheletal.,2019] PaulMichel,OmerLevy,andGrahamNeubig. Aresixteenheadsreallybetter\nthanone? Advancesinneuralinformationprocessingsystems,32,2019.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos,\nErichElsen,DavidGarcia,BorisGinsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,\nandHaoWu. Mixedprecisiontraining. InProceedingsofInternationalConferenceonLearning\nRepresentations,2018.\n[Mielkeetal.,2021] SabrinaJ.Mielke,ZaidAlyafeai,ElizabethSalesky,ColinRaffel,MananDey,\nMatthiasGallé,ArunRaja,ChengleiSi,WilsonY.Lee,BenoîtSagot,andSamsonTan. Between\nwordsandcharacters: Abriefhistoryofopen-vocabularymodelingandtokenizationinnlp. arXiv\npreprintarXiv:2112.10508,2021.\n[Miettinen,1999] KaisaMiettinen.Nonlinearmultiobjectiveoptimization,volume12.SpringerScience\n&BusinessMedia,1999.",
    "[Mikolovetal., 2010] TomasMikolov, MartinKarafiát, LukasBurget, JanCernocky`, andSanjeev\nKhudanpur. Recurrentneuralnetworkbasedlanguagemodel. InProceedingsofInterspeech,pages\n1045–1048,2010.\n[Mikolovetal.,2013] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimation\nofwordrepresentationsinvectorspace. InProceedingsoftheInternationalConferenceonLearning\nRepresentations(ICLR2013),2013a.\n[Mikolovetal.,2013] TomasMikolov,QuocVLe,andIlyaSutskever. Exploitingsimilaritiesamong\nlanguagesformachinetranslation. arXivpreprintarXiv:1309.4168,2013b.\n[Mikolovetal., 2013] TomasMikolov, IlyaSutskever, KaiChen, GregCorrado, andJeffreyDean.\nDistributed representations of words and phrases and their compositionality. In Proceedings of\nthe26thInternationalConferenceonNeuralInformationProcessingSystems-Volume2, pages\n3111–3119,2013c.\n[Mikolovetal.,2013] TomasMikolov,Wen-tauYih,andGeoffreyZweig. Linguisticregularitiesin",
    "continuousspacewordrepresentations. InProceedingsofthe2013conferenceofthenorthamerican\nchapter of the association for computational linguistics: Human language technologies, pages\n746–751,2013d.\n[Miller et al., 2016] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine\nBordes, and Jason Weston. Key-value memory networks for directly reading documents. In\nProceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages\n1400–1409,2016.\n[Minetal.,2019] SewonMin,VictorZhong,LukeZettlemoyer,andHannanehHajishirzi. Multi-hop\nreadingcomprehensionthroughquestiondecompositionandrescoring. InProceedingsofthe57th\nAnnualMeetingoftheAssociationforComputationalLinguistics,pages6097–6109,2019.\n[Minaeeetal.,2024] ShervinMinaee,TomasMikolov,NarjesNikzad,MeysamChenaghlu,Richard\nSocher, XavierAmatriain, andJianfengGao. Largelanguagemodels: Asurvey. arXivpreprint\narXiv:2402.06196,2024.\n[MinskyandPapert,1969] MarvinMinskyandSeymourPapert. Perceptrons. MITpress,1969.",
    "[Mishraetal.,2022] SwaroopMishra,DanielKhashabi,ChittaBaral,andHannanehHajishirzi. Cross-",
    "BIBLIOGRAPHY 677\ntask generalization via natural language crowdsourcing instructions. In Proceedings of the 60th\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages\n3470–3487,2022.\n[MitchellandLapata,2010] JeffMitchellandMirellaLapata. Compositionindistributionalmodelsof\nsemantics. Cognitivescience,34(8):1388–1429,2010.\n[Mitchell,1997] TomM.Mitchell. MachineLearning. McGraw-HillEducation,1997.\n[MnihandKavukcuoglu,2013] AndriyMnihandKorayKavukcuoglu. Learningwordembeddings\nefficientlywithnoise-contrastiveestimation. Advancesinneuralinformationprocessingsystems,26,\n2013.\n[Mnihetal.,2016] VolodymyrMnih,AdriàPuigdomènechBadia,MehdiMirza,AlexGraves,Tim\nHarley, TimothyPLillicrap, DavidSilver, andKorayKavukcuoglu. Asynchronousmethodsfor\ndeepreinforcementlearning. InProceedingsofthe33rdInternationalConferenceonInternational\nConferenceonMachineLearning,pages1928–1937,2016.\n[Mohrietal., 2018] MehryarMohri, AfshinRostamizadeh, andAmeetTalwalkar. Foundationsof",
    "MachineLearning(2nded.). MITPress,2018.\n[MohtashamiandJaggi,2024] AmirkeivanMohtashamiandMartinJaggi. Random-accessinfinite\ncontextlengthfortransformers. AdvancesinNeuralInformationProcessingSystems,36,2024.\n[Montague,1974] RichardMontague. Universalgrammar. InR.Thomason,editor,FormalPhilosophy:\nSelectedPapersofRichardMontague.YaleUniversityPress,1974.\n[Muetal.,2024] JesseMu,XiangLi,andNoahGoodman. Learningtocompresspromptswithgist\ntokens. AdvancesinNeuralInformationProcessingSystems,36,2024.\n[Müller and Sennrich, 2021] Mathias Müller and Rico Sennrich. Understanding the properties of\nminimumbayesriskdecodinginneuralmachinetranslation. InProceedingsofthe59thAnnual\nMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConference\nonNaturalLanguageProcessing(Volume1: LongPapers),pages259–272,2021.\n[Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave",
    "no context behind: Efficient infinite context transformers with infini-attention. arXiv preprint\narXiv:2404.07143,2024.\n[Murphy,2012] KevinP.Murphy. MachineLearning: AProbabilisticPerspective. MITPress,2012.\n[MurrayandChiang,2018] KentonMurrayandDavidChiang.Correctinglengthbiasinneuralmachine\ntranslation. InProceedingsoftheThirdConferenceonMachineTranslation: ResearchPapers,pages\n212–223,2018.\n[Nageletal.,2021] MarkusNagel,MariosFournarakis,RanaAliAmjad,YelyseiBondarenko,Mart\nVanBaalen,andTijmenBlankevoort. Awhitepaperonneuralnetworkquantization. arXivpreprint\narXiv:2106.08295,2021.\n[NairandHinton,2009] VinodNairandGeoffreyEHinton. 3dobjectrecognitionwithdeepbelief\nnets. Advancesinneuralinformationprocessingsystems,22,2009.\n[Nakanoetal.,2021] ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,Christina\nKim,ChristopherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,\nTyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John",
    "Schulman. Webgpt: Browser-assistedquestion-answeringwithhumanfeedback. arXivpreprint\narXiv:2112.09332,2021.",
    "678 BIBLIOGRAPHY\n[Narayananetal.,2021] DeepakNarayanan,MohammadShoeybi,JaredCasper,PatrickLeGresley,\nMostofaPatwary,VijayKorthikanti,DmitriVainbrand,PrethviKashinkunti,JulieBernauer,Bryan\nCatanzaro,AmarPhanishayee,andMateiZaharia. Efficientlarge-scalelanguagemodeltraining\non gpu clusters using megatron-lm. In Proceedings of the International Conference for High\nPerformanceComputing,Networking,StorageandAnalysis,pages1–15,2021.\n[Neelakantanetal.,2015] ArvindNeelakantan,LukeVilnis,QuocVLe,IlyaSutskever,LukaszKaiser,\nKarolKurach,andJamesMartens. Addinggradientnoiseimproveslearningforverydeepnetworks.\narXivpreprintarXiv:1511.06807,2015.\n[Neisser,2014] UlricNeisser. CognitivePsychology: ClassicEdition. PsychologyPress,2014.\n[Nesterov, 1983] Yurii E Nesterov. A method for solving the convex programming problem with\nconvergencerateo(1/kˆ2). InDokl.akad.naukSssr,volume269,pages543–547,1983.\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russell. Policy invariance under",
    "rewardtransformations: Theoryandapplicationtorewardshaping. InProceedingsoftheSixteenth\nInternationalConferenceonMachineLearning,pages278–287,1999.\n[Nguyenetal.,2020] Xuan-PhiNguyen,ShafiqJoty,StevenHoi,andRichardSocher. Tree-structured\nattentionwithhierarchicalaccumulation. InProceedingsofthe8thInternationalConferenceon\nLearningRepresentationsICLR,2020.\n[Nvidia, 2025] Nvidia. Nvidia nim llms benchmarking. https://docs.nvidia.com/nim/\nbenchmarking/llm/latest/metrics.html,2025. Retrieved2025-03-17.\n[Och, 2003] Franz Josef Och. Minimum error rate training in statistical machine translation. In\nProceedingsofthe41stannualmeetingoftheAssociationforComputationalLinguistics, pages\n160–167,2003.\n[Och and Ney, 2002] Franz Josef Och and Hermann Ney. Discriminative training and maximum\nentropymodelsforstatisticalmachinetranslation. InProceedingsofthe40thAnnualMeetingofthe\nAssociationforComputationalLinguistics,pages295–302,2002.",
    "[OchandNey,2003] FranzJosefOchandHermannNey. Asystematiccomparisonofvariousstatistical\nalignmentmodels. ComputationalLinguistics,29(1):19–51,2003.\n[Olive,2022] DavidOlive.Robuststatistics,2022.URLhttp://parker.ad.siu.edu/Olive/\nol-bookp.htm.\n[OlshausenandField,1997] BrunoAOlshausenandDavidJField.Sparsecodingwithanovercomplete\nbasisset: Astrategyemployedbyv1? Visionresearch,37(23):3311–3325,1997.\n[Oord et al., 2017] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete\nrepresentationlearning. Advancesinneuralinformationprocessingsystems,30,2017.\n[Oordetal.,2018] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwith\ncontrastivepredictivecoding. arXivpreprintarXiv:1807.03748,2018.\n[OpenAI,2024] OpenAI. Learningtoreasonwithllms,September2024. URLhttps://openai.\ncom/index/learning-to-reason-with-llms/.\n[OpitzandMaclin,1999] DavidOpitzandRichardMaclin. Popularensemblemethods: Anempirical\nstudy. Journalofartificialintelligenceresearch,11:169–198,1999.",
    "[OppenheimandSchafer,1975] AlanVOppenheimandRonaldWSchafer. Digitalsignalprocess-\ning(book). Prentice-Hall,1975.",
    "BIBLIOGRAPHY 679\n[Orvieto et al., 2023] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar\nGulcehre,RazvanPascanu,andSohamDe.Resurrectingrecurrentneuralnetworksforlongsequences.\narXivpreprintarXiv:2303.06349,2023.\n[Osgood,1952] CharlesEOsgood. Thenatureandmeasurementofmeaning. Psychologicalbulletin,\n49(3):197,1952.\n[Ottetal., 2018] MyleOtt, MichaelAuli, DavidGrangier, andMarc’AurelioRanzato. Analyzing\nuncertaintyinneuralmachinetranslation. InInternationalConferenceonMachineLearning,pages\n3956–3965.PMLR,2018a.\n[Ottetal.,2018] MyleOtt,SergeyEdunov,DavidGrangier,andMichaelAuli. Scalingneuralmachine\ntranslation. InProceedingsoftheThirdConferenceonMachineTranslation: ResearchPapers,pages\n1–9,October2018b.\n[Ottetal.,2019] MyleOtt,SergeyEdunov,AlexeiBaevski,AngelaFan,SamGross,NathanNg,David\nGrangier,andMichaelAuli.fairseq:Afast,extensibletoolkitforsequencemodeling.InProceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational",
    "Linguistics(Demonstrations),pages48–53,2019.\n[Ouyangetal.,2022] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,\nPamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman,\nJacobHilton,FraserKelton,LukeMiller,MaddieSimens,AmandaAskell,PeterWelinder,PaulF.\nChristiano,JanLeike,andRyanLowe. Traininglanguagemodelstofollowinstructionswithhuman\nfeedback. AdvancesinNeuralInformationProcessingSystems,35:27730–27744,2022.\n[Padó and Lapata, 2007] Sebastian Padó and Mirella Lapata. Dependency-based construction of\nsemanticspacemodels. ComputationalLinguistics,33(2):161–199,2007.\n[Paletal.,2023] KoyenaPal,JiudingSun,AndrewYuan,ByronCWallace,andDavidBau.Futurelens:\nAnticipatingsubsequenttokensfromasinglehiddenstate. InProceedingsofthe27thConferenceon\nComputationalNaturalLanguageLearning(CoNLL),pages548–560,2023.\n[Panetal.,2022] AlexanderPan,KushBhatia,andJacobSteinhardt. Theeffectsofrewardmisspec-",
    "ification: Mappingandmitigatingmisalignedmodels. InInternationalConferenceonLearning\nRepresentations,2022.\n[Pan et al., 2024] Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and\nWilliamYangWang. Automaticallycorrectinglargelanguagemodels: Surveyingthelandscape\nof diverse automated correction strategies. Transactions of the Association for Computational\nLinguistics,12:484–506,2024.\n[Pang et al., 2002] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. Thumbs up? sentiment\nclassificationusingmachinelearningtechniques.InProceedingsofthe2002ConferenceonEmpirical\nMethodsinNaturalLanguageProcessing(EMNLP2002),pages79–86,2002.\n[Papinenietal.,2002] KishorePapineni,SalimRoukos,ToddWard,andWeijingZhu. Bleu: amethod\nforautomaticevaluationofmachinetranslation. InProceedingsofthe40thAnnualmeetingofthe\nAssociationforComputationalLinguistics,pages311–318,2002.\n[Parisietal.,2022] AaronParisi,YaoZhao,andNoahFiedel. Talm: Toolaugmentedlanguagemodels.",
    "arXivpreprintarXiv:2205.12255,2022.\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan\nWermter. Continuallifelonglearningwithneuralnetworks: Areview. Neuralnetworks,113:54–71,",
    "680 BIBLIOGRAPHY\n2019.\n[Park et al., 2019] Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho. Relational knowledge\ndistillation. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,\npages3967–3976,2019.\n[Parmaretal.,2018] NikiParmar,AshishVaswani,JakobUszkoreit,LukaszKaiser,NoamShazeer,\nAlexanderKu,andDustinTran. Imagetransformer. InInternationalconferenceonmachinelearning,\npages4055–4064.PMLR,2018.\n[Pascanuetal.,2013] RazvanPascanu,TomasMikolov,andYoshuaBengio.Onthedifficultyoftraining\nrecurrent neural networks. In International conference on machine learning, pages 1310–1318.\nPMLR,2013.\n[Pateletal.,2024] PratyushPatel,EshaChoukse,ChaojieZhang,AashakaShah,ÍñigoGoiri,Saeed\nMaleki,andRicardoBianchini. Splitwise: Efficientgenerativellminferenceusingphasesplitting.\nIn2024ACM/IEEE51stAnnualInternationalSymposiumonComputerArchitecture(ISCA),pages\n118–132.IEEE,2024.\n[Pearson,1901] KarlPearson. Onlinesandplanesofclosestfittosystemsofpointsinspace. The",
    "London, Edinburgh, and Dublin philosophical magazine and journal of science, 2(11):559–572,\n1901.\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,\nAlessandroCappelli,HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay.\nTherefinedwebdatasetforfalconllm: outperformingcuratedcorporawithwebdata,andwebdata\nonly. arXivpreprintarXiv:2306.01116,2023.\n[Pengetal.,2019] BaoyunPeng,XiaoJin,JiahengLiu,DongshengLi,YichaoWu,YuLiu,Shunfeng\nZhou,andZhaoningZhang. Correlationcongruenceforknowledgedistillation. InProceedingsof\ntheIEEE/CVFInternationalConferenceonComputerVision,pages5007–5016,2019.\n[Pengetal.,2023] BoPeng,EricAlcaide,QuentinAnthony,AlonAlbalak,SamuelArcadinho,Stella\nBiderman,HuanqiCao,XinCheng,MichaelChung,LeonDerczynski,XingjianDu,MatteoGrella,\nKranthiGv,XuzhengHe,HaowenHou,PrzemyslawKazienko,JanKocon,JiamingKong,Bartlomiej\nKoptyra,HaydenLau,JiajuLin,KrishnaSriIpsitMantri,FerdinandMom,AtsushiSaito,Guangyu",
    "Song,XiangruTang,JohanS.Wind,StanislawWozniak,ZhenyuanZhang,QinghuaZhou,JianZhu,\nandRui-JieZhu. Rwkv: Reinventingrnnsforthetransformerera. arXivpreprintarXiv:2305.13048,\n2023.\n[Pengetal.,2024] BowenPeng,JeffreyQuesnelle,HongluFan,andEnricoShippole. YaRN:Efficient\ncontextwindowextensionoflargelanguagemodels. InTheTwelfthInternationalConferenceon\nLearningRepresentations,2024.\n[Pengetal.,2021] HPeng,NPappas,DYogatama,RSchwartz,NSmith,andLKong. Randomfeature\nattention. InProceedingsofInternationalConferenceonLearningRepresentations(ICLR2021),\n2021.\n[Penningtonetal.,2014] JeffreyPennington,RichardSocher,andChristopherD.Manning. Glove:\nGlobalvectorsforwordrepresentation. InProceedingsofEmpiricalMethodsinNaturalLanguage\nProcessing(EMNLP),pages1532–1543,2014.\n[Pérezetal.,2018] JorgePérez,JavierMarinkovic´,andPabloBarceló. Ontheturingcompleteness\nofmodernneuralnetworkarchitectures. InProceedingsofInternationalConferenceonLearning\nRepresentations,2018.",
    "BIBLIOGRAPHY 681\n[Perozzietal.,2014] BryanPerozzi,RamiAl-Rfou,andStevenSkiena. Deepwalk: Onlinelearning\nofsocialrepresentations. InProceedingsofthe20thACMSIGKDDinternationalconferenceon\nKnowledgediscoveryanddatamining,pages701–710,2014.\n[Petersetal.,2018] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,\nKentonLee,andLukeZettlemoyer. Deepcontextualizedwordrepresentations. InProceedingsofthe\n2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:\nHumanLanguageTechnologies,Volume1(LongPapers),2018.\n[Petronietal.,2019] FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin,\nYuxiangWu,andAlexanderMiller. Languagemodelsasknowledgebases? InProceedingsofthe\n2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternational\nJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP),pages2463–2473,2019.\n[Pham et al., 2019] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Müller, Sebastian",
    "Stüker,andAlexanderWaibel. Verydeepself-attentionnetworksforend-to-endspeechrecognition.\narXivpreprintarXiv:1904.13377,2019.\n[Picone,1993] JosephWPicone. Signalmodelingtechniquesinspeechrecognition. Proceedingsof\ntheIEEE,81(9):1215–1247,1993.\n[Piresetal.,2023] TelmoPessoaPires,AntónioVLopes,YannickAssogba,andHendraSetiawan.\nOnewidefeedforwardisallyouneed. arXivpreprintarXiv:2309.01826,2023.\n[Plackett, 1975] Robin L Plackett. The analysis of permutations. Journal of the Royal Statistical\nSocietySeriesC:AppliedStatistics,24(2):193–202,1975.\n[Plautetal.,1986] DavidCPlaut,StevenJNowlan,andGeoffreyEHinton. Experimentsonlearning\nbybackpropagation. Technicalreport,Carnegie-MellonUniversity,1986.\n[Polyak,1964] BorisTPolyak. Somemethodsofspeedinguptheconvergenceofiterationmethods.\nUssrcomputationalmathematicsandmathematicalphysics,4(5):1–17,1964.\n[Popeetal.,2023] ReinerPope,SholtoDouglas,AakankshaChowdhery,JacobDevlin,JamesBradbury,",
    "JonathanHeek,KefanXiao,ShivaniAgrawal,andJeffDean.Efficientlyscalingtransformerinference.\nInProceedingsofMachineLearningandSystems,2023.\n[Porter,1980] MartinFPorter. Analgorithmforsuffixstripping. Program,1980.\n[PostandVilar,2018] MattPostandDavidVilar. Fastlexicallyconstraineddecodingwithdynamic\nbeamallocationforneuralmachinetranslation. InProceedingsofthe2018ConferenceoftheNorth\nAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,\nVolume1(LongPapers),pages1314–1324,2018.\n[Prasadetal.,2023] ArchikiPrasad,PeterHase,XiangZhou,andMohitBansal. Grips: Gradient-\nfree, edit-based instruction search for prompting large language models. In Proceedings of the\n17thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics,pages\n3845–3864,2023.\n[Prechelt,1998] LutzPrechelt. Earlystopping-butwhen? InNeuralNetworks: Tricksofthetrade,\npages55–69.Springer,1998.\n[Pressetal.,2021] OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwith",
    "linear biases enables input length extrapolation. In Proceedings of International Conference on\nLearningRepresentations,2021.\n[Pressetal.,2022] OfirPress,NoahSmith,andMikeLewis. Trainshort,testlong: Attentionwith",
    "682 BIBLIOGRAPHY\nlinear biases enables input length extrapolation. In Proceedings of International Conference on\nLearningRepresentations,2022.\n[Pressetal.,2023] OfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahASmith,andMike\nLewis. Measuringandnarrowingthecompositionalitygapinlanguagemodels. InFindingsofthe\nAssociationforComputationalLinguistics: EMNLP2023,pages5687–5711,2023.\n[Provilkovetal.,2020] IvanProvilkov,DmitriiEmelianenko,andElenaVoita. Bpe-dropout: Simple\nandeffectivesubwordregularization. InProceedingsofthe58thAnnualMeetingoftheAssociation\nforComputationalLinguistics,pages1882–1892,2020.\n[Pryzantetal.,2023] ReidPryzant,DanIter,JerryLi,YinTatLee,ChenguangZhu,andMichaelZeng.\nAutomaticpromptoptimizationwith\"gradientdescent\"andbeamsearch. InThe2023Conference\nonEmpiricalMethodsinNaturalLanguageProcessing,2023.\n[Qiu et al., 2020] Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang.",
    "Blockwise self-attention for long document understanding. In Findings of the Association for\nComputationalLinguistics: EMNLP2020,pages2555–2565,2020a.\n[Qiuetal.,2020] XipengQiu,TianxiangSun,YigeXu,YunfanShao,NingDai,andXuanjingHuang.\nPre-trainedmodelsfornaturallanguageprocessing:Asurvey. ScienceChinaTechnologicalSciences,\n63(10):1872–1897,2020b.\n[Rabiner and Juang, 1993] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of speech\nrecognition. Prentice-Hall,Inc.,1993.\n[RabinerandGold,1975] LawrenceRRabinerandBernardGold. Theoryandapplicationofdigital\nsignalprocessing. Prentice-Hall,1975.\n[Radfordetal.,2018] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improving\nlanguageunderstandingbygenerativepre-training. OpenAI,2018.\n[Radfordetal.,2019] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,andIlya\nSutskever. Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8),2019.\n[Radfordetal.,2021] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,",
    "SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,\nand Ilya Sutskever. Learning transferable visual models from natural language supervision. In\nInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.\n[Raeetal., 2019] JackWRae, AnnaPotapenko, SiddhantMJayakumar, ChloeHillier, andTimo-\nthyPLillicrap. Compressivetransformersforlong-rangesequencemodelling. InProceedingsof\nInternationalConferenceonLearningRepresentations,2019a.\n[Raeetal.,2019] JackWRae,AnnaPotapenko,SiddhantMJayakumar,ChloeHillier,andTimothyP\nLillicrap. Compressivetransformersforlong-rangesequencemodelling. InInternationalConference\nonLearningRepresentations,2019b.\n[Rafailovetal.,2024] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,Stefano\nErmon,andChelseaFinn. Directpreferenceoptimization: Yourlanguagemodelissecretlyareward\nmodel. AdvancesinNeuralInformationProcessingSystems,36,2024.\n[Raffeletal.,2017] ColinRaffel,Minh-ThangLuong,PeterJLiu,RonJWeiss,andDouglasEck.",
    "Onlineandlinear-timeattentionbyenforcingmonotonicalignments. InProceedingsofthe34th\nInternationalConferenceonMachineLearning-Volume70,pages2837–2846,2017.\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,",
    "BIBLIOGRAPHY 683\nMichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwith\naunifiedtext-to-texttransformer. JournalofMachineLearningResearch,21(140):1–67,2020.\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for\nactivationfunctions. arXivpreprintarXiv:1710.05941,2017.\n[RamshawandMarcus,1995] LanceRamshawandMitchMarcus. Textchunkingusingtransformation-\nbasedlearning. InThirdWorkshoponVeryLargeCorpora,1995.\n[Reddy,1976] DRajReddy. Speechrecognitionbymachine: Areview. ProceedingsoftheIEEE,64\n(4):501–531,1976.\n[ReimersandGurevych,2019] NilsReimersandIrynaGurevych. Sentence-bert:Sentenceembeddings\nusing siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing(EMNLP-IJCNLP),pages3982–3992,2019.\n[ReisingerandMooney,2010] JosephReisingerandRaymondMooney. Multi-prototypevector-space",
    "modelsofwordmeaning. InHumanLanguageTechnologies: The2010AnnualConferenceofthe\nNorthAmericanChapteroftheAssociationforComputationalLinguistics,pages109–117,2010.\n[Renetal.,2017] ZhouRen,XiaoyuWang,NingZhang,XutaoLv,andLi-JiaLi. Deepreinforcement\nlearning-basedimagecaptioningwithembeddingreward. InProceedingsoftheIEEEconferenceon\ncomputervisionandpatternrecognition,pages290–298,2017.\n[Rifaietal.,2011] SalahRifai,PascalVincent,XavierMuller,XavierGlorot,andYoshuaBengio. Con-\ntractiveauto-encoders: Explicitinvarianceduringfeatureextraction. InProceedingsofInternational\nConferenceonMachineLearning,2011.\n[Rogersetal.,2018] AnnaRogers,ShashwathHosurAnanthakrishna,andAnnaRumshisky. What’s\ninyourembedding,andhowitpredictstaskperformance. InProceedingsofthe27thInternational\nConferenceonComputationalLinguistics,pages2690–2703,2018.\n[Rolnicketal.,2019] DavidRolnick,ArunAhuja,JonathanSchwarz,TimothyLillicrap,andGregory",
    "Wayne. Experience replay for continual learning. Advances in Neural Information Processing\nSystems,32,2019.\n[Romeroetal.,2014] AdrianaRomero,NicolasBallas,SamiraEbrahimiKahou,AntoineChassang,\nCarloGatta,andYoshuaBengio. Fitnets: Hintsforthindeepnets. arXivpreprintarXiv:1412.6550,\n2014.\n[Rosenblatt,1957] FrankRosenblatt. Theperceptron,aperceivingandrecognizingautomatonProject\nPara. CornellAeronauticalLaboratory,1957.\n[Rosenfeldetal.,2020] JonathanSRosenfeld,AmirRosenfeld,YonatanBelinkov,andNirShavit. A\nconstructivepredictionofthegeneralizationerroracrossscales. InProceedingsofInternational\nConferenceonLearningRepresentations,2020.\n[Ross,1924] WilliamDavidRoss. Aristotle’smetaphysics. ClarendonPress,1924.\n[Rostietal.,2007] Antti-VeikkoRosti,SpyrosMatsoukas,andRichardSchwartz. Improvedword-level\nsystem combination for machine translation. In Proceedings of the 45th Annual Meeting of the\nAssociationofComputationalLinguistics,pages312–319,2007.",
    "[Roy et al., 2021] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient\ncontent-based sparse attention with routing transformers. Transactions of the Association for\nComputationalLinguistics,9:53–68,2021.",
    "684 BIBLIOGRAPHY\n[Ruanetal.,2024] JunhaoRuan,LongMeng,WeiqiaoShan,TongXiao,andJingboZhu. Asurveyof\nllmsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs,2024.\n[Rubenstein and Goodenough, 1965] Herbert Rubenstein and John B Goodenough. Contextual\ncorrelatesofsynonymy. CommunicationsoftheACM,8(10):627–633,1965.\n[Rubinetal.,2022] OhadRubin,JonathanHerzig,andJonathanBerant. Learningtoretrieveprompts\nforin-contextlearning. InProceedingsofthe2022ConferenceoftheNorthAmericanChapterofthe\nAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages2655–2671,2022.\n[Ruder, 2017] Sebastian Ruder. Deep learning for nlp best practices. https://ruder.io/\ndeep-learning-nlp-best-practices/index.html,2017.\n[Rumelhartetal.,1986] DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams. Learning\nrepresentationsbyback-propagatingerrors. nature,323(6088):533–536,1986.\n[RushandCollins,2012] AlexanderMRushandMJCollins. Atutorialondualdecompositionand",
    "lagrangianrelaxationforinferenceinnaturallanguageprocessing. JournalofArtificialIntelligence\nResearch,45:305–362,2012.\n[Rushetal.,2015] AlexanderMRush,SumitChopra,andJasonWeston. Aneuralattentionmodelfor\nabstractivesentencesummarization. InProceedingsofthe2015ConferenceonEmpiricalMethods\ninNaturalLanguageProcessing,pages379–389,2015.\n[Russell,2019] StuartRussell. HumanCompatible: ArtificialIntelligenceandtheProblemofControls.\nViking,2019.\n[Russell and Norvig, 2010] Stuart J. Russell and Peter Norvig. Artificial Intelligence: A Modern\nApproach(3nded.). PrenticeHall,2010.\n[Sanhetal.,2020] VictorSanh,ThomasWolf,andAlexanderRush. Movementpruning: Adaptive\nsparsitybyfine-tuning. AdvancesinNeuralInformationProcessingSystems,33:20378–20389,2020.\n[Sanhetal.,2022] VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,Zaid\nAlyafeai, AntoineChaffin, ArnaudStiegler, ArunRaja, MananDey, MSaifulBari, CanwenXu,\nUrmishThakker,ShanyaSharmaSharma,ElizaSzczechla,TaewoonKim,GunjanChhablani,Nihal",
    "Nayak,DebajyotiDatta,JonathanChang,MikeTian-JianJiang,HanWang,MatteoManica,Sheng\nShen,ZhengXinYong,HarshitPandey,RachelBawden,ThomasWang,TrishalaNeeraj,JosRozen,\nAbheeshtSharma,AndreaSantilli,ThibaultFevry,JasonAlanFries,RyanTeehan,TevenLeScao,\nStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training\nenables zero-shot task generalization. In Proceedings of International Conference on Learning\nRepresentations,2022.\n[Sankaranetal.,2016] BaskaranSankaran,HaitaoMi,YaserAl-Onaizan,andAbeIttycheriah. Tempo-\nralattentionmodelforneuralmachinetranslation. arXivpreprintarXiv:1608.02927,2016.\n[Santacroceetal.,2023] MichaelSantacroce,ZixinWen,YelongShen,andYuanzhiLi. Whatmatters\ninthestructuredpruningofgenerativelanguagemodels? arXivpreprintarXiv:2302.03773,2023.\n[SantosandGatti,2014] CícerodosSantosandMaíraGatti. Deepconvolutionalneuralnetworksfor\nsentimentanalysisofshorttexts.InProceedingsofCOLING2014,the25thInternationalConference",
    "onComputationalLinguistics: TechnicalPapers,pages69–78,2014.\n[SchacterandBuckner,1998] DanielLSchacterandRandyLBuckner. Primingandthebrain. Neuron,\n20(2):185–195,1998.\n[Schapire, 1990] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):",
    "BIBLIOGRAPHY 685\n197–227,1990.\n[Schicketal.,2023] TimoSchick,JaneA.Yu,ZhengbaoJiang,FabioPetroni,PatrickLewis,Gautier\nIzacard,QingfeiYou,ChristoforosNalmpantis,EdouardGrave,andSebastianRiedel. PEER:A\ncollaborativelanguagemodel. InProceedingsofTheEleventhInternationalConferenceonLearning\nRepresentations,2023.\n[Schicketal.,2024] TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,\nEricHambro,LukeZettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Language\nmodelscanteachthemselvestousetools. AdvancesinNeuralInformationProcessingSystems,36,\n2024.\n[Schlagetal.,2021] ImanolSchlag,KazukiIrie,andJürgenSchmidhuber. Lineartransformersare\nsecretlyfastweightprogrammers. InProceedingsofInternationalConferenceonMachineLearning,\npages9355–9366.PMLR,2021.\n[Schmidhuber,2015] JürgenSchmidhuber. Deeplearninginneuralnetworks: Anoverview. Neural\nnetworks,61:85–117,2015.\n[Schnabeletal.,2015] TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval-",
    "uation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on\nempiricalmethodsinnaturallanguageprocessing,pages298–307,2015a.\n[Schnabeletal.,2015] TobiasSchnabel,IgorLabutov,DavidMimno,andThorstenJoachims. Eval-\nuation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on\nempiricalmethodsinnaturallanguageprocessing,pages298–307,2015b.\n[Schneider et al., 2019] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli.\nwav2vec: Unsupervisedpre-trainingforspeechrecognition. InINTERSPEECH,2019.\n[Schulmanetal.,2015] JohnSchulman,SergeyLevine,PhilippMoritz,MichaelJordan,andPieter\nAbbeel. Trustregionpolicyoptimization. InProceedingsofthe32ndInternationalConferenceon\nInternationalConferenceonMachineLearning-Volume37,pages1889–1897,2015.\n[Schulman et al., 2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg\nKlimov. Proximalpolicyoptimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.",
    "[SchusterandNakajima, 2012] MikeSchusterandKaisukeNakajima. Japaneseandkoreanvoice\nsearch. InProceedingsofInternationalConferenceonAcoustics,SpeechandSignalProcessing,\npages5149–5152,2012.\n[Schusteretal.,2022] TalSchuster,AdamFisch,JaiGupta,MostafaDehghani,DaraBahri,VinhTran,\nYiTay,andDonaldMetzler. Confidentadaptivelanguagemodeling. AdvancesinNeuralInformation\nProcessingSystems,35:17456–17472,2022.\n[Schwartzetal.,2020] RoySchwartz,GabrielStanovsky,SwabhaSwayamdipta,JesseDodge,and\nNoahASmith. Therighttoolforthejob:Matchingmodelandinstancecomplexities. InProceedings\nofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics, pages6640–6651,\n2020.\n[See,2018] AbigailSee. Deeplearning,structureandinnatepriors: Adiscussionbetweenyannlecun\nand christopher manning, 02 2018. URL http://www.abigailsee.com/2018/02/21/\ndeep-learning-structure-and-innate-priors.html.\n[Seeetal.,2017] AbigailSee,PeterJLiu,andChristopherDManning.Gettothepoint:Summarization",
    "withpointer-generatornetworks. InProceedingsofthe55thAnnualMeetingoftheAssociationfor",
    "686 BIBLIOGRAPHY\nComputationalLinguistics(Volume1: LongPapers),pages1073–1083,2017.\n[Seni et al., 2010] Giovanni Seni, John Elder, and Robert Grossman. Ensemble Methods in Data\nMining: ImprovingAccuracyThroughCombiningPredictions. MorganandClaypoolPublishers,\n2010.\n[Sennrichetal.,2016] RicoSennrich,BarryHaddow,andAlexandraBirch. Improvingneuralmachine\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages86–96,2016a.\n[Sennrichetal.,2016] RicoSennrich,BarryHaddow,andAlexandraBirch. Neuralmachinetranslation\nofrarewordswithsubwordunits. InProceedingsofthe54thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages1715–1725,2016b.\n[Seoetal.,2017] MinjoonSeo,AniruddhaKembhavi,AliFarhadi,andHannanehHajishirzi. Bidirec-\ntionalattentionflowformachinecomprehension. InProceedingsofInternationalConferenceon\nLearningRepresentations,2017.",
    "[Shannon,1948] C.E.Shannon. Amathematicaltheoryofcommunication. TheBellSystemTechnical\nJournal,27(3):379–423,1948a.\n[Shannon,1948] ClaudeE.Shannon. Amathematicaltheoryofcommunication. Report,BellLabs,\n1948b.\n[Shannon,1951] ClaudeEShannon. Predictionandentropyofprintedenglish. Bellsystemtechnical\njournal,30(1):50–64,1951.\n[Shawetal., 2018] PeterShaw, JakobUszkoreit, andAshishVaswani. Self-attentionwithrelative\npositionrepresentations. InProceedingsofthe2018ConferenceoftheNorthAmericanChapterof\ntheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume2(Short\nPapers),pages464–468,2018.\n[Shazeer,2019] NoamShazeer. Fasttransformerdecoding: Onewrite-headisallyouneed. arXiv\npreprintarXiv:1911.02150,2019.\n[Shazeer,2020] NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,\n2020.\n[Shazeer et al., 2017] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc",
    "Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated\nmixture-of-expertslayer. InProceedingsofInternationalConferenceonLearningRepresentations,\n2017.\n[Shenetal.,2020] DinghanShen,MingzhiZheng,YelongShen,YanruQu,andWeizhuChen.Asimple\nbuttough-to-beatdataaugmentationapproachfornaturallanguageunderstandingandgeneration.\narXivpreprintarXiv:2009.13818,2020a.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,\nMichaelWMahoney,andKurtKeutzer. Q-bert: Hessianbasedultralowprecisionquantizationof\nbert. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages8815–8821,\n2020b.\n[Shen et al., 2016] Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and\nYang Liu. Minimum risk training for neural machine translation. In Proceedings of the 54th\nAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages\n1683–1692,2016.",
    "BIBLIOGRAPHY 687\n[Shenetal.,2019] TianxiaoShen,MyleOtt,MichaelAuli,andMarc’AurelioRanzato. Mixturemodels\nfordiversemachinetranslation: Tricksofthetrade. InInternationalconferenceonmachinelearning,\npages5719–5728,2019.\n[Shietal.,2016] XingShi,InkitPadhi,andKevinKnight. Doesstring-basedneuralmtlearnsource\nsyntax? InProceedingsofthe2016conferenceonempiricalmethodsinnaturallanguageprocessing,\npages1526–1534,2016.\n[Shinn et al., 2023] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and\nShunyuYao. Reflexion: Languageagentswithverbalreinforcementlearning. AdvancesinNeural\nInformationProcessingSystems,36:8634–8652,2023.\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared\nCasper,andBryanCatanzaro. Megatron-lm: Trainingmulti-billionparameterlanguagemodelsusing\nmodelparallelism. arXivpreprintarXiv:1909.08053,2019.\n[ShortenandKhoshgoftaar,2019] ConnorShortenandTaghiMKhoshgoftaar. Asurveyonimagedata",
    "augmentationfordeeplearning. Journalofbigdata,6(1):1–48,2019.\n[Silver et al., 2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja\nHuang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen,\nTimothyLillicrap,FanHui,LaurentSifre,GeorgevandenDriessche,ThoreGraepel,andDemis\nHassabis. Masteringthegameofgowithouthumanknowledge. nature,550(7676):354–359,2017.\n[Singhal,2005] AmitSinghal. Introducingtheknowledgegraph: things,notstrings,2005.\n[Skalseetal.,2022] JoarSkalse,NikolausHowe,DmitriiKrasheninnikov,andDavidKrueger.Defining\nandcharacterizingrewardgaming. AdvancesinNeuralInformationProcessingSystems,35:9460–\n9471,2022.\n[Skorskietal.,2021] MaciejSkorski,AlessandroTemperoni,andMartinTheobald. Revisitingweight\ninitializationofdeepneuralnetworks. InAsianConferenceonMachineLearning,pages1192–1207.\nPMLR,2021.\n[Smithetal.,2017] SamuelLSmith,DavidHPTurban,StevenHamblin,andNilsYHammerla.Offline",
    "bilingualwordvectors,orthogonaltransformationsandtheinvertedsoftmax. InProceedingsofthe\n5thInternationalConferenceonLearningRepresentations(ICLR),2017.\n[Smithetal.,2018] SamuelL.Smith,Pieter-JanKindermans,ChrisYing,andQuocV.Le. Don’tdecay\nthelearningrate,increasethebatchsize. InProceedingsofthe6thInternationalConferenceon\nLearningRepresentationsICLR,2018.\n[Snelletal.,2022] CharlieSnell,DanKlein,andRuiqiZhong. Learningbydistillingcontext. arXiv\npreprintarXiv:2209.15189,2022.\n[Snell et al., 2024] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-\ntime compute optimally can be more effective than scaling model parameters. arXiv preprint\narXiv:2408.03314,2024.\n[Snelletal.,2025] CharlieVictorSnell,JaehoonLee,KelvinXu,andAviralKumar. ScalingLLM\ntest-timecomputeoptimallycanbemoreeffectivethanscalingparametersforreasoning. InThe\nThirteenthInternationalConferenceonLearningRepresentations,2025.",
    "[Soetal.,2019] DavidSo,QuocLe,andChenLiang. Theevolvedtransformer. InProceedingsof\nInternationalconferenceonmachinelearning,pages5877–5886.PMLR,2019.\n[Socheretal.,2011] RichardSocher,CliffCLin,ChrisManning,andAndrewYNg. Parsingnatural",
    "688 BIBLIOGRAPHY\nscenesandnaturallanguagewithrecursiveneuralnetworks. InProceedingsofthe28thinternational\nconferenceonmachinelearning(ICML-11),pages129–136,2011.\n[Socheretal.,2013] RichardSocher,AlexPerelygin,JeanWu,JasonChuang,ChristopherDManning,\nAndrewYNg,andChristopherPotts. Recursivedeepmodelsforsemanticcompositionalityovera\nsentimenttreebank. InProceedingsofthe2013conferenceonempiricalmethodsinnaturallanguage\nprocessing,pages1631–1642,2013.\n[Søgaard, 2016] Anders Søgaard. Evaluating word embeddings with fmri and eye-tracking. In\nProceedingsofthe1stworkshoponevaluatingvector-spacerepresentationsforNLP,pages116–121,\n2016.\n[Solorio-Fernández et al., 2020] Saúl Solorio-Fernández, J Ariel Carrasco-Ochoa, and José Fco\nMartínez-Trinidad. A review of unsupervised feature selection methods. Artificial Intelligence\nReview,53(2):907–948,2020.\n[Song et al., 2019] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked",
    "sequencetosequencepre-trainingforlanguagegeneration. InInternationalConferenceonMachine\nLearning,pages5926–5936.PMLR,2019.\n[Sperberetal., 2018] MatthiasSperber, JanNiehues, GrahamNeubig, SebastianStüker, andAlex\nWaibel. Self-attentionalacousticmodels. InProceedingsofInterspeech2018,pages3723–3727,\n2018.\n[Srivastava et al., 2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and\nRuslanSalakhutdinov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting. The\njournalofmachinelearningresearch,15(1):1929–1958,2014.\n[Srivastavaetal.,2015] RupeshKumarSrivastava,KlausGreff,andJürgenSchmidhuber. Highway\nnetworks. arXivpreprintarXiv:1505.00387,2015.\n[StahlbergandByrne,2019] FelixStahlbergandBillByrne. Onnmtsearcherrorsandmodelerrors:\nCatgotyourtongue? InProceedingsofthe2019ConferenceonEmpiricalMethodsinNatural\nLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing\n(EMNLP-IJCNLP),pages3356–3362,2019.",
    "[Stahlbergetal.,2016] FelixStahlberg,EvaHasler,AurelienWaite,andBillByrne. Syntactically\nguidedneuralmachinetranslation. InProceedingsofthe54thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume2: ShortPapers),pages299–305,2016.\n[Sternberg,1996] RobertJSternberg. Cognitivepsychology. HarcourtBraceCollegePublishers,1996.\n[Stewart,1993] GilbertWStewart. Ontheearlyhistoryofthesingularvaluedecomposition. SIAM\nreview,35(4):551–566,1993.\n[Stiennonetal.,2020] NisanStiennon,LongOuyang,JeffreyWu,DanielZiegler,RyanLowe,Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human\nfeedback. AdvancesinNeuralInformationProcessingSystems,33:3008–3021,2020.\n[Stocketal.,2021] PierreStock,AngelaFan,BenjaminGraham,EdouardGrave,RémiGribonval,\nHerveJegou,andArmandJoulin. Trainingwithquantizationnoiseforextrememodelcompression.\nInProceedingsofInternationalConferenceonLearningRepresentations,2021.",
    "[Strubelletal.,2018] EmmaStrubell,PatrickVerga,DanielAndor,DavidWeiss,andAndrewMcCal-\nlum. Linguistically-informedself-attentionforsemanticrolelabeling. InProceedingsofthe2018\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages5027–5038,2018.",
    "BIBLIOGRAPHY 689\n[Suetal.,2021] JianlinSu,YuLu,ShengfengPan,BoWen,andYunfengLiu. Roformer: Enhanced\ntransformerwithrotarypositionembedding. arXivpreprintarXiv:2104.09864,2021.\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.\nRoformer: Enhancedtransformerwithrotarypositionembedding. Neurocomputing,568:127063,\n2024.\n[Suetal.,2022] YixuanSu,TianLan,YanWang,DaniYogatama,LingpengKong,andNigelCollier.\nAcontrastiveframeworkforneuraltextgeneration. AdvancesinNeuralInformationProcessing\nSystems,35:21548–21561,2022.\n[Sukhbaataretal.,2015] SainbayarSukhbaatar,ArthurSzlam,JasonWeston,andRobFergus. End-to-\nendmemorynetworks. Advancesinneuralinformationprocessingsystems,28,2015.\n[Sukhbaataretal.,2019] SainbayarSukhbaatar,ÉdouardGrave,PiotrBojanowski,andArmandJoulin.\nAdaptiveattentionspanintransformers.InProceedingsofthe57thAnnualMeetingoftheAssociation\nforComputationalLinguistics,pages331–335,2019.",
    "[Sunetal.,2023] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,Jianyong\nWang, andFuruWei. Retentivenetwork: Asuccessortotransformerforlargelanguagemodels.\narXivpreprintarXiv:2307.08621,2023.\n[Sunetal.,2020] ZeweiSun,ShujianHuang,Hao-RanWei,Xin-yuDai,andJiajunChen. Generating\ndiversetranslationbymanipulatingmulti-headattention. InProceedingsoftheAAAIConferenceon\nArtificialIntelligence,volume34,pages8976–8983,2020a.\n[Sunetal.,2020] ZhiqingSun,HongkunYu,XiaodanSong,RenjieLiu,YimingYang,andDenny\nZhou. Mobilebert: acompacttask-agnosticbertforresource-limiteddevices. InProceedingsofthe\n58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages2158–2170,2020b.\n[Sundermeyeretal.,2012] MartinSundermeyer,RalfSchlüter,andHermannNey.Lstmneuralnetworks\nforlanguagemodeling. InProceedingsoftheThirteenthannualconferenceoftheinternational\nspeechcommunicationassociation,2012.\n[Sutskever,2013] IlyaSutskever. Trainingrecurrentneuralnetworks. UniversityofTorontoToronto,\n2013.",
    "[Sutskeveretal.,2013] IlyaSutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton. Onthe\nimportanceofinitializationandmomentumindeeplearning. InInternationalconferenceonmachine\nlearning,pages1139–1147.PMLR,2013.\n[Sutskeveretal.,2014] IlyaSutskever,OriolVinyals,andQuocVLe. Sequencetosequencelearning\nwithneuralnetworks. Advancesinneuralinformationprocessingsystems,27,2014.\n[SuttonandMcCallum,2012] CharlesSuttonandAndrewMcCallum. Anintroductiontoconditional\nrandomfields. FoundationsandTrends®inMachineLearning,4(4):267–373,2012.\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An\nIntroduction(2nded.). TheMITPress,2018.\n[Szabó, 2020] ZoltánGendlerSzabó. Compositionality. InEdwardN.Zalta, editor, TheStanford\nEncyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Fall 2020 edition,\n2020.\n[Szegedyetal.,2014] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,Dumitru",
    "Erhan,IanGoodfellow,andRobFergus. Intriguingpropertiesofneuralnetworks. InProceedingsof\nthe2ndInternationalConferenceonLearningRepresentations,2014a.",
    "690 BIBLIOGRAPHY\n[Szegedyetal.,2014] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,Dumitru\nErhan,IanGoodfellow,andRobFergus. Intriguingpropertiesofneuralnetworks. InProceedingsof\n2ndInternationalConferenceonLearningRepresentations(ICLR2014),2014b.\n[Szegedyetal.,2016] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonShlens,andZbigniew\nWojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE\nconferenceoncomputervisionandpatternrecognition,pages2818–2826,2016.\n[Szepesvári,2010] CsabaSzepesvári. Algorithmsforreinforcementlearning. SynthesisLectureson\nArtificialIntelligenceandMachineLearning,4(1):1–103,2010.\n[Taietal.,2015] KaiShengTai,RichardSocher,andChristopherDManning. Improvedsemantic\nrepresentationsfromtree-structuredlongshort-termmemorynetworks. InProceedingsofthe53rd\nAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJoint\nConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages1556–1566,2015.",
    "[Talmor and Berant, 2018] Alon Talmor and Jonathan Berant. The web as a knowledge-base for\nansweringcomplexquestions. arXivpreprintarXiv:1803.06643,2018.\n[TanandLe,2019] MingxingTanandQuocLe. Efficientnet: Rethinkingmodelscalingforconvolu-\ntionalneuralnetworks. InInternationalconferenceonmachinelearning,pages6105–6114.PMLR,\n2019.\n[Tangetal., 2015] DuyuTang, BingQin, andTingLiu. Documentmodelingwithgatedrecurrent\nneuralnetworkforsentimentclassification. InProceedingsofthe2015conferenceonempirical\nmethodsinnaturallanguageprocessing,pages1422–1432,2015.\n[Tank and Hopfield, 1987] David W Tank and JJ Hopfield. Neural computation by concentrating\ninformationintime. ProceedingsoftheNationalAcademyofSciences,84(7):1896–1900,1987.\n[Taorietal.,2023] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,Carlos\nGuestrin,PercyLiang,andTatsunoriB.Hashimoto. Stanfordalpaca: Aninstruction-followingllama\nmodel. https://github.com/tatsu-lab/stanford_alpaca,2023.",
    "[Taskaretal.,2005] BenTaskar,SimonLacoste-Julien,andDanKlein. Adiscriminativematching\napproach to word alignment. In Proceedings of Human Language Technology Conference and\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages73–80,2005.\n[Tayetal.,2020] YiTay,DaraBahri,LiuYang,DonaldMetzler,andDa-ChengJuan. Sparsesinkhorn\nattention. In Proceedings of International Conference on Machine Learning, pages 9438–9447.\nPMLR,2020a.\n[Tayetal.,2020] YiTay,MostafaDehghani,DaraBahri,andDonaldMetzler. Efficienttransformers:\nAsurvey. CoRR,abs/2009.06732,2020b.\n[Teametal., 2024] GemmaTeam, MorganeRiviere, ShreyaPathak, PierGiuseppeSessa, Cassidy\nHardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre\nRamé, et al. Gemma 2: Improving open language models at a practical size. arXiv preprint\narXiv:2408.00118,2024.\n[Teknium,2023] Teknium. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassis-",
    "tants,2023. URLhttps://huggingface.co/datasets/teknium/OpenHermes-2.5.\n[Telgarsky,2016] MatusTelgarsky. Benefitsofdepthinneuralnetworks. InConferenceonlearning\ntheory,pages1517–1539.PMLR,2016.\n[Tenney et al., 2019] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical",
    "BIBLIOGRAPHY 691\nnlp pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics,pages4593–4601,2019a.\n[Tenney et al., 2019] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas\nMcCoy,NajoungKim,BenjaminVanDurme,SamBowman,DipanjanDas,andElliePavlick. What\ndoyoulearnfromcontext? probingforsentencestructureincontextualizedwordrepresentations. In\nProceedingsofInternationalConferenceonLearningRepresentations,2019b.\n[Timonin et al., 2022] Denis Timonin, BoYang Hsueh, and Vinh Nguyen.\nAccelerated inference for large transformer models using nvidia tri-\nton inference server. https://developer.nvidia.com/blog/\naccelerated-inference-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inference-server/,\n2022.\n[Tissieretal.,2017] JulienTissier,ChristopheGravier,andAmauryHabrard. Dict2vec: Learningword\nembeddingsusinglexicaldictionaries. InProceedingsofthe2017ConferenceonEmpiricalMethods",
    "inNaturalLanguageProcessing,pages254–263,2017.\n[Tjong Kim Sang, 2002] Erik F. Tjong Kim Sang. Introduction to the CoNLL-2002 shared task:\nLanguage-independentnamedentityrecognition.InProceedingsofCOLING-02:The6thConference\nonNaturalLanguageLearning2002(CoNLL-2002),2002.\n[TjongKimSangandBuchholz,2000] ErikF.TjongKimSangandSabineBuchholz. Introduction\ntotheCoNLL-2000sharedtaskchunking. InProceedingsofFourthConferenceonComputational\nNaturalLanguageLearningandtheSecondLearningLanguageinLogicWorkshop,2000.\n[Touvronetal.,2023] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne\nLachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient\nfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,",
    "YasmineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,\nLukasBlecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,DavidEsiobu,JudeFernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,\nIsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,Jenya\nLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,PushkarMishra,\nIgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,Alan\nSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,\nRossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,\nAngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov,\nand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288,2023b.",
    "[TrentinandGori,2001] EdmondoTrentinandMarcoGori. Asurveyofhybridann/hmmmodelsfor\nautomaticspeechrecognition. Neurocomputing,37(1-4):91–126,2001.\n[Tsvetkovetal.,2015] YuliaTsvetkov,ManaalFaruqui,WangLing,GuillaumeLample,andChris\nDyer. Evaluationofwordvectorrepresentationsbysubspacealignment. InProceedingsofthe2015\nConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages2049–2054,2015.\n[Tuetal.,2016] ZhaopengTu,ZhengdongLu,YangLiu,XiaohuaLiu,andHangLi.Modelingcoverage\nforneuralmachinetranslation. InProceedingsofthe54thAnnualMeetingoftheAssociationfor",
    "692 BIBLIOGRAPHY\nComputationalLinguistics(Volume1: LongPapers),pages76–85,2016.\n[Tulving and Schacter, 1990] Endel Tulving and Daniel L Schacter. Priming and human memory\nsystems. Science,247(4940):301–306,1990.\n[Uesatoetal.,2022] JonathanUesato,NateKushman,RamanaKumar,FrancisSong,NoahSiegel,\nLisaWang,AntoniaCreswell,GeoffreyIrving,andIrinaHiggins. Solvingmathwordproblemswith\nprocess-andoutcome-basedfeedback. arXivpreprintarXiv:2211.14275,2022.\n[Uffink,2017] JosUffink. Boltzmann’sWorkinStatisticalPhysics. InEdwardN.Zalta,editor,The\nStanfordEncyclopediaofPhilosophy.MetaphysicsResearchLab,StanfordUniversity,Spring2017\nedition,2017.\n[Ulyanovetal.,2016] DmitryUlyanov,AndreaVedaldi,andVictorLempitsky. Instancenormalization:\nThemissingingredientforfaststylization. arXivpreprintarXiv:1607.08022,2016.\n[VanderMaatenandHinton,2008] LaurensVanderMaatenandGeoffreyHinton. Visualizingdata\nusingt-sne. Journalofmachinelearningresearch,9(11),2008.",
    "[Vapnik and Chervonenkis, 1971] Vladimir Vapnik and Alexey Chervonenkis. On the uniform\nconvergence of relative frequencies of events to their probabilities. Theory of Probability & Its\nApplications,16(2):264–279,1971.\n[Vaswanietal.,2017] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,\nAidanNGomez,ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InProceedingsof\nAdvancesinNeuralInformationProcessingSystems,volume30,2017.\n[Velicˇkovic´ et al., 2018] Petar Velicˇkovic´, Guillem Cucurull, Arantxa Casanova, Adriana Romero,\nPietroLiò,andYoshuaBengio. Graphattentionnetworks. InInternationalConferenceonLearning\nRepresentations,2018.\n[Vijayakumaretal.,2018] AshwinVijayakumar,MichaelCogswell,RamprasaathSelvaraju,QingSun,\nStefanLee, DavidCrandall, andDhruvBatra. Diversebeamsearchforimproveddescriptionof\ncomplexscenes. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,2018.",
    "[Vincentetal.,2010] PascalVincent,HugoLarochelle,IsabelleLajoie,YoshuaBengio,Pierre-Antoine\nManzagol,andLéonBottou. Stackeddenoisingautoencoders: Learningusefulrepresentationsina\ndeepnetworkwithalocaldenoisingcriterion. Journalofmachinelearningresearch,11(12),2010.\n[Vinyalsetal.,2015] OriolVinyals,ŁukaszKaiser,TerryKoo,SlavPetrov,IlyaSutskever,andGeoffrey\nHinton. Grammarasaforeignlanguage. Advancesinneuralinformationprocessingsystems,28,\n2015.\n[Viterbi,1967] AndrewJViterbi. Errorboundsforconvolutionalcodesandanasymptoticallyoptimum\ndecodingalgorithm. IEEETransactionsonInformationTheory,1967.\n[Vogel et al., 1996] Stephan Vogel, Hermann Ney, and Christoph Tillmann. Hmm-based word\nalignmentinstatisticaltranslation. InCOLING1996Volume2: The16thInternationalConference\nonComputationalLinguistics,1996.\n[Voitaetal.,2018] ElenaVoita,PavelSerdyukov,RicoSennrich,andIvanTitov. Context-awareneural\nmachinetranslationlearnsanaphoraresolution. InProceedingsofthe56thAnnualMeetingofthe",
    "AssociationforComputationalLinguistics(Volume1: LongPapers),pages1264–1274,2018.\n[Voitaetal.,2019] ElenaVoita,DavidTalbot,FedorMoiseev,RicoSennrich,andIvanTitov. Ana-\nlyzingmulti-headself-attention: Specializedheadsdotheheavylifting,therestcanbepruned. In\nProceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics,pages",
    "BIBLIOGRAPHY 693\n5797–5808,2019.\n[VonOswaldetal.,2023] JohannesVonOswald,EyvindNiklasson,EttoreRandazzo,JoãoSacramento,\nAlexanderMordvintsev,AndreyZhmoginov,andMaxVladymyrov. Transformerslearnin-context\nby gradient descent. In Proceedings of International Conference on Machine Learning, pages\n35151–35174.PMLR,2023.\n[Waibel et al., 1989] Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and\nKevin J Lang. Phoneme recognition using time-delay neural networks. IEEE transactions on\nacoustics,speech,andsignalprocessing,37(3):328–339,1989.\n[Wallaceetal.,2019] EricWallace,YizhongWang,SujianLi,SameerSingh,andMattGardner. Donlp\nmodelsknownumbers? probingnumeracyinembeddings. InProceedingsofthe2019Conference\nonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConference\nonNaturalLanguageProcessing(EMNLP-IJCNLP),pages5307–5315,2019.\n[Wangetal.,2024] ChenglongWang,HangZhou,YiminHu,YifuHuo,BeiLi,TongranLiu,Tong",
    "Xiao,andJingboZhu. Esrl:Efficientsampling-basedreinforcementlearningforsequencegeneration.\nInProceedingsoftheAAAIConferenceonArtificialIntelligence,pages19107–19115,2024.\n[Wangetal., 2020] HanruiWang, ZhanghaoWu, ZhijianLiu, HanCai, LigengZhu, ChuangGan,\nand Song Han. Hat: Hardware-aware transformers for efficient natural language processing. In\nProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics,pages\n7675–7688,2020a.\n[Wangetal.,2022] HongyuWang,ShumingMa,LiDong,ShaohanHuang,DongdongZhang,and\nFuruWei. Deepnet: Scalingtransformersto1,000layers. arXivpreprintarXiv:2203.00555,2022a.\n[Wangetal.,2022] HongyuWang,ShumingMa,ShaohanHuang,LiDong,WenhuiWang,Zhiliang\nPeng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav\nChaudhary,XiaSong,andFuruWei. Foundationtransformers. arXivpreprintarXiv:2210.06423,\n2022b.\n[Wang et al., 2022] Jue Wang, Ke Chen, Gang Chen, Lidan Shou, and Julian McAuley. Skipbert:",
    "Efficientinferencewithshallowlayerskipping. InProceedingsofthe60thAnnualMeetingofthe\nAssociationforComputationalLinguistics(Volume1: LongPapers),pages7287–7301,2022c.\n[Wang and Yoon, 2021] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher\nlearningforvisualintelligence: Areviewandnewoutlooks. IEEEtransactionsonpatternanalysis\nandmachineintelligence,44(6):3048–3068,2021.\n[Wangetal.,2023] LiyuanWang,XingxingZhang,HangSu,andJunZhu. Acomprehensivesurveyof\ncontinuallearning: Theory,methodandapplication. arXivpreprintarXiv:2302.00487,2023a.\n[Wangetal.,2023] PeihaoWang,RameswarPanda,LucasTorrobaHennigen,PhilipGreengard,Leonid\nKarlinsky,RogerioFeris,DavidDanielCox,ZhangyangWang,andYoonKim. Learningtogrow\npretrainedmodelsforefficienttransformertraining. InProceedingsofTheEleventhInternational\nConferenceonLearningRepresentations,2023b.\n[Wangetal.,2018] QiangWang,FuxueLi,TongXiao,YanyangLi,YinqiaoLi,andJingboZhu. Multi-",
    "layerrepresentationfusionforneuralmachinetranslation. InProceedingsofthe27thInternational\nConferenceonComputationalLinguistics,pages3015–3026,2018a.\n[Wangetal.,2019] QiangWang,BeiLi,TongXiao,JingboZhu,ChangliangLi,DerekFWong,and\nLidiaSChao. Learningdeeptransformermodelsformachinetranslation. InProceedingsofthe57th",
    "694 BIBLIOGRAPHY\nAnnualMeetingoftheAssociationforComputationalLinguistics,pages1810–1822,2019a.\n[Wangetal.,2020] SinongWang,BelindaZLi,MadianKhabsa,HanFang,andHaoMa. Linformer:\nSelf-attentionwithlinearcomplexity. arXivpreprintarXiv:2006.04768,2020b.\n[Wang et al., 2019] Wei Wang, Vincent Wenchen Zheng, Han Yu, and Chunyan Miao. A survey\nofzero-shotlearning. ACMTransactionsonIntelligentSystemsandTechnology(TIST),10:1–37,\n2019b.\n[Wangetal.,2018] XinWang,FisherYu,Zi-YiDou,TrevorDarrell,andJosephEGonzalez. Skipnet:\nLearningdynamicroutinginconvolutionalnetworks. InProceedingsoftheEuropeanConferenceon\nComputerVision(ECCV),pages409–424,2018b.\n[Wangetal.,2022] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,andDennyZhou.\nRationale-augmentedensemblesinlanguagemodels. arXivpreprintarXiv:2207.00747,2022d.\n[Wang et al., 2023] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan\nNarang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought",
    "reasoninginlanguagemodels. InProceedingsofTheEleventhInternationalConferenceonLearning\nRepresentations,2023c.\n[Wangetal.,2020] YaqingWang,QuanmingYao,JamesT.Kwok,andLionelM.Ni. Generalizing\nfromafewexamples: Asurveyonfew-shotlearning. ACMComputingSurveys,53(3):1–34,2020c.\n[Wang et al., 2022] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi,\nAmirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar,\nDavidStap,EshaanPathak,GiannisKaramanolakis,HaizhiGaryLai,IshanPurohit,IshaniMondal,\nJacobAnderson,KirbyKuznia,KrimaDoshi,KuntalKumarPal,MaitreyaPatel,MehradMoradshahi,\nMihirParmar,MiraliPurohit,NeerajVarshney,PhaniRohithaKaza,PulkitVerma,RavsehajSingh\nPuri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A,\nSumanta Patro, Tanay Dixit, and Xudong Shen. Super-naturalinstructions: Generalization via\ndeclarativeinstructionson1600+nlptasks. InProceedingsofthe2022ConferenceonEmpirical",
    "MethodsinNaturalLanguageProcessing,pages5085–5109,2022e.\n[Wangetal.,2023] YizhongWang,HamishIvison,PradeepDasigi,JackHessel,TusharKhot,Khy-\nathiRaghaviChandu,DavidWadden,KelseyMacMillan,NoahA.Smith,IzBeltagy,andHannaneh\nHajishirzi. How far can camels go? exploring the state of instruction tuning on open resources.\nAdvancesinNeuralInformationProcessingSystems,36:74764–74786,2023d.\n[Wangetal.,2023] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel\nKhashabi,andHannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generated\ninstructions. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics(Volume1: LongPapers),pages13484–13508,2023e.\n[Wangetal.,2023] ZhenyiWang,EnnengYang,LiShen,andHengHuang. Acomprehensivesurvey\nofforgettingindeeplearningbeyondcontinuallearning. arXivpreprintarXiv:2307.09218,2023f.\n[Wang et al., 2020] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large",
    "languagemodels.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing(EMNLP),pages6151–6162,2020d.\n[Warstadtetal.,2019] AlexWarstadt,AmanpreetSingh,andSamuelRBowman. Neuralnetwork\nacceptabilityjudgments. TransactionsoftheAssociationforComputationalLinguistics,7:625–641,\n2019.",
    "BIBLIOGRAPHY 695\n[WebsterandKit,1992] JonathanJWebsterandChunyuKit. Tokenizationastheinitialphaseinnlp.\nInProceedingsofCOLING1992volume4: The14thinternationalconferenceoncomputational\nlinguistics,1992.\n[Weietal.,2022] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,\nNanDu,AndrewMDai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. In\nProceedingsofInternationalConferenceonLearningRepresentations,2022a.\n[Weietal.,2022] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,\nDaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,EdH.Chi,TatsunoriHashimoto,\nOriolVinyals, PercyLiang, JeffDean, andWilliamFedus. Emergentabilitiesoflargelanguage\nmodels. arXivpreprintarXiv:2206.07682,2022b.\n[Weietal.,2022] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia,\nEdH.Chi,QuocV.Le,andDennyZhou. Chain-of-thoughtpromptingelicitsreasoninginlarge\nlanguagemodels. AdvancesinNeuralInformationProcessingSystems,35:24824–24837,2022c.",
    "[Weiss et al., 2021] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In\nProceedingsofInternationalConferenceonMachineLearning,pages11080–11090.PMLR,2021.\n[Wellecketal.,2023] SeanWelleck,XimingLu,PeterWest,FaezeBrahman,TianxiaoShen,Daniel\nKhashabi,andYejinChoi. Generatingsequencesbylearningtoself-correct. InProceedingsofThe\nEleventhInternationalConferenceonLearningRepresentations,2023.\n[Weng, 2021] Lilian Weng. How to train really large models on many gpus? lil-\nianweng.github.io, Sep 2021. URL https://lilianweng.github.io/posts/\n2021-09-25-train-large/.\n[Werbos, 1990] Paul J Werbos. Backpropagation through time: what it does and how to do it.\nProceedingsoftheIEEE,78(10):1550–1560,1990.\n[Weston et al., 2015] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In\nProceedingsofthe3rdInternationalConferenceonLearningRepresentations,ICLR2015,2015.\n[Wiener,1960] NorbertWiener. Somemoralandtechnicalconsequencesofautomation: Asmachines",
    "learntheymaydevelopunforeseenstrategiesatratesthatbaffletheirprogrammers. Science,131\n(3410):1355–1358,1960.\n[WiggsandMartin,1998] CheriLWiggsandAlexMartin. Propertiesandmechanismsofperceptual\npriming. Currentopinioninneurobiology,8(2):227–233,1998.\n[Wiheretal.,2022] GianWiher,ClaraMeister,andRyanCotterell. Ondecodingstrategiesforneural\ntextgenerators. TransactionsoftheAssociationforComputationalLinguistics,10:997–1012,2022.\n[Williamsetal.,2018] AdinaWilliams,NikitaNangia,andSamuelBowman. Abroad-coveragechal-\nlengecorpusforsentenceunderstandingthroughinference. InProceedingsofthe2018Conference\noftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguage\nTechnologies,Volume1(LongPapers),pages1112–1122,2018.\n[Williams,1992] RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionist\nreinforcementlearning. Machinelearning,8:229–256,1992.\n[WilliamsandPeng,1990] RonaldJWilliamsandJingPeng. Anefficientgradient-basedalgorithmfor",
    "on-linetrainingofrecurrentnetworktrajectories. Neuralcomputation,2(4):490–501,1990.\n[WilliamsandZipser,1989] RonaldJWilliamsandDavidZipser. Alearningalgorithmforcontinually\nrunningfullyrecurrentneuralnetworks. Neuralcomputation,1(2):270–280,1989.",
    "696 BIBLIOGRAPHY\n[Wingateetal.,2022] DavidWingate,MohammadShoeybi,andTaylorSorensen. Promptcompression\nandcontrastiveconditioningforcontrollabilityandtoxicityreductioninlanguagemodels.InFindings\noftheAssociationforComputationalLinguistics: EMNLP2022,pages5621–5634,2022.\n[Wittgenstein,1953] LudwigWittgenstein. Philosophicalinvestigations.PhilosophischeUntersuchun-\ngen. Macmillan,1953.\n[Wold et al., 1987] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.\nChemometricsandintelligentlaboratorysystems,2(1-3):37–52,1987.\n[Wolpert, 1996] David H. Wolpert. The lack of a priori distinctions between learning algorithms.\nNeuralComputatoin,8(7):1341–1390,1996.\n[WolpertandMacready,1997] DavidH.WolpertandWilliamG.Macready. Nofreelunchtheorems\nforoptimization. IEEETransactionsonEvolutionaryComputation,1(1):67–82,1997.\n[WozengraftandReiffen,1961] JohnM.WozengraftandBarneyReiffen. SequentialDecoding. The\nMITPress,1961.",
    "[Wright and Ma, 2022] John Wright and Yi Ma. High-Dimensional Data Analysis with Low-\nDimensional Models: Principles, Computation, and Applications. Cambridge University Press,\n2022.\n[Wuetal.,2023] BingyangWu,YinminZhong,ZiliZhang,ShengyuLiu,FangyueLiu,YuanhangSun,\nGangHuang,XuanzheLiu,andXinJin. Fastdistributedinferenceservingforlargelanguagemodels.\narXivpreprintarXiv:2305.05920,2023a.\n[Wuetal.,2018] FelixWu,AngelaFan,AlexeiBaevski,YannDauphin,andMichaelAuli. Payless\nattentionwithlightweightanddynamicconvolutions. InProceedingsofInternationalConferenceon\nLearningRepresentations,2018a.\n[Wuetal.,2019] FelixWu,AngelaFan,AlexeiBaevski,YannDauphin,andMichaelAuli. Payless\nattentionwithlightweightanddynamicconvolutions. InProceedingsofInternationalConferenceon\nLearningRepresentations,2019.\n[Wuetal.,2024] WilsonWu,JohnXMorris,andLionelLevine. Dolanguagemodelsplanforfuture\ntokens? arXivpreprintarXiv:2404.00859,2024.",
    "[Wuetal.,2020] XuanfuWu,YangFeng,andChenzeShao. Generatingdiversetranslationfrommodel\ndistributionwithdropout. InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural\nLanguageProcessing(EMNLP),pages1088–1097,2020a.\n[Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgangMacherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,JeffKlingner,Apurva\nShah,MelvinJohnson,XiaobingLiu,ŁukaszKaiser,StephanGouws,YoshikiyoKato,TakuKudo,\nHideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey\nDean. Google’sneuralmachinetranslationsystem: Bridgingthegapbetweenhumanandmachine\ntranslation. arXivpreprintarXiv:1609.08144,2016.\n[Wu et al., 2021] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.\nMemorizingtransformers. InProceedingsofInternationalConferenceonLearningRepresentations,\n2021.",
    "[WuandHe,2018] YuxinWuandKaimingHe. Groupnormalization. InProceedingsoftheEuropean\nconferenceoncomputervision(ECCV),pages3–19,2018.",
    "BIBLIOGRAPHY 697\n[Wuetal.,2023] ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,AlaneSuhr,PrithvirajAmmanabrolu,\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives\nbetterrewardsforlanguagemodeltraining. InThirty-seventhConferenceonNeuralInformation\nProcessingSystems,2023b.\n[Wuetal.,2020] ZhanghaoWu,ZhijianLiu,JiLin,YujunLin,andSongHan. Litetransformerwith\nlong-shortrangeattention. InProceedingsofInternationalConferenceonLearningRepresentations\n(ICLR),2020b.\n[Wuetal., 2018] ZuxuanWu, TusharNagarajan, AbhishekKumar, StevenRennie, LarrySDavis,\nKristenGrauman,andRogerioFeris. Blockdrop: Dynamicinferencepathsinresidualnetworks. In\nProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages8817–8826,\n2018b.\n[Xiaetal.,2024] MengzhouXia,SadhikaMalladi,SuchinGururangan,SanjeevArora,andDanqiChen.\nLess: Selectinginfluentialdatafortargetedinstructiontuning. arXivpreprintarXiv:2402.04333,\n2024.",
    "[Xiaoetal.,2024] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis. Efficient\nstreaming language models with attention sinks. In Proceedings of The Twelfth International\nConferenceonLearningRepresentations,2024.\n[Xiaoetal.,2013] TongXiao,JingboZhu,andTongranLiu. Baggingandboostingstatisticalmachine\ntranslationsystems. ArtificialIntelligence,195:496–527,2013.\n[Xiaoetal.,2019] TongXiao,YinqiaoLi,JingboZhu,ZhengtaoYu,andTongranLiu.Sharingattention\nweightsforfasttransformer. InProceedingsoftheTwenty-EighthInternationalJointConferenceon\nArtificialIntelligence(IJCAI-19),pages5292–5298,2019.\n[Xieetal.,2017] SainingXie,RossGirshick,PiotrDollár,ZhuowenTu,andKaimingHe. Aggregated\nresidual transformations for deep neural networks. In Proceedings of the IEEE conference on\ncomputervisionandpatternrecognition,pages1492–1500,2017.\n[Xieetal.,2022] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa. Anexplanation",
    "ofin-contextlearningasimplicitbayesianinference. InProceedingsofInternationalConferenceon\nLearningRepresentations,2022.\n[Xinetal.,2020] JiXin,RaphaelTang,JaejunLee,YaoliangYu,andJimmyLin. Deebert: Dynamic\nearly exiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the\nAssociationforComputationalLinguistics,pages2246–2251,2020.\n[Xiongetal.,2020] RuibinXiong,YunchangYang,DiHe,KaiZheng,ShuxinZheng,ChenXing,\nHuishuaiZhang,YanyanLan,LiweiWang,andTieyanLiu.Onlayernormalizationinthetransformer\narchitecture. InInternationalConferenceonMachineLearning,pages10524–10533,2020.\n[Xuetal.,2024] CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,Chongyang\nTao,QingweiLin,andDaxinJiang. Wizardlm: Empoweringlargepre-trainedlanguagemodelsto\nfollowcomplexinstructions. InTheTwelfthInternationalConferenceonLearningRepresentations,\n2024.\n[XuandMcauley,2023] CanwenXuandJulianMcauley. Asurveyondynamicneuralnetworksfor",
    "naturallanguageprocessing. InFindingsoftheAssociationforComputationalLinguistics: EACL\n2023,pages2325–2336,2023.\n[Xuetal.,2021] ChenXu,BojieHu,YanyangLi,YuhaoZhang,ShenHuang,QiJu,TongXiao,and",
    "698 BIBLIOGRAPHY\nJingboZhu. Stackedacoustic-and-textualencoding: Integratingthepre-trainedmodelsintospeech\ntranslationencoders.InProceedingsofthe59thAnnualMeetingoftheAssociationforComputational\nLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:\nLongPapers),pages2619–2630,2021a.\n[Xuetal.,2023] ChenXu,RongYe,QianqianDong,ChengqiZhao,TomKo,MingxuanWang,Tong\nXiao,andJingboZhu. Recentadvancesindirectspeech-to-texttranslation. InProceedingsofthe\nThirty-SecondInternationalJointConferenceonArtificialIntelligence(IJCAI-23): SurveyTrack,\npages6796–6804,2023a.\n[Xuetal.,2023] ChenXu,YuhaoZhang,ChengboJiao,XiaoqianLiu,ChiHu,XinZeng,TongXiao,\nAnxiangMa,HuizhenWang,andJingboZhu. Bridgingthegranularitygapforacousticmodeling. In\nFindingsoftheAssociationforComputationalLinguistics: ACL2023,pages10816–10833,2023b.\n[Xuetal.,2020] HongfeiXu,QiuhuiLiu,JosefvanGenabith,DeyiXiong,andJingyiZhang. Lipschitz",
    "constrainedparameterinitializationfordeeptransformers.InProceedingsofthe58thAnnualMeeting\noftheAssociationforComputationalLinguistics,pages397–402,July2020.\n[Xu et al., 2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan\nSalakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption\ngenerationwithvisualattention. InInternationalconferenceonmachinelearning,pages2048–2057.\nPMLR,2015.\n[Xuetal.,2023] PengXu,XiatianZhu,andDavidAClifton. Multimodallearningwithtransformers:\nAsurvey. IEEETransactionsonPatternAnalysisandMachineIntelligence,2023c.\n[Xuetal.,2021] ZenanXu,DayaGuo,DuyuTang,QinliangSu,LinjunShou,MingGong,Wanjun\nZhong, Xiaojun Quan, Daxin Jiang, and Nan Duan. Syntax-enhanced pre-trained model. In\nProceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npages5412–5422,2021b.",
    "[Yangetal., 2024] AnYang, BaosongYang, BeichenZhang, BinyuanHui, BoZheng, BowenYu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv\npreprintarXiv:2412.15115,2024.\n[Yangetal.,2018] BaosongYang,ZhaopengTu,DerekFWong,FandongMeng,LidiaSChao,and\nTongZhang. Modelinglocalnessforself-attentionnetworks. InProceedingsofthe2018Conference\nonEmpiricalMethodsinNaturalLanguageProcessing,pages4449–4458,2018a.\n[Yangetal.,2018] YilinYang,LiangHuang,andMingboMa. Breakingthebeamsearchcurse: A\nstudyof(re-)scoringmethodsandstoppingcriteriaforneuralmachinetranslation. InProceedings\nofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3054–3059,\n2018b.\n[Yangetal.,2023] ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,Zicheng\nLiu, andLijuan Wang. The dawnof lmms: Preliminary explorationswith gpt-4v(ision). arXiv\npreprintarXiv:2309.17421,2023a.\n[Yangetal.,2019] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RussRSalakhutdinov,",
    "andQuocVLe. Xlnet:Generalizedautoregressivepretrainingforlanguageunderstanding. Advances\ninneuralinformationprocessingsystems,32,2019.\n[Yangetal.,2023] ZiYang,SamridhiChoudhary,SiegfriedKunzmann,andZhengZhang.Quantization-\nawareandtensor-compressedtrainingoftransformersfornaturallanguageunderstanding. arXiv",
    "BIBLIOGRAPHY 699\npreprintarXiv:2306.01076,2023b.\n[Yangetal.,2016] ZichaoYang,DiyiYang,ChrisDyer,XiaodongHe,AlexSmola,andEduardHovy.\nHierarchicalattentionnetworksfordocumentclassification. InProceedingsofthe2016conference\noftheNorthAmericanchapteroftheassociationforcomputationallinguistics: humanlanguage\ntechnologies,pages1480–1489,2016.\n[Yaoetal.,2024] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,and\nKarthikNarasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels.\nAdvancesinNeuralInformationProcessingSystems,36,2024.\n[Yaoetal.,2007] YuanYao,LorenzoRosasco,andAndreaCaponnetto. Onearlystoppingingradient\ndescentlearning. ConstructiveApproximation,26:289–315,2007.\n[Yarowsky, 1994] David Yarowsky. Decision lists for lexical ambiguity resolution: Application\nto accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the\nAssociationforComputationalLinguistics,pages88–95,1994.",
    "[Yarowsky, 1995] David Yarowsky. Unsupervised word sense disambiguation rivaling supervised\nmethods. InProceedingsofthe33rdannualmeetingoftheassociationforcomputationallinguistics,\npages189–196,1995.\n[Yeetal.,2021] RongYe,MingxuanWang,andLeiLi. End-to-endspeechtranslationviacross-modal\nprogressivetraining. arXivpreprintarXiv:2104.10380,2021.\n[Yinetal.,2023] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen.\nAsurveyonmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.\n[Youetal.,2020] WeiqiuYou,SimengSun,andMohitIyyer. Hard-codedgaussianattentionforneural\nmachinetranslation.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputational\nLinguistics,pages7689–7700,2020.\n[Yuetal.,2022] Gyeong-InYu,JooSeongJeong,Geon-WooKim,SoojeongKim,andByung-Gon\nChun. Orca: Adistributedservingsystemfor{Transformer-Based}generativemodels. In16th\nUSENIXSymposiumonOperatingSystemsDesignandImplementation(OSDI22),pages521–538,\n2022.",
    "[Yuetal.,2023] YaodongYu,SamBuchanan,DruvPai,TianzheChu,ZiyangWu,ShengbangTong,\nBenjaminDHaeffele,andYiMa. White-boxtransformersviasparseratereduction. arXivpreprint\narXiv:2306.01129,2023a.\n[Yuetal.,2023] ZihanYu,LiangHe,ZhenWu,XinyuDai,andJiajunChen. Towardsbetterchain-of-\nthoughtpromptingstrategies: Asurvey. arXivpreprintarXiv:2310.04959,2023b.\n[Yukseletal.,2012] SenihaEsenYuksel,JosephNWilson,andPaulDGader. Twentyyearsofmixture\nofexperts. IEEEtransactionsonneuralnetworksandlearningsystems,23(8):1177–1193,2012.\n[Yun et al., 2019] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and\nSanjivKumar. Aretransformersuniversalapproximatorsofsequence-to-sequencefunctions? In\nProceedingsofInternationalConferenceonLearningRepresentations,2019.\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie,\nC.Alberti,S.Ontañón,PhilipPham,AnirudhRavula,QifanWang,L.Yang,andA.Ahmed. Big",
    "bird: Transformersforlongersequences. Advancesinneuralinformationprocessingsystems,33:\n17283–17297,2020.\n[Zaslavskiyetal.,2009] MikhailZaslavskiy,MarcDymetman,andNicolaCancedda. Phrase-based",
    "700 BIBLIOGRAPHY\nstatisticalmachinetranslationasatravelingsalesmanproblem.InProceedingsoftheJointConference\nof the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural\nLanguageProcessingoftheAFNLP,pages333–341,2009.\n[Zeiler, 2012] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701,2012.\n[Zellersetal.,2018] RowanZellers,YonatanBisk,RoySchwartz,andYejinChoi. Swag: Alarge-scale\nadversarialdatasetforgroundedcommonsenseinference. InProceedingsofthe2018Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessing,pages93–104,2018.\n[Zhangetal.,2021] AstonZhang,ZacharyC.Lipton,MuLi,andAlexanderJ.Smola. Diveintodeep\nlearning. arXivpreprintarXiv:2106.11342,2021.\n[ZhangandSennrich,2019] BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization.\nAdvancesinNeuralInformationProcessingSystems,32,2019.\n[Zhangetal.,2018] BiaoZhang,DeyiXiong,andJinsongSu. Acceleratingneuraltransformervia",
    "anaverageattentionnetwork. InProceedingsofthe56thAnnualMeetingoftheAssociationfor\nComputationalLinguistics(Volume1: LongPapers),pages1789–1798,2018a.\n[Zhangetal.,2019] BiaoZhang,IvanTitov,andRicoSennrich. Improvingdeeptransformerwith\ndepth-scaledinitializationandmergedattention.InProceedingsofthe2019ConferenceonEmpirical\nMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNatural\nLanguageProcessing(EMNLP-IJCNLP),pages898–909,2019a.\n[Zhang et al., 2020] Jiajun Zhang, Long Zhou, Yang Zhao, and Chengqing Zong. Synchronous\nbidirectionalinferenceforneuralsequencegeneration. ArtificialIntelligence,281:103234,2020a.\n[Zhang et al., 2019] Juexiao Zhang, Yubei Chen, Brian Cheung, and Bruno A Olshausen. Word\nembeddingvisualizationviadictionarylearning. arXivpreprintarXiv:1910.03833,2019b.\n[Zhangetal.,2020] WeiEmmaZhang,QuanZSheng,AhoudAlhazmi,andChenliangLi. Adversarial\nattacksondeep-learningmodelsinnaturallanguageprocessing: Asurvey. ACMTransactionson",
    "IntelligentSystemsandTechnology(TIST),11(3):1–41,2020b.\n[Zhang et al., 2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional\nnetworksfortextclassification. Advancesinneuralinformationprocessingsystems,28,2015.\n[Zhangetal.,2018] XiangwenZhang,JinsongSu,YueQin,YangLiu,RongrongJi,andHongjiWang.\nAsynchronousbidirectionaldecodingforneuralmachinetranslation. InProceedingsoftheAAAI\nconferenceonartificialintelligence,volume32,2018b.\n[ZhangandYang,2021] YuZhangandQiangYang.Asurveyonmulti-tasklearning.IEEETransactions\nonKnowledgeandDataEngineering,pages1–1,2021.\n[Zhangetal.,2024] YunxiangZhang,MuhammadKhalifa,LajanugenLogeswaran,JaekyeomKim,\nMoontaeLee,HonglakLee,andLuWang.Smalllanguagemodelsneedstrongverifierstoself-correct\nreasoning. InACL(Findings),2024.\n[Zhangetal.,2020] ZhuoshengZhang,YuweiWu,JunruZhou,SufengDuan,HaiZhao,andRuiWang.\nSg-net: Syntax-guidedmachinereadingcomprehension. InProceedingsoftheAAAIConferenceon\nArtificialIntelligence,pages9636–9643,2020c.",
    "[Zhangetal., 2023] ZhuoshengZhang, YaoYao, AstonZhang, XiangruTang, XinbeiMa, Zhiwei\nHe, Yiming Wang, Mark Gerstein, Rui Wang, Gongshen Liu, and Hai Zhao. Igniting language\nintelligence: The hitchhiker’s guide from chain-of-thought reasoning to language agents. arXiv",
    "BIBLIOGRAPHY 701\npreprintarXiv:2311.11797,2023a.\n[Zhangetal.,2023] ZhuoshengZhang,AstonZhang,MuLi,andAlexSmola. Automaticchainof\nthoughtpromptinginlargelanguagemodels. InTheEleventhInternationalConferenceonLearning\nRepresentations,2023b.\n[Zhaoetal.,2006] HaiZhao,Chang-NingHuang,MuLi,andBao-LiangLu. Effectivetagsetselection\ninChinesewordsegmentationviaconditionalrandomfieldmodeling. InProceedingsofthe20th\nPacificAsiaConferenceonLanguage,InformationandComputation,pages87–94,2006.\n[Zhaoetal.,2024] HaoZhao,MaksymAndriushchenko,FrancescoCroce,andNicolasFlammarion.\nLongismoreforalignment: Asimplebuttough-to-beatbaselineforinstructionfine-tuning. arXiv\npreprintarXiv:2402.04833,2024.\n[Zhaoetal.,2023] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,\nYingqianMin, BeichenZhang, JunjieZhang, ZicanDong, YifanDu, ChenYang, YushuoChen,\nZ.Chen,JinhaoJiang,RuiyangRen,YifanLi,XinyuTang,ZikangLiu,PeiyuLiu,JianyunNie,and",
    "JirongWen. Asurveyoflargelanguagemodels. arXivpreprintarXiv:2303.18223,2023.\n[Zhengetal.,2019] BaigongZheng,RenjieZheng,MingboMa,andLiangHuang. Simplerandfaster\nlearningofadaptivepoliciesforsimultaneoustranslation. InProceedingsofthe2019Conferenceon\nEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon\nNaturalLanguageProcessing(EMNLP-IJCNLP),pages1349–1354,2019.\n[Zhengetal.,2018] ZaixiangZheng,HaoZhou,ShujianHuang,LiliMou,XinyuDai,JiajunChen,\nandZhaopengTu. Modelingpastandfutureforneuralmachinetranslation. Transactionsofthe\nAssociationforComputationalLinguistics,6:145–157,2018.\n[Zhongetal.,2024] YinminZhong,ShengyuLiu,JundaChen,JianboHu,YiboZhu,XuanzheLiu,\nXinJin,andHaoZhang. {DistServe}: Disaggregatingprefillanddecodingforgoodput-optimized\nlarge language model serving. In 18th USENIX Symposium on Operating Systems Design and\nImplementation(OSDI24),pages193–210,2024.\n[Zhouetal.,2023] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,Xuezhe",
    "Ma,AviaEfrat,PingYu,LiliYu,SusanZhang,GargiGhosh,MikeLewis,LukeZettlemoyer,and\nOmerLevy. Lima: Lessismoreforalignment. arXivpreprintarXiv:2305.11206,2023a.\n[Zhouetal.,2023] DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,\nDaleSchuurmans,ClaireCui,OlivierBousquet,QuocV.Le,andEdH.Chi.Least-to-mostprompting\nenablescomplexreasoninginlargelanguagemodels. InProceedingsofTheEleventhInternational\nConferenceonLearningRepresentations,2023b.\n[Zhouetal.,2021] HaoyiZhou,ShanghangZhang,JieqiPeng,ShuaiZhang,JianxinLi,HuiXiong,\nandWancaiZhang. Informer: Beyondefficienttransformerforlongsequencetime-seriesforecasting.\nInProceedingsoftheAAAIconferenceonartificialintelligence,volume35,pages11106–11115,\n2021.\n[Zhouetal., 2017] LongZhou, WenpengHu, JiajunZhang, andChengqingZong. Neuralsystem\ncombinationformachinetranslation. InProceedingsofthe55thAnnualMeetingoftheAssociation\nforComputationalLinguistics(Volume2: ShortPapers),pages378–384,2017.",
    "[Zhouetal.,2020] WangchunshuZhou,CanwenXu,TaoGe,JulianMcAuley,KeXu,andFuruWei.\nBert loses patience: Fast and robust inference with early exit. Advances in Neural Information\nProcessingSystems,33:18330–18341,2020.",
    "702 BIBLIOGRAPHY\n[Zhouetal.,2023] YongchaoZhou,AndreiIoanMuresanu,ZiwenHan,KeiranPaster,SilviuPitis,\nHarrisChan, andJimmyBa. Largelanguagemodelsarehuman-levelpromptengineers. InThe\nEleventhInternationalConferenceonLearningRepresentations,2023c.\n[Zhou, 2012] Zhi-Hua Zhou. Ensemble Methods: Foundations and Algorithms. Chapman and\nHall/CRC,2012a.\n[Zhou,2012] Zhi-HuaZhou. Ensemblemethods: foundationsandalgorithms. CRCpress,2012b.\n[ZophandLe,2016] BarretZophandQuocLe. Neuralarchitecturesearchwithreinforcementlearning.\nInProceedingsofInternationalConferenceonLearningRepresentations,2016.\n[Zophetal.,2020] BarretZoph,GolnazGhiasi,Tsung-YiLin,YinCui,HanxiaoLiu,EkinDogus\nCubuk,andQuocLe. Rethinkingpre-trainingandself-training. Advancesinneuralinformation\nprocessingsystems,33:3833–3845,2020.",
    "Index\nk-NN,445 AGI,361\nk-NNLM,447 AIC,53\nk-NNlanguagemodeling,447 ALiBi,456\nk-nearestneighbors,42,445 alignment,415\nl regularization,100 alignmentlink,66\n2\nl norm,21 alignmentscores,220\np\nn-gramlanguagemodeling,80 alternativehypothesis,58\np-norm,21 ARprocesses,180\np-normdistance,22 artificialgeneralintelligence,361\np-value,58 Artificialneuralnetworks,71\n(Higher-order)Runge-KuttaMethods,306 Associativity,20\n0-1Loss,44 attacks,107\n0-1Masking,286 attentionfield,313\nattentionhead,229\nlong-termmemory,182 attentionweight,219\nattentionwithlinearbiases,456\nA*search,262\nauto-encoders,108\nA2C,560\nauto-encoding,46,62\nabsolutepositionalencoding,197,273\nauto-regressive,61\naction-valuefunction,555\nautomatedmachinelearning,336,515\nAdaDelta,93\nautomaticpromptdesign,515\nAdaGrad,92\nAutoML,336,515\nAdam,93\nautonomousagents,513\nAdaptiveGradientDescent,92\nautoregressiveprocesses,180\nAdaptiveMomentEstimation,93\naveragingpooling,87\nadd-αsmoothing,36\nAdditiveAttention,221 backtranslation,263",
    "additivesmoothing,36 back-propagationthroughtime,177\naddressing,237 back-translation,107\nadvantage,559 backgroundtask,153\nadvantageactor-critic,560 backwardpass,78\nadversarialmachinelearning,107 BART,385\nadversarialsamples,107 batch,95\naffinetransformation,72 batchgradientdescent,95\nAgent,417 batching,95",
    "704 INDEX\nBeamsearch,247 coding,182\nbeamsize,247 columnvector,19\nbeamwidth,247 combinatorialoptimizationproblems,254\nBernoullinaiveBayes,33 Commutativity,20\nBERT,365 completion,370\nBest-of-N sampling,583 componentmodels,49\nBGD,95 componentsystems,260\nbi-directionalmodels,180 compositionalgeneralization,499\nBIC,53 compositionalmodels,209\nbilineartransform,333 Compositionality,140\nbinaryclassification,27 Computationgraphs,75\nbinaryvariable,22 conditionalcomputation,337\nBoltzmanndistribution,104 ConditionalProbability,23\nBoNsampling,583 conditionalrandomfields,130,204\nBPE,131 connectionisttemporalclassification,202\nBPTT,177 ConstantInitialization,96\nBradley-Terrymodel,562 constituentsystems,260\nbranches,288 constrainedoptimization,258\nBytePairEncoding,131 constrainedsearch,266\ncontextwindow,143\nCAE,112 Contextuality,141\ncalculationannotation,488 Continuousbatching,611\ncanonicalization,125 continuousmemory,321\ncapacity,182 continuousoptimization,256",
    "catastrophicforgetting,401 contractionmapping,113\ncategories,26 contractiveauto-encoder,112\ncausallanguagemodeling,374 contrastivelearning,45\nchainofthought,489 ContrastiveLoss,45\nchainrule,23 convolution,189\nchainruleofdifferentiation,77 convolutionkernels,85\nchain-of-thoughtprompting,422 convolutionoperation,188\ncheckpointensembling,261 convolutionproduct,189\nchunking,61 convolutionallayer,85\nClassification,26 Convolutionalneuralnetworks,85\nclassificationmodel,27 correlationcoefficients,144\nclassificationsystem,26 corruptedinput,114\nclassifier,26 corruption,114\nCloze,294 cosine,196\nCNN,85 CosineAttention,220\nco-adaptation,101 costfunctions,43\nco-attention,237 CoT,489\ncode,109 COTprompting,422",
    "INDEX 705\ncoverage,240 distantreward,38\ncoveragevector,241 distributedrepresentation,139\nCRFs,130,204 distributedrepresentations,80\ncross-entropy,25 distribution,24\ncross-linguallanguagemodels,395 distributionalhypothesis,142\ncross-validation,56 distributionalrepresentation,142\nCTC,202 distributionalsemantics,142,169\ncumulativereward,556 distributionalwordrepresentation,142\ncurvefitting,43 Distributivity,20\nDivergence-basedLoss,43\nDAEs,113 DocumentRotation,385\ndataaugmentation,107,263,342 dotproduct,20\ndecayfactor,93 Dot-productAttention,220\ndecisionboundary,31 DPO,575\ndecisionsurface,31 dropout,101\ndecoder,109 dropoutrate,101\ndecoder-onlyarchitecture,273 DTG,504\nDecoding,212,593 duration,182\ndecoding,203 DynamicNeuralNetworks,337\ndecodingblocks,272\ndecodinglayers,272 earlyexitclassifier,340\ndecodingsystem,214 earlystop,48\ndeduction,39 earlystopping,94,337\ndeeplearning,71 early-stop,249\ndeepneuralnetwork,41 edgeprobing,293\ndeepneuralnetworks,71 effectivenumberofparameters,53",
    "deliberate-then-generate,504 embeddingmatrix,82\ndeltarule,91 emergentabilities,292,433\ndemonstrations,371 emissionprobability,204\ndenoising,113 Emission-likeFeatures,205\nDenoisingauto-encoders,113 encoder,109,171\ndependentvariable,49 encoder-decoder,63\ndepth,74 encoder-decoderarchitecture,212\ndepthgrowth,303 encoder-decoderattention,225\ndepth-firstsearch,255 encoder-onlyarchitecture,273\ndiagonalstate-spacemodels,335 encodingblocks,269\ndiagonalization,335 encodinglayers,269\ndilatedcontextwindow,315 encodingsystem,214\ndirectpreferenceoptimization,575 end-to-endmemorynetworks,183\ndiscriminantfunction,30 ensemblelearning,55\ndiscriminativemodels,32,33 ensembling,260\nDistance-basedLoss,43 errorgradient,177",
    "706 INDEX\nError-basedLoss,46 gate,87\nerror-drivenlearning,34 gatedlinearunit,428\nerror-propagation,78 gatedrecurrentunits,185\nEuclideannorm,22 gaussianerrorlinearunit,428\nEulermethod,306 Gaussianmixturemodels,209\neventextraction,67 GaussiannaiveBayes,33\nevidencelowerbound(ELBO),117 Gaussiannoise,106\nexpectation,24 GeLU,428\nexpectedembedding,257 generalization,47\nexpectedrepresentation,221 generalizationerror,50\nexpectedvalue,24 generativemodels,32\nexploding and vanishing gradient problems, Globalvectors,157\n177 GloVe,157\nexternalmemories,183,445 GLU,429\nExtrapolation,453 GMMs,209\nExtrinsicEvaluation,163 GPT,365\nGQA,451\nfactoranalysis,153 gradientdescent,91\nfastText,162 GradientDescentwithMomentum,92\nfeature,28 graphicalmodels,203\nfeaturelearning,108 greedystrategy,246\nfeaturemap,189 Groupedqueryattention,451\nFeaturemapping,40 GRUs,185\nFeatureselection,152\nhardprompts,519\nfeaturesub-spaces,226\nhead,229\nfeaturevector,72\nheads,275\nfeed-forwardneuralnetworkbasedlanguage\nheld-outdata,48",
    "model,80\nhiddenlayer,82\nfeed-forwardneuralnetworks,74\nhiddenMarkovmodel,130\nfew-shotCOTprompting,423\nhiddenMarkovmodels,203\nfew-shotlearning,57\nhingeloss,45\nFFNNLM,80\nhistogrampruning,248\nFFNNs,74\nHMM,130\nfilters,85\nHMMs,203\nFisher’slineardiscriminant,152\nhomonymy,141\nFixedLearningRates,97\nhumanpreferencealignment,533\nforgetgate,184\nhypothesisselection,261\nforwardpass,76\nfractionalcount,136 ICA,153\nframes,200 ICL,422\nFrobeniusnorm,113 ICT,371\nfullyconnected,73 identitymatrix,18",
    "INDEX 707\nIDF,146 labelsmoothing,105\nimagecaptioning,213 labeledsamples,26\nimage-to-textgeneration,213 labels,26\nimportancesampling,564 languagemodeling,61\nimpulsenoise,114 large-margintraining,44\nin-contextlearning,371,422,467 Lassoregularization,100\nindependentcomponentanalysis,153 latentDirichletallocation,152\nindependentvariable,49 latentsemanticanalysis,147\nindicatorfunction,34 latentsemanticindexing,147\ninduction,39 layer,72\ninductivebias,39 layerdropout,305\ninductiveinference,38 Layer-sensitiveInitialization,96\ninductivereasoning,38 LDA,152\nInferenceEngine,611 LearningfromHumanFeedback,415\ninference-timescaling,621 learningrate,91\ninformationextraction,67 LearningRateDecay,97\ninformationgain,134 learningratescheduling,97\nInitializationwithPredefinedDistributions,96 least-to-mostprompting,495\ninputgate,184 LeCuninitialization,96\ninputinversion,546 left-singularvectors,148\ninstructionalignment,533 lemma,125\ninstructionfine-tuning,411,536 lemmatization,125",
    "interactivemachinetranslation,252 lengthnormalization,240\ninterference,396 lengthreward,239\ninternalmemories,445 lexicalsemantics,140\nInterpolation,453 linearattentionmodel,326\nIntrinsicEvaluation,163 Linearclassifier,29\ninversedocumentfrequency,146 lineardiscriminantanalysis,152\nirreducibleerror,435 lineardiscriminantfunction,30\niteration-basedscheduling,611 LinearMulti-stepMethods,306\nlineartransformation,72\nJacobianmatrix,112\nlinear-chainCRF,205\nJointProbability,23\nlinearizedtrees,210\nlinearlyseparable,40\nkernelfusion,347\nlinguisticregularity,165\nkernelmethods,40,324\nLipschitzconstant,302\nkey-valuecache,438,591\nlocalattention,234\nkeywordextraction,67\nlocaltruncationerror,306\nKnowledgedistillation,341\nlog-linear,33\nKullback-Leibler(KL)divergence,25\nlogisticregression,33\nKVcache,438,591\nLongshort-termmemory,184\nlabelmapping,479 long-contextLLMs,436",
    "708 INDEX\nlossfunctions,43 mixture-of-experts,309\nlow-rankapproximation,149 MLE,32\nLSA,147 MML,53\nLSI,147 mode,264\nLSTM,184 modelaveraging,55\nmodelcapacity,52\nMAE,44 modelcomplexity,52\nMAP,263 modeldepth,74\nmargin,44 modelerrors,50\nMargin-basedLoss,44 Modelevaluation,50\nMarginalProbability,23 modelfunction,336\nMarkovassumption,130 modelgrowth,303\nmaskedlanguagemodel,294 Modelselection,50\nmaskedlanguagemodeling,366,374 modelwidth,74\nmaskingnoise,114 MoE,309\nmatrix,18 momentum,92\nmatrixaddition,19 monotonicity,254\nMatrixproduct,21 morphologicalanalysis,124\nmatrix-matrixproduct,21 movingaverage,332\nmaxpooling,86 MQA,451\nmaximumaposteriori,263 MSE,44\nmaximumlikelihoodestimation,32 multi-branchneuralnetworks,229\nmaximumnorm,22 Multi-classclassification,27\nmBERT,394 Multi-headattention,226\nMBR,264 multi-headattention,275\nMDL,53 Multi-labelclassification,28\nmean,24 multi-layerattention,232\nmeanabsoluteerror,44 multi-layerneuralnetwork,73\nmeansquareerror,44 multi-lingualBERT,394",
    "memory,182 multi-queryattention,328,451\nmemorycell,184 multinomialnaiveBayes,33\nmemory-basedmethods,445 multiplelinearregressionmodel,180\nMERT,47 multiplicativeattention,220\nmetriclearning,65 multivariateregression,43\nmini-batchgradientdescent,95\nminibatch,282 namedentityrecognition,61\nminimumBayesrisk,264 NAS,336,515\nminimumdescriptionlength,53 NER,61\nminimumerror-ratetraining,47 NeuralArchitectureSearch,336\nminimum-spanningtree,292 neuralarchitecturesearch,515\nmining,49 neurallanguagemodels,80\nmixturemodel,55 neuralmachinetranslation,215",
    "INDEX 709\nneuralnets,71 parametricmethods,41\nneuralnetworks,71 parametrictest,58\nneuralTuringmachines,183 parsetree,63\nneurons,71 part-of-speechtagging,60\nnextsentenceprediction,377 patches,354\nNMT,215 paths,288\nnon-autoregressivedecoding,266 PCA,108,149\nnon-autoregressivegeneration,266 perceptrons,71\nNon-linearactivationfunctions,41 PerformanceEstimation,516\nnon-parametric,58 performancefunction,557\nNon-parametricmethods,41 performancegaprecovered,551\nnorm,21 permutedlanguagemodeling,375\nnormalization,88,125 PGR,551\nNSP,377 Plackett-Lucemodel,569\nnucleussampling,601 PMI,144\nnullhypothesis,58 pointwisemutualinformation,134,144\npolysemy,141\nobjectivefunction,42\npoolinglayer,85\nOccam’sRazor,42\nPOStagging,60\nODEs,305\npositionalencoding,194\nofflinereinforcementlearning,578\npost-norm,270\none-hot,138\npost-trainingquantization,346\none-hotrepresentations,80\nPPO,419,566\none-shotCOTprompting,423\npre-norm,277\nonlinesequence-to-sequencesystem,252\npre-training,38\nOOV,35\nPrefilling,593",
    "open-vocabulary,35\nprefixfine-tuning,523\noptimization,42\nprefixlanguagemodeling,381\nordinarydifferentialequations,305\nprimeword,167\nORMs,629\npriming,166\northogonalvectors,148\nprincipalcomponent,151\nout-of-vocabulary,35\nprincipalcomponentanalysis,149\noutcomerewardmodels,629\nprincipalcomponentcoefficients,149\nOutcome-basedApproaches,580\nprincipalcomponentloadings,149\noutputequation,332\nprincipalcomponentsanalysis,108\nover-translation,240\nPRM,629\noverfitting,39\nProbability,22\noveroptimizationproblem,573\nprobabilitydensity,23\npadding,85 probabilitydistribution,24\npairwisemethod,45 probabilityfunction,22\nparallelscaling,627 probabilitymeasure,22\nparametersharing,85 probes,291",
    "710 INDEX\nprobingclassifier,292 reorderingproblem,234\nprobingpredictor,292 reparameterizationtrick,117\nproblemdecomposition,492 request-levelscheduling,611\nprocessrewardmodel,629 resetgate,185\nProcess-basedApproaches,580 residualconnections,89,179\nproductrule,23 Residualneuralnetworks,89\nprogressivedownsampling,344 retrieval-augmentedapproach,322\npromptembeddings,528 retrieval-augmentedgeneration,448\npromptengineering,467 return,556\npromptoptimization,515 reversibleresidualnetworks,328\nPromptSearchSpace,515 rewardgaming,573\npromptingengineering,420 rewardhacking,573\nproximalpolicyoptimization,419,566 RewardModel,417\nRidgeregularization,100\nQ-valuefunction,555\nright-singularvectors,148\nQKVattention,223\nrisk,46\nquery-key-valueattention,223\nRLHF,416,534\nRAG,448 RMSProp,93\nrandomprocesses,173 RNNs,83\nrandomvariable,22 RoBERTa,393\nRanking,49 robuststatistics,105\nRanking-basedLoss,45 routingmodel,309\nratiofunction,564 rowvector,19\nreceiver,221 RPR,297\nreceptivefield,85,187",
    "reconstructionloss,109 salt-and-peppernoise,114\nrectifiedlinearunit,428 sampleefficient,57,547\nrecurrentcell,83,184 samples,26\nRecurrentneuralnetworks,83 saturatingactivationfunctions,178\nrecurrentunit,83 Scalar,18\nregression,43 scalarproduct,19\nregularexpressions,125 scaleddot-productattention,220\nreinforcementlearning,38 scalinglaws,345,433\nreinforcementlearningfromhumanfeedback, Scheduler,611\n416,534 searcherrors,50,248\nrejectionsampling,584 searchproblem,50\nrelationextraction,67,482 self-attention,226\nrelativeentropy,25 self-consistency,507\nrelativepositionalencoding,197 self-instruct,543\nrelativepositionalrepresentation,297 self-pacedreading,167\nReLU,428 self-supervisedlearning,38,367\nremove-one,136 self-training,367",
    "INDEX 711\nsemi-orthogonal,148 sourcesequence,212\nsemi-supervisedlearning,37 source-sidesequence,212\nsemi-unitary,148 SpanMasking,385\nsender,221 spanprediction,67\nsensorymemory,182 sparseattentionmodels,296\nsentenceembedding,210 sparseauto-encoders,111\nsentencelengthprediction,294 sparsecoding,111\nSentenceReordering,385 sparseexpertmodels,338\nsentence-leveldepth-adaptivemodel,338 sparsitypenalty,111\nseq2seq,211 sparsityratio,313\nSequenceEncodingModels,368 Speculativedecoding,604\nSequenceGenerationModels,368 speculativeexecution,604\nSequencelabeling,60 speechencoder,352\nsequencelabeling,129 speed-accuracytrade-off,250\nsequence-to-sequence,62,211 SSMs,332\nsequentialscaling,627 standarddeviation,24\nSFT,415,534 standardization,89\nSGD,94 stateequation,332\nshallow-to-deeptraining,303 statevariables,332\nsharedencoder,352 state-spacemodels,332\nshort-termmemory,182 state-valuefunction,555\nshortcutconnections,89 Statisticallanguagemodeling,61",
    "shrinkageestimator,104 statisticalmachinetranslation,240\nsignificancelevel,58 statisticalparsing,63\nsignificancetests,58 steepestdescent,91\nsimilarityfunction,65 stem,126\nsimilaritylearning,65 stemming,126\nsimplex,257 stepfunction,72\nsimultaneoustranslation,252 stochasticgradientdescent,94\nsine,195 stochasticprocesses,173\nsingle-labelclassification,28 Stoppingcriterion,94\nsingle-layerattention,232 StrongCeilingPerformance,550\nsingle-layerneuralnetwork,73 Structureprediction,49\nsingle-layerperceptrons,71 structureprediction,27\nsingle-roundprediction,538 structuredpruning,342\nsingularvaluedecomposition,147 sub-layerdropout,305\nsingularvalues,148 Sub-problemGeneration,495\nskipconnections,89,179 Sub-problemSolving,495\nSMT,240 sub-spacemethod,260\nSoftMasking,287 subword,131\nsoftprompts,519 suffixstripping,126\nsoftwordalignmentmatrix,66 superficialalignmenthypothesis,547",
    "712 INDEX\nSuperviseddimensionreduction,152 TF,145\nSupervisedFine-tuning,415 TF-IDF,146\nsupervisedfine-tuning,534 theAkaikeinformationcriterion,53\nSupervisedlearning,37 thebag-of-words(BOW)model,28\nsupervisedlearning,367 theBayesianapproach,46\nsupportvectormachines,41 theBayesianinformationcriterion,53\nSurfaceFormsofWordsandSentences,294 theBayesianrisk,46\nsurrogateobjective,565 theCBOWmodel,155\nSVD,147 Thecontinuousbag-of-wordsmodel,155\nSyntacticandSemanticLabels,293 Thecontinuousskip-grammodel,156\nsyntactichierarchy,290 thecurseofdimensionality,39\nsyntacticparser,63 TheExpectationStep,136\nsyntaxtree,63 theExpectation-Maximization(EM)algorithm,\nsyntax-awareTransformerencoders,284 135\nsystemcombinationmethods,260 thekerneltrick,41\nsystematicerror,54 Thelearningproblem,27\nTheMaximizationStep,136\nt-distributed stochastic neighbor embedding, theminimummessagelength,53\n168 Themodelingproblem,27\nt-SNE,168 themulti-storemodel,182\nT5,380 Thenofreelunchtheorem,52\ntags,26 Thepredictionproblem,27",
    "targetsequence,212 theprincipleofcompositionality,140\ntargetword,167 theskip-grammodel,156\ntarget-sidesequence,212 theVapnik-Chervonenkisdimension,52\nTD,560 theVCdimension,52\nteacherforcing,239 thresholdpruning,248\nteacher-studenttraining,341 timeseries,173\ntemplatefilling,67 TokenDeletion,385\ntemporaldifference,560 TokenMasking,385\ntermfrequency,145 tokenpruning,344\ntermfrequency-inversedocumentfrequency, token-leveldepth-adaptivemodel,338\n146 Tokenization,123\nterm-documentco-occurrencematrix,145 tokens,123\nterm-termco-occurrencematrix,143 Topicmodels,152\ntesterror,50 trainingepochs,94\ntextcompletion,483 trainingerror,50\ntextembedding,210 trainingstep,91\ntextencoder,352 transcription,200\ntextgeneration,212 transcriptionlabels,200\ntexttransformation,483 transcriptionunits,200\ntext-to-imagegeneration,213 transferlearning,341",
    "INDEX 713\nTransformer,269 VQA,237,355\nTransformer-XL,319\nWarmupandDecay,97\ntransitionprobability,204\nWeakPerformance,550\nTransition-likeFeatures,205\nweak-to-stronggeneralization,550\ntranslationlanguagemodeling,395\nWeak-to-strongPerformance,550\ntranspose,19\nweightdecay,100\ntreelinearization,284\nweightsharing,85\nTrees,292\nWeightedDot-productAttention,220\ntrustregions,565\nwidth,308\nTustin’smethod,333\nWordalignment,234\nunder-translation,240 wordalignment,66\nundercompleteauto-encoder,110 wordalignmentweightmatrix,66\nunderfitting,47 wordclustering,37\nundirectedgraphicalmodels,205 worddistance,164\nunfolded,83 wordembedding,82,139\nuni-directionalmodels,180 WordRepresentationLearning,123\nunigram,134 wordsegmentation,124\nunrolled,83 wordsemanticdistance,164\nunseenwords,35 wordsense,137\nunstructuredpruning,342 wordsensedisambiguation,142\nunsupervised bilingual dictionary induction, word-documentco-occurrencematrix,145\n37 word-wordco-occurrencematrix,143\nunsupervisedlearning,37,367 Word2Vec,155",
    "updategate,185 WSD,142\nupdaterule,91\nXavierinitialization,96\nupdatestep,91\nXLMs,395\nVAEs,115\nzeromatrix,18\nvalidationdata,48\nzeropadding,282\nvaluefunction,262\nzero-shotCOT,423\nvalue-basedsearch,262\nzero-shotlearning,414\nvariable,22\nZipf’sLaw,36\nvariance,24\nvariationalauto-encoders,115\nVector,18\nvectordatabase,321\nvectorfunction,30\nVisionTransformer,353\nvisualquestionanswering,237,355\nViT,353\nViterbidecoding,203\nvLBL,162"
  ],
  "metadata": [
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    },
    {
      "source": "nlp-book.pdf"
    }
  ]
}