# Query OOM Fix - Giáº£i quyáº¿t CUDA Out of Memory

## ğŸ”´ Váº¥n Ä‘á»

**Query bá»‹ CUDA OOM vÃ  cháº­m:**
- LLM: 12GB VRAM
- KV cache (context lá»›n): 4GB VRAM
- **Total: 16GB â†’ OOM trÃªn GPU 16GB!** ğŸ’¥

---

## ğŸ” NguyÃªn nhÃ¢n chi tiáº¿t

### Pipeline Query (TrÆ°á»›c khi fix):

```python
# 1. Retrieve
top_50 = faiss.search(query, 50)

# 2. Rerank
top_5 = rerank(top_50, 5)  # 5 chunks

# 3. Build context
chunks = [
    chunk1: 1500 words â‰ˆ 2000 tokens
    chunk2: 1500 words â‰ˆ 2000 tokens  
    chunk3: 1500 words â‰ˆ 2000 tokens
    chunk4: 1500 words â‰ˆ 2000 tokens
    chunk5: 1500 words â‰ˆ 2000 tokens
]
# Total: 10,000 tokens! ğŸ’¥

# 4. LLM generate
# KV cache for 10,000 tokens: ~6GB VRAM
# LLM weights: 12GB VRAM
# Total: 18GB â†’ OOM!
```

### VRAM Breakdown (TrÆ°á»›c):
```
LLM weights (4-bit):     12GB
KV cache (10k tokens):    6GB  â† Váº¤N Äá»€!
Activation:               1GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                   19GB â†’ OOM! ğŸ’¥
```

---

## âœ… Giáº£i phÃ¡p (3 fixes)

### Fix 1: **Giáº£m chunk size**

**TrÆ°á»›c:**
```python
MIN_CHUNK_SIZE = 800   # words
MAX_CHUNK_SIZE = 1500  # words
```

**Sau:**
```python
MIN_CHUNK_SIZE = 400   # words (giáº£m 50%)
MAX_CHUNK_SIZE = 800   # words (giáº£m 47%)
```

**Lá»£i Ã­ch:**
- Má»—i chunk nhá» hÆ¡n â†’ Ãt tokens hÆ¡n
- 800 words â‰ˆ 1000 tokens (thay vÃ¬ 2000)

---

### Fix 2: **Hard cap token limit**

**ThÃªm:**
```python
MAX_CONTEXT_TOKENS = 2000  # Hard limit
TOKENS_PER_WORD = 1.3      # Estimate

def truncate_context(chunks, max_tokens=2000):
    """Giá»›i háº¡n tá»•ng token context"""
    context_parts = []
    total_tokens = 0
    
    for chunk, score in chunks:
        chunk_tokens = len(chunk.split()) * 1.3
        
        if total_tokens + chunk_tokens > max_tokens:
            break  # Stop!
        
        context_parts.append(chunk)
        total_tokens += chunk_tokens
    
    return "\n\n".join(context_parts)
```

**Lá»£i Ã­ch:**
- Äáº£m báº£o context KHÃ”NG BAO GIá»œ vÆ°á»£t 2000 tokens
- Tá»± Ä‘á»™ng truncate náº¿u cáº§n

---

### Fix 3: **Giáº£m TOP_K_RERANK**

**TrÆ°á»›c:**
```python
TOP_K_RERANK = 5  # chunks
```

**Sau:**
```python
TOP_K_RERANK = 3  # chunks (giáº£m 40%)
```

**Lá»£i Ã­ch:**
- Ãt chunks hÆ¡n â†’ Ãt tokens hÆ¡n
- Váº«n Ä‘á»§ context Ä‘á»ƒ tráº£ lá»i

---

## ğŸ“Š Káº¿t quáº£

### Pipeline Query (Sau khi fix):

```python
# 1. Retrieve
top_50 = faiss.search(query, 50)

# 2. Rerank
top_3 = rerank(top_50, 3)  # 3 chunks (tá»« 5)

# 3. Build context vá»›i hard cap
chunks = [
    chunk1: 600 words â‰ˆ 780 tokens
    chunk2: 600 words â‰ˆ 780 tokens
    chunk3: 600 words â‰ˆ 780 tokens
]
# Total: ~2340 tokens
# Truncated to: 2000 tokens âœ…

# 4. LLM generate
# KV cache for 2000 tokens: ~1.5GB VRAM
# LLM weights: 12GB VRAM
# Total: 13.5GB âœ… (safe!)
```

### VRAM Breakdown (Sau):
```
LLM weights (4-bit):     12GB
KV cache (2k tokens):   1.5GB  âœ… Giáº£m 75%!
Activation:             0.5GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                 14GB âœ… (safe for 16GB GPU)
```

---

## ğŸ“ˆ So sÃ¡nh Before/After

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| **Chunk size** | 800-1500 words | 400-800 words | -50% |
| **TOP_K** | 5 chunks | 3 chunks | -40% |
| **Max tokens** | Unlimited | 2000 (hard cap) | âœ… |
| **Context tokens** | ~10,000 | ~2,000 | -80% |
| **KV cache VRAM** | 6GB | 1.5GB | -75% |
| **Total VRAM** | 19GB (OOM!) | 14GB âœ… | -26% |
| **Query status** | OOM ğŸ’¥ | Works âœ… | Fixed! |

---

## ğŸš€ CÃ¡ch test

### BÆ°á»›c 1: Re-index vá»›i chunk nhá» hÆ¡n
```bash
# Index láº¡i vá»›i chunk size má»›i (400-800)
uv run rag_index.py --force
```

### BÆ°á»›c 2: Query
```bash
# Query vá»›i hard token limit
uv run rag_query.py
```

**Output má»›i:**
```
ğŸ§‘ Báº¡n: NLP lÃ  gÃ¬?

ğŸ¤– Äang xá»­ lÃ½...
   ğŸ” Searching...
   ğŸ“„ Found 50 chunks
   ğŸ¯ Reranking to top 3...
   âœ… Selected 3 best chunks
   âœ… Using all 3 chunks (1850 tokens)  â† Hard cap works!
   ğŸ¤– Generating answer...
   â±ï¸ Total: 6.5s

ğŸ“ Tráº£ lá»i:
Natural Language Processing (NLP) is...
```

---

## ğŸ’¡ Táº¡i sao fix nÃ y hiá»‡u quáº£?

### 1. **Chunk nhá» hÆ¡n**
- Má»—i chunk: 400-800 words thay vÃ¬ 800-1500
- Dá»… fit vÃ o token limit
- Váº«n Ä‘á»§ context

### 2. **Hard cap token**
- Äáº£m báº£o KHÃ”NG BAO GIá»œ vÆ°á»£t 2000 tokens
- Tá»± Ä‘á»™ng truncate
- An toÃ n 100%

### 3. **Ãt chunks hÆ¡n**
- 3 chunks thay vÃ¬ 5
- Giáº£m context size
- Váº«n Ä‘á»§ Ä‘á»ƒ tráº£ lá»i

---

## âœ… Káº¿t luáº­n

**Váº¥n Ä‘á»:** Query OOM do context quÃ¡ lá»›n (10k tokens) â†’ KV cache 6GB

**Giáº£i phÃ¡p:**
1. âœ… Giáº£m chunk size (400-800 words)
2. âœ… Hard cap token (2000 max)
3. âœ… Giáº£m TOP_K (3 chunks)

**Káº¿t quáº£:**
- Context: 2000 tokens (giáº£m 80%)
- KV cache: 1.5GB (giáº£m 75%)
- Total VRAM: 14GB (safe!)
- **No more OOM!** ğŸ‰

---

**BÃ¢y giá» hÃ£y re-index vÃ  test!** ğŸš€
