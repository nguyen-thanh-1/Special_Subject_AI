# Fix: LLM váº«n cháº¡y CPU - Lazy Loading

## ğŸ”´ Váº¥n Ä‘á» tiáº¿p theo

Máº·c dÃ¹ Ä‘Ã£ Ä‘á»•i thá»© tá»± load trong `rag_query.py`, LLM váº«n cháº¡y CPU.

## ğŸ” NguyÃªn nhÃ¢n thá»±c sá»±

**File `Llama_3_1_8B_Instruct_v2.py` load model NGAY KHI IMPORT:**

```python
# File: Llama_3_1_8B_Instruct_v2.py
import torch
from transformers import ...

# âŒ Load ngay khi import (dÃ²ng 12-28)
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(...)  # Load ngay!
print("Model loaded!")

def generate_response(...):
    # DÃ¹ng model Ä‘Ã£ load
```

**Váº¥n Ä‘á»:**
1. Khi `rag_query.py` gá»i `get_llm()` â†’ Import `Llama_3_1_8B_Instruct_v2`
2. Import â†’ **Model load NGAY** (khÃ´ng Ä‘á»£i gá»i function)
3. LÃºc nÃ y náº¿u cÃ³ models khÃ¡c Ä‘Ã£ chiáº¿m VRAM â†’ LLM bá»‹ Ä‘áº©y xuá»‘ng CPU

**Thá»© tá»± thá»±c táº¿:**
```python
# rag_query.py
get_llm()  # Import Llama_3_1_8B_Instruct_v2
           # â†’ Model load NGAY táº¡i Ä‘Ã¢y!
           # â†’ Náº¿u VRAM Ä‘Ã£ bá»‹ chiáº¿m â†’ CPU

get_embedder()  # Load sau
get_reranker()  # Load sau
```

---

## âœ… Giáº£i phÃ¡p: Lazy Loading

**Chá»‰ load model khi Gá»ŒI function láº§n Ä‘áº§u, khÃ´ng pháº£i khi import:**

### File: `Llama_3_1_8B_Instruct_v2.py`

**TrÆ°á»›c (Eager Loading):**
```python
# âŒ Load ngay khi import
print("Loading model...")
model = AutoModelForCausalLM.from_pretrained(...)
print("Model loaded!")

def generate_response(...):
    # DÃ¹ng model Ä‘Ã£ load
    inputs = tokenizer(prompt).to(model.device)
```

**Sau (Lazy Loading):**
```python
# âœ… Chá»‰ khai bÃ¡o biáº¿n global
_model = None
_tokenizer = None

def _load_model():
    """Lazy load - chá»‰ load khi gá»i láº§n Ä‘áº§u"""
    global _model, _tokenizer
    
    if _model is not None:
        return _model, _tokenizer  # ÄÃ£ load rá»“i
    
    # Load model (chá»‰ cháº¡y 1 láº§n)
    print("Loading model...")
    _model = AutoModelForCausalLM.from_pretrained(...)
    _tokenizer = AutoTokenizer.from_pretrained(...)
    print("Model loaded!")
    
    return _model, _tokenizer

def generate_response(...):
    # Load model khi gá»i function láº§n Ä‘áº§u
    model, tokenizer = _load_model()
    
    # DÃ¹ng model
    inputs = tokenizer(prompt).to(model.device)
```

---

## ğŸ“Š So sÃ¡nh

### Eager Loading (TrÆ°á»›c):
```python
# rag_query.py
from rag_pro_v2 import get_llm

get_llm()  # Import Llama_3_1_8B_Instruct_v2
           # â†’ "Loading model..." (load NGAY)
           # â†’ Náº¿u VRAM bá»‹ chiáº¿m â†’ CPU âŒ
```

### Lazy Loading (Sau):
```python
# rag_query.py
from rag_pro_v2 import get_llm

get_llm()  # Import Llama_3_1_8B_Instruct_v2
           # â†’ KhÃ´ng load gÃ¬ cáº£ (chá»‰ import)
           
# Khi query láº§n Ä‘áº§u:
rag.query("NLP lÃ  gÃ¬?")
  â†’ generate_response()
    â†’ _load_model()  # Load BÃ‚Y GIá»œ
    â†’ "Loading model..." (load lÃºc nÃ y)
    â†’ Chiáº¿m GPU âœ…
```

---

## ğŸ¯ Lá»£i Ã­ch Lazy Loading

### 1. **Kiá»ƒm soÃ¡t thá»i Ä‘iá»ƒm load**
```python
# CÃ³ thá»ƒ load ÄÃšNG LÃšC cáº§n
get_llm()        # Chá»‰ import, chÆ°a load
get_embedder()   # Load embedding
get_reranker()   # Load reranker

# Query láº§n Ä‘áº§u â†’ LLM má»›i load
rag.query(...)   # Load LLM BÃ‚Y GIá»œ â†’ GPU
```

### 2. **TrÃ¡nh load khÃ´ng cáº§n thiáº¿t**
```python
# Náº¿u chá»‰ import nhÆ°ng khÃ´ng dÃ¹ng
from Llama_3_1_8B_Instruct_v2 import generate_response
# â†’ KhÃ´ng load gÃ¬ (tiáº¿t kiá»‡m thá»i gian)

# Chá»‰ load khi thá»±c sá»± gá»i
generate_response("hello")  # Load lÃºc nÃ y
```

### 3. **Dá»… debug**
```python
# Biáº¿t chÃ­nh xÃ¡c khi nÃ o model load
print("Before import")
from Llama_3_1_8B_Instruct_v2 import generate_response
print("After import")  # KhÃ´ng load

generate_response("test")
# â†’ "Loading model..." (load á»Ÿ Ä‘Ã¢y)
```

---

## ğŸš€ CÃ¡ch cháº¡y láº¡i

```bash
# Stop táº¥t cáº£ chÆ°Æ¡ng trÃ¬nh Ä‘ang cháº¡y (Ctrl+C)

# Cháº¡y láº¡i rag_query.py
uv run rag_query.py
```

**Output má»›i:**
```
ğŸ”„ Loading LLM (GPU priority)...
   âœ… Llama 3.1 8B loaded  â† ChÆ°a load model, chá»‰ import

ğŸ”„ Loading index from disk...
   âœ… Loaded 88 chunks

ğŸ”„ Loading embedding & reranker (CPU)...
   âœ… Embedding model loaded (CPU)
   âœ… Reranker loaded (CPU)

ğŸ’¬ CÃ¢u há»i: NLP lÃ  gÃ¬?

ğŸ¤– Äang xá»­ lÃ½...
Loading model...  â† Load LÃšC NÃ€Y (khi query)
Model loaded!     â† Chiáº¿m GPU
   ğŸ” Searching...
   ğŸ¯ Reranking...
   ğŸ¤– Generating answer... (5s) â† GPU NHANH!
```

---

## âœ… Káº¿t luáº­n

**Váº¥n Ä‘á»:** Model load ngay khi import â†’ KhÃ´ng kiá»ƒm soÃ¡t Ä‘Æ°á»£c thá»i Ä‘iá»ƒm load

**Giáº£i phÃ¡p:** Lazy loading â†’ Load khi gá»i function láº§n Ä‘áº§u

**Káº¿t quáº£:** LLM load Ä‘Ãºng lÃºc, chiáº¿m GPU, cháº¡y nhanh! ğŸš€

---

## ğŸ“ Pattern: Lazy Loading

**Ãp dá»¥ng cho má»i model náº·ng:**

```python
# Global variables
_model = None

def _load_model():
    global _model
    if _model is not None:
        return _model
    
    # Load chá»‰ 1 láº§n
    _model = load_heavy_model()
    return _model

def use_model(...):
    model = _load_model()  # Load khi cáº§n
    return model.predict(...)
```

**Lá»£i Ã­ch:**
- âœ… Kiá»ƒm soÃ¡t thá»i Ä‘iá»ƒm load
- âœ… TrÃ¡nh load khÃ´ng cáº§n thiáº¿t
- âœ… Dá»… debug
- âœ… Tiáº¿t kiá»‡m VRAM
