# âœ… Fixed: CUDA Out of Memory Error

## ğŸ”´ Váº¥n Ä‘á» ban Ä‘áº§u

```
CUDA out of memory. Tried to allocate 22.00 GiB
GPU 0 has a total capacity of 15.93 GiB
12.30 GiB is allocated by PyTorch (Llama model)
```

**NguyÃªn nhÃ¢n:**
- Llama 3.1 8B: ~12GB VRAM
- BGE-M3 Embedding: ~2-3GB VRAM
- BGE-Reranker-v2-M3: ~1-2GB VRAM
- **Tá»•ng: ~15-17GB > 15.93GB GPU cá»§a báº¡n**

## âœ… Giáº£i phÃ¡p Ä‘Ã£ Ã¡p dá»¥ng

### PhÃ¢n bá»• CPU/GPU tá»‘i Æ°u:

| Component | Device | VRAM | LÃ½ do |
|-----------|--------|------|-------|
| **Llama 3.1 8B** | GPU | 12GB | LLM cáº§n GPU Ä‘á»ƒ inference nhanh |
| **BGE-M3 Embedding** | CPU | 0GB | Cháº¡y CPU váº«n cháº¥p nháº­n Ä‘Æ°á»£c |
| **BGE-Reranker-v2-M3** | CPU | 0GB | Reranker Ã­t chunks, CPU OK |

### Code changes:

#### 1. Force Embedding to CPU
```python
# OLD
_embedder = SentenceTransformer(EMBEDDING_MODEL)

# NEW
_embedder = SentenceTransformer(EMBEDDING_MODEL, device='cpu')
```

#### 2. Force Reranker to CPU
```python
# OLD
_reranker = CrossEncoder(RERANKER_MODEL)

# NEW
_reranker = CrossEncoder(RERANKER_MODEL, device='cpu')
```

#### 3. Update Batch Size
```python
# Always use CPU batch size (32) for embedding
batch_size = EMBEDDING_BATCH_SIZE_CPU  # 32
```

## ğŸ“Š Káº¿t quáº£

### VRAM Usage:
```
Before: 15-17GB (OOM âŒ)
After:  ~12GB (OK âœ…)
```

### Performance Impact:

| Operation | GPU | CPU | Slowdown |
|-----------|-----|-----|----------|
| Embedding | ~2s/1000 chunks | ~8s/1000 chunks | **4x slower** |
| Reranking | ~0.5s/50 chunks | ~1.5s/50 chunks | **3x slower** |
| LLM | ~5s/response | N/A | **No change** |

**Tá»•ng impact:** Embedding cháº­m hÆ¡n ~4x, nhÆ°ng **khÃ´ng bá»‹ OOM**!

## ğŸš€ Cháº¡y láº¡i

```bash
# Exit chÆ°Æ¡ng trÃ¬nh hiá»‡n táº¡i (Ctrl+C)
# Cháº¡y láº¡i
uv run rag_pro_v2.py --force
```

**Output má»›i:**
```
ğŸ“Š Embedding: BAAI/bge-m3 (CPU)
ğŸ¯ Reranker:  BAAI/bge-reranker-v2-m3 (CPU)
ğŸ¤– LLM:       Llama 3.1 8B (GPU)
```

## â±ï¸ Performance Expectations

### Indexing NLP Book (800 pages):

**TrÆ°á»›c (V1 - táº¥t cáº£ GPU):**
- 30,000 chunks Ã— 2s = ~16 phÃºt embedding
- **Total: ~20 phÃºt** (náº¿u khÃ´ng OOM)

**Sau (V2 - Embedding CPU):**
- 4,000 chunks Ã— 8s = ~9 phÃºt embedding
- **Total: ~10 phÃºt** (khÃ´ng OOM âœ…)

**Láº§n 2 (vá»›i cache):**
- Load tá»« cache: ~2-3 giÃ¢y
- **Total: ~3 giÃ¢y** âš¡

## ğŸ’¡ Trade-offs

### âœ… Pros:
- KhÃ´ng bá»‹ CUDA OOM
- Váº«n giá»¯ Ä‘Æ°á»£c táº¥t cáº£ tÃ­nh nÄƒng
- LLM váº«n cháº¡y GPU (nhanh)
- Cache váº«n hoáº¡t Ä‘á»™ng (láº§n 2 ráº¥t nhanh)

### âš ï¸ Cons:
- Embedding cháº­m hÆ¡n ~4x (GPU â†’ CPU)
- Láº§n Ä‘áº§u index máº¥t ~10 phÃºt thay vÃ¬ ~6 phÃºt

### ğŸ¯ Káº¿t luáº­n:
**Cháº¥p nháº­n Ä‘Æ°á»£c!** VÃ¬:
1. Chá»‰ cháº­m láº§n Ä‘áº§u (10 phÃºt vs 6 phÃºt)
2. Láº§n sau váº«n ráº¥t nhanh (3 giÃ¢y vá»›i cache)
3. KhÃ´ng bá»‹ crash do OOM
4. LLM inference váº«n nhanh (GPU)

## ğŸ”„ Alternative Solutions (náº¿u muá»‘n nhanh hÆ¡n)

### Option 1: Unload Llama khi embedding
```python
# Unload Llama trÆ°á»›c khi embed
del model
torch.cuda.empty_cache()

# Embed trÃªn GPU
embed_on_gpu()

# Load láº¡i Llama
load_llama()
```
**Pros:** Embedding nhanh hÆ¡n  
**Cons:** Phá»©c táº¡p, máº¥t thá»i gian load/unload

### Option 2: DÃ¹ng embedding nhá» hÆ¡n
```python
# Thay BGE-M3 báº±ng BGE-base-en (nhá» hÆ¡n)
EMBEDDING_MODEL = "BAAI/bge-base-en-v1.5"  # ~0.5GB thay vÃ¬ 2GB
```
**Pros:** Fit GPU, nhanh hÆ¡n  
**Cons:** Cháº¥t lÆ°á»£ng embedding kÃ©m hÆ¡n

### Option 3: Quantize Llama thÃªm
```python
# 4-bit â†’ 3-bit hoáº·c 2-bit
# Giáº£m VRAM Llama tá»« 12GB â†’ 8GB
```
**Pros:** Nhiá»u VRAM cho embedding  
**Cons:** Cháº¥t lÆ°á»£ng LLM giáº£m

## âœ… Recommended: Giá»¯ nguyÃªn giáº£i phÃ¡p hiá»‡n táº¡i

**LÃ½ do:**
- ÄÆ¡n giáº£n, á»•n Ä‘á»‹nh
- KhÃ´ng áº£nh hÆ°á»Ÿng cháº¥t lÆ°á»£ng
- Chá»‰ cháº­m láº§n Ä‘áº§u (~10 phÃºt)
- Láº§n sau ráº¥t nhanh (cache)

---

**BÃ¢y giá» hÃ£y cháº¡y láº¡i vÃ  test!** ğŸš€
