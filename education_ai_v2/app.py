"""
ğŸ“ Education AI v2 - Trá»£ lÃ½ GiÃ¡o dá»¥c thÃ´ng minh vá»›i RAG
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Features:
- Upload tÃ i liá»‡u (PDF, TXT, MD, CSV)
- Tá»± Ä‘á»™ng xá»­ lÃ½ vÃ  index
- Há»i Ä‘Ã¡p dá»±a trÃªn tÃ i liá»‡u (RAG)
- Streaming response
- Hiá»ƒn thá»‹ tráº¡ng thÃ¡i xá»­ lÃ½ real-time

Cháº¡y: streamlit run app.py
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import streamlit as st
import os
import time
from datetime import datetime

# Page Config
st.set_page_config(
    page_title="Education AI v2",
    page_icon="ğŸ“",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS - Dark Theme
st.markdown("""
<style>
    .status-pending { color: #FFA500; font-weight: bold; }
    .status-processing { color: #00BFFF; font-weight: bold; }
    .status-completed { color: #00FF7F; font-weight: bold; }
    .status-error { color: #FF6B6B; font-weight: bold; }
    .file-card {
        padding: 12px 15px;
        border-radius: 10px;
        margin: 8px 0;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        border: 1px solid #0f3460;
        color: #e0e0e0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
    }
    .file-card b {
        color: #00d4ff;
    }
    .file-card small {
        color: #a0a0a0;
    }
    .stats-box {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 15px;
        border-radius: 10px;
        color: white;
        text-align: center;
    }
    /* Dark sidebar */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0f0f1a 0%, #1a1a2e 100%);
    }
    [data-testid="stSidebar"] .stMarkdown {
        color: #e0e0e0;
    }
</style>
""", unsafe_allow_html=True)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INITIALIZE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
UPLOADS_DIR = "./uploads"
os.makedirs(UPLOADS_DIR, exist_ok=True)

# Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

if "file_status" not in st.session_state:
    st.session_state.file_status = {}

if "models_loaded" not in st.session_state:
    st.session_state.models_loaded = False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOAD MODELS (Cached)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_resource(show_spinner=False)
def load_rag_engine():
    """Load RAG Engine (embedding + reranker)"""
    from rag_engine import get_rag_engine
    return get_rag_engine()


@st.cache_resource(show_spinner=False)
def load_llm():
    """Load LLM"""
    from engine_llm import EducationalLLM
    return EducationalLLM()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SIDEBAR - File Upload & Status
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with st.sidebar:
    st.title("ğŸ“ Education AI v2")
    st.caption("Trá»£ lÃ½ há»c táº­p thÃ´ng minh vá»›i RAG")
    
    st.markdown("---")
    
    # Model Loading Status
    st.markdown("### ğŸ”§ Tráº¡ng thÃ¡i há»‡ thá»‘ng")
    
    if not st.session_state.models_loaded:
        with st.spinner("Äang táº£i models..."):
            try:
                rag_engine = load_rag_engine()
                llm = load_llm()
                st.session_state.models_loaded = True
                st.success("âœ… Há»‡ thá»‘ng sáºµn sÃ ng!")
            except Exception as e:
                st.error(f"âŒ Lá»—i: {e}")
                st.stop()
    else:
        rag_engine = load_rag_engine()
        llm = load_llm()
        st.success("âœ… Há»‡ thá»‘ng sáºµn sÃ ng!")
    
    # Stats
    stats = rag_engine.get_stats()
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ğŸ“ Files", stats['files'])
    with col2:
        st.metric("ğŸ“¦ Chunks", stats['chunks'])
    
    st.markdown("---")
    
    # File Upload
    st.markdown("### ğŸ“¤ Upload tÃ i liá»‡u")
    uploaded_files = st.file_uploader(
        "KÃ©o tháº£ hoáº·c chá»n files",
        type=["pdf", "txt", "md", "csv"],
        accept_multiple_files=True,
        key="file_uploader"
    )
    
    # Process uploaded files
    if uploaded_files:
        new_files = []
        for file in uploaded_files:
            if file.name not in st.session_state.file_status:
                new_files.append(file)
        
        if new_files:
            st.markdown("#### ğŸ”„ Äang xá»­ lÃ½...")
            progress_bar = st.progress(0)
            
            for i, file in enumerate(new_files):
                # Update status
                st.session_state.file_status[file.name] = {
                    "status": "processing",
                    "chunks": 0,
                    "time": None
                }
                
                # Save file
                filepath = os.path.join(UPLOADS_DIR, file.name)
                with open(filepath, "wb") as f:
                    f.write(file.getbuffer())
                
                # Process
                try:
                    start_time = time.time()
                    file_info = rag_engine.add_file(filepath, file.name)
                    rag_engine.process_queue()
                    elapsed = time.time() - start_time
                    
                    # Update status
                    st.session_state.file_status[file.name] = {
                        "status": "completed",
                        "chunks": rag_engine.vector_store.files.get(file.name, {}).get('chunks', 0),
                        "time": f"{elapsed:.1f}s"
                    }
                except Exception as e:
                    st.session_state.file_status[file.name] = {
                        "status": "error",
                        "error": str(e)
                    }
                
                progress_bar.progress((i + 1) / len(new_files))
            
            progress_bar.empty()
            st.rerun()
    
    # File Status List
    if st.session_state.file_status:
        st.markdown("#### ğŸ“‹ Danh sÃ¡ch tÃ i liá»‡u")
        
        for filename, info in st.session_state.file_status.items():
            status = info.get("status", "pending")
            
            if status == "completed":
                chunks = info.get('chunks', 0)
                time_taken = info.get('time', '')
                st.markdown(f"""
                <div class="file-card">
                    âœ… <b>{filename}</b><br/>
                    <small>ğŸ“¦ {chunks} chunks | â±ï¸ {time_taken}</small>
                </div>
                """, unsafe_allow_html=True)
            elif status == "processing":
                st.markdown(f"""
                <div class="file-card">
                    ğŸ”„ <b>{filename}</b><br/>
                    <small class="status-processing">Äang xá»­ lÃ½...</small>
                </div>
                """, unsafe_allow_html=True)
            elif status == "error":
                error = info.get('error', 'Unknown error')
                st.markdown(f"""
                <div class="file-card">
                    âŒ <b>{filename}</b><br/>
                    <small class="status-error">{error[:50]}</small>
                </div>
                """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Clear buttons
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ XÃ³a chat"):
            st.session_state.messages = []
            st.rerun()
    with col2:
        if st.button("ğŸ”„ Refresh"):
            st.rerun()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN CHAT INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.title("ğŸ’¬ Há»i Ä‘Ã¡p vá»›i TÃ i liá»‡u")

# Instructions
if not st.session_state.file_status:
    st.info("""
    ğŸ“ **Education AI v2 - Trá»£ lÃ½ Há»c táº­p ThÃ´ng minh**
    
    **Báº¡n cÃ³ thá»ƒ há»i ngay!** KhÃ´ng cáº§n upload tÃ i liá»‡u.
    
    **TÃ­nh nÄƒng:**
    - ğŸ’¬ **Chat Mode**: Há»i Ä‘Ã¡p trá»±c tiáº¿p vá»›i AI
    - ğŸ“š **RAG Mode**: Upload tÃ i liá»‡u â†’ AI tráº£ lá»i dá»±a trÃªn ná»™i dung
    - ğŸ”¢ ToÃ¡n há»c | âš›ï¸ Váº­t lÃ½ | ğŸ§ª HÃ³a há»c | ğŸ”¤ Tiáº¿ng Anh
    - ğŸ¤– Tá»± Ä‘á»™ng nháº­n diá»‡n mÃ´n há»c vÃ  ngÃ´n ngá»¯
    """)

# Display chat history
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input
if user_input := st.chat_input("Nháº­p cÃ¢u há»i..."):
    # Display user message
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # Analyze question (subject & language detection)
    analysis = llm.analyze_question(user_input)
    
    # Determine mode: RAG or Normal Chat
    has_documents = bool(st.session_state.file_status)
    use_rag = False
    context_chunks = []
    
    if has_documents:
        with st.spinner("ğŸ” Äang tÃ¬m kiáº¿m trong tÃ i liá»‡u..."):
            result = rag_engine.query(user_input)
            if isinstance(result, list) and len(result) > 0:
                context_chunks = result
                use_rag = True
    
    # Show detection info with mode indicator
    mode_text = "ğŸ“š RAG Mode" if use_rag else "ğŸ’¬ Chat Mode"
    mode_color = "#00d4ff" if use_rag else "#FFD700"
    
    st.markdown(f"""
    <div style="background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); 
                padding: 10px 15px; border-radius: 8px; margin: 10px 0;
                border-left: 4px solid {mode_color};">
        {analysis['subject_emoji']} <b>MÃ´n há»c:</b> {analysis['subject_name']} | 
        ğŸŒ <b>NgÃ´n ngá»¯:</b> {analysis['language_name']} |
        <span style="color: {mode_color};">{mode_text}</span>
    </div>
    """, unsafe_allow_html=True)
    
    # Generate response
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        
        try:
            if use_rag:
                # RAG Mode: Answer with context
                for token in llm.answer_with_context(user_input, context_chunks, stream=True):
                    full_response += token
                    response_placeholder.markdown(full_response + "â–Œ")
            else:
                # Normal Chat Mode: Use LLM directly (like education_ai)
                history = [
                    {"role": m["role"], "content": m["content"]} 
                    for m in st.session_state.messages[:-1]  # Exclude current message
                ][-10:]  # Last 10 messages for context
                
                for token in llm.chat_without_rag(user_input, history, stream=True):
                    full_response += token
                    response_placeholder.markdown(full_response + "â–Œ")
            
            response_placeholder.markdown(full_response)
        except Exception as e:
            full_response = f"âŒ Lá»—i khi xá»­ lÃ½: {e}"
            response_placeholder.markdown(full_response)
        
        # Save assistant message
        st.session_state.messages.append({"role": "assistant", "content": full_response})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FOOTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.markdown("---")
st.caption("""
ğŸ“ **Education AI v2** | Powered by Llama 3.1 8B + BGE-M3 + Reranker  
ğŸ“š Upload tÃ i liá»‡u â†’ Tá»± Ä‘á»™ng xá»­ lÃ½ â†’ Há»i Ä‘Ã¡p thÃ´ng minh
""")
