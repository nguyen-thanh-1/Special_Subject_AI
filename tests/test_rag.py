"""
Test RAG m·∫´u s·ª≠ d·ª•ng RAGAnything v·ªõi Local LLM (Llama 3.1)
Th∆∞ vi·ªán: https://github.com/HKUDS/RAG-Anything
C√†i ƒë·∫∑t: pip install raganything sentence-transformers
"""

import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc
import torch

# Import model local
try:
    from Llama_3_1_8B_Instruct_v2 import generate_response
except ImportError:
    print("L·ªói: Kh√¥ng t√¨m th·∫•y file Llama_3_1_8B_Instruct_v2.py ho·∫∑c kh√¥ng th·ªÉ import model.")
    exit(1)

# Import sentence_transformers cho embedding
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("L·ªói: Ch∆∞a c√†i ƒë·∫∑t sentence-transformers. Vui l√≤ng ch·∫°y: pip install sentence-transformers")
    exit(1)


# ======================== C·∫§U H√åNH LOCAL EMBEDDING ========================
# Load model embedding (nh·∫π, ch·∫°y CPU/GPU ƒë·ªÅu ·ªïn)
embedding_model_name = "all-MiniLM-L6-v2"
print(f"Loading embedding model: {embedding_model_name}...")
embedder = SentenceTransformer(embedding_model_name)
print("Embedding model loaded!")

async def local_embedding_func(texts):
    """H√†m t·∫°o embedding s·ª≠ d·ª•ng sentence-transformers (async wrapper)"""
    # LightRAG expects numpy array with .size attribute, NOT list
    return embedder.encode(texts)

embedding_func = EmbeddingFunc(
    embedding_dim=384, # all-MiniLM-L6-v2 c√≥ dim l√† 384
    max_token_size=256, # 512, nh∆∞ng ƒë·ªÉ an to√†n 256
    func=local_embedding_func
)


# ======================== C·∫§U H√åNH LOCAL LLM ========================

async def local_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
    """Bridge function g·ªçi t·ªõi Llama 3.1 local (async wrapper)"""
    print(f"ü§ñ Calling Local LLM with prompt len: {len(prompt)}")
    
    # RAGAnything/LightRAG c√≥ th·ªÉ truy·ªÅn history_messages ph·ª©c t·∫°p,
    # nh∆∞ng ·ªü ƒë√¢y ta ƒë∆°n gi·∫£n h√≥a truy·ªÅn v√†o h√†m generate_response
    
    # Chu·∫©n b·ªã history format cho h√†m generate_response
    # H√†m generate_response mong ƒë·ª£i history l√† list dict [{"role":..., "content":...}]
    
    # N·∫øu c√≥ history_messages t·ª´ RAG, ta d√πng n√≥
    chat_history = history_messages if history_messages else []
    
    # G·ªçi h√†m generate t·ª´ file script c·ªßa user
    response = generate_response(
        user_input=prompt,
        history=chat_history,
        system_prompt=system_prompt,
        max_new_tokens=1024, # TƒÉng token cho c√¢u tr·∫£ l·ªùi d√†i h∆°n
        temperature=0.1 # Gi·∫£m temperature ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c h∆°n cho RAG
    )
    
    return response


async def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y RAG"""
    
    # ======================== C·∫§U H√åNH RAG ========================
    config = RAGAnythingConfig(
        working_dir="./rag_storage_local", # Thay ƒë·ªïi th∆∞ m·ª•c ƒë·ªÉ kh√¥ng ƒë√® l√™n c√°i c≈©
        parser="mineru", 
        parse_method="txt", # D√πng txt cho nhanh v√† ƒë∆°n gi·∫£n v·ªõi demo
        enable_image_processing=False, # T·∫Øt image processing v√¨ model vision ch∆∞a setup local
        enable_table_processing=False,
        enable_equation_processing=False,
    )
    
    # ======================== KH·ªûI T·∫†O RAG ========================
    rag = RAGAnything(
        config=config,
        llm_model_func=local_llm_func,
        # vision_model_func=vision_model_func, # B·ªè qua vision model cho demo text thu·∫ßn
        embedding_func=embedding_func,
    )
    
    print("‚úÖ RAG Initialized with Local LLM & Embeddings")
    
    # ======================== T·∫†O D·ªÆ LI·ªÜU M·∫™U ========================
    # T·∫°o m·ªôt file txt m·∫´u ƒë·ªÉ test
    sample_file = "sample_knowledge.txt"
    with open(sample_file, "w", encoding="utf-8") as f:
        f.write("""
        RAGAnything l√† m·ªôt framework RAG t·∫•t c·∫£ trong m·ªôt.
        N√≥ h·ªó tr·ª£ x·ª≠ l√Ω ƒëa ph∆∞∆°ng th·ª©c (h√¨nh ·∫£nh, b·∫£ng bi·ªÉu, c√¥ng th·ª©c).
        Vi·ªác s·ª≠ d·ª•ng Local LLM gi√∫p b·∫£o m·∫≠t d·ªØ li·ªáu v√† ti·∫øt ki·ªám chi ph√≠ API.
        Llama 3.1 8B l√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ m·∫°nh m·∫Ω c·ªßa Meta.
        """)
    
    # ======================== X·ª¨ L√ù T√ÄI LI·ªÜU ========================
    print(f"Processing {sample_file}...")
    await rag.process_document_complete(
        file_path=sample_file,
        output_dir="./output_local",
        parse_method="txt"
    )
    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω t√†i li·ªáu")
    
    # ======================== TRUY V·∫§N ========================
    query = "L·ª£i √≠ch c·ªßa vi·ªác s·ª≠ d·ª•ng Local LLM l√† g√¨?"
    print(f"\n‚ùì Query: {query}")
    
    result = await rag.aquery(
        query,
        mode="hybrid"
    )
    print(f"\nüìù K·∫øt qu·∫£:\n{result}")

if __name__ == "__main__":
    asyncio.run(main())
