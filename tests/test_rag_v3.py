"""
RAG Demo - Script demo ƒë·∫ßy ƒë·ªß workflow Index + Query
ƒê√¢y l√† b·∫£n all-in-one ƒë·ªÉ test, trong production n√™n t√°ch th√†nh:
  - index_docs.py: Ch·∫°y offline ƒë·ªÉ index
  - query_rag.py: Ch·∫°y online ƒë·ªÉ query nhanh

Ch·∫°y: uv run test_rag_v3.py
"""

import asyncio
import os
from datetime import datetime

# Import RAGAnything
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc

# Import shared config
from rag_config import (
    COURSES_FOLDER, OUTPUT_DIR, RAG_STORAGE,
    SUPPORTED_EXTENSIONS, EMBEDDING_MODEL_NAME, EMBEDDING_DIM,
    EMBEDDING_MAX_TOKENS, LLM_MAX_NEW_TOKENS, LLM_TEMPERATURE,
    RAG_CONFIG, ensure_directories, get_supported_files, get_file_info
)


# ======================== MODELS SETUP ========================
print("=" * 60)
print("üöÄ RAG DEMO - Local LLM + RAGAnything")
print("=" * 60)

# Import Llama model
try:
    from Llama_3_1_8B_Instruct_v2 import generate_response
    print("‚úÖ Llama 3.1 8B loaded")
except ImportError:
    print("‚ùå Kh√¥ng t√¨m th·∫•y file Llama_3_1_8B_Instruct_v2.py")
    exit(1)

# Import sentence_transformers
try:
    from sentence_transformers import SentenceTransformer
except ImportError:
    print("‚ùå Ch∆∞a c√†i ƒë·∫∑t sentence-transformers")
    exit(1)

# Load embedding model
print(f"Loading embedding: {EMBEDDING_MODEL_NAME}...")
embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
print("‚úÖ Embedding model loaded")


# ======================== ASYNC FUNCTIONS ========================
async def local_embedding_func(texts):
    """Async embedding function"""
    return embedder.encode(texts)

embedding_func = EmbeddingFunc(
    embedding_dim=EMBEDDING_DIM,
    max_token_size=EMBEDDING_MAX_TOKENS,
    func=local_embedding_func
)

async def local_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
    """Async LLM function"""
    chat_history = history_messages if history_messages else []
    response = generate_response(
        user_input=prompt,
        history=chat_history,
        system_prompt=system_prompt,
        max_new_tokens=LLM_MAX_NEW_TOKENS,
        temperature=LLM_TEMPERATURE
    )
    return response


# ======================== MAIN ========================
async def main():
    """Main workflow: Index + Query"""
    
    # Ensure directories
    ensure_directories()
    
    print(f"\nüìÅ C·∫•u h√¨nh:")
    print(f"   - T√†i li·ªáu: {COURSES_FOLDER}")
    print(f"   - Output: {OUTPUT_DIR}")
    print(f"   - Database: {RAG_STORAGE}")
    
    # Initialize RAG
    print("\nüîß Initializing RAGAnything...")
    config = RAGAnythingConfig(**RAG_CONFIG)
    rag = RAGAnything(
        config=config,
        llm_model_func=local_llm_func,
        embedding_func=embedding_func,
    )
    print("‚úÖ RAGAnything initialized")
    
    # ======================== PHASE 1: INDEXING ========================
    print("\n" + "=" * 60)
    print("üìö PHASE 1: INDEXING DOCUMENTS")
    print("=" * 60)
    
    # Get files
    files = get_supported_files(COURSES_FOLDER)
    if not files:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file trong {COURSES_FOLDER}")
        print(f"   H·ªó tr·ª£: {SUPPORTED_EXTENSIONS}")
        return
    
    print(f"üìã T√¨m th·∫•y {len(files)} file(s):")
    for i, f in enumerate(files, 1):
        info = get_file_info(COURSES_FOLDER, f)
        print(f"   {i}. {f} ({info['size_mb']:.2f} MB)")
    
    # Ask user confirmation
    print(f"\n‚ö†Ô∏è  B·∫°n c√≥ mu·ªën index {len(files)} file(s) kh√¥ng?")
    print("   Nh·∫•n Enter ƒë·ªÉ ti·∫øp t·ª•c, ho·∫∑c 'skip' ƒë·ªÉ b·ªè qua indexing...")
    
    user_input = input("   > ").strip().lower()
    
    if user_input != "skip":
        # Index files
        for i, filename in enumerate(files, 1):
            file_path = os.path.join(COURSES_FOLDER, filename)
            print(f"\n[{i}/{len(files)}] Indexing: {filename}")
            
            try:
                start = datetime.now()
                await rag.process_document_complete(
                    file_path=file_path,
                    output_dir=OUTPUT_DIR,
                    parse_method="auto"
                )
                elapsed = (datetime.now() - start).total_seconds()
                print(f"   ‚úÖ Done in {elapsed:.1f}s")
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)}")
    else:
        print("‚è≠Ô∏è  Skipped indexing")
    
    # ======================== PHASE 2: QUERYING ========================
    print("\n" + "=" * 60)
    print("üîç PHASE 2: QUERYING")
    print("=" * 60)
    
    # Demo queries
    demo_queries = [
        "T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa c√°c t√†i li·ªáu",
        "Event-Driven Design l√† g√¨ v√† l·ª£i √≠ch c·ªßa n√≥?",
    ]
    
    print("üìù Demo queries:")
    for query in demo_queries:
        print(f"\n‚ùì {query}")
        try:
            result = await rag.aquery(query, mode="hybrid")
            print(f"ÔøΩ {result}")
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
    
    # ======================== PHASE 3: INTERACTIVE ========================
    print("\n" + "=" * 60)
    print("üí¨ PHASE 3: INTERACTIVE Q&A")
    print("=" * 60)
    print("G√µ c√¢u h·ªèi v√† nh·∫•n Enter. G√µ 'exit' ƒë·ªÉ tho√°t.")
    
    while True:
        try:
            user_query = input("\nüßë B·∫°n: ").strip()
            
            if user_query.lower() in ["exit", "quit", "q"]:
                print("üëã T·∫°m bi·ªát!")
                break
            
            if not user_query:
                continue
            
            print("ü§ñ ƒêang x·ª≠ l√Ω...")
            result = await rag.aquery(user_query, mode="hybrid")
            print(f"ü§ñ AI: {result}")
            
        except KeyboardInterrupt:
            print("\nüëã T·∫°m bi·ªát!")
            break
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")


if __name__ == "__main__":
    asyncio.run(main())

