"""
Test RAG v5 - LightRAG v·ªõi Gemini Query
- Index: D√πng database ƒë√£ c√≥ (rag_storage_v4)
- Query: D√πng Gemini API (ch√≠nh x√°c h∆°n local Llama)

Ch·∫°y: uv run test_rag_v5_gemini_query.py

Y√™u c·∫ßu: GEMINI_API_KEY trong file .env
"""

import asyncio
import os
import time
from dotenv import load_dotenv

load_dotenv()

# Import LightRAG
from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc
import numpy as np

# ======================== CONFIG ========================
RAG_STORAGE = "./rag_storage_v4"  # D√πng database ƒë√£ index

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = "gemini-2.5-flash"

# Embedding
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_DIM = 384
EMBEDDING_MAX_TOKENS = 512


# ======================== GEMINI LLM ========================
def create_gemini_llm():
    """T·∫°o Gemini LLM function cho query"""
    import google.generativeai as genai
    
    if not GEMINI_API_KEY:
        raise ValueError("‚ùå Thi·∫øu GEMINI_API_KEY trong .env")
    
    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL)
    
    async def gemini_llm(prompt, system_prompt=None, history_messages=[], **kwargs):
        full_prompt = ""
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n"
        
        for msg in history_messages:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            full_prompt += f"{role}: {content}\n"
        
        full_prompt += prompt
        
        try:
            response = model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.1,
                    max_output_tokens=2048,
                )
            )
            return response.text
        except Exception as e:
            return f"‚ùå Gemini error: {e}"
    
    return gemini_llm


# ======================== EMBEDDING ========================
def create_embedding():
    from sentence_transformers import SentenceTransformer
    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
    
    async def embedding_func(texts):
        return embedder.encode(texts)
    
    return EmbeddingFunc(
        embedding_dim=EMBEDDING_DIM,
        max_token_size=EMBEDDING_MAX_TOKENS,
        func=embedding_func
    )


# ======================== MAIN ========================
async def main():
    print("=" * 60)
    print("üöÄ TEST RAG v5 - Gemini Query")
    print("   Database: rag_storage_v4 (ƒë√£ index)")
    print("   Query LLM: Gemini API (ch√≠nh x√°c h∆°n)")
    print("=" * 60)
    
    if not GEMINI_API_KEY:
        print("\n‚ùå Thi·∫øu GEMINI_API_KEY!")
        print("   Th√™m v√†o file .env: GEMINI_API_KEY=your_key")
        return
    
    print(f"‚úÖ Gemini API Key: {GEMINI_API_KEY[:10]}...")
    
    # Check database
    if not os.path.exists(RAG_STORAGE):
        print(f"\n‚ùå Database kh√¥ng t·ªìn t·∫°i: {RAG_STORAGE}")
        print("   Ch·∫°y test_rag_v4_lightrag.py tr∆∞·ªõc ƒë·ªÉ index!")
        return
    
    # Load models
    print("\nüîÑ Loading models...")
    gemini_llm = create_gemini_llm()
    print("   ‚úÖ Gemini LLM ready")
    embedding = create_embedding()
    print("   ‚úÖ Embedding loaded")
    
    # Initialize LightRAG
    print("\nüîß Loading LightRAG...")
    rag = LightRAG(
        working_dir=RAG_STORAGE,
        llm_model_func=gemini_llm,
        embedding_func=embedding,
    )
    await rag.initialize_storages()
    print("‚úÖ LightRAG ready!")
    
    # Interactive query
    print("\n" + "=" * 60)
    print("üí¨ CH·∫æ ƒê·ªò H·ªéI ƒê√ÅP (Gemini Query)")
    print("=" * 60)
    print("G√µ c√¢u h·ªèi. 'exit' ƒë·ªÉ tho√°t.")
    print("'mode:hybrid/local/global/naive' ƒë·ªÉ ƒë·ªïi mode")
    print("-" * 60)
    
    current_mode = "hybrid"
    
    while True:
        try:
            user_input = input(f"\nüßë [{current_mode}] B·∫°n: ").strip()
            
            if user_input.lower() in ["exit", "quit", "q"]:
                print("üëã T·∫°m bi·ªát!")
                break
            
            if not user_input:
                continue
            
            if user_input.startswith("mode:"):
                new_mode = user_input.split(":")[1].strip()
                if new_mode in ["hybrid", "local", "global", "naive"]:
                    current_mode = new_mode
                    print(f"‚úÖ Mode: {current_mode}")
                continue
            
            print("ü§ñ ƒêang x·ª≠ l√Ω (Gemini)...")
            start = time.time()
            
            result = await rag.aquery(user_input, param=QueryParam(mode=current_mode))
            
            elapsed = time.time() - start
            print(f"\nü§ñ AI ({elapsed:.1f}s):\n{result}")
            
        except KeyboardInterrupt:
            print("\nüëã T·∫°m bi·ªát!")
            break
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")


if __name__ == "__main__":
    asyncio.run(main())
