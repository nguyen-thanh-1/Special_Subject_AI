"""
RAG Hybrid - 2-Stage RAG with Question Routing
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ARCHITECTURE:
    User Question
          â”‚
          â–¼
    [Question Router]
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                   â”‚
    â–¼                   â–¼
  rag_lite           rag_pro
  (fast)             (deep)
    â”‚                   â”‚
    â–¼                   â–¼
  LLM + Prior      Strict RAG
  Knowledge        (No hallucination)

ROUTING RULES:
- rag_pro: "theo tÃ i liá»‡u", "trong sÃ¡ch", "chÆ°Æ¡ng X", specific citations
- rag_lite: General knowledge, definitions, common concepts

PROMPTS:
- rag_lite: HYBRID (context + LLM general knowledge)
- rag_pro: STRICT (only document context)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import time
import re
from typing import Tuple, Optional

# Add project root to path
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SIMILARITY_THRESHOLD = 0.5  # Below this, use LLM general knowledge

# Keywords that trigger rag_pro (strict mode)
RAG_PRO_KEYWORDS = [
    "theo tÃ i liá»‡u", "trong sÃ¡ch", "trong tÃ i liá»‡u", "theo sÃ¡ch",
    "chÆ°Æ¡ng", "trang", "section", "chapter", "page",
    "Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a", "Ä‘Æ°á»£c mÃ´ táº£", "Ä‘Æ°á»£c giáº£i thÃ­ch",
    "so sÃ¡nh trong tÃ i liá»‡u", "trÃ­ch dáº«n", "quote",
    "theo nhÆ°", "dá»±a theo", "nhÆ° Ä‘Ã£ nÃ³i"
]

# Keywords that trigger rag_lite (fast + hybrid)
RAG_LITE_KEYWORDS = [
    "lÃ  gÃ¬", "what is", "Ä‘á»‹nh nghÄ©a", "definition",
    "giáº£i thÃ­ch", "explain", "cÃ³ nghÄ©a lÃ  gÃ¬",
    "táº¡i sao", "why", "nhÆ° tháº¿ nÃ o", "how",
    "vÃ­ dá»¥", "example", "á»©ng dá»¥ng", "application"
]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMPT TEMPLATES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
HYBRID_PROMPT = """Based on the following context, answer the question.

RULES:
1. Prefer using the provided context if relevant
2. If context is insufficient, you may use general AI knowledge
3. Clearly indicate when the answer is based on general knowledge
4. Answer in the same language as the question

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

STRICT_PROMPT = """Based on the following context, answer the question accurately.

IMPORTANT RULES:
1. ONLY use information from the context below
2. If the answer is NOT in the context, say "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u."
3. Be specific and cite which part of the context you're using
4. Answer in the same language as the question

CONTEXT:
{context}

QUESTION: {question}

ANSWER:"""

NO_CONTEXT_PROMPT = """Answer the following question using your general AI knowledge.

RULES:
1. Be accurate and educational
2. Answer in the same language as the question
3. If you're unsure, indicate your uncertainty

QUESTION: {question}

ANSWER:"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUESTION ROUTER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class QuestionRouter:
    """
    Route questions to appropriate RAG pipeline:
    - rag_pro: Document-specific questions (strict mode)
    - rag_lite: General knowledge questions (hybrid mode)
    - llm_only: When no relevant context found
    """
    
    def __init__(self):
        pass
    
    def classify(self, question: str, context_score: float = 0.0) -> str:
        """
        Classify question into routing mode
        
        Returns:
            "rag_pro" - Use strict RAG with only document context
            "rag_lite" - Use hybrid RAG with LLM general knowledge
            "llm_only" - Use LLM without RAG context
        """
        question_lower = question.lower()
        
        # Rule 1: Check for rag_pro keywords (document-specific)
        for keyword in RAG_PRO_KEYWORDS:
            if keyword in question_lower:
                return "rag_pro"
        
        # Rule 2: Check similarity score
        if context_score < SIMILARITY_THRESHOLD:
            return "llm_only"
        
        # Rule 3: Default to rag_lite (hybrid)
        return "rag_lite"
    
    def get_prompt(self, mode: str, question: str, context: str) -> str:
        """Get appropriate prompt based on mode"""
        if mode == "rag_pro":
            return STRICT_PROMPT.format(context=context, question=question)
        elif mode == "rag_lite":
            return HYBRID_PROMPT.format(context=context, question=question)
        else:  # llm_only
            return NO_CONTEXT_PROMPT.format(question=question)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAG HYBRID PIPELINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class RAGHybrid:
    """
    Hybrid RAG system that intelligently routes between:
    - rag_lite (fast, hybrid prompt)
    - rag_pro (deep, strict prompt)
    """
    
    def __init__(self):
        self.router = QuestionRouter()
        self.rag_lite = None
        self.rag_pro = None
        self.llm = None
    
    def _load_rag_lite(self):
        """Lazy load rag_lite"""
        if self.rag_lite is None:
            print("   ğŸ“¦ Loading RAG Lite...")
            from rag_systems.rag_lite.rag_lite import RAGLite, get_embedder, get_reranker
            self.rag_lite = RAGLite()
            self.rag_lite.load()
            get_embedder()
            get_reranker()
            print("   âœ… RAG Lite ready")
    
    def _load_rag_pro(self):
        """Lazy load rag_pro"""
        if self.rag_pro is None:
            print("   ğŸ“¦ Loading RAG Pro...")
            from rag_systems.rag_pro.rag_pro_v2 import RAGProV2, get_embedding_model, get_reranker
            self.rag_pro = RAGProV2()
            self.rag_pro.load()
            print("   âœ… RAG Pro ready")
    
    def _load_llm(self):
        """Load LLM"""
        if self.llm is None:
            print("   ğŸ“¥ Loading Llama 3.1 8B...")
            from llm_models.Llama_3_1_8B_Instruct_v2 import generate_response, _load_model
            _load_model()
            self.llm = generate_response
            print("   âœ… Llama 3.1 8B loaded (GPU)")
    
    def query_lite(self, question: str) -> Tuple[str, float]:
        """Query using rag_lite (fast)"""
        self._load_rag_lite()
        
        # Get context
        from rag_systems.rag_lite.rag_lite import TOP_K_RETRIEVE, TOP_K_RERANK, rerank
        
        retrieved = self.rag_lite.vector_store.search(question, TOP_K_RETRIEVE)
        if not retrieved:
            return "", 0.0
        
        reranked = rerank(question, retrieved, TOP_K_RERANK)
        if not reranked:
            return "", 0.0
        
        # Get top score
        top_score = reranked[0][1] if reranked else 0.0
        
        # Build context
        context_parts = []
        for i, (chunk, score) in enumerate(reranked, 1):
            context_parts.append(f"[Äoáº¡n {i}]\n{chunk}")
        context = "\n\n---\n\n".join(context_parts)
        
        return context, top_score
    
    def query_pro(self, question: str) -> Tuple[str, float]:
        """Query using rag_pro (deep)"""
        self._load_rag_pro()
        
        from rag_systems.rag_pro.rag_pro_v2 import TOP_K_RETRIEVE, TOP_K_RERANK, rerank
        
        retrieved = self.rag_pro.vector_store.search(question, TOP_K_RETRIEVE)
        if not retrieved:
            return "", 0.0
        
        reranked = rerank(question, retrieved, TOP_K_RERANK)
        if not reranked:
            return "", 0.0
        
        top_score = reranked[0][1] if reranked else 0.0
        
        context_parts = []
        for i, (chunk, score) in enumerate(reranked, 1):
            context_parts.append(f"[Äoáº¡n {i}]\n{chunk}")
        context = "\n\n---\n\n".join(context_parts)
        
        return context, top_score
    
    def generate(self, prompt: str) -> str:
        """Generate answer using LLM"""
        self._load_llm()
        
        system_prompt = "You are a helpful educational AI assistant. Be accurate, clear, and helpful."
        
        response = self.llm(
            user_input=prompt,
            history=[],
            system_prompt=system_prompt,
            max_new_tokens=700,
            temperature=0.21,
        )
        
        return response
    
    def query(self, question: str, verbose: bool = True) -> str:
        """
        Main query method with intelligent routing
        """
        import torch
        start = time.time()
        
        # Step 1: Quick classification based on keywords
        initial_mode = self.router.classify(question)
        
        if verbose:
            print(f"   ğŸ” Initial classification: {initial_mode}")
        
        # Step 2: Get context based on initial classification
        if initial_mode == "rag_pro":
            if verbose:
                print(f"   ğŸ“š Using RAG Pro (strict mode)...")
            context, score = self.query_pro(question)
            mode = "rag_pro"
        else:
            if verbose:
                print(f"   âš¡ Using RAG Lite (fast mode)...")
            context, score = self.query_lite(question)
            
            # Re-evaluate based on score
            mode = self.router.classify(question, score)
            if mode == "llm_only":
                if verbose:
                    print(f"   â„¹ï¸  Low relevance score ({score:.2f}) â†’ Using LLM general knowledge")
        
        if verbose:
            print(f"   ğŸ“Š Context score: {score:.2f}")
            print(f"   ğŸ¯ Final mode: {mode}")
        
        # Step 3: Generate answer
        if verbose:
            print(f"   ğŸ¤– Generating answer...")
        
        prompt = self.router.get_prompt(mode, question, context)
        answer = self.generate(prompt)
        
        # Clear CUDA cache
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            torch.cuda.empty_cache()
        
        elapsed = time.time() - start
        if verbose:
            print(f"   â±ï¸ Total: {elapsed:.1f}s")
        
        return answer
    
    def preload_lite(self):
        """Preload rag_lite for fast queries"""
        self._load_rag_lite()
        self._load_llm()
    
    def preload_all(self):
        """Preload all models (uses more memory)"""
        self._load_rag_lite()
        self._load_rag_pro()
        self._load_llm()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    import argparse
    parser = argparse.ArgumentParser(description="RAG Hybrid - 2-Stage RAG System")
    parser.add_argument('--query', '-q', type=str, help='Single query mode')
    parser.add_argument('--preload', choices=['lite', 'all'], default='lite', 
                        help='Preload mode: lite (default) or all')
    args = parser.parse_args()
    
    print("â•" * 60)
    print("ğŸš€ RAG HYBRID - 2-Stage RAG System")
    print("â•" * 60)
    print("   ğŸ“Š Strategy: Question Router â†’ rag_lite / rag_pro")
    print("   âš¡ Fast mode: RAG Lite + LLM General Knowledge")
    print("   ğŸ“š Deep mode: RAG Pro (Strict Document Only)")
    print("â•" * 60)
    
    rag = RAGHybrid()
    
    print(f"\nğŸ”„ Preloading ({args.preload} mode)...")
    if args.preload == 'all':
        rag.preload_all()
    else:
        rag.preload_lite()
    
    # Single query mode
    if args.query:
        print("\n" + "â•" * 60)
        print(f"\nâ“ {args.query}")
        print("\nğŸ¤– Äang xá»­ lÃ½...")
        answer = rag.query(args.query)
        print(f"\nğŸ“ Tráº£ lá»i:\n{answer}")
        return
    
    # Interactive mode
    print("\n" + "â•" * 60)
    print("ğŸ’¬ INTERACTIVE MODE")
    print("â•" * 60)
    print("GÃµ cÃ¢u há»i. 'exit' Ä‘á»ƒ thoÃ¡t.")
    print("")
    print("ğŸ’¡ Tips:")
    print("   - 'NLP lÃ  gÃ¬?' â†’ Fast mode (hybrid)")
    print("   - 'Theo tÃ i liá»‡u, NLP lÃ  gÃ¬?' â†’ Deep mode (strict)")
    print("-" * 60)
    
    while True:
        try:
            question = input("\nğŸ§‘ Báº¡n: ").strip()
            
            if question.lower() in ["exit", "quit", "q"]:
                print("ğŸ‘‹ Táº¡m biá»‡t!")
                break
            
            if not question:
                continue
            
            print("\nğŸ¤– Äang xá»­ lÃ½...")
            answer = rag.query(question)
            print(f"\nğŸ“ Tráº£ lá»i:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break


if __name__ == "__main__":
    main()
