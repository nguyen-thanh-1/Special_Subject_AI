"""
Qwen2.5-14B RAG System
K·∫øt h·ª£p LightRAG v·ªõi Qwen2.5-14B-Instruct
- Ch·∫∑n token ngo·∫°i ng·ªØ (Trung, Nga, Nh·∫≠t, H√†n...)
- Auto-index files t·ª´ ./courses
- Query v·ªõi nhi·ªÅu mode: hybrid, local, global, naive

Ch·∫°y: uv run Qwen2.5_14B_RAG.py
"""

import sys
import codecs
sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())

import asyncio
import json
import hashlib
import os
import time
from datetime import datetime
from threading import Thread

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from lightrag import LightRAG, QueryParam
from lightrag.utils import EmbeddingFunc

# ======================== CONFIG ========================
COURSES_FOLDER = "./courses"
RAG_STORAGE = "./rag_storage_qwen14b"
INDEX_TRACKER_FILE = os.path.join(RAG_STORAGE, "indexed_files.json")

SUPPORTED_EXTENSIONS = [".txt", ".md", ".csv", ".pdf"]

# Embedding
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_DIM = 384
EMBEDDING_MAX_TOKENS = 512

# LLM
LLM_MAX_NEW_TOKENS = 512
MODEL_NAME = "Qwen/Qwen2.5-14B-Instruct"


# ======================== INDEX TRACKER ========================
class IndexTracker:
    def __init__(self, tracker_file: str):
        self.tracker_file = tracker_file
        self.indexed_files = self._load()
    
    def _load(self) -> dict:
        if os.path.exists(self.tracker_file):
            try:
                with open(self.tracker_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save(self):
        os.makedirs(os.path.dirname(self.tracker_file), exist_ok=True)
        with open(self.tracker_file, 'w', encoding='utf-8') as f:
            json.dump(self.indexed_files, f, indent=2, ensure_ascii=False)
    
    def _get_file_hash(self, file_path: str) -> str:
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(65536), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def needs_indexing(self, file_path: str) -> bool:
        if not os.path.exists(file_path):
            return False
        filename = os.path.basename(file_path)
        current_hash = self._get_file_hash(file_path)
        if filename not in self.indexed_files:
            return True
        if self.indexed_files[filename].get('hash') != current_hash:
            return True
        return False
    
    def mark_indexed(self, file_path: str):
        filename = os.path.basename(file_path)
        self.indexed_files[filename] = {
            'hash': self._get_file_hash(file_path),
            'indexed_at': datetime.now().isoformat(),
            'size_bytes': os.path.getsize(file_path),
        }
        self._save()
    
    def get_indexed_count(self) -> int:
        return len(self.indexed_files)


# ======================== FILE READERS ========================
def read_pdf_file(file_path: str) -> str:
    try:
        import pdfplumber
    except ImportError:
        import subprocess
        subprocess.run(["uv", "pip", "install", "pdfplumber"], check=True)
        import pdfplumber
    
    text_content = []
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                text_content.append(text)
    return "\n\n".join(text_content)


def read_text_file(file_path: str) -> str:
    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
    for encoding in encodings:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                return f.read()
        except UnicodeDecodeError:
            continue
    with open(file_path, 'rb') as f:
        return f.read().decode('utf-8', errors='ignore')


def read_file(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        return read_pdf_file(file_path)
    return read_text_file(file_path)


# ======================== QWEN RAG CLASS ========================
class QwenRAG:
    def __init__(self):
        print("=" * 60)
        print("üöÄ QWEN 2.5-14B RAG SYSTEM")
        print("=" * 60)
        
        # 1. Load Qwen model
        self._load_qwen_model()
        
        # 2. Load Embedding model
        self._load_embedding_model()
        
        # 3. Initialize LightRAG
        self._init_lightrag()
        
        # System prompt cho RAG
        self.system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n gi√°o d·ª•c.
S·ª≠ d·ª•ng th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
CH·ªà tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
N·∫øu kh√¥ng c√≥ th√¥ng tin trong t√†i li·ªáu, h√£y n√≥i th·∫≥ng.
Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c."""

    def _load_qwen_model(self):
        print(f"\nüîÑ Loading Qwen model: {MODEL_NAME}...")
        start = time.time()
        
        # Quantization config
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            torch_dtype=torch.bfloat16
        )
        
        print(f"   ‚úÖ Model loaded ({time.time()-start:.1f}s)")
        
        # T·∫°o bad_words_ids ƒë·ªÉ ch·∫∑n token ngo·∫°i ng·ªØ
        print("   üîß T·∫°o danh s√°ch ch·∫∑n token ngo·∫°i ng·ªØ...")
        self.bad_words_ids = self._get_non_vietnamese_bad_words()
        print(f"   ‚úÖ ƒê√£ ch·∫∑n {len(self.bad_words_ids)} token ngo·∫°i ng·ªØ!")
    
    def _get_non_vietnamese_bad_words(self):
        """Ch·∫∑n token KH√îNG PH·∫¢I ti·∫øng Vi·ªát/Latin"""
        bad_words = []
        
        def is_allowed_char(ch):
            if ord(ch) < 128:
                return True
            if '\u00c0' <= ch <= '\u01b0':
                return True
            if '\u1ea0' <= ch <= '\u1ef9':
                return True
            if ch in '‚Äì‚Äî''""‚Ä¶‚Ä¢¬∑√ó√∑¬±‚â†‚â§‚â•':
                return True
            return False
        
        for i in range(self.tokenizer.vocab_size):
            token = self.tokenizer.decode([i])
            if any(not is_allowed_char(ch) for ch in token):
                bad_words.append([i])
        
        return bad_words
    
    def _load_embedding_model(self):
        print(f"\nüîÑ Loading Embedding model: {EMBEDDING_MODEL_NAME}...")
        from sentence_transformers import SentenceTransformer
        self.embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
        print("   ‚úÖ Embedding loaded!")
    
    def _init_lightrag(self):
        print("\nüîß Initializing LightRAG...")
        os.makedirs(RAG_STORAGE, exist_ok=True)
        
        # T·∫°o async LLM function cho LightRAG
        async def qwen_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return self._generate_response(prompt, system_prompt)
        
        # T·∫°o async embedding function
        async def embedding_func(texts):
            return self.embedder.encode(texts)
        
        embedding_wrapper = EmbeddingFunc(
            embedding_dim=EMBEDDING_DIM,
            max_token_size=EMBEDDING_MAX_TOKENS,
            func=embedding_func
        )
        
        self.rag = LightRAG(
            working_dir=RAG_STORAGE,
            llm_model_func=qwen_llm_func,
            embedding_func=embedding_wrapper,
        )
        print("   ‚úÖ LightRAG initialized!")
    
    def _generate_response(self, prompt: str, system_prompt: str = None) -> str:
        """Generate response t·ª´ Qwen model (sync)"""
        messages = [
            {"role": "system", "content": system_prompt or self.system_prompt},
            {"role": "user", "content": prompt}
        ]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=LLM_MAX_NEW_TOKENS,
                do_sample=False,
                num_beams=1,
                repetition_penalty=1.2,
                bad_words_ids=self.bad_words_ids,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(
            outputs[0][model_inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        return response
    
    async def initialize(self):
        """Kh·ªüi t·∫°o async cho LightRAG"""
        await self.rag.initialize_storages()
        print("‚úÖ RAG storages ready!")
    
    async def auto_index_new_files(self):
        """T·ª± ƒë·ªông ph√°t hi·ªán v√† index file m·ªõi"""
        tracker = IndexTracker(INDEX_TRACKER_FILE)
        
        if not os.path.exists(COURSES_FOLDER):
            print(f"‚ö†Ô∏è Folder {COURSES_FOLDER} kh√¥ng t·ªìn t·∫°i")
            return 0
        
        all_files = []
        for f in os.listdir(COURSES_FOLDER):
            ext = os.path.splitext(f)[1].lower()
            if ext in SUPPORTED_EXTENSIONS:
                all_files.append(f)
        
        new_files = []
        for f in all_files:
            file_path = os.path.join(COURSES_FOLDER, f)
            if tracker.needs_indexing(file_path):
                new_files.append(f)
        
        if not new_files:
            print(f"‚úÖ Kh√¥ng c√≥ file m·ªõi. Database: {tracker.get_indexed_count()} files")
            return 0
        
        print(f"\nüÜï Ph√°t hi·ªán {len(new_files)} file m·ªõi:")
        for f in new_files:
            print(f"   - {f}")
        
        print("\nüì• ƒêang index...")
        indexed = 0
        
        for i, filename in enumerate(new_files, 1):
            file_path = os.path.join(COURSES_FOLDER, filename)
            
            try:
                start = time.time()
                text = read_file(file_path)
                await self.rag.ainsert(text)
                tracker.mark_indexed(file_path)
                elapsed = time.time() - start
                print(f"   [{i}/{len(new_files)}] ‚úÖ {filename} ({elapsed:.1f}s)")
                indexed += 1
            except Exception as e:
                print(f"   [{i}/{len(new_files)}] ‚ùå {filename}: {e}")
        
        print(f"\nüìä ƒê√£ index: {indexed}/{len(new_files)} files")
        return indexed
    
    async def query(self, question: str, mode: str = "hybrid") -> str:
        """Query RAG v·ªõi question"""
        try:
            result = await self.rag.aquery(question, param=QueryParam(mode=mode))
            return result
        except Exception as e:
            return f"‚ùå L·ªói: {e}"
    
    async def interactive_mode(self):
        """Ch·∫ø ƒë·ªô h·ªèi ƒë√°p t∆∞∆°ng t√°c"""
        print("\n" + "=" * 60)
        print("üí¨ CH·∫æ ƒê·ªò H·ªéI ƒê√ÅP")
        print("=" * 60)
        print("G√µ c√¢u h·ªèi v√† Enter. 'exit' ƒë·ªÉ tho√°t.")
        print("'mode:hybrid/local/global/naive' ƒë·ªÉ ƒë·ªïi mode")
        print("'clear' ƒë·ªÉ x√≥a m√†n h√¨nh")
        print("-" * 60)
        
        current_mode = "hybrid"
        
        while True:
            try:
                user_input = input(f"\nüßë [{current_mode}] B·∫°n: ").strip()
                
                if user_input.lower() in ["exit", "quit", "q", "tho√°t"]:
                    print("üëã T·∫°m bi·ªát!")
                    break
                
                if not user_input:
                    continue
                
                if user_input.startswith("mode:"):
                    new_mode = user_input.split(":")[1].strip()
                    if new_mode in ["hybrid", "local", "global", "naive"]:
                        current_mode = new_mode
                        print(f"‚úÖ Mode: {current_mode}")
                    continue
                
                if user_input.lower() == "clear":
                    os.system('cls' if os.name == 'nt' else 'clear')
                    continue
                
                print("ü§ñ ƒêang x·ª≠ l√Ω...")
                start = time.time()
                result = await self.query(user_input, mode=current_mode)
                elapsed = time.time() - start
                print(f"\nü§ñ AI ({elapsed:.1f}s):\n{result}")
                
            except KeyboardInterrupt:
                print("\nüëã T·∫°m bi·ªát!")
                break


# ======================== MAIN ========================
async def main():
    # 1. Kh·ªüi t·∫°o h·ªá th·ªëng
    qwen_rag = QwenRAG()
    await qwen_rag.initialize()
    
    # 2. Auto-index new files
    print("\n" + "=" * 60)
    print("üìÅ KI·ªÇM TRA FILE M·ªöI")
    print("=" * 60)
    await qwen_rag.auto_index_new_files()
    
    # 3. Test queries
    print("\n" + "=" * 60)
    print("üß™ TEST QUERIES")
    print("=" * 60)
    
    test_questions = [
        "RAG l√† g√¨?",
        "Machine Learning c√≥ nh·ªØng lo·∫°i n√†o?",
    ]
    
    for q in test_questions:
        print(f"\n‚ùì {q}")
        start = time.time()
        answer = await qwen_rag.query(q, mode="hybrid")
        elapsed = time.time() - start
        # Truncate long answers
        if len(answer) > 300:
            print(f"üìù ({elapsed:.1f}s): {answer[:300]}...")
        else:
            print(f"üìù ({elapsed:.1f}s): {answer}")
    
    # 4. Interactive mode
    print("\n" + "=" * 60)
    print("üí° V√†o ch·∫ø ƒë·ªô h·ªèi ƒë√°p? (y/n)")
    choice = input().strip().lower()
    if choice == 'y':
        await qwen_rag.interactive_mode()


if __name__ == "__main__":
    asyncio.run(main())
