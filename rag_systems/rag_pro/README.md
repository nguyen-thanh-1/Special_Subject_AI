# RAG Pro - Quick Start Guide

## ğŸ“ Location

```
rag_systems/rag_pro/
â”œâ”€â”€ rag_pro_v2.py      # Main RAG implementation
â”œâ”€â”€ rag_index.py       # Index-only script
â”œâ”€â”€ rag_query.py       # Query-only script
â””â”€â”€ rag_config.py      # Configuration
```

---

## ğŸš€ Quick Start

### 1. Index Documents

```bash
# Navigate to rag_pro folder
cd rag_systems/rag_pro

# Index documents (first time or force rebuild)
uv run rag_index.py --force

# Or from project root
uv run rag_systems/rag_pro/rag_index.py --force
```

**What it does:**
- Loads documents from `data/courses/`
- Chunks text (400-800 words)
- Embeds chunks using BGE-M3 (GPU)
- Saves to `storage/rag_storage_pro_v2/`

**Time:** ~6-10 min for 800-page PDF (first time)

---

### 2. Query

```bash
# Navigate to rag_pro folder
cd rag_systems/rag_pro

# Start interactive query
uv run rag_query.py

# Or from project root
uv run rag_systems/rag_pro/rag_query.py
```

**What it does:**
- Loads index from disk
- Embeds query (CPU)
- Searches FAISS index (50 chunks)
- Reranks to top 3 (CPU)
- Generates answer with LLM (GPU)

**Time:** ~6-7 sec per query

---

## ğŸ“‚ File Paths Reference

### Project Structure
```
Special_Subject_AI/                    # Project root
â”œâ”€â”€ llm_models/                        # LLM wrappers
â”‚   â””â”€â”€ Llama_3_1_8B_Instruct_v2.py   # Used by rag_pro
â”œâ”€â”€ rag_systems/
â”‚   â””â”€â”€ rag_pro/                       # â† You are here
â”‚       â”œâ”€â”€ rag_pro_v2.py             # Main implementation
â”‚       â”œâ”€â”€ rag_index.py              # Index script
â”‚       â”œâ”€â”€ rag_query.py              # Query script
â”‚       â””â”€â”€ rag_config.py             # Config
â”œâ”€â”€ data/
â”‚   â””â”€â”€ courses/                       # Input documents
â””â”€â”€ storage/
    â””â”€â”€ rag_storage_pro_v2/           # Index storage
        â”œâ”€â”€ faiss_index.bin           # FAISS index
        â”œâ”€â”€ chunks.pkl                # Chunk data
        â”œâ”€â”€ indexed_files.json        # File tracker
        â””â”€â”€ embedding_cache.pkl       # Cache
```

### Absolute Paths

**Scripts:**
- `C:\Users\Admin\Desktop\Special_Subject_AI\rag_systems\rag_pro\rag_index.py`
- `C:\Users\Admin\Desktop\Special_Subject_AI\rag_systems\rag_pro\rag_query.py`
- `C:\Users\Admin\Desktop\Special_Subject_AI\rag_systems\rag_pro\rag_pro_v2.py`

**Dependencies:**
- LLM: `C:\Users\Admin\Desktop\Special_Subject_AI\llm_models\Llama_3_1_8B_Instruct_v2.py`

**Data:**
- Input: `C:\Users\Admin\Desktop\Special_Subject_AI\data\courses\`
- Storage: `C:\Users\Admin\Desktop\Special_Subject_AI\storage\rag_storage_pro_v2\`

---

## âš™ï¸ Configuration

Edit `rag_config.py` or modify constants in `rag_pro_v2.py`:

```python
# Chunking
MIN_CHUNK_SIZE = 400   # words
MAX_CHUNK_SIZE = 800   # words
CHUNK_OVERLAP = 100    # words

# Retrieval
TOP_K_RETRIEVE = 50    # FAISS search
TOP_K_RERANK = 3       # Reranker output

# Context (OOM prevention)
MAX_CONTEXT_TOKENS = 2000  # Hard limit

# Models
EMBEDDING_MODEL = "BAAI/bge-m3"
RERANKER_MODEL = "BAAI/bge-reranker-v2-m3"
```

---

## ğŸ¯ Usage Examples

### Example 1: Index from Different Folder

```bash
# From project root
cd C:\Users\Admin\Desktop\Special_Subject_AI
uv run rag_systems/rag_pro/rag_index.py --force
```

### Example 2: Query from Different Folder

```bash
# From project root
cd C:\Users\Admin\Desktop\Special_Subject_AI
uv run rag_systems/rag_pro/rag_query.py
```

### Example 3: Single Query

```bash
cd rag_systems/rag_pro
uv run rag_query.py --query "What is NLP?"
```

---

## ğŸ”§ Troubleshooting

### Issue: ModuleNotFoundError

**Error:**
```
ModuleNotFoundError: No module named 'Llama_3_1_8B_Instruct_v2'
```

**Solution:**
The script automatically adds project root to `sys.path`. If this fails:

```python
# Add to top of script
import sys
import os
project_root = r"C:\Users\Admin\Desktop\Special_Subject_AI"
sys.path.insert(0, project_root)
```

### Issue: Index not found

**Error:**
```
âŒ Lá»—i: Index not found
ğŸ’¡ HÃ£y cháº¡y rag_index.py trÆ°á»›c Ä‘á»ƒ táº¡o index!
```

**Solution:**
```bash
cd rag_systems/rag_pro
uv run rag_index.py --force
```

### Issue: CUDA OOM

**Error:**
```
torch.cuda.OutOfMemoryError
```

**Solution:**
Already fixed! The code uses:
- Smaller chunks (400-800 words)
- Hard token limit (2000 tokens)
- TOP_K_RERANK = 3

If still OOM, reduce `MAX_CONTEXT_TOKENS` in `rag_pro_v2.py`:
```python
MAX_CONTEXT_TOKENS = 1500  # From 2000
```

---

## ğŸ“Š Performance

### Indexing (rag_index.py)
```
Documents: 800-page PDF
Chunks: ~4,000 (semantic chunking)
Time: 6-10 min (first time)
Time: 2-3 sec (with cache)
VRAM: ~3GB (embedding on GPU)
```

### Querying (rag_query.py)
```
Pipeline:
  1. Embed query (CPU): 0.05s
  2. FAISS search: 0.5s
  3. Rerank (CPU): 1.5s
  4. LLM generate (GPU): 5s
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total: ~7s

VRAM: ~13.5GB (LLM 12GB + KV cache 1.5GB)
```

---

## ğŸ“š Related Documentation

- **Main Guide:** `docs/guides/RAG_PRO_V2_QUICKSTART.md`
- **OOM Fix:** `docs/fixes/QUERY_OOM_FIX.md`
- **V1 vs V2:** `docs/guides/RAG_PRO_V1_VS_V2.md`
- **Split Index/Query:** `docs/guides/SPLIT_INDEX_QUERY_GUIDE.md`

---

## âœ… Checklist

Before running:
- [ ] Documents in `data/courses/`
- [ ] GPU available (for LLM)
- [ ] ~16GB VRAM (recommended)
- [ ] Python packages installed (`uv sync`)

First time:
- [ ] Run `rag_index.py --force`
- [ ] Wait for indexing to complete
- [ ] Run `rag_query.py`

---

**Ready to use!** ğŸš€
