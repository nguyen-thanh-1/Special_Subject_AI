"""
RAG Pro V2 - QUERY ONLY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Chá»‰ dÃ¹ng Ä‘á»ƒ query, KHÃ”NG index.
Load index tá»« disk, LLM cháº¡y trÃªn GPU.

USAGE:
  uv run rag_query.py                    # Interactive mode
  uv run rag_query.py --query "cÃ¢u há»i" # Single query
  
PERFORMANCE:
  - LLM GPU: Nhanh
  - VRAM: ~12GB (LLM)
  - Query time: ~7.5s
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import argparse

# Import tá»« rag_pro_v2
from rag_pro_v2 import (
    RAG_STORAGE,
    EMBEDDING_CACHE_FILE,
    TRACKER_FILE,
    EmbeddingCache,
    IndexTracker,
    VectorStore,
    get_embedder,
    get_reranker,
    get_llm,
    embed_query,
    rerank,
    generate_answer,
    TOP_K_RETRIEVE,
    TOP_K_RERANK,
    MIN_CHUNK_SIZE,
    MAX_CHUNK_SIZE,
)

import time


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUERY ENGINE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class RAGQuery:
    """Chá»‰ dÃ¹ng Ä‘á»ƒ query, khÃ´ng index"""
    
    def __init__(self):
        self.vector_store = VectorStore(RAG_STORAGE)
        self.tracker = IndexTracker(TRACKER_FILE)
        
        # Load index
        print("\nğŸ”„ Loading index from disk...")
        if not self.vector_store.load():
            raise RuntimeError(
                "âŒ KhÃ´ng tÃ¬m tháº¥y index! Vui lÃ²ng cháº¡y rag_index.py trÆ°á»›c."
            )
        
        print(f"   âœ… Loaded {len(self.vector_store.chunks)} chunks")
    
    def query(self, question: str, verbose: bool = True) -> str:
        """Query RAG pipeline"""
        start = time.time()
        
        # Step 1: Retrieve from FAISS
        if verbose:
            print(f"   ğŸ” Searching...")
        retrieved_chunks = self.vector_store.search(question, TOP_K_RETRIEVE)
        
        if not retrieved_chunks:
            return "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong database."
        
        if verbose:
            print(f"   ğŸ“„ Found {len(retrieved_chunks)} chunks")
        
        # Step 2: Rerank
        if verbose:
            print(f"   ğŸ¯ Reranking to top {TOP_K_RERANK}...")
        reranked = rerank(question, retrieved_chunks, TOP_K_RERANK)
        
        if verbose:
            print(f"   âœ… Selected {len(reranked)} best chunks")
        
        # Step 3: Generate answer
        if verbose:
            print(f"   ğŸ¤– Generating answer...")
        answer = generate_answer(question, reranked)
        
        elapsed = time.time() - start
        if verbose:
            print(f"   â±ï¸ Total: {elapsed:.1f}s")
        
        return answer
    
    def get_stats(self):
        """Láº¥y thá»‘ng kÃª"""
        return {
            'total_files': self.tracker.get_indexed_count(),
            'total_chunks': self.tracker.get_total_chunks(),
            'indexed_files': self.tracker.indexed_files
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    parser = argparse.ArgumentParser(description="RAG Pro V2 - Query Only")
    parser.add_argument('--query', '-q', type=str, help='Single query mode')
    args = parser.parse_args()
    
    print("â•" * 60)
    print("ğŸš€ RAG PRO V2 - QUERY ONLY")
    print("â•" * 60)
    print(f"   ğŸ“Š Embedding: BAAI/bge-m3 (CPU)")
    print(f"   ğŸ¯ Reranker:  BAAI/bge-reranker-v2-m3 (CPU)")
    print(f"   ğŸ¤– LLM:       Llama 3.1 8B (GPU)")
    print(f"   âš¡ Chunking:  Semantic ({MIN_CHUNK_SIZE}-{MAX_CHUNK_SIZE} words)")
    print("â•" * 60)
    
    # CRITICAL: Load LLM FIRST to ensure it gets GPU
    print("\nğŸ”„ Loading LLM (GPU priority)...")
    get_llm()
    
    # Initialize query engine (loads index)
    try:
        rag = RAGQuery()
    except Exception as e:
        print(f"\nâŒ Lá»—i: {e}")
        print("\nğŸ’¡ HÃ£y cháº¡y rag_index.py trÆ°á»›c Ä‘á»ƒ táº¡o index!")
        return
    
    # Load embedding and reranker AFTER LLM (on CPU)
    print("\nğŸ”„ Loading embedding & reranker (CPU)...")
    get_embedder()
    get_reranker()
    
    # Show stats
    stats = rag.get_stats()
    print("\n" + "â•" * 60)
    print("ğŸ“Š DATABASE STATS")
    print("â•" * 60)
    print(f"   Total files: {stats['total_files']}")
    print(f"   Total chunks: {stats['total_chunks']}")
    print("â•" * 60)
    
    # Single query mode
    if args.query:
        print("\n" + "â•" * 60)
        print("ğŸ” QUERY")
        print("â•" * 60)
        print(f"\nâ“ {args.query}")
        answer = rag.query(args.query)
        print(f"\nğŸ¤– Answer:\n{answer}")
        return
    
    # Interactive mode
    print("\n" + "â•" * 60)
    print("ğŸ’¬ INTERACTIVE MODE")
    print("â•" * 60)
    print("GÃµ cÃ¢u há»i. 'exit' Ä‘á»ƒ thoÃ¡t, 'stats' Ä‘á»ƒ xem thá»‘ng kÃª.")
    print("-" * 60)
    
    while True:
        try:
            question = input("\nğŸ§‘ Báº¡n: ").strip()
            
            if question.lower() in ["exit", "quit", "q"]:
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            
            if not question:
                continue
            
            if question.lower() == "stats":
                stats = rag.get_stats()
                print(f"\nğŸ“Š Thá»‘ng kÃª:")
                print(f"   Files: {stats['total_files']}")
                print(f"   Chunks: {stats['total_chunks']}")
                continue
            
            print("\nğŸ¤– Äang xá»­ lÃ½...")
            answer = rag.query(question)
            print(f"\nğŸ“ Tráº£ lá»i:\n{answer}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break


if __name__ == "__main__":
    main()
