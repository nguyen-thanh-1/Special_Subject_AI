"""
RAG Pro V2 - INDEX ONLY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Chá»‰ dÃ¹ng Ä‘á»ƒ index tÃ i liá»‡u, KHÃ”NG query.
Embedding cháº¡y trÃªn GPU Ä‘á»ƒ tá»‘i Æ°u tá»‘c Ä‘á»™.

USAGE:
  uv run rag_index.py --force     # Re-index táº¥t cáº£
  uv run rag_index.py             # Chá»‰ index file má»›i
  
PERFORMANCE:
  - Embedding GPU: 3-4x nhanh hÆ¡n CPU
  - VRAM: ~3GB (chá»‰ Embedding)
  - Index 800-page PDF: ~2-3 phÃºt
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import time
import argparse
from pathlib import Path

# Import tá»« rag_pro_v2
from rag_pro_v2 import (
    RAG_STORAGE,
    COURSES_FOLDER,
    SUPPORTED_EXTENSIONS,
    EmbeddingCache,
    IndexTracker,
    VectorStore,
    read_file,
    chunk_text_semantic,
    EMBEDDING_CACHE_FILE,
    TRACKER_FILE,
    MIN_CHUNK_SIZE,
    MAX_CHUNK_SIZE,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EMBEDDING MODEL - FORCE GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_embedder = None

def get_embedder_gpu():
    """Load embedding model trÃªn GPU"""
    global _embedder
    if _embedder is None:
        from sentence_transformers import SentenceTransformer
        import torch
        
        EMBEDDING_MODEL = "BAAI/bge-m3"
        print(f"   ğŸ“¥ Loading {EMBEDDING_MODEL} on GPU...")
        
        # Force GPU
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        _embedder = SentenceTransformer(EMBEDDING_MODEL, device=device)
        
        print(f"   âœ… Embedding model loaded ({device.upper()})")
        
        if device == 'cpu':
            print("   âš ï¸  WARNING: GPU not available, using CPU (slower)")
    
    return _embedder


def embed_texts_gpu(texts, cache, batch_size=128):
    """Embed texts vá»›i GPU vÃ  cache"""
    import numpy as np
    
    if not texts:
        return np.array([])
    
    # Get cached and to-embed
    cached_indices, to_embed, embed_indices = cache.get_batch(texts)
    
    # Initialize result array
    embeddings = np.zeros((len(texts), 1024), dtype=np.float32)
    
    # Fill cached embeddings
    for idx in cached_indices:
        embeddings[idx] = cache.get(texts[idx])
    
    # Embed new texts on GPU
    if to_embed:
        print(f"   ğŸ”„ Embedding {len(to_embed)} new chunks (cached: {len(cached_indices)})...")
        
        embedder = get_embedder_gpu()
        new_embeddings = embedder.encode(
            to_embed,
            batch_size=batch_size,
            show_progress_bar=True,
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        # Fill new embeddings and cache them
        for i, (text, emb) in enumerate(zip(to_embed, new_embeddings)):
            idx = embed_indices[i]
            embeddings[idx] = emb
            cache.set(text, emb)
    else:
        print(f"   âœ… All {len(texts)} chunks from cache!")
    
    return embeddings


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INDEXER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class RAGIndexer:
    """Chá»‰ dÃ¹ng Ä‘á»ƒ index, khÃ´ng query"""
    
    def __init__(self):
        self.vector_store = VectorStore(RAG_STORAGE)
        self.tracker = IndexTracker(TRACKER_FILE)
        self.cache = EmbeddingCache(EMBEDDING_CACHE_FILE)
    
    def index_file(self, file_path: str) -> int:
        """Index má»™t file"""
        text = read_file(file_path)
        chunks = chunk_text_semantic(text)
        
        if chunks:
            # Embed vá»›i GPU
            embeddings = embed_texts_gpu(chunks, self.cache)
            
            # Add to vector store
            self.vector_store.add_chunks(chunks, os.path.basename(file_path), embeddings)
            self.tracker.mark_indexed(file_path, len(chunks))
        
        return len(chunks)
    
    def index_folder(self, folder: str, force: bool = False) -> int:
        """Index toÃ n bá»™ folder"""
        if force:
            self.vector_store.clear()
            self.tracker.indexed_files = {}
            self.tracker._save()
            # Don't clear cache - reuse it!
        
        if not os.path.exists(folder):
            print(f"âš ï¸ Folder khÃ´ng tá»“n táº¡i: {folder}")
            return 0
        
        # Find files
        all_files = [f for f in os.listdir(folder) 
                     if os.path.splitext(f)[1].lower() in SUPPORTED_EXTENSIONS]
        
        # Filter new files
        new_files = [f for f in all_files 
                     if self.tracker.needs_indexing(os.path.join(folder, f))]
        
        if not new_files:
            print(f"âœ… KhÃ´ng cÃ³ file má»›i. Database: {self.tracker.get_indexed_count()} files, {self.tracker.get_total_chunks()} chunks")
            return 0
        
        print(f"\nğŸ†• PhÃ¡t hiá»‡n {len(new_files)} file cáº§n index:")
        
        total_chunks = 0
        for i, filename in enumerate(new_files, 1):
            file_path = os.path.join(folder, filename)
            try:
                print(f"   [{i}/{len(new_files)}] {filename}...", end=" ", flush=True)
                start = time.time()
                chunks = self.index_file(file_path)
                elapsed = time.time() - start
                print(f"âœ… {chunks} chunks ({elapsed:.1f}s)")
                total_chunks += chunks
            except Exception as e:
                print(f"âŒ {e}")
        
        # Save cache
        self.cache.save()
        
        # Print stats
        cache_stats = self.cache.get_stats()
        print(f"\nğŸ“Š Indexing Stats:")
        print(f"   Total chunks: {total_chunks}")
        print(f"   Cache hit rate: {cache_stats['hit_rate']:.1f}%")
        
        return total_chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    parser = argparse.ArgumentParser(description="RAG Pro V2 - Index Only")
    parser.add_argument('--force', '-f', action='store_true', help='Force re-index')
    args = parser.parse_args()
    
    print("â•" * 60)
    print("ğŸš€ RAG PRO V2 - INDEX ONLY")
    print("â•" * 60)
    print(f"   ğŸ“Š Embedding: BAAI/bge-m3 (GPU)")
    print(f"   âš¡ Chunking:  Semantic ({MIN_CHUNK_SIZE}-{MAX_CHUNK_SIZE} words)")
    print(f"   ğŸ’¾ Cache:     Enabled")
    print(f"   ğŸ“ Folder:    {COURSES_FOLDER}")
    print("â•" * 60)
    
    # Initialize
    print("\nğŸ”„ Loading embedding model...")
    indexer = RAGIndexer()
    
    # Load embedding model
    get_embedder_gpu()
    
    # Index
    print("\n" + "â•" * 60)
    print("ğŸ“ INDEXING")
    print("â•" * 60)
    
    start = time.time()
    total_chunks = indexer.index_folder(COURSES_FOLDER, force=args.force)
    elapsed = time.time() - start
    
    print("\n" + "â•" * 60)
    print("âœ… INDEXING COMPLETE")
    print("â•" * 60)
    print(f"   Total chunks: {total_chunks}")
    print(f"   Total time: {elapsed/60:.1f} minutes")
    print(f"   Storage: {RAG_STORAGE}")
    print("\nğŸ’¡ BÃ¢y giá» cÃ³ thá»ƒ cháº¡y rag_query.py Ä‘á»ƒ query!")
    print("â•" * 60)


if __name__ == "__main__":
    main()
