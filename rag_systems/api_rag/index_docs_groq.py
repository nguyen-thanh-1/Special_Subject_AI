"""
Index Documents v·ªõi Groq API - NHANH 10x so v·ªõi local LLM
S·ª≠ d·ª•ng Groq API mi·ªÖn ph√≠ cho entity extraction

Ch·∫°y: uv run index_docs_groq.py
Ho·∫∑c: uv run index_docs_groq.py --force

Y√™u c·∫ßu: ƒê·∫∑t GROQ_API_KEY trong environment ho·∫∑c file .env
"""

import asyncio
import argparse
import json
import hashlib
import os
from datetime import datetime
from dotenv import load_dotenv

# Load .env file
load_dotenv()

# Import RAGAnything
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc

# ======================== GROQ CONFIG ========================
# L·∫•y API key t·ª´ environment
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
# D√πng model 8B instant: nhanh h∆°n, limit 500K tokens/ng√†y (thay v√¨ 100K)
GROQ_MODEL = "llama-3.1-8b-instant"

# ======================== PATHS (RI√äNG BI·ªÜT) ========================
COURSES_FOLDER = "./courses"
OUTPUT_DIR = "./output_courses_groq"  # Output ri√™ng
RAG_STORAGE = "./rag_storage_groq"  # Storage ri√™ng
INDEX_TRACKER_FILE = os.path.join(RAG_STORAGE, "indexed_files.json")

SUPPORTED_EXTENSIONS = [".pdf", ".txt", ".docx", ".doc", ".xlsx", ".xls", ".csv", ".pptx", ".ppt", ".md"]

# Embedding config (v·∫´n d√πng local)
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
EMBEDDING_DIM = 384
EMBEDDING_MAX_TOKENS = 256

# Retry config
MAX_RETRIES = 5
INITIAL_BACKOFF = 2  # seconds


# ======================== INDEX TRACKER ========================
class IndexTracker:
    """Qu·∫£n l√Ω danh s√°ch file ƒë√£ index"""
    
    def __init__(self, tracker_file: str):
        self.tracker_file = tracker_file
        self.indexed_files = self._load()
    
    def _load(self) -> dict:
        if os.path.exists(self.tracker_file):
            try:
                with open(self.tracker_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save(self):
        os.makedirs(os.path.dirname(self.tracker_file), exist_ok=True)
        with open(self.tracker_file, 'w', encoding='utf-8') as f:
            json.dump(self.indexed_files, f, indent=2, ensure_ascii=False)
    
    def _get_file_hash(self, file_path: str) -> str:
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(65536), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def needs_indexing(self, file_path: str) -> bool:
        if not os.path.exists(file_path):
            return False
        filename = os.path.basename(file_path)
        current_hash = self._get_file_hash(file_path)
        if filename not in self.indexed_files:
            return True
        if self.indexed_files[filename].get('hash') != current_hash:
            return True
        return False
    
    def mark_indexed(self, file_path: str):
        filename = os.path.basename(file_path)
        self.indexed_files[filename] = {
            'hash': self._get_file_hash(file_path),
            'indexed_at': datetime.now().isoformat(),
            'size_bytes': os.path.getsize(file_path),
            'method': 'groq'
        }
        self._save()
    
    def get_indexed_count(self) -> int:
        return len(self.indexed_files)


# ======================== GROQ LLM FUNCTION ========================
def create_groq_llm_func():
    """T·∫°o async LLM function s·ª≠ d·ª•ng Groq API v·ªõi retry logic"""
    import time
    
    try:
        from groq import Groq, RateLimitError
    except ImportError:
        print("‚ùå Ch∆∞a c√†i groq. Ch·∫°y: uv pip install groq")
        raise
    
    if not GROQ_API_KEY:
        raise ValueError(
            "‚ùå Ch∆∞a c√≥ GROQ_API_KEY!\n"
            "   1. ƒêƒÉng k√Ω t·∫°i: https://console.groq.com\n"
            "   2. T·∫°o API Key\n"
            "   3. T·∫°o file .env v·ªõi n·ªôi dung: GROQ_API_KEY=your_key_here"
        )
    
    client = Groq(api_key=GROQ_API_KEY)
    
    async def groq_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        """G·ªçi Groq API v·ªõi retry v√† exponential backoff"""
        messages = []
        
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        for msg in history_messages:
            messages.append(msg)
        
        messages.append({"role": "user", "content": prompt})
        
        # Retry with exponential backoff
        for attempt in range(MAX_RETRIES):
            try:
                response = client.chat.completions.create(
                    model=GROQ_MODEL,
                    messages=messages,
                    temperature=0.1,
                    max_tokens=4096,
                )
                return response.choices[0].message.content
            except RateLimitError as e:
                if attempt < MAX_RETRIES - 1:
                    wait_time = INITIAL_BACKOFF * (2 ** attempt)
                    print(f"   ‚è≥ Rate limit, ƒë·ª£i {wait_time}s r·ªìi th·ª≠ l·∫°i...")
                    time.sleep(wait_time)
                else:
                    print(f"   ‚ùå Rate limit sau {MAX_RETRIES} l·∫ßn th·ª≠")
                    raise
            except Exception as e:
                print(f"   ‚ö†Ô∏è Groq API error: {e}")
                raise
    
    return groq_llm_func


# ======================== EMBEDDING FUNCTION ========================
def create_embedding_func():
    """T·∫°o embedding function (v·∫´n d√πng local)"""
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("‚ùå Ch∆∞a c√†i sentence-transformers")
        raise
    
    print(f"   Loading embedding: {EMBEDDING_MODEL_NAME}...")
    embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("   ‚úÖ Embedding model loaded")
    
    async def embedding_func(texts):
        return embedder.encode(texts)
    
    return EmbeddingFunc(
        embedding_dim=EMBEDDING_DIM,
        max_token_size=EMBEDDING_MAX_TOKENS,
        func=embedding_func
    )


# ======================== HELPER FUNCTIONS ========================
def ensure_directories():
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(RAG_STORAGE, exist_ok=True)


def get_supported_files(folder: str) -> list:
    if not os.path.exists(folder):
        return []
    files = []
    for f in os.listdir(folder):
        ext = os.path.splitext(f)[1].lower()
        if ext in SUPPORTED_EXTENSIONS:
            files.append(f)
    return files


def get_file_info(folder: str, filename: str) -> dict:
    file_path = os.path.join(folder, filename)
    if not os.path.exists(file_path):
        return None
    stat = os.stat(file_path)
    return {
        "filename": filename,
        "path": file_path,
        "size_bytes": stat.st_size,
        "size_mb": stat.st_size / (1024 * 1024),
    }


# ======================== MAIN INDEXING ========================
async def index_documents(force_reindex: bool = False):
    """Index t√†i li·ªáu v·ªõi Groq API"""
    
    print("=" * 60)
    print("üöÄ RAG INDEXER v·ªõi GROQ API (NHANH 10x)")
    print("=" * 60)
    
    # Check API key
    if not GROQ_API_KEY:
        print("\n‚ùå Thi·∫øu GROQ_API_KEY!")
        print("   1. ƒêƒÉng k√Ω t·∫°i: https://console.groq.com")
        print("   2. T·∫°o API Key")
        print("   3. T·∫°o file .env v·ªõi: GROQ_API_KEY=your_key_here")
        return
    
    print(f"‚úÖ Groq API Key: {GROQ_API_KEY[:10]}...")
    print(f"üì¶ Model: {GROQ_MODEL}")
    
    # Ensure directories
    ensure_directories()
    
    print(f"\nüìÅ C·∫•u h√¨nh:")
    print(f"   - T√†i li·ªáu: {COURSES_FOLDER}")
    print(f"   - Output: {OUTPUT_DIR}")
    print(f"   - Database: {RAG_STORAGE}")
    
    # Setup tracker
    tracker = IndexTracker(INDEX_TRACKER_FILE)
    print(f"üìä ƒê√£ c√≥ {tracker.get_indexed_count()} file(s) trong database")
    
    # Get files
    all_files = get_supported_files(COURSES_FOLDER)
    if not all_files:
        print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y file trong {COURSES_FOLDER}")
        return
    
    print(f"üìÅ T√¨m th·∫•y {len(all_files)} file(s)")
    
    # Determine files to index
    if force_reindex:
        files_to_index = all_files
        print("‚ö†Ô∏è  Force re-index: S·∫Ω index l·∫°i T·∫§T C·∫¢ files")
    else:
        files_to_index = []
        for f in all_files:
            file_path = os.path.join(COURSES_FOLDER, f)
            if tracker.needs_indexing(file_path):
                files_to_index.append(f)
        
        if not files_to_index:
            print("‚úÖ T·∫•t c·∫£ files ƒë√£ ƒë∆∞·ª£c index. Kh√¥ng c√≥ g√¨ m·ªõi.")
            return
        
        print(f"üÜï {len(files_to_index)} file(s) c·∫ßn index")
    
    # Show files
    print("\nüìã Files s·∫Ω ƒë∆∞·ª£c index:")
    for i, f in enumerate(files_to_index, 1):
        info = get_file_info(COURSES_FOLDER, f)
        print(f"   {i}. {f} ({info['size_mb']:.2f} MB)")
    
    # Setup models
    print("\nüîÑ Loading models...")
    groq_llm = create_groq_llm_func()
    print("   ‚úÖ Groq LLM ready")
    embedding_func = create_embedding_func()
    
    # Initialize RAG
    print("\nüîß Initializing RAGAnything v·ªõi Groq...")
    config = RAGAnythingConfig(
        working_dir=RAG_STORAGE,
        parser="mineru",
        parse_method="auto",
        enable_image_processing=False,
        enable_table_processing=True,
        enable_equation_processing=False,
    )
    
    rag = RAGAnything(
        config=config,
        llm_model_func=groq_llm,
        embedding_func=embedding_func,
    )
    print("‚úÖ RAGAnything initialized v·ªõi Groq")
    
    # Index files
    print("\n" + "=" * 60)
    print("üöÄ B·∫ÆT ƒê·∫¶U INDEXING (Groq API)")
    print("=" * 60)
    
    success_count = 0
    error_count = 0
    
    for i, filename in enumerate(files_to_index, 1):
        file_path = os.path.join(COURSES_FOLDER, filename)
        info = get_file_info(COURSES_FOLDER, filename)
        
        print(f"\n[{i}/{len(files_to_index)}] üìÑ {filename}")
        print(f"    Size: {info['size_mb']:.2f} MB")
        
        try:
            start_time = datetime.now()
            
            await rag.process_document_complete(
                file_path=file_path,
                output_dir=OUTPUT_DIR,
                parse_method="auto"
            )
            
            elapsed = (datetime.now() - start_time).total_seconds()
            tracker.mark_indexed(file_path)
            
            print(f"    ‚úÖ Ho√†n th√†nh trong {elapsed:.1f}s")
            success_count += 1
            
        except Exception as e:
            print(f"    ‚ùå L·ªói: {str(e)}")
            error_count += 1
            continue
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä K·∫æT QU·∫¢ INDEXING (Groq)")
    print("=" * 60)
    print(f"   ‚úÖ Th√†nh c√¥ng: {success_count} file(s)")
    print(f"   ‚ùå L·ªói: {error_count} file(s)")
    print(f"   üì¶ T·ªïng trong database: {tracker.get_indexed_count()} file(s)")
    print(f"\nüíæ Database l∆∞u t·∫°i: {RAG_STORAGE}")
    print("üöÄ Ch·∫°y 'uv run query_rag_groq.py' ƒë·ªÉ h·ªèi ƒë√°p!")


# ======================== CLI ========================
def main():
    parser = argparse.ArgumentParser(description="Index v·ªõi Groq API (nhanh)")
    parser.add_argument('--force', '-f', action='store_true', help='Force re-index')
    args = parser.parse_args()
    
    asyncio.run(index_documents(force_reindex=args.force))


if __name__ == "__main__":
    main()
