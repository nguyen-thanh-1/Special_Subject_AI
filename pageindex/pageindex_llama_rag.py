"""
PageIndex + Llama 3.1 8B RAG System
Há»‡ thá»‘ng RAG hoÃ n chá»‰nh sá»­ dá»¥ng PageIndex methodology
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from pathlib import Path

# Import PageIndex core
from pageindex_core import LocalPageIndex, format_context_for_prompt

# ==================== LLM Wrapper ====================
class LlamaLLM:
    """Wrapper cho Llama 3.1 8B model"""
    
    def __init__(self, model_id="meta-llama/Llama-3.1-8B-Instruct"):
        self.model_id = model_id
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        """Load model vá»›i 4-bit quantization"""
        print(f"ğŸ”„ Äang load model {self.model_id}...")
        
        try:
            # Cáº¥u hÃ¬nh quantization
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            
            # Load model
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                device_map="auto",
                quantization_config=bnb_config,
            )
            
            print("âœ… Model Ä‘Ã£ load thÃ nh cÃ´ng!")
            
        except Exception as e:
            print(f"âŒ Lá»—i khi load model: {e}")
            print("\nğŸ’¡ Thá»­ load model khÃ´ng quantization...")
            
            # Fallback: Load without quantization
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_id,
                    device_map="auto",
                    torch_dtype=torch.float16,
                )
                print("âœ… Model Ä‘Ã£ load thÃ nh cÃ´ng (FP16)!")
            except Exception as e2:
                print(f"âŒ KhÃ´ng thá»ƒ load model: {e2}")
                raise
    
    def generate(self, prompt, max_new_tokens=512, temperature=0.2):
        """Sinh text tá»« prompt"""
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("Model chÆ°a Ä‘Æ°á»£c load!")
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=temperature > 0,
                repetition_penalty=1.1,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode (chá»‰ láº¥y pháº§n má»›i sinh ra)
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def chat(self, messages, max_new_tokens=512, temperature=0.2):
        """Chat vá»›i history"""
        # Apply chat template
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        return self.generate(prompt, max_new_tokens, temperature)


# ==================== RAG System ====================
class PageIndexRAG:
    """
    RAG system káº¿t há»£p PageIndex vá»›i Llama 3.1 8B
    
    Äáº·c Ä‘iá»ƒm:
    - Vectorless retrieval: KhÃ´ng dÃ¹ng vector database
    - Tree-structured indexing: Cáº¥u trÃºc phÃ¢n cáº¥p tá»± nhiÃªn
    - LLM-based reasoning: Sá»­ dá»¥ng LLM Ä‘á»ƒ tráº£ lá»i
    """
    
    def __init__(self, documents_dir="./courses", model_id="meta-llama/Llama-3.1-8B-Instruct"):
        # Khá»Ÿi táº¡o PageIndex
        self.page_index = LocalPageIndex(documents_dir)
        self.page_index.build_index()
        
        # Khá»Ÿi táº¡o LLM
        self.llm = LlamaLLM(model_id)
        
        # System prompt
        self.system_prompt = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI giÃ¡o dá»¥c thÃ´ng minh vÃ  chuyÃªn nghiá»‡p.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn thÃ´ng tin tá»« tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
- Giáº£i thÃ­ch rÃµ rÃ ng, chi tiáº¿t vÃ  cÃ³ cáº¥u trÃºc
- Sá»­ dá»¥ng vÃ­ dá»¥ cá»¥ thá»ƒ khi cáº§n thiáº¿t

QUY Táº®C Báº®T BUá»˜C:
1. Tráº£ lá»i HOÃ€N TOÃ€N báº±ng tiáº¿ng Viá»‡t
2. Dá»±a vÃ o thÃ´ng tin trong tÃ i liá»‡u Ä‘á»ƒ tráº£ lá»i
3. Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u, hÃ£y nÃ³i rÃµ "ThÃ´ng tin nÃ y khÃ´ng cÃ³ trong tÃ i liá»‡u"
4. TrÃ­ch dáº«n nguá»“n khi cáº§n thiáº¿t
5. Tráº£ lá»i ngáº¯n gá»n nhÆ°ng Ä‘áº§y Ä‘á»§ Ã½
6. KhÃ´ng bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong tÃ i liá»‡u
"""
    
    def query(self, question, max_new_tokens=512, temperature=0.2, max_sections=3):
        """
        Truy váº¥n há»‡ thá»‘ng RAG
        
        Args:
            question: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
            max_new_tokens: Sá»‘ token tá»‘i Ä‘a Ä‘á»ƒ sinh
            temperature: Nhiá»‡t Ä‘á»™ sampling
            max_sections: Sá»‘ sections tá»‘i Ä‘a Ä‘á»ƒ retrieve
            
        Returns:
            response: CÃ¢u tráº£ lá»i
            sources: Danh sÃ¡ch nguá»“n tham kháº£o
        """
        # Láº¥y context tá»« PageIndex
        context, sources = self.page_index.get_context(question, max_sections=max_sections)
        
        if context is None:
            return "âš ï¸ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u. Vui lÃ²ng thÃªm tÃ i liá»‡u hoáº·c há»i cÃ¢u há»i khÃ¡c.", []
        
        # XÃ¢y dá»±ng prompt
        user_prompt = format_context_for_prompt(question, context, sources)
        
        # Táº¡o messages
        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # Gá»i LLM
        try:
            response = self.llm.chat(
                messages,
                max_new_tokens=max_new_tokens,
                temperature=temperature
            )
            
            return response, sources
            
        except Exception as e:
            return f"âŒ Lá»—i khi sinh cÃ¢u tráº£ lá»i: {e}", []
    
    def rebuild_index(self):
        """XÃ¢y dá»±ng láº¡i index (khi cÃ³ tÃ i liá»‡u má»›i)"""
        print("\nğŸ”„ Äang xÃ¢y dá»±ng láº¡i index...")
        self.page_index = LocalPageIndex(self.page_index.documents_dir)
        self.page_index.build_index()
    
    def get_statistics(self):
        """Láº¥y thá»‘ng kÃª vá» há»‡ thá»‘ng"""
        return self.page_index.get_statistics()


# ==================== Interactive Interface ====================
def main():
    print("=" * 70)
    print("ğŸš€ PageIndex + Llama 3.1 8B RAG System")
    print("=" * 70)
    print("\nğŸ“Œ Äáº·c Ä‘iá»ƒm cá»§a PageIndex:")
    print("  âœ… KhÃ´ng sá»­ dá»¥ng vector database (vectorless)")
    print("  âœ… Cáº¥u trÃºc cÃ¢y phÃ¢n cáº¥p tá»± nhiÃªn (tree-structured)")
    print("  âœ… Reasoning-based retrieval (LLM-powered)")
    print("  âœ… Báº£o toÃ n ngá»¯ cáº£nh tÃ i liá»‡u (context-preserving)")
    print("=" * 70)
    
    # Khá»Ÿi táº¡o RAG system
    print("\nğŸ”§ Äang khá»Ÿi táº¡o há»‡ thá»‘ng...")
    try:
        rag = PageIndexRAG(documents_dir="./courses")
    except Exception as e:
        print(f"\nâŒ Lá»—i khá»Ÿi táº¡o: {e}")
        print("\nğŸ’¡ Vui lÃ²ng kiá»ƒm tra:")
        print("  1. Model Llama 3.1 8B Ä‘Ã£ Ä‘Æ°á»£c download chÆ°a")
        print("  2. GPU cÃ³ Ä‘á»§ VRAM khÃ´ng (tá»‘i thiá»ƒu 6GB)")
        print("  3. ThÆ° má»¥c ./courses cÃ³ tÃ i liá»‡u chÆ°a")
        return
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    stats = rag.get_statistics()
    print(f"\nğŸ“Š Thá»‘ng kÃª há»‡ thá»‘ng:")
    print(f"  â€¢ Tá»•ng sá»‘ tÃ i liá»‡u: {stats['total_documents']}")
    print(f"  â€¢ Tá»•ng sá»‘ sections: {stats['total_sections']}")
    if stats['documents']:
        print(f"  â€¢ Danh sÃ¡ch tÃ i liá»‡u:")
        for doc in stats['documents']:
            print(f"    - {doc}")
    
    print("\nâœ… Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng!")
    print("\nğŸ“ Lá»‡nh Ä‘áº·c biá»‡t:")
    print("  â€¢ 'rebuild' - XÃ¢y dá»±ng láº¡i index tá»« tÃ i liá»‡u")
    print("  â€¢ 'stats' - Hiá»ƒn thá»‹ thá»‘ng kÃª há»‡ thá»‘ng")
    print("  â€¢ 'exit' hoáº·c 'quit' - ThoÃ¡t chÆ°Æ¡ng trÃ¬nh")
    print("=" * 70)
    
    # Interactive loop
    while True:
        print("\n")
        user_input = input("ğŸ’¬ CÃ¢u há»i cá»§a báº¡n: ").strip()
        
        if not user_input:
            continue
        
        if user_input.lower() in ["exit", "quit"]:
            print("\nğŸ‘‹ Táº¡m biá»‡t!")
            break
        
        if user_input.lower() == "rebuild":
            rag.rebuild_index()
            stats = rag.get_statistics()
            print(f"âœ… ÄÃ£ rebuild! Tá»•ng: {stats['total_documents']} docs, {stats['total_sections']} sections")
            continue
        
        if user_input.lower() == "stats":
            stats = rag.get_statistics()
            print(f"\nğŸ“Š Thá»‘ng kÃª:")
            print(f"  â€¢ TÃ i liá»‡u: {stats['total_documents']}")
            print(f"  â€¢ Sections: {stats['total_sections']}")
            print(f"  â€¢ Danh sÃ¡ch: {', '.join(stats['documents'])}")
            continue
        
        print("\nğŸ¤– Äang xá»­ lÃ½...")
        print("=" * 70)
        
        try:
            response, sources = rag.query(user_input, max_new_tokens=512, temperature=0.2)
            
            print("\nğŸ“ Tráº£ lá»i:")
            print(response)
            
            if sources:
                print("\nğŸ“š Nguá»“n tham kháº£o:")
                for idx, source in enumerate(sources, 1):
                    print(f"  {idx}. {source}")
            
        except Exception as e:
            print(f"\nâŒ Lá»—i: {e}")
            import traceback
            traceback.print_exc()
        
        print("=" * 70)


if __name__ == "__main__":
    main()
